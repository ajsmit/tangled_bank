---
title: "BCB744 Practical Exam Assessment Instructions (2025)"
output:
  pdf_document: default
  html_document: default
date: "2025-05-31"
format:
  docx:
    lang: "en-GB"
  html:
    code-tools: false
    embed-resources: true
    theme: pandoc
    highlight-style: tango
    fig-height: 4.25
    fig-width: 6
    fig-format: jpeg
    fig-dpi: 300
---

Assessment Instructions for BCB744 Practical Exam (2025)

## INPUT


- The rubric is defined in BCB744_Prac_Exam_Rubric_2025.pdf (attached once only at the start).
- The worked out answers which will guide the assessment in BCB744_Biostats_Proac_Exam_2025.pdf (attached once at the start)
- Each student’s response will be in a .html, .docx, or .pdf output file.
- Assessment criteria apply per task and question, with overall weightings per task provided.

## STEP-BY-STEP ASSESSMENT PROCEDURE

### STEP 1: Parse and identify the student file

1.	Read the student answer file.
2.	Identify and extract answers corresponding to:
  - Task 1 (with subcomponents 1.1 and 1.2)
  - Task 2.1 (1 and 2), 2.2 (1, 2, 3), and 2.3
  - Task 3 (1–4)
  - Task 4.1 and 4.2
  - Task 5.1 through 5.5
  - Task 6 (Write-up)

### STEP 2: Evaluate each component using the rubric

For each sub-question or component:

1.	Apply the rubric section relevant to that task:
  - Use the four assessment dimensions:
    - Technical Accuracy (50%)
    - Depth of Analysis (20%)
    - Clarity and Communication (20%)
    - Critical Thinking (10%)
  - Each is scored on a 0–100 scale for that component.
2.	Multiply each score by the weighting for that component as defined in the rubric:
  - E.g., Task 1.1 is 50% of Task 1 (worth 10%), so max contribution is 5 points.
  - Task 5.3 is one of five sub-tasks in Task 5 (30% total), so it’s ~6%.
3.	Tally sub-task scores to compute the task total (e.g., Task 3 might yield 17.4/20).
4.	Round task scores to one decimal place.

### STEP 3: Write feedback and save to .txt

For each student, generate a .txt file named identically to their input file (but with .txt extension):

A. Feedback Report Structure

1.	Narrative feedback for each task (Tasks 1–6)
  - One paragraph per task.
  - Highlight:
    - Strengths (e.g., well-structured code, clear visualisations)
    - Weaknesses (e.g., incorrect model use, insufficient explanation)
    - Areas for improvement (e.g., mention VIF or DW test next time)
    - Must be constructive and written for student learning.
2.	Marks per component
  - Use format: Task 1.1: 43/50 or Task 2.2 (2): 12/20
  - One line per sub-question (lowest possible granularity)
3.	Task total
  - Use format: Task 1: 8.6/10
4.	Final total
  - Use format: Total mark: 84.5/100

### STEP 4: Generate .csv with marks

For the same student, create a .csv file (named identically but with .csv extension) with the following structure:

Task	Mark
Task 1	8.6
Task 2	9.2
Task 3	17.4
Task 4	9.0
Task 5	27.0
Task 6	9.3
Total	80.5

## ADDITIONAL GUIDELINE FOR CONSISTENCY

- Use the same rubric for all students.
- Apply point deductions proportionally across the four dimensions of the rubric.
- Do not penalise for choices beyond the scope of the taught material (e.g., not using mixed models).
- Award partial marks for attempts that demonstrate correct reasoning, even if syntax is flawed.
- Always refer to the original “Notes to Assessor” where included for guidance on expected answers.

## SUMMARY

Output Type	Content:

- .txt	Narrative feedback, component marks, task marks, total mark
- .csv	Tabular summary of marks per task + total
