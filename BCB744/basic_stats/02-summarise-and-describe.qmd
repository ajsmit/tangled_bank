---
date: "2021-01-01"
title: "2. Exploring With Summaries and Descriptions"
subtitle: "The first steps of exploratory data analysis"
---

> *"I think it is much more interesting to live with uncertainty than to live with answers that might be wrong."*
>
> ---- Richard Feynman

```{r}
#| message: false
#| warning: false
#| echo: false
library(tidyverse) # because we depend heavily on the tidyverse
```

::: {.callout-note appearance="simple"}
## In this Chapter

-   Data summaries
-   Measures of central tendency
-   Measures of dispersal
-   Descriptive statistics by group
:::

::: {.callout-important appearance="simple"}
## Tasks to complete in this Chapter

* Task B 1-10
:::

![](/images/internet_truth.jpg){fig-align="center" width="600"}

# Introduction

**Exploratory data analysis (EDA)** is an essential initial step in every statistical analysis project aimed at understanding the structure, patterns, and characteristics of the data. During EDA, we visually and quantitatively examine the data by creating plots, summary statistics, and descriptive measures to identify trends, outliers, and potential relationships among variables. This process helps us formulate hypotheses, choose appropriate statistical models, and identify potential issues or anomalies that may require further investigation or data cleaning. EDA serves as a foundation for making informed decisions and guiding the subsequent steps of a statistical analysis project.

In this Chapter we will focus on the basic components of EDA, and these initial forays are built around measures of the central tendency (the **mean**, **median**, **mode**) and the dispersion and variability (e.g. **standard deviations**, **standard error**, **variance**, **quantiles**) of the data. I advocate a workflow in which EDA and graphical assessments [(Chapter 3)](03-visualise.qmd) precede inferential statistics [(Chapter 7)](07-t_tests.qmd).

But first, I want to define a few principles that you will encounter throughout this course on biostatistics.

::: {.callout-note appearance="simple"}
## Variables and parameters

In a statistical framework, a **parameter** is a fixed, unknown (to be determined) value that characterises a population or distribution, and is often estimated using sample data. On the other hand, a **variable** is a measurable characteristic that can vary among individuals or objects in a population or sample. Parameters are used to describe the distribution of a population, while variables are the data points that are collected and analysed to draw conclusions about the population. Variables are used to estimate parameters through statistical methods such as regression and hypothesis testing.
:::

::: {.callout-note appearance="simple"}
## Samples and populations

In statistics, samples and populations are two fundamental concepts used to describe data sets and the process of drawing conclusions from them.

-   **Population** A population is the entire set of individuals, objects, or events of interest in a study. It represents the complete collection of data points that you would ideally like to examine to answer a specific research question. Populations can be finite or infinite, depending on the context. For example, the population could be all the trees in a forest, all the people living in a country, or all the bacteria in a particular environment.

-   **Sample** A sample is a smaller *random* subset of the population, selected to represent the larger group. Due to practical constraints, such as time, cost, and accessibility, it is often impossible or impractical to collect data from every member of a population. Instead, we collect data from a carefully chosen sample, with the goal of making inferences about the broader population based on the information gathered from this smaller group. Ideally, a sample should be representative of the population, meaning that it accurately reflects the characteristics and variability present in the entire group. When drawing more samples from the population, the more representative it will be, the smaller the variance will be, and the better the accuracy of the estimated mean (@fig-simul).

```{r}
#| echo: false
#| fig-cap: "Drawing increasingly larger sample sizes from a population with a true mean of 13 and an SD of 1."
#| label: fig-simul

set.seed(666)

# pre-allocate the tibble
normal_takes_shape <- tibble(number_draws = c(), draws = c())

# simulate increasingly larger samples
for (i in c(2, 5, 10, 50, 100, 500, 1000, 10000, 100000)) {
  draws_i <-
    tibble(
      number_draws = c(rep.int(
        x = paste(as.integer(i), " draws"),
        times = i
      )),
      draws = c(rnorm(
        n = i,
        mean = 13,
        sd = 1
      ))
    )

  normal_takes_shape <- rbind(normal_takes_shape, draws_i)
  rm(draws_i)
}

normal_takes_shape |>
  mutate(number_draws = as_factor(number_draws)) |>
  ggplot(aes(x = draws)) +
  geom_density(colour = "indianred3") +
  theme_minimal() +
  facet_wrap(
    vars(number_draws),
    scales = "free_y"
  ) +
  labs(
    x = "Mean",
    y = "Density"
  )
```
:::

::: {.callout-note appearance="simple"}
## When is something random?

This is a bit of a rant. I often get annoyed by the imprecise use of language and disrespect for scientific terms. 'Random' is such a word (as are 'literally' and 'energy'). The scientific concept of randomness and the colloquial usage of the word differ in their precision and context. Understanding these distinctions can help clarify the role of randomness in various settings.

-   **Scientific concept of randomness** In the scientific context, randomness refers to a specific pattern or sequence of events that completely lacks predictability, order, or regularity. The biostatistician often associates the term 'random' with *probability theory and statistical models*. In experiments, we use random sampling techniques to ensure that samples are representative of the population, and we employ random assignment to minimise biases in experimental groups. In computer science, algorithms generate pseudo-random numbers, which approximate true randomness but are ultimately determined by deterministic processes.

-   **Colloquial usage of randomness** In everyday language, the term 'random' is often used more loosely to describe events or situations that appear unexpected, surprising, or unrelated to a specific context. Colloquially, randomness may imply a lack of explanation or discernible pattern, even though in most cases the event might not be genuinely random in a strict scientific sense. For example, when someone says, "I had a random encounter with an old friend," they mean the meeting was unexpected or unplanned, rather than being the result of a stochastic process. In these instances, there most certainly is an explanation for why the two friends hapazzardly met.

An example of a truly random physical process is *radioactive decay*. Radioactive decay is a spontaneous process in which an unstable atomic nucleus loses energy by emitting radiation, such as alpha particles, beta particles, or gamma rays. The exact timing of when a specific atom will decay is unpredictable and follows a random pattern. The decay events are independent of each other and are governed by the principles of quantum mechanics, making radioactive decay a truly random physical process.

Sometimes we will see the term **stochastic** in the scientific literature. This term is similar to random *stricto sensu*, but it is used in a different context. The term 'random' refers to a lack of predictability or discernible pattern in a sequence of events, often associated with outcomes that are equally likely or follow a specific probability distribution. 'Stochastic', on the other hand, describes *processes* that incorporate an element of randomness or uncertainty, with outcomes that depend on both deterministic factors and random variables. Both terms are used in the context of probability theory and statistical models to describe events or processes that exhibit some level of unpredictability. Stochastic processes are widespread in nature and may explain a large fraction of ecological variation.

An example of a stochastic ecological process is the population dynamics of a species subject to unpredictable environmental fluctuations. In this scenario, the growth or decline of a species' population is influenced by both deterministic factors, such as birth and death rates, and random factors, like unpredictable weather events, disease outbreaks, or variations in food availability. These random factors introduce *uncertainty and variability* in the population dynamics, making it a stochastic ecological process.

Okay, you may ask, "Weather models can predict tomorrow's weather to some degree, albeit not with complete accuracy, so is there truly such a thing as unpredictable weather events"? Weather events might not be truly random in the strictest sense, as weather patterns are influenced by various physical factors, such as temperature, air pressure, humidity, and interactions between different atmospheric layers. However, due to the immense complexity and chaotic nature of these factors, weather events can be challenging to predict with absolute certainty, especially over longer time scales.

In this context, "unpredictable weather events" are considered random from a practical standpoint because they introduce uncertainty and variability in the system. While they may not be truly random, like quantum-level events, their unpredictability and the difficulty in forecasting their occurrence make them effectively random for many ecological and statistical analyses.
:::

# The Palmer penguin dataset

The [Palmer penguin dataset](https://apreshill.github.io/palmerpenguins-useR-2022/#/title-slide) in the **palmerpenguins** package is a modern alternative to the classic Iris dataset, often used for teaching data exploration, visualisation, and statistical modeling. The dataset was introduced by Dr. Kristen Gorman and the Palmer Station Long Term Ecological Research (LTER) program. It contains information collected from penguins living in the Palmer Archipelago, located in the Western Antarctic Peninsula, and focuses on three different penguin `species`: Adélie, Chinstrap, and Gentoo.

The dataset includes measurements such as bill length (`bill_length_mm`), bill depth (`bill_depth_mm`), flipper length (`flipper_length_mm`), body mass (`body_mass_g`), and sex (`sex`), along with the species of the penguins. It provides an opportunity to explore various aspects of data analysis, such as EDA, data visualisation, data preprocessing, hypothesis testing, and machine learning techniques like classification and clustering.

Let's start with loading the data:

```{r}
library(palmerpenguins)
```

# Exploring the properties of the data structure

Several functions help us get a *broad view of the data structure* (the dataset) itself: it's size (number of rows and columns), the data classes contained, the presence of missing values, and so forth. This table lists some of the most important ways to get such an overview:

| Purpose                   | Function      |
|:--------------------------|:--------------|
| The class of the dataset  | `class()`     |
| The head of the dataframe | `head()`      |
| The tail of the dataframe | `tail()`      |
| Printing the data         | `print()`     |
| Glimpse the data          | `glimpse()`   |
| Show number of rows       | `nrow()`      |
| Show number of columns    | `ncol()`      |
| The column names          | `colnames()`  |
| The row names             | `row.names()` |
| The dimensions            | `dim()`       |
| The dimension names       | `dimnames()`  |
| The data structure        | `str()`       |

First, it is a good idea to understand the class of the data structure. We can do this with the `class()` function. Applying it to the `penguins` dataset, we see that the dataset is a tibble, which is a modern take on the old-fashioned dataframe:

```{r}
class(penguins)
```

We can convert between tibbles and dataframes using the commands `as.data.frame()` and `as_tibble()`:

```{r}
penguins_df <- as.data.frame(penguins)
class(penguins_df)
```

And *vice versa*:

```{r}
penguins_tbl <- as_tibble(penguins_df)
class(penguins_tbl)
```

Note that the `print()` methods for tibbles and dataframes differ somewhat. You'll see that the `print()` function applied to a dataframe does not result in a particularly nice-looking table. Applied to a tibble, on the other hand, `print()` results in a neat, concise display of the dataset's content:

```{r}
# print the dataframe
print(penguins_df[1:5,]) # I limit to 5 rows

# print the tibble
print(penguins_tbl)
```

The `glimpse()` function is not dissimilar to `print()` as it gives more-or-less the same kind of output, but with a horisontal orientation:

```{r}
glimpse(penguins)
```

I suggest converting all your dataframes to tibbles to benefit from the Tidyverse philosophy around which tibbles were designed.

The tops and bottoms of tibbles (or dataframes) can be viewed with the `head()` and `tail()` functions. Both take the `n = ...` argument so that you can specify how many rows to display:

```{r}
head(penguins)
tail(penguins, n = 3)
```

You can wrap `head()` and `tail()` in `print()` for a nicer display:

```{r}
print(head(penguins))
print(tail(penguins, n = 3))
```

Three functions tell us about the size of the tibble. We can return the number of rows with `nrow()`, the number of columns with `ncol()`, or both the number of rows and columns with `dim()`:

```{r}
nrow(penguins)
ncol(penguins)
dim(penguins)
```

If we wanted to return the names of the columns we could use `colnames()` as such:

```{r}
colnames(penguins)
```

Tibbles do not have row names, but dataframes can have (although they are not frequently used). If we *did* have row names and wanted to know their names, we could use `row.names()` (note the `.` in the function name). We can also use `colnames()` (`names()` returns an identical output) and `row.names()` to assign new names to the dataframes or tibbles---see the respective help files for more information about this use. There is also the `dimnames()` function.

::: callout-important
## Task B
1.    Explain the output of `dimnames()` when applied to the `penguins` dataset.
:::

The last function to introduce in this section is `str()`. It can be applied to any R object (i.e. any data structure or model output).

```{r}
str(penguins)
```

::: callout-important
## Task B
2. Explain the output of `str()` when applied to the `penguins` dataset.
:::

# Exploring the properties of the data

Thus far we have looked at the properties of the data structure. Now we turn to the *properties of the data contained within a data structure*. This might simply be to see the data class of the different variables, or to provide some basic descriptive statistics. Before we go to deeply into the data summaries, we need to first review basic descriptive statistics.

**Descriptive statistics** refer to the process of *summarising* and organising data collected from our various kinds of studies in a meaningful way. The process of **summary** implies that we take something detailed (like a large number of individual measurements) and reduce its complexity to a view---a 'statistic'---that tells us something informative about the nature of the population. Descriptive statistics allow us to gain an initial understanding of the data's main features, including the **sample size**, **central tendency**, **variability**, and **distribution**. We will look at data distributions in [Chapter 4](04-distributions.qmd).

The sample size is simply the number of data points ($n$) per sample. The measures of central tendency include the **mean**, **median**, and **mode**, while variability is most often described using the **range**, **interquartile range**, **variance**, and **standard deviation**. Additionally, descriptive statistics can involve visual representations, such as histograms, box plots, and scatter plots, to further illustrate the data's characteristics, and we will cover these in [Chapter 3](03-visualise.qmd). Descriptive statistics serve as a foundation for more advanced statistical analyses, as it helps us make sense of our data and identify patterns, trends, or potential anomalies.

## Measures of central tendency

| Statistic | Function     | Package   |
|:----------|:-------------|:----------|
| Mean      | `mean()`     | **base**  |
| Median    | `median()`   | **base**  |
| Mode      | **Do it!**   |           |
| Skewness  | `skewness()` | **e1071** |
| Kurtosis  | `kurtosis()` | **e1071** |

Central tendency is a fundamental concept in statistics, referring to the *central* or *typical* value of a dataset that best represents its overall distribution. The measures of central tendency are also sometimes called 'location' statistics. As a key component of descriptive statistics, central tendency is essential for summarising and simplifying complex data. It provides a single representative value that captures the data's general behaviour and which might tell us something about the bigger population from which the random samples were drawn. 

A thorough assessment of the central tendency in EDA serves several purposes:

-   **Summary of data** Measures of central tendency, such as the mean, median, and mode, provide a single value that represents the center or typical value of a dataset. They help summarise the data and allow us to gain an early insight into the dataset's general properties and behaviour.

-   **Comparing groups or distributions** Central tendency measures allow us to compare different datasets or groups within a dataset. They can help identify differences or similarities in the data. This may be useful for hypothesis testing and inferential statistics.

-   **Data transformation decisions** Understanding the central tendency of our data can inform decisions on whether to apply transformations to the data to better meet the assumptions of certain statistical tests or improve the interpretability of the results.

-   **Identifying potential issues** Examining the central tendency can help reveal issues with the data, such as outliers or data entry errors, that could influence the results of inferential statistics. Outliers, for example, can greatly impact the mean, making the median a more robust measure of central tendency in such cases.

Understanding the central tendency informs the choice of inferential statistics in the following ways:

-   **Assumptions of statistical tests** Many inferential statistical tests have assumptions about the distribution of the data, such as normality, linearity, or homoscedasticity. Analysing the central tendency helps assess whether these assumptions are met and informs the choice of an appropriate test.

-   **Choice of statistical models** The central tendency can influence the choice of statistical models or the selection of dependent and independent variables in regression analyses---certain models or relationships may be more appropriate depending on the data's distribution and central tendencies.

-   **Choice of estimators** Central tendency measures can influence our choice of estimators for further inferential statistics, depending on the data's distribution and presence of outliers (e.g., mean vs. median).

Before I discuss each central tendency statistic, I'll generate some random data to represent normal and skewed distributions. I'll use these data in my discussions, below.

```{r}
# Generate random data from a normal distribution
n <- 5000 # Number of data points
mean <- 0
sd <- 1
normal_data <- rnorm(n, mean, sd)

# Generate random data from a slightly
# right-skewed beta distribution
alpha <- 2
beta <- 5
right_skewed_data <- rbeta(n, alpha, beta)

# Generate random data from a slightly
# left-skewed beta distribution
alpha <- 5
beta <- 2
left_skewed_data <- rbeta(n, alpha, beta)

# Generate random data with a bimodal distribution
mean1 <- 0
mean2 <- 10
sd1 <- 3
sd2 <- 4

# Generate data from two normal distributions
data1 <- rnorm(n, mean1, sd1)
data2 <- rnorm(n, mean2, sd2)

# Combine the data from both distributions to
# create a bimodal distribution
bimodal_data <- c(data1, data2)
```

```{r}
#| fig-cap: "A series of plots with histograms for the previously generated normal, right-skewed, and left-skewed distributions."
#| label: fig-histos

# Set up a three-panel plot layout
par(mfrow = c(2, 2))

# Plot the histogram of the normal distribution
hist(normal_data, main = "Normal Distribution",
     xlab = "Value", ylab = "Frequency",
     col = "lightblue", border = "black")

# Plot the histogram of the right-skewed distribution
hist(right_skewed_data, main = "Right-Skewed Distribution",
     xlab = "Value", ylab = "Frequency",
     col = "lightgreen", border = "black")

# Plot the histogram of the left-skewed distribution
hist(left_skewed_data, main = "Left-Skewed Distribution",
     xlab = "Value", ylab = "Frequency",
     col = "lightcoral", border = "black")

# Plot the histogram of the left-skewed distribution
hist(bimodal_data, main = "Bimodal Distribution",
     xlab = "Value", ylab = "Frequency",
     col = "khaki2", border = "black")

# Reset the plot layout to default
par(mfrow = c(1, 1))
```

### The sample mean 

The mean is the arithmetic average of the data, and it is calculated by summing all the data and dividing it by the sample size, *n* (@eq-mean).

::: {.column-margin}
**The mean, $\bar{x}$, is calculated thus:**
$$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_{i} = \frac{x_{1} + x_{2} + \cdots + x_{n}}{n}$$ {#eq-mean}
where $x_{1} + x_{2} + \cdots + x_{n}$ are the observations and $n$ is the number of observations in the sample.
:::

We can calculate the mean of a sample using the ... wait for it ... `mean()` function:

```{r}
round(mean(normal_data), 3)
```

::: callout-important
## Task B
3. How would you manually calculate the mean value for the `normal_data`? Do it now!
:::

```{r}
#| eval: false
#| echo: false
round(sum(normal_data) / length(normal_data), 3)
```

The mean is quite sensitive to the presence of outliers or extreme values in the data, and it is advised that its use be reserved for normally distributed data from which the extremes/outliers have been removed. When extreme values are indeed part of our data and not simply 'noise,' then we have to resort to a different measure of central tendency: the median.

### The median

The median indicates the center value in our dataset. The simplest way to explain what is is is to describe how it is determined. It can be calculated by 'hand' (if you have a small enough amount of data) by arranging all the numbers in sequence from low to high, and then finding the middle value. If there are five numbers, say `5, 2, 6, 13, 1`, then you would arrange them from low to high, i.e. `1, 2, 5, 6, 13`. The middle number is `5`. This is the median. But there is no middle if we have an even number of values. What now? Take this example sequence of six integers (they may also be floating point numbers), which has already been ordered for your pleasure: `1, 2, 5, 6, 9, 13`. Find the middle two numbers (i.e. `5, 6`) and take the mean. It is `5.5`. That is the median.

The median is therefore the value that separates the lower half of the sample data from the upper half. In normally distributed continuous data the median is equal to the mean. Comparable concepts to the median are the **1st** and **3rd quartiles**, which, respectively, separate the first quarter of the data from the last quarter---see the later in the section on 'Measures of variance and dispersal' in this Chapter. The advantage of the median over the mean is that it is unaffected by extreme values or outliers. The median is also used to provide a robust description of non-parametric data (see [Chapter 4](04-distributions.qmd) for a discussion on normal data and other data distributions).

What is the median of the `normal_data` dataset? We use the `median()` function for this:

```{r}
round(median(normal_data), 3)
```

It is easier to see what the median is by looking at a much smaller dataset. Let's take 11 random data points:

```{r}
small_normal_data <- round(rnorm(11, 13, 3), 1)
sort(small_normal_data)
median(small_normal_data)
```

The mean and median together provide a comprehensive understanding of the data's central tendency and underlying distribution.

:::{.callout-note appearance="simple"}
## What is the relationship between the median and quantiles?
The relation between the median and quantiles lies in their roles as measures that describe the relative position of data points within a dataset. Quantiles are values that partition a dataset into equal intervals, with each interval containing the same proportion of the data. The most common types of quantiles are quartiles, quintiles, deciles, and percentiles.

The median is a special case of a quantile, specifically the 50th percentile or the second quartile (Q2). It divides the dataset into two equal halves, with 50% of the data points falling below the median and 50% of the data points falling above the median. In this sense, the median is a central quantile that represents the middle value of the dataset.

Both the median and quantiles help describe the distribution and spread of a dataset, with the median providing information about the center and other quantiles (such as quartiles) offering insights into the overall shape, skewness, and dispersion of the data.
:::

### The mode

The mode is a measure that represents the value or values that occur most frequently in a dataset. Unlike the mean and median, the mode can be used with both numerical and categorical data, making it quite versatile. For a dataset with a single value that appears most often, the distribution is considered **unimodal**. However, datasets can also be **bimodal** (having two modes) or **multimodal** (having multiple modes) when there are multiple values that occur with the same highest frequency.

While the mode may not always be a good representative of the dataset's center, especially in the presence of extreme values or skewed distributions, it can still offer valuable information about the data's characteristics when used alongside the other measures of central tendency.

There is no built-in function to calculate the mode of a numeric vector, but you can make one if you need it. There are some examples on the internet that you will be able to adapt to your needs, but my cursory evaluation of them does not suggest they are particularly useful. The easiest way to see the data's mode(s) is to examine a histogram of your data. All the data we have explored above are examples of unimodal distributions, but a bimodal distribution can also be seen in @fig-histos.

### Skewness

Skewness is a measure of symmetry (or asymmetry) of the data distribution, and it is best understood by understanding the location of the median relative to the mean. A distribution with a skewness of zero is considered symmetric, with both tails extending equally on either side of the mean. Here, the mean will be the same as the median. A negative skewness indicates that the mean of the data is less than their median---the data distribution is left-skewed; that is, there is a longer or heavier tail to the left of the mean. A positive skewness results from data that have a mean that is larger than their median; these data have a right-skewed distribution; so there will be a longer or heavier tail to the right of the mean. Base R does not have a built-in skewness function, but we can use the one included with the **e1071** package:

```{r}
library(e1071)
# Positive skewness
skewness(right_skewed_data)

# Is the mean larger than the median?
mean(right_skewed_data) > median(right_skewed_data)

# Negative skewness
skewness(left_skewed_data)

# Is the mean less than the median?
mean(left_skewed_data) < median(left_skewed_data)
```

### Kurtosis

Kurtosis describes the tail shape of the data's distribution. Kurtosis is effectively a measure of the 'tailedness' or the concentration of data in the tails of a distribution, relative to a normal distribution. A normal distribution has zero kurtosis (or close to) and thus the standard tail shape (**mesokurtic**). Negative kurtosis indicates data with a thin-tailed (**platykurtic**) distribution. Positive kurtosis indicates a fat-tailed distribution (**leptokurtic**).

Similarly as to skewness, we use the **e1071** package for a kurtosis function. All the output shown below suggests a tendency towards thin-tailedness, but it is subtle.

```{r}
kurtosis(normal_data)
kurtosis(right_skewed_data)
kurtosis(left_skewed_data)
```

I have seldom used the concepts of the skewness or kurtosis in any EDA, but it is worth being aware of them. The overall purpose of examining data using the range of central tendency statistics is to get an idea of whether our data are **normally distributed**---*a normal distribution is a key requirement for all parametric inferential statistics*. See [Chapter 4](04-distributions.qmd) for a discourse of data distributions. These central tendency statistics will serve you well as a first glance, but formal tests for normality do exist and I encourage their use before embarking on the rest of the journey. We will explore these formal tests in [Chapter 7](07-t_tests.qmd).

::: callout-important
## Task B
4. Find the `faithful` dataset and describe both variables in terms of their measures of central tendency. Include graphs in support of your answers (use `ggplot()`), and conclude with a brief statement about the data distribution.
:::

```{r}
#| eval: false
#| echo: false

# Load the faithful dataset
data(faithful)

# Set up a two-panel plot layout
par(mfrow = c(1, 2))

# Plot the histogram of the 'eruptions' variable
hist(faithful$eruptions, main = "Eruptions", xlab = "Duration (minutes)", ylab = "Frequency", col = "lightblue", border = "black")

# Plot the histogram of the 'waiting' variable
hist(faithful$waiting, main = "Waiting", xlab = "Time (minutes)", ylab = "Frequency", col = "lightgreen", border = "black")

# Reset the plot layout to default
par(mfrow = c(1, 1))

library(e1071)
kurtosis(faithful$eruptions)
kurtosis(faithful$waiting)
skewness(faithful$eruptions)
skewness(faithful$waiting)
```

## Measures of variance or dispersion around the center

| Statistic            | Function     |
|:---------------------|:-------------|
| Variance             | `var()`      |
| Standard deviation   | `sd()`       |
| Minimum              | `min()`      |
| Maximum              | `max()`      |
| Range                | `range()`    |
| Quantile             | `quantile()` |
| Inter Quartile Range | `IQR()`      |
<!-- | Covariance           | `cov()`      | -->
<!-- | Correlation          | `cor()`      | -->

A good understanding of variability, or variation around the central point, is crucial in EDA for several reasons:

-   **Signal vs. noise** Variability helps distinguish between the signal (true underlying pattern) and noise (random fluctuations that might arise from stochastic processes, measurement or experimental error, or other unaccounted for influences) in the data. High variability can make it difficult to detect meaningful patterns or relationships in the data, while low variability may indicate a strong underlying pattern.

-   **Precision and reliability** Variability is related to the precision and reliability of measurements. Smaller variability indicates more consistent and precise measurements, whereas larger variability suggests inconsistency and potential issues with the data collection process.

-   **Comparing groups** Understanding variability is essential when comparing different groups or datasets. Even if two groups have similar central tendencies, their variability may differ significantly, leading to different interpretations of the data.

-   **Assumptions of statistical tests** Many inferential statistical tests have assumptions about the variability of the data, such as homoscedasticity (equal variances across groups) or independence of observations. Assessing variability helps determine whether these assumptions are met and informs the choice of appropriate tests.

-   **Effect sizes and statistical power** Variability plays a role in determining the effect size (magnitude of the difference between groups or strength of relationships) and the statistical power (ability to detect a true effect) of a study. High variability can make it harder to detect significant effects, requiring larger sample sizes to achieve adequate statistical power.

Understanding variability informs the choice of inferential statistics:

-   **Parametric vs non-parametric tests** If the data exhibit normality and homoscedasticity, parametric tests may be appropriate (see [Chapter 7](07-t_tests.qmd)). However, if the data have high variability or violates the assumptions of parametric tests, non-parametric alternatives may be more suitable.

-   **Choice of estimators** Variability can influence the choice of estimators (e.g., mean vs. median) for central tendency, depending on the data's distribution and presence of outliers.

-   **Sample size calculations** Variability informs sample size calculations for inferential statistics. Higher variability typically requires larger sample sizes to achieve sufficient statistical power.

-   **Model selection** Variability can influence the choice of statistical models, as certain models may better accommodate the variability in the data than others (e.g., linear vs. non-linear models, fixed vs. random effects).

Let us now look at the estimators of variance.

### Variance and standard deviation

Variance and standard deviation (SD) are examples of interval estimates. The sample variance, $S^{2}$, may be calculated according to the following formula (@eq-sd). If we cannot be bothered to calculate the variance and SD by hand, we may use the built-in functions `var()` and `sd()`:

::: {.column-margin}
**The sample variance:**
$$S^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}$$ {#eq-sd}

This reads, "the sum of the squared differences from the mean, divided by the sample size minus 1." To get the standard deviation, $S$, we take the square root of the variance, i.e. $S = \sqrt{S^{2}}$. 
:::

```{r}
var(normal_data)
sd(normal_data)
```

::: callout-important
## Task B
5. How would you manually calculate the variance and SD for the `normal_data`? Do it now! Make sure your answer is the same as those reported above.
:::

The interpretation of the concepts of mean and median are fairly straight forward and intuitive. Not so for the measures of variance. What does $S$ represent? Firstly, the unit of measurement of $S$ is the same as that of $\bar{x}$ (but the variance doesn't share this characteristic). If temperature is measured in °C, then $S$ also takes a unit of °C. Since $S$ measures the dispersion *around* the mean, we write it as $\bar{x} \pm S$ (note that often the mean and standard deviation are written with the letters *mu* and *sigma*, respectively; i.e. $\mu \pm \sigma$). The smaller $S$ the closer the sample data are to $\bar{x}$, and the larger the value is the further away they will spread out from $\bar{x}$. So, it tells us about the proportion of observations above and below $\bar{x}$. But what proportion? We invoke the the 68-95-99.7 rule: \~68% of the population (as represented by a random sample of $n$ observations taken from the population) falls within 1$S$ of $\bar{x}$ (i.e. \~34% below $\bar{x}$ and \~34% above $\bar{x}$); \~95% of the population falls within 2$S$; and \~99.7% falls within 3$S$ (@fig-expectednormal).

::: {.column-margin}
![The idealised Normal distribution showing the proportion of data within 1, 2, and 3SD from the mean.](../../images/Standard_deviation_diagram.svg){#fig-expectednormal}
:::

Like the mean, $S$ is affected by extreme values and outliers, so before we attach $S$ as a summary statistic to describe some data, we need to ensure that the data are in fact normally distributed. We will talk about how to do this in [Chapter 7](07-t_tests.qmd), where we will go over the numerous ways to check the assumption of normality. When the data are found to be non-normal, we need to find appropriate ways to express the spread of the data. Enter the [quartiles](#quartiles-and-the-interquartile-range).

### The minimum, maximum, and range

A description of the extremes (edges of the distribution) of the data can also be provided by the functions `min()`, `max()` and `range()`. These concepts are straight forward and do not require elaboration. They apply to data of any distribution, and not only to normal data. These statistics are often the first places you want to start when looking at the data for the first time. Note that `range()` does something different from `min()` and `max()`:

```{r}
min(normal_data)
max(normal_data)
range(normal_data)
```

`range()` actually gives us the minimum and maximum values, and not the difference between them. To find the range value properly we must be a bit more clever:

```{r}
range(normal_data)[2] - range(normal_data)[1]
```

### Quartiles and the interquartile range

A more forgiving approach (forgiving of the extremes, often called 'robust') is to divide the distribution of ordered data into quarters and finding the points below which 25% (0.25, the first quartile; Q1), 50% (0.50, the median; Q2) and 75% (0.75, the third quartile; Q3) of the data are distributed. These are called quartiles (for 'quarter;' not to be confused with *quantile*, which is a more general concept that divides the distribution into any arbitrary proportion from 0 to 1). 

The interquartile range (IQR) is a measure of statistical dispersion that provides information about the spread of the middle 50% of a dataset. It is calculated by subtracting the first quartile (25th percentile) from the third quartile (75th percentile).

The quartiles and IQR have several important uses:

-   **Identifying central tendency** As I have shown earlier, the second quartile, or median, is a measure of central tendency that is less sensitive to outliers than the mean. It offers a more robust estimate of the typical value in skewed distributions or those with extreme values.

-   **Measure of variability** The IQR is a robust measure of variability that is less sensitive to outliers and extreme values compared to other measures like the range or standard deviation. It gives a better understanding of the data spread in the middle part of the distribution.

-   **Identifying outliers** The IQR can be used to identify potential outliers in the data. A common method is to define outliers as data points falling below the first quartile minus 1.5 times the IQR or above the third quartile plus 1.5 times the IQR.

-   **Describing skewed data** For skewed distributions, the quartiles and IQR provide a better description of the data spread than the standard deviation, as it is not influenced by the skewness of the data. It can help reveal the degree of asymmetry in the distribution and the concentration of values in the middle portion.

-   **Comparing distributions** The IQR can be used to compare the variability or spread of two or more distributions. It provides a more robust comparison than the standard deviation or range when the distributions have outliers or are not symmetric, and the median reveals departures from normality.

-   **Box plots** The quartiles and IQR are key components of box plots, which are graphical representations of the distribution of a dataset. Box plots display the median, first quartile, third quartile, and potential outliers, providing a visual representation of the data's central tendency, spread, and potential outliers.

In R we use the `quantile()` function to provide the quartiles. Here is a demonstration:

```{r}
# Look at the normal data
quantile(normal_data, p = 0.25)
quantile(normal_data, p = 0.75)

# Look at skewed data
quantile(left_skewed_data, p = 0.25)
quantile(left_skewed_data, p = 0.75)
```

::: callout-important
## Task B
6. Write a few lines of code to demonstrate that the above results indeed conform to the formal definition for what quantiles are? I.e., show manually how you can determine that 25% of the observations indeed falls below `r round(quantile(normal_data, p = 0.25), 2)` for the `normal_data`. Explain the rationale to your approach.  
:::

We calculate the interquartile range using the `IQR()` function:

```{r}
IQR(normal_data)
```

# Data summaries

| Purpose                                 | Function          | Package             |
|:----------------------------------------|:------------------|:--------------------|
| Summary of the data properties          | `summary()`       | **base**            |
|                                         | `describe()`      | **psych**           |
|                                         | `skim()`          | **skimr**           |
|                                         | `descriptives()`  | **jmv**             |
|                                         | `dfSummary()`     | **summarytools**    |


R has a built-in function that provides a broad overview, not only of the classes within a dataframe, but also of some coarsely calculated descriptive statistics that might serve well as a starting place for EDA. There also also several packages that offer similar views of our data. I'll introduce these below, but leave it to you to study and to choose one you like best.

Above, I said, "coarsely calculated". Why? Because it ignores the grouping structure of the data.

::: callout-important
## Task B
7. Why is it important to consider the grouping structures that might be present within our datasets?  
:::

## `summary()`

The first method is a generic function that can be applied to a range of R data structures, and whose output depends on the class of the structure. It is called `summary()`. This function can be applied to the dataset itself (here a tibble) and also to the output of some models fitted to the data (later we will see, for instance, how it is applied to *t*-tests, ANOVAs, correlations, and regressions). When applied to a dataframe or tibble, we will be presented with something quite useful. Let us return to the Palmer penguin dataset, and you'll see many familiar descriptive statistics:

```{r}
summary(penguins)
```

::: callout-important
## Task B
8. Explain the output of `summary()` when applied to the `penguins` dataset.
:::

## `psych::describe()`

The **psych** package has the `describe()` function, which provides a somewhat more verbose output containing many of the descriptive statistics I introduced earlier in this Chapter:

```{r}
psych::describe(penguins)
```

## `skimr::skim()`

The **skimr** package offers something similar, but different. The `skim()` function returns:

```{r}
library(skimr)
skim(penguins)
```

## `jmv::descriptives()`

Here's yet another view into our data, this time courtesy of the **jmv** package:

```{r}
library(jmv)
descriptives(penguins, freq = TRUE)
```

## `summarytools::dfSummary()`

And lastly, there is the **summarytools** package and the `dfSummary()` function within:

```{r}
library(summarytools)
print(dfSummary(penguins, 
                varnumbers   = FALSE, 
                valid.col    = FALSE, 
                graph.magnif = 0.76),
      method = 'render')
```

As you can see, there are many option and you may use the one you least dislike. I'll not be prescriptive or openly opinionated about it.

<!--- ### Covariance --->

<!-- include the equation -->

<!--- ### Correlation

The correlation coefficient of two matched (paired) variables is equal to their covariance divided by the product of their individual standard deviations. It is a normalised measurement of how linearly related the two variables are. --->

<!-- include example here -->

# Descriptive statistics by group

In this Chapter, we have explored the fundamental types of summary statistics typically encountered during EDA and the methods for calculating them. While these basic measures provide valuable insights, delving deeper into the data requires recognising the inherent grouping structures. By identifying how group membership influences measured variables, we can answer questions such as, "Are there differences in bill length, bill depth, flipper length, and body mass between species?" or "Does the island where individuals were studied affect these variables?" Additionally, we might be interested in investigating the effects of sex and year. Numerous approaches can be taken to dissect this structured dataset, each shedding light on unique aspects of the data's structure and relationships.

Here's an example. Remember the `ChickWeight` dataset introduced in [Chapter 4](../intro_r/04-graphics.qmd) of Intro R? In this dataset, we calculated the mean (etc.) for all the chickens, over all the diet groups to which they had been assigned (there are four factors, i.e. Diets 1 to 4), and over the entire duration of the experiment (the experiment lasted 21 days). It would be more useful to see what the weights are of the chickens in each of the four groups at the end of the experiment---we can compare means (± SD) and medians (± interquartile ranges, etc.), for instance. You'll notice now how the measures of central tendency are being combined with the measures of variability/range. Further, we can augment this statistical summary with many kinds of graphical summaries, which will be far more revealing of differences (if any) amongst groups.

An analysis of the `ChickWeights` dataset that recognises the effect of diet and time (start and end of experiment) might reveal something like this:

```{r}
#| echo: false
grp_stat <- ChickWeight |> 
  filter(Time %in% c(0, 21)) |> 
  group_by(Diet, Time) |> 
  summarise(mean_wt = round(mean(weight, na.rm = TRUE), 1),
            sd_wt = round(sd(weight, na.rm = TRUE), 1),
            min_wt = min(weight),
            qrt1_wt = quantile(weight, p = 0.25),
            med_wt = median(weight),
            qrt3_wt = quantile(weight, p = 0.75),
            max_wt = max(weight),
            n_wt = n(),
            .groups = "drop") |> 
  as_tibble()
  
print(grp_stat)
```

We typically report the measure of central tendency together with the associated variation. So, in a table we would want to include the mean ± SD. For example, this table is almost ready for including in a publication:

```{r}
#| echo: false
library(knitr)
library(kableExtra)

grp_stat2 <- grp_stat |> 
  mutate(Diet = as.numeric(Diet)) |> 
  select(Diet, Time, mean_wt, sd_wt) |> 
  unite(mean_wt, sd_wt, col = `Mean ± SD`, sep = " ± ") |> 
  as_tibble()

grp_stat2 |> 
  kbl(caption = "Mean ± SD for the `ChickWeight` dataset as a function of Diet and Time.",
      align = "ccr") |> 
  column_spec(1, width = "2.5cm") |> 
  column_spec(2, width = "2.5cm") |> 
  column_spec(3, width = "5.0cm") |> 
  kable_classic(full_width = F, html_font = "Cambria")
```


Further, we want to supplement this EDA with some figures that visually show the effects. Here I show a few options for displaying the effects in different ways: @fig-chicks shows the spread of the raw data, the mean or median, as well as the appropriate accompanying indicators of variation around the mean or median. I will say much more about using figures in EDA in [Chapter 3](../basic_stats/03-visualise.Rmd).

```{r}
#| echo: false
#| label: fig-chicks
#| fig-cap: "The figures represent A) a scatterplot of the mean and raw chicken mass values; B) a bar graph of the chicken mass values, showing whiskers indicating 1 ±SD; C) a box and whisker plot of the chicken mass data; and D) chicken mass as a function of both `Diet` and `Time` (10 and 21 days)."
#| column: margin

library(ggpubr) # needed for arranging multi-panel plots
plt1 <- ChickWeight %>%
  filter(Time == 21) %>% 
  ggplot(aes(x = Diet, y = weight)) +
  geom_jitter(width = 0.05) + # geom_point() if jitter not required
  geom_point(data = filter(grp_stat, Time == 21), aes(x = Diet, y = mean_wt),
             col = "black", fill = "red", shape = 23, size = 3) +
  labs(y = "Mass (g)") + 
  theme_pubr()

plt2 <- ggplot(data = filter(grp_stat, Time == 21), aes(x = Diet, y = mean_wt)) +
  geom_bar(position = position_dodge(), stat = "identity", 
           col = NA, fill = "salmon") +
  geom_errorbar(aes(ymin = mean_wt - sd_wt, ymax = mean_wt + sd_wt),
                width = .2) +
  labs(y = "Mass (g)") + 
  theme_pubr()
# position_dodge() places bars side-by-side
# stat = "identity" prevents the default count from being plotted

# a description of the components of a boxplot is provided in the help file
# geom_boxplot()
plt3 <- ChickWeight %>%
  filter(Time == 21) %>% 
  ggplot(aes(x = Diet, y = weight)) +
  geom_boxplot(fill = "salmon") +
  geom_jitter(width = 0.05, fill = "white", col = "blue", shape = 21) +
  labs(y = "Mass (g)") + 
  theme_pubr()

plt4 <- ChickWeight %>%
  filter(Time %in% c(10, 21)) %>% 
  ggplot(aes(x = Diet, y = weight, fill = as.factor(Time))) +
  geom_boxplot() +
  geom_jitter(shape = 21, width = 0.1) +
  labs(y = "Mass (g)", fill = "Time") +
  theme_pubr()

ggarrange(plt1, plt2, plt3, plt4, ncol = 2, nrow = 2, labels = "AUTO")
```

::: callout-important
## Task B
9. Using a tidy workflow, assemble a summary table of the **palmerpenguins** dataset that has a similar appearance as that produced by `psych::describe(penguins)`.

    - For bonus marks of up to 10% added to Task B, apply a beautiful and creative styling to the table using the [**kable**](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html) package. Try and make it as publication ready as possible. Refer to a few journal articles to see how to professionally typeset tables.

10. Still using the **palmerpenguins** dataset, perform an exploratory data analysis to investigate the relationship between penguin species and their morphological traits (bill length, bill depth, flipper length, and body mass). Employ the tidyverse approaches learned earlier in the module to explore the data and account for the grouping structures present within the dataset. Provide visualisations (use @fig-chicks for inspiration) and summary statistics to support your findings and elaborate on any observed patterns or trends.
:::



