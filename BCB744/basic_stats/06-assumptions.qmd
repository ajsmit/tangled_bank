---
date: "2021-01-01"
title: "6. Assumptions for parametric statistics"
subtitle: "When assumptions fail alchemy starts"
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
```


::: {.callout-note appearance="simple"}
## In this Chapter

-   Revisiting assumptions
-   Normality and homogeneity of variance tests
-   Revisiting the non-parametric tests
-   Log transformation
-   Square-root transformation
-   Arcsine transformation
-   Power transformation
-   Lesser-used transformation
:::

::: {.callout-important appearance="simple"}
## Tasks to complete in this Chapter

-   None
:::

![](../../images/paranormal_distributions.jpeg){fig-align="center" width="600"}

# Introduction

Parametric statistical tests such as *t*-tests, ANOVAs, regressions, and correlations are built on some assumptions about the nature of our data. In life, however, sometimes things get messy and the assumptions go unmet. In this Section we will look in more detail into testing the assumptions and what do do when the assumptions are violated.

<!-- The first challenge involves *assessing whether our data satisfy the necessary assumptions* preceding the main inferential statistical procedure---we will explore the various formal assumption tests later on in this section. -->

<!-- The second challenge can be more tricky, depending on the chosen solution. Suppose the data fail to meet one or more of the necessary assumptions: we now have to decide on the next course of action. First prize is to select a suitable non-parametric alternative to the inferential statistical method required to test our hypotheses. Non-parametric tests do not require assumptions about the underlying population distribution, making them a robust alternative. However, if a suitable non-parametric alternative is not available the only feasible way forward is to apply some form of data transformation to the original dataset to 'force' the data to meet the necessary assumptions and thus permitting us to use a parametric statistical test. This process can be complicated as the physical nature of the data may not allow for easy transformation, and different transformations can lead to different conclusions. -->

<!-- In the sections below we will look at tests for the various assumptions, then we will consider the non-parametric substitutes for the parametric tests, and lastly, we will discuss the alchemy of data transformations. -->

# Testing the assumptions {#sec-assum}

::: {.column-margin}
![](../../images/wahlberg_assumptions.jpeg){fig-align="center" width="600"}
:::

The parametric statistical tests that we are frequently required to perform demand that our data fulfil a few crucial assumptions that are not guaranteed to hold. These assumptions are frequently violated as real-world data, particularly biological data, are typically complex and often contain measurement errors or other sources of variability. Therefore, we need to ensure that:

-   the data are **normally distributed**, meaning that the data follow a Gaussian distribution;
-   that the data are **homoscedastic**, i.e. the variance should be the same across all levels of the independent variable, and in particular, that there are no outliers;
-   the dependent variable must be **continuous**,
-   the observations in the groups being compared are **independent** of each other,

I view the last two points as expectations, not an assumptions, because these two aspects of our data are entirely under our control. No amount of transformation or manipulation can make data independent or continuous if they were not produced to be so right from the outset.

::: {.callout-note appearance="simple"}
## *i.i.d.*
Sometimes we will see the term *i.i.d.* which stands for "independent and identically distributed". i.i.d. is a general way of stating the assumptions, particularly bullet points 2-4 above.

**Independent** means that the occurrence of one event does not affect the occurrence of any other event in the sample. In other words, the observations are unrelated to each other.

**Identically distributed** means that each observation is drawn from the same underlying probability distribution. In other words, the statistical properties of each observation are the same.
:::

How do we know that the assumptions of normality and homoscedasticity are not violated? Here are your options. Although I mention the stats tests to use, I will not explain each in detail with examples. You should easily be able to figure it out at this stage of your R journey.

## Tests for normality

```{r}
#| fig-cap: "Histograms showing two randomly generated normal distributions."
#| label: fig-histo1
#| column: margin
#| echo: false
#| message: false

# Random normal data
set.seed(666)
r_dat <- data.frame(dat = c(rnorm(n = 1000, mean = 10, sd = 3),
                            rnorm(n = 1000, mean = 8, sd = 2)),
                    sample = c(rep("A", 1000), rep("B", 1000)))

# Create histogram
h <- ggplot(data = r_dat, aes(x = dat, fill = sample)) +
  geom_histogram(position = "dodge", binwidth = 1,
                 colour = "black", alpha = 0.8) +
  geom_density(aes(y = ..count.., fill = sample),
               colour = "black", alpha = 0.2) +
  labs(x = "value") +
  theme_minimal()
h
```

Remember from [Chapter 4](04-distributions.qmd) what a normal distribution is/looks like? Let's have a peek below to remind ourselves (@fig-histo1). 

Whereas histograms may be a pretty way to check the normality of our data, there are actual statistical tests for this, which is preferable to a visual inspection alone. But remember that you should *always* visualise your data before performing any statistics on them.

::: {.callout-note}
## Hypothesis for normailty
$H_{0}$: The distribution of our data is not different from normal (or, the variable is normally distributed).
:::

The **Shapiro-Wilk** test is frequently used to assess the normality of a dataset. It is known to have good power and accuracy for detecting departures from normality, even for small sample sizes, and it is also robust to outliers, making it useful for analysing data that may contain extreme values.

It tests the *H*~0~ that *the population from which the sample, $x_{1},..., x_{n}$, was drawn is not significantly different from normal*. The test does so by sorting the data from lowest to highest, and a test statistic, $W$, is calculated based on the deviations of the observed values from the expected values under a normal distribution (@eq-shapiro). $W$ is compared to a critical value, based on the sample size and significance level, to determine whether to reject or fail to reject the *H*~0~. 

::: {.column-margin}
**The Shapiro-Wilk test**:
$$W = \frac{(\sum_{i=1}^n a_i x_{(i)})^2}{\sum_{i=1}^n (x_i - \overline{x})^2}$$ {#eq-shapiro}

Here, $W$ represents the Shapiro-Wilk test statistic, $a_{i}$ are coefficients that depend on the sample size and distribution of the data, $x_{(i)}$ represents the $i$-th order statistic, or the $i$-th smallest value in the sample, and $\overline{x}$ represents the sample mean.
:::

The Shapiro-Wilk test is available within base R as the function `shapiro.test()`. If the *p*-value is **above** 0.05 we may assume the data to be normally distributed. In order to demonstrate what the output of `shapiro.test()` looks like we will run it on all of the random data we generated.

```{r}
shapiro.test(r_dat$dat)
```

Note that this shows that the data are *not* normally distributed. This is because we have incorrectly run this function simultaneously on two different samples of data. To perform this test correctly, and in the tidy way, we need to recognise the grouping structure (Groups A and B) and select only the second piece of information from the `shapiro.test()` output and ensure that it is presented as a numeric value:

```{r}
# we use the square bracket notation to select only the *-value;
# had we used `[1]` we'd have gotten W
r_dat %>% 
  group_by(sample) %>% 
  summarise(norm_dat = as.numeric(shapiro.test(dat)[2]))
```

Now we see that our two sample sets are indeed normally distributed.

Several other tests are available to test whether our data are consistent with a normal distribution:

-   **Kolmogorov-Smirnov test** This test is a non-parametric test that compares the empirical distribution of a sample with a hypothesised normal distribution. It is based on the maximum absolute difference between the cumulative distribution function of the sample and the theoretical normal distribution function. This test can also be used to see if one's own data are consistent with other kinds of data distributions. In R the Kolmogorov-Smirnov test is available as `ks.test()`.

-   **Anderson-Darling test** Similar to the Shapiro-Wilk test, the Anderson-Darling test is used to test the hypothesis that a sample comes from a normal (or any other) distribution. It is based on the squared differences between the empirical distribution function of the sample and the theoretical normal distribution function. This function is not natively available in base R but the function `ad.test()` is made available in two packages (that I know of), namely, **nortest** and **kSamples**. Read the help files---even though the name of the function is the same in the two packages, they are implemented differently.

-   **Lilliefors test** This test is a modification of the Kolmogorov-Smirnov test that is specifically designed for small sample sizes. It is based on the maximum difference between the empirical distribution function of the sample and the normal distribution function. Some R packages such as **nortest** and **descTools** seem to use Lilliefors synonymously with Kolmogorov-Smirnov. These functions are called `lillie.test()` and `LillieTest()`, respectively. 

-   **Jarque-Bera test** This test is based on the skewness and kurtosis of a sample and tests whether the sample has the skewness and kurtosis expected from a normal distribution. Find it in R as `jarque.bera.test()` in the **DescTools** and **tseries** packages. Again, read the help files as a function with the same name appears in two independent packages and I cannot give assurance that it implemented consistently.

-   **Cramer-Von Mises test** The Cramer-Von Mises test is used to assess the goodness of fit of a distribution to a sample of data. The test is based on the cumulative distribution function (CDF) of the sample and the theoretical distribution being tested. See the `cvm.test()` function in the **goftest** package.

Take your pick. The Shapiro-Wilk and Kolmogorov-Smirnov tests are the most frequently used normality tests in my experience but be adventurous and use the Cramer-Von Mises test and surprise your supervisor in an interesting way---more than likely, they will not have heard of it before. When you decide, however, do your homework and read about these pros and cons of the tests as they are not all equally robust to all the surprises data can throw at them.

## Tests for homogeneity of variances {#sec-homogeneity}

Besides requiring that our data are normally distributed, we must also ensure that they are **homoscedastic**. This word means that the scedasticity (variance) of our samples is homogeneous (similar). In practical terms this means that the variance of the samples we are comparing should not be more than two to four times greater than one another. In R, we use the function `var()` to check the variance in a sample:

```{r}
r_dat %>% 
  group_by(sample) %>% 
  summarise(sample_var = var(dat))
```

Above we see that the variance of our two samples is homoscedastic because the variance of one is not more than two to four times greater than the other. However, there are formal tests to establish the equality of variances, as we can see in the following hypothesis tests:

::: {.callout-note}
## Hypotheses for equality of variances
The two-sided and one-sided formulations are:

$H_{0}: \sigma^{2}_{A} = \sigma^{2}_{B}$ and $H_{a}: \sigma^{2}_{A} \ne \sigma^{2}_{B}$

$H_{0}: \sigma^{2}_{A} \le \sigma^{2}_{B}$ and $H_{a}: \sigma^{2}_{A} \gt \sigma^{2}_{B}$

$H_{0}: \sigma^{2}_{A} \ge \sigma^{2}_{B}$ and $H_{a}: \sigma^{2}_{A} \lt \sigma^{2}_{B}$

where $\sigma^{2}_{A}$ and $\sigma^{2}_{B}$ are the variances for samples $A$ and $B$, respectively.
:::

The most commonly used test for equality of variances is **Levene's test**, `leveneTest()`. Levene's test assess the equality of variances between two or more groups in a dataset. The *H*~0~ is that the variances of the groups are equal. It is a non-parametric test that does not assume anything about the data's normality and as such it is more robust than the *F*-test.

The test is commonly used in *t*-tests and ANOVA to check that the variances of the dependent variable are the same across all levels of the independent variable. Violating this assumption can lead to incorrect conclusions made from the test outcome, such as those resulting from Type I and Type II errors.

In Levene's test, the absolute deviations of the observations from their group medians are calculated, and the test statistic is computed as the ratio of the sum of the deviations to the degrees of freedom (@eq-levene). The test statistic follows an *F* distribution under the *H*~0~, and a significant result indicates that the variances of the groups are significantly different.

::: {.column-margin}
**Levene's test**:

$$W = \frac{(N-k)}{(k-1)} \cdot \frac{\sum_{i=1}^k n_i (\bar{z}_i - \bar{z})^2}{\sum_{i=1}^k \sum_{j=1}^{n_i} (z_{ij} - \bar{z}_i)^2}$$ {#eq-levene}

where $W$ represents the Levene's test statistic, $N$ is the total sample size, $k$ is the number of groups being compared, $n_i$ is the sample size of the $i$-th group, $z_{ij}$ is the $j$-th observation in the $i$-th group, $z_{i}$ is the mean of the ith group, and $\bar{z}$ is the overall mean of all groups combined.

The test statistic is calculated by comparing the deviations of the observations within each group from their group mean ($\bar{z}_i$) to the deviations of the group means from the overall mean ($\bar{z}$).
:::

Levene's test is considered robust to non-normality and outliers, making it a useful tool for analysing data that do not meet the assumptions of normality, but it can be sensitive to unequal sample sizes and may not be appropriate for very small sample sizes. 

Several other statistical tests are available to assess the homogeneity of variances in a dataset:

-   ***F*-test** This test is also known as the variance ratio test. Use the `var.test()` function in R. It assumes that the underlying data follows a normal distribution and is designed to test the *H*~0~ that the variances of two populations are equal. The test statistic is the ratio of the variances of the two populations. You will often see this test used in the context of an ANOVA to test for homogeneity of variance across groups.

-   **Bartlett's test** This test is similar to Levene's test and is used to assess the equality of variances across multiple groups. The test is based on the $\chi$-squared distribution and assumes that the data are normally distributed. Base R has the `bartlett.test()` function.

-   **Brown-Forsythe test** This test is a modification of the Levene's test that uses the absolute deviations of the observations from their respective group medians instead of means. This makes the test more robust to outliers and non-normality. It is available in **onewaytests** as the function `bf.test()`.

-   **Fligner-Killeen test** This is another non-parametric test that uses the medians of the groups instead of the means. It is based on the $\chi$-squared distribution and is also robust to non-normality and outliers. The Fligner test is available in Base R as `fligner.test()`.

As always, supplement your analysis with these checks: i) perform any of the diagnostic plots we covered in the earlier Chapters, or ii) compare the variances and see if they differ by more than a factor of four.

See [this discussion](https://stats.stackexchange.com/questions/91872/alternatives-to-one-way-anova-for-heteroskedastic-data) if you would like to know about some more advanced options when faced with heteroscedastic data.

# Epic fail... now what? Non-parametric statistics! {#sec-nonparam}

Tests for these two assumptions fail often with real data. Once we have evaluated our data against the two critical assumptions and discovered that one or both of the assumptions are not met, we are presented with two options. Firstly, we can select an appropriate non-parametric test. If that is not possible, then we may need to consider transforming our data. However, it is preferable to avoid the latter option whenever feasible.

::: {.callout-note appearance="simple"}
## Parametric and non-parametric tests
**Parametric tests** assume that the data follow a specific distribution, such as the normal distribution, and that the population parameters are known or can be estimated from the sample data. Examples of parametric tests include *t*-tests, ANOVA, and regression analysis.

**Non-parametric tests** make fewer assumptions about the underlying distribution of the data and are used when the data do not meet the assumptions of parametric tests. Non-parametric tests do not rely on population parameters but instead use ranks or other non-numerical measures of the data. Examples of non-parametric tests include the **Wilcoxon rank-sum test** (non-parametric substitute for *t*-tests) or the **Kruskal-Wallis test** (substitute for ANOVA).

In general, parametric tests are more powerful than non-parametric tests when the assumptions are met, but non-parametric tests are more robust and can be used in a wider range of situations. The choice between parametric and non-parametric tests depends on the nature of the data and the research question being addressed.
:::

Non-parametric tests do not assume a specific probability distribution for the data. These tests are often used when the assumptions of normality and homoscedasticity underlying parametric tests are violated. The non-parametric substitutes for the parametric tests that we will often have to use are discussed in [Chapter 11](11-glance.qmd). Below, however, we will talk about data transformations.

# Data transformations

Data transformation is used to change the scale of the data in a way that makes it conform to the assumptions of normality and homoscedasticity so that we can proceed with parametric tests in the usual way. Transformations can be used to *change the shape of the distribution* of the data, *reduce the effects of outliers*, or *stabilise the variance* across levels of the independent variable. However, we must often first identify the way in which our data are distributed (refer to [Chapter 4](04-distributions.qmd)) so we may better decide how to transformation them in an attempt to coerce them into a format that will pass the assumptions of normality and homoscedasticity.

Common data transformations include **logarithmic**, **square root**, and **reciprocal** transformations, among others. These transformations can be applied to the dependent variable, independent variable, or both, depending on the nature of the data and the research question of interest.

When selecting a data transformation method, it is important to consider the goals of the analysis, as well as the properties of the data. Different transformations can have different effects on the distribution of the data, and may lead to different conclusions or interpretations of the results.

When transforming data, one does a mathematical operation on the observations and then use these transformed numbers in the statistical tests. After one as conducted the statistical analysis and calculated the mean ± SD (or ± 95% CI), these values are back transformed (i.e. by applying the reverse of the transformation function) to the original scale before being reported. Note that in back-transformed data the SD (or CI) are not necessarily symmetrical, so one cannot simply compute one (e.g. the upper) and then assumed the lower one would be the same distance away from the mean.

> “*Torture numbers and they will confess to anything*” --- Gregg Easterbrook

When transforming data, it is a good idea to know a bit about how data within your field of study are usually transformed---try and use the same approach in your own work. Don't try all the various transformations until you find one that works, else it might seem as if you are trying to massage the data into an acceptable outcome. The effects of transformations are often difficult to see on the shape of data distributions, especially when you have few samples, so trust that what you are doing is correct. Unfortunately, as I said before, transforming data requires a bit of experience and knowledge with the subject matter, so read widely before you commit to one.

Some of the texts below come from [this discussion](http://fmwww.bc.edu/repec/bocode/t/transint.html) and from [John H. McDonald](http://www.biostathandbook.com/transformation.html). Below (i.e. the text on log transformation, square-root transformation, and arcsine transformation) I have extracted, often verbatim, the excellent text produced by John H MacDonald from his 'Handbook of Biological Statistics'. Please attribute this text directly to him. I have made minor editorial changes to point towards some R code, but aside from that the text is more-or-less used as is. I strongly suggest reading the preceding text under his 'Data transformations' section, as well as consulting the textbook for in-depth reading about biostatistics. Highly recommended!

## Log transformation

Log transformation is often applied to positively skewed data. It consists of taking the log of each observation. You can use either base-10 logs (`log10(x)`) or base-$e$ logs, also known as natural logs (`log(x)`). It makes no difference for a statistical test whether you use base-10 logs or natural logs, because they differ by a constant factor; the base- 10 log of a number is just 2.303...× the natural log of the number. You should specify which log you're using when you write up the results, as it will affect things like the slope and intercept in a regression. I prefer base-10 logs, because it's possible to look at them and see the magnitude of the original number: $log(1) = 0$, $log(10) = 1$, $log(100) = 2$, etc.

The back transformation is to raise 10 or $e$ to the power of the number; if the mean of your base-10 log-transformed data is 1.43, the back transformed mean is $10^{1.43} = 26.9$ (in R, `10^1.43`). If the mean of your base-$e$ log-transformed data is 3.65, the back transformed mean is $e^{3.65} = 38.5$ (in R, `exp(3.65)`). If you have zeros or negative numbers, you can't take the log; you should add a constant to each number to make them positive and non-zero (i.e. `log10(x + 1))`. If you have count data, and some of the counts are zero, the convention is to add 0.5 to each number.

Many variables in biology have log-normal distributions, meaning that after log-transformation, the values are normally distributed. This is because if you take a bunch of independent factors and multiply them together, the resulting product is log-normal. For example, let's say you've planted a bunch of weed seeds, then 10 years later you see how tall the trees are. The height of an individual tree would be affected by the nitrogen in the soil, the amount of water, amount of sunlight, amount of insect damage, etc. Having more nitrogen might make a tree 10% larger than one with less nitrogen; the right amount of water might make it 30% larger than one with too much or too little water; more sunlight might make it 20% larger; less insect damage might make it 15% larger, etc. Thus the final size of a tree would be a function of nitrogen × water × sunlight × insects, and mathematically, this kind of function turns out to be log-normal.

## Arcsine transformation

Arcsine transformation is commonly used for proportions, which range from 0 to 1, or percentages that go from 0 to 100. Specifically, this transformation is quite useful when the data follow a binomial distribution and have extreme proportions close to 0 or 1.

A biological example of the type of data suitable for arcsine transformation is the proportion of offspring that survives or the proportion of plants that succumbs to a disease; such data often follow a binomial distribution.

This transformation involves of taking the arcsine of the square root of a number (in R, `arcsin(sqrt(x))`). (The result is given in radians, not degrees, and can range from −π/2 to π/2). The numbers to be arcsine transformed must be in the range 0 to 1. [...] the back-transformation is to square the sine of the number (in R, `sin(x)^2`).

## Square root transformation

The square root transformation (in R, `sqrt(x)`) is often used to stabilise the variance of data that have a non-linear relationship between the mean and variance (heteroscedasticity). It is effective for reducing right-skewness (positively skewed). Taking the square root of each observation has the effect of compressing the data towards zero and reducing the impact of extreme values. It is a monotonic transformation, which means that it preserves the order of the data and does not change the relative rankings of the observations.

The square root transformation does not work with negative values, but one could add a constant to each number to make them positive.

A square root transformation is most frequently applied where the data are counts or frequencies, such as the number of individuals in a population or the number of events in a certain time period. Count data are prone to the variance increasing with the mean due to the discrete nature of the data. In these cases, the data tend to follow a Poisson distribution, which is characterised by a variance that is equal to the mean. The same applies to some environmental data, such as rainfall or wind; these may also exhibit heteroscedasticity due to extreme weather phenomena.

## Square transformation

Another transformation available for dealing with heteroscedasticity is the square transformation. As the name suggests, it involves taking the square of each observation in a dataset (`x^2`). The effect sought is to reduce left skewness.

This transformation has the effect of magnifying the differences between values and so increasing the influence of extreme values. However, this can make outliers more prominent and can make it more challenging to interpret the results of statistical analysis. 

The square transformation is often used in situations where the data are related to areas or volumes, such as the size of cells or the volume of an organ, where the data may follow a nonlinear relationship between the mean and variance.

## Cube transformation

This transformation also applies to heteroscedastic data. It is sometimes used with moderately left skewed data. This transformation is more drastic than a square transformation, and the drawback are more severe.

The cube transformation is less commonly used than other data transformations such as square-root or log transformation. Use with caution.

## Reciprocal transformation

It involves taking the reciprocal or inverse of each observation in a dataset (`1/x`). It is another variance stabilising transformation and is used with severely positively skewed data.

## Anscombe transformation

Another variance stabilising transformation is the Anscombe transformation, `sqrt(max(x+1)-x)`. It is applied to negatively skewed data. This transformation can be used to shift the data and compress it towards zero, and remove the influence of extreme values. It is a monotonic transformation, which means that it preserves the order of the data and does not change the relative rankings of the observations.

The Anscombe transformation is useful when dealing with count or frequency data that have a non-linear relationship between the mean and variance; such data are characteristic of Poisson-distributed count data.

<!-- ## Other transformations -->

<!-- -- `log10(max(x + 1) - x)` for negatively skewed data -->

<!-- -- `1/(max(x + 1) - x)` or higher powers than cubes for negatively skewed data -->

# Transformation and regressions

Regression models do not necessarily require data transformations to deal with heteroscedasticity. Generalised Linear Models (GLM) can be used with a variety of variance and error structures in the residuals via so-called link functions. Please consult the `glm()` function for details.

The linearity requirement specifically applies to linear regressions. However, regressions do not have to be linear. Some degree of curvature can be accommodated by additive (polynomial) models, which are like linear regressions, but with additional terms (you already have the knowledge you need to fit such models). More complex departures from linearity can be modelled by non-linear models (e.g. exponential, logistic, Michaelis-Menten, Gompertz, von Bertalanffy, and their ilk) or Generalised Additive Models (GAM)---these more complex relationships will not be covered in this module. The `gam()` function in the **mgcv** package fits GAMs. After fitting these parametric or semi-parametric models to accommodate non-linear regressions, the residual error structure still does to meet the normality requirements, and these can be tested as before with simple linear regressions.

# Conclusion

Knowing how to successfully implement transformations can be as much alchemy as science and requires a great deal of experience to get right. Due to the multitude of options I cannot offer comprehensive examples to deal with all eventualities---so I will not provide any examples at all! I suggest reading widely on the internet or textbooks, and practising by yourselves on your own datasets.

<!-- ::: callout-important -->
<!-- ## Task I -->
<!-- 1. Find one of the days of measurement where the chicken weights do not pass the assumptions of normality, and another day (not day 21!) in which they do. -->

<!-- 2. transformation the data so that they may pass the assumptions. -->
<!-- ::: -->
