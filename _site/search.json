[
  {
    "objectID": "vignettes/chl_localisation.html",
    "href": "vignettes/chl_localisation.html",
    "title": "Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data",
    "section": "",
    "text": "This analysis uses the chlorophylla data downloaded in Retrieving Chlorophyll-a Data from ERDDAP Servers.\nThe purpose of this analysis is to extract chlorophyll-a (chl-a) data collocated with the position of whale sightings. The geographical locality of each whale sighting is used to define a centre point in the gridded chl-a dataset. This centre point is then expanded by a specified radius, and all the pixels located within the expanded area’s bounding box are then aggregated along the latitude and longitude dimensions. This is repeated for each whale sighting since the start of the chl-a record time period (i.e. since 2003-01-01)."
  },
  {
    "objectID": "vignettes/chl_localisation.html#load-libraries",
    "href": "vignettes/chl_localisation.html#load-libraries",
    "title": "Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data",
    "section": "\n1 Load libraries",
    "text": "1 Load libraries\nI use tidyverse (of course) for basic data processing, lubridate for date calculations (specifically the ceiling of a pre-defined time interval such as week), and stars and sf for some specific geographical computations.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stars)\nlibrary(sf)\n\n\n# devtools::session_info()"
  },
  {
    "objectID": "vignettes/chl_localisation.html#load-the-whale-sightings-data",
    "href": "vignettes/chl_localisation.html#load-the-whale-sightings-data",
    "title": "Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data",
    "section": "\n2 Load the whale sightings data",
    "text": "2 Load the whale sightings data\nThese data are provided as CSV file and are easy to load:\n\n# sightings\nsights_data &lt;- read_csv(\n  \"../data/occurences_sampled.Mn.txt\",\n  show_col_types = FALSE\n)\nmin(sights_data$date); max(sights_data$date)\n\n[1] \"1996-09-01 UTC\"\n\n\n[1] \"2022-05-07 15:41:51 UTC\""
  },
  {
    "objectID": "vignettes/chl_localisation.html#reading-chlorophyll-a-from-csv-rdata",
    "href": "vignettes/chl_localisation.html#reading-chlorophyll-a-from-csv-rdata",
    "title": "Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data",
    "section": "\n3 Reading chlorophyll-a from CSV (RData)",
    "text": "3 Reading chlorophyll-a from CSV (RData)\nNext I load the chlorophyll-a data. These data were downloaded as flat CSV file and stored in a more compact RData file. See 1_ERDDAP_download.html for information about the download process.\nIt is important that we are clear about the start times (especially) of the sightings dataset.\n\n# chlorophyll-*a*\nchlDir &lt;- \"/Users/ajsmit/Documents/R/R_in_Ocean_Science/_development/ERDDAP/\"\nload(paste0(chlDir, \"MODIS_chl_data.Rdata\"))\nmin(chl_data$time); max(chl_data$time)\n\n[1] \"2003-01-01\"\n\n\n[1] \"2022-07-27\"\n\n# for testing only (legacy code)\n\n# chl_df &lt;- chl_data |&gt;\n#   dplyr::filter(time &gt;= \"2003-01-01\") |&gt;\n#   dplyr::mutate(date_ceiling = ceiling_date(time, unit = \"week\",\n#                                         week_start = 3)) |&gt;\n#   dplyr::group_by(longitude, latitude, date_ceiling) |&gt;\n#   dplyr::select(-time) |&gt;\n#   dplyr::summarise(chlorophyll = median(chlorophyll, na.rm = TRUE), .groups = \"drop\")\n# \n# chl_date_df &lt;- data.frame(date = unique(chl_df$date_ceiling))\n\n# check! this must align with the first date of the whale sighting data after\n# we calculate the date ceiling\n# min(chl_df$date_ceiling); max(chl_df$date_ceiling)\n# class(chl_df$date_ceiling)"
  },
  {
    "objectID": "vignettes/chl_localisation.html#reading-chlorophyll-a-from-netcdf-alternative",
    "href": "vignettes/chl_localisation.html#reading-chlorophyll-a-from-netcdf-alternative",
    "title": "Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data",
    "section": "\n4 Reading chlorophyll-a from netCDF (alternative)",
    "text": "4 Reading chlorophyll-a from netCDF (alternative)\nNote that we can also download (in 1_ERDDAP_download.html) the data in netCDF format, and load the netCDF directly as a stars object. In the future I’ll probably go this route because I like the convenience of netCDF. An example workflow is provided for the netCDF approach, but the subsequent analysis proceeds with the data loaded from CSV.\n\nchl_nc &lt;-\n  stars::read_ncdf(paste0(chlDir, \"chl_data.nc\"),\n                   var = \"chlorophyll\",\n                   proxy = TRUE)\n\n# 'warp' to regular grid (for some reason it was not properly registered as a\n# regular grid, even though it is one)\nchl_st1 &lt;- st_warp(chl_nc, crs = st_crs(4326))\n\n# check the CRS\n# st_crs(chl_st1)\n\n# make a vector of proper dates\ndates &lt;-\n  as.Date(st_get_dimension_values(chl_st1, which = \"time\"),\n          format = \"%Y-%m-%d\")\n\n# transform this to a vector of 'date floors'\n# dates &lt;- ceiling_date(dates, unit = \"week\")\n\n# assign the proper dates to the coordinate dimension\nchl_st1 &lt;- st_set_dimensions(chl_st1, which = \"time\",\n                             values = dates)"
  },
  {
    "objectID": "vignettes/chl_localisation.html#processing",
    "href": "vignettes/chl_localisation.html#processing",
    "title": "Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data",
    "section": "\n5 Processing",
    "text": "5 Processing\nBoth the sightings and chlorophyll-a data have a date/time vector comprised of daily dates. But will they align after we calculate the weekly data?\nThe chlorophyll-a data are available since 2002-12-29 and the whale sighting data since much earlier. I will use 2003-01-01 as the date from which to calculate the date ceiling and thus align the datasets along the time dimension. Note that I also calculated the weekly chl-a medians.\nI convert the dataframe to a stars object and do all subsequent calculations (subsetting, cropping, etc.) there. This seems to be a bit faster than working in a dataframe.\n\nchl_st &lt;- chl_data |&gt;\n  dplyr::select(longitude, latitude, time, chlorophyll) |&gt;\n  st_as_stars(dims = c(\"longitude\", \"latitude\", \"time\"),\n              raster = \"chlorophyll\") |&gt;\n  sf::st_set_crs(4326) |&gt; \n  filter(time &gt;= \"2003-01-01\") |&gt;\n  aggregate(by = \"7 days\", FUN = median, na.rm = TRUE)\n\nchl_st &lt;- st_warp(chl_st, crs = st_crs(4326))\nprint(chl_st)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                 Min.   1st Qu.   Median      Mean   3rd Qu.     Max.  NA's\nchlorophyll  0.101503 0.2204348 0.256319 0.2894689 0.3211997 2.784282 46694\ndimension(s):\n     from   to     offset      delta refsys x/y\nx       1  191    -31.625  0.0421596 WGS 84 [x]\ny       1   71    39.7777 -0.0421596 WGS 84 [y]\ntime    1 1022 2003-01-01     7 days   Date    \n\nchl_date_st &lt;- data.frame(date = unique(st_get_dimension_values(chl_st, which = \"time\")))\nmin(st_get_dimension_values(chl_st, which = \"time\")); max(st_get_dimension_values(chl_st, which = \"time\"))\n\n[1] \"2003-01-01\"\n\n\n[1] \"2022-07-27\"\n\n\nI want to create a date ceiling for the sights date and I want to ensure that the week starts on a Wednesday (2003-01-01). I also want to ensure that the class of the date vector here is the same as that of the chl-a data (this last step is probably not necessary).\n\nsights &lt;- sights_data |&gt; \n  filter(date &gt;= \"2003-01-01\") |&gt; \n  mutate(\n    date_ceiling = as.Date(ceiling_date(date, unit = \"week\",\n                                        week_start = 3))\n  ) |&gt; \n  select(-date) |&gt; \n  arrange(date_ceiling)\n\n# check! it aligns with the first date ceiling in the chl-*a* time series\nmin(sights$date_ceiling); max(sights$date_ceiling)\n\n[1] \"2003-01-01\"\n\n\n[1] \"2022-05-11\"\n\nsort(unique(sights$date_ceiling)[1:20])\n\n [1] \"2003-01-01\" \"2003-05-21\" \"2003-05-28\" \"2003-07-09\" \"2004-01-07\"\n [6] \"2004-04-14\" \"2004-05-12\" \"2004-05-19\" \"2006-05-10\" \"2006-05-31\"\n[11] \"2007-03-28\" \"2008-01-02\" \"2008-04-30\" \"2008-05-14\" \"2008-05-21\"\n[16] \"2010-05-26\" \"2011-01-05\" \"2011-04-06\" \"2011-04-27\" \"2011-05-25\"\n\nclass(sights$date_ceiling)\n\n[1] \"Date\"\n\n\nFor each line of the sightings data, I find the point of interest (longitude and latitude), add a buffer around it, and create a circular polygon that specifies the spatial extent around the point. This circular polygon will be used to crop the area of interest around each whale sighting on a particular date, and all the chl-a values in the pixels within the circular polygon will be aggregated.\n\n# for testing...\n\n# arbitrarily selecting the date at the 200th line in the sightings\n# dataset\nbuffer &lt;- 0.2\nt_step &lt;- 100\n(date_val &lt;- as.Date(sights$date_ceiling[t_step]))\n\n[1] \"2015-04-15\"\n\n# the coordinates for that particular whale sighting\nlon_val &lt;- sights$lon[t_step]\nlat_val &lt;- sights$lat[t_step]\n\n# calculate the bounding box for the sighting\ncir_pt &lt;- sf::st_point(c(lon_val, lat_val))\ncir_sfg &lt;- sf::st_buffer(cir_pt, buffer) # approx. 22.2 km radius\ncir_sfc &lt;- sf::st_sfc(cir_sfg, crs = st_crs(4326))\ncir_bbox &lt;- sf::st_bbox(cir_sfc)\n\nPlot of the full data extent on the first day of the chl-a dataset, showing the area to be cropped and aggregated:\n\nplot(chl_st[, , , 1], reset = FALSE)\nplot(cir_sfc, col = NA, border = 'red', add = TRUE, lwd = 2)\n\n\n\n\nCropped data within a circular sf geometry region (circular polygon) around central point:\n\nplot(chl_st[cir_sfc][, , , 1], reset = FALSE)\nplot(cir_sfc, col = NA, border = 'red', add = TRUE, lwd = 2)\n\n\n\n\nData within a rectangular bbox:\n\nplot(chl_st[cir_bbox][, , , 1], reset = FALSE)\n\n\n\n\nI extract the chl-a data within the sf geometry at the exact time step as that the whale sighting, and calculate their median value. The output is one value, which can be appended to to original sightings dataset, one value per line of whale sighting.\nWe also need to be able to calculate the median chl-a value at certain lags before the date of whale sightings. Because the data are aggregated to weeky values, we must ensure that the value provided to the lag argument is a multiple of 7 days (i.e. 7, 14, 21, 28, etc.). So, to accommodate the lag calculated for the full region (see 2_sightings.html), the value closest to 54 day is 8 weeks x 7 days = 56 days. In my function I will only allow the user to enter the number of full weeks as lags.\nFor example, the median chlorophyll-a concentration 8 weeks prior to the date on which the greatest number of cetaceans observations were made, within the bounding box for one particular observation, is:\n\n# for testing only (legacy code)\n# calculates the median chl-a value within bbox rectangle\n\n# chl_conc &lt;- chl_df |&gt;\n#   filter(date_ceiling == date_val - as.difftime(0, unit = \"weeks\")) |&gt;\n#   group_by(date_ceiling) |&gt;\n#   filter(between(longitude, cir_bbox[['xmin']], cir_bbox[['xmax']]),\n#          between(latitude, cir_bbox[['ymin']], cir_bbox[['ymax']])) |&gt;\n#   summarise(med_chl = median(chlorophyll, na.rm = TRUE))\n# \n# chl_conc\n\n\n# calculates median chl-a conc within a circle\nchl_conc &lt;- chl_st |&gt; \n  filter(time == date_val - as.difftime(0, unit = \"weeks\")) |&gt;\n  aggregate(by = cir_sfc, FUN = median, na.rm = TRUE) |&gt; \n  as.data.frame()\n\nchl_conc[1,3]\n\n[1] 0.2370576\n\n\nNow I know how to do the data extraction and processing for one line in the sights dataset. The next trick is to do it line by line for the whole sights dataset, i.e. once for each whale sighting."
  },
  {
    "objectID": "vignettes/chl_localisation.html#make-a-function-to-apply",
    "href": "vignettes/chl_localisation.html#make-a-function-to-apply",
    "title": "Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data",
    "section": "\n6 Make a function to apply",
    "text": "6 Make a function to apply\n\n# for testing (legacy code)\n\n# buffer &lt;- 0.2\n# \n# # function for method applied to dataframe\n# chl_calc &lt;- function(df, lag = 0) {\n#   cir_bbox &lt;-\n#     sf::st_bbox(st_sfc(st_buffer(st_point(\n#       c(as.numeric(df[1]), as.numeric(df[2]))\n#     ), buffer),\n#     crs = st_crs(4326)))\n# \n#   chl_conc &lt;- chl_df |&gt;\n#     filter(date_ceiling == as.Date(df[[\"date_ceiling\"]]) -\n#              as.difftime(lag, unit = \"weeks\")) |&gt;\n#     group_by(date_ceiling) |&gt;\n#     filter(between(longitude, cir_bbox[['xmin']], cir_bbox[['xmax']]),\n#            between(latitude, cir_bbox[['ymin']], cir_bbox[['ymax']])) |&gt;\n#     summarise(med_chl = median(chlorophyll, na.rm = TRUE))\n# \n#   return(chl_conc)\n# }\n\n# chl_calc_safe &lt;- possibly(chl_calc, \"Error\")\n\n\nbuffer &lt;- 0.2\n\n# function for method applied to stars object\nchl_calc &lt;- function(df, lag = 0) {\n  cir_sfc &lt;-\n    st_sfc(st_buffer(st_point(\n      c(as.numeric(df[1]), as.numeric(df[2]))\n    ), buffer),\n    crs = st_crs(4326))\n  \n  chl_conc &lt;- chl_st |&gt; \n    filter(time == as.Date(df[[\"date_ceiling\"]]) -\n             as.difftime(lag, unit = \"weeks\")) |&gt;\n    aggregate(by = cir_sfc, FUN = median, na.rm = TRUE) |&gt; \n    as.data.frame()\n  \n  return(chl_conc[1,2:3])\n}\n\nchl_calc_safe &lt;- possibly(chl_calc, \"Error\")\n\nTest the function on one line of sights:\n\nchl_calc_safe(sights[1,], lag = 0) # works\n\n\n\n\ntime\nchlorophyll\n\n\n2003-01-01\n0.216853\n\n\n\n\n\nMake each row of sights a unique list element and map the chl_calc function to each element in the list, list_rbind it into a dataframe:\n\nchl_lag_0 &lt;- sights |&gt; \n  split(seq(nrow(sights))) |&gt;\n  map(\\(df) chl_calc_safe(df)) |&gt; \n  list_rbind(names_to = \"row.num\") |&gt; \n  mutate(row.num = as.integer(row.num))\n\nCombine the output with the original sights dataset and also add a column with months:\n\nsights_chl_lag_0 &lt;- sights |&gt; \n  mutate(row.num = row_number()) |&gt; \n  left_join(chl_lag_0, by = \"row.num\") |&gt; \n  select(-row.num) |&gt; \n  mutate(month = month(date_ceiling, label = TRUE, abbr = TRUE)) |&gt; \n  rename(sight_date = date_ceiling,\n         chl_date = time)\n\nhead(sights_chl_lag_0)\n\n\n\n\nlon\nlat\nsight_date\nchl_date\nchlorophyll\nmonth\n\n\n\n-29.39461\n39.38061\n2003-01-01\n2003-01-01\n0.2168530\nJan\n\n\n-27.97184\n38.91912\n2003-01-01\n2003-01-01\n0.1926668\nJan\n\n\n-28.37714\n38.40140\n2003-01-01\n2003-01-01\n0.1795655\nJan\n\n\n-28.63333\n38.38333\n2003-05-21\n2003-05-21\n0.2062395\nMay\n\n\n-28.66667\n38.45000\n2003-05-21\n2003-05-21\n0.1970190\nMay\n\n\n-28.36667\n38.40000\n2003-05-21\n2003-05-21\n0.2283610\nMay\n\n\n\n\n\n\nAlso do this with a lag of 8 weeks:\n\nchl_lag_8 &lt;- sights |&gt; \n  split(seq(nrow(sights))) |&gt;\n  map(\\(df) chl_calc(df, lag = 8)) |&gt; \n  list_rbind(names_to = \"row.num\") |&gt; \n  mutate(row.num = as.integer(row.num))\n\n\nsights_chl_lag_8 &lt;- sights |&gt; \n  mutate(row.num = row_number()) |&gt; \n  left_join(chl_lag_8, by = \"row.num\") |&gt; \n  select(-row.num) |&gt; \n  mutate(month = month(date_ceiling, label = TRUE, abbr = TRUE)) |&gt; \n  mutate(diff.time = date_ceiling - time) |&gt; # check\n  rename(sight_date = date_ceiling,\n         chl_date = time)\n\nhead(sights_chl_lag_8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlon\nlat\nsight_date\nchl_date\nchlorophyll\nmonth\ndiff.time\n\n\n\n-29.39461\n39.38061\n2003-01-01\nNA\nNA\nJan\nNA days\n\n\n-27.97184\n38.91912\n2003-01-01\nNA\nNA\nJan\nNA days\n\n\n-28.37714\n38.40140\n2003-01-01\nNA\nNA\nJan\nNA days\n\n\n-28.63333\n38.38333\n2003-05-21\n2003-03-26\n0.373149\nMay\n56 days\n\n\n-28.66667\n38.45000\n2003-05-21\n2003-03-26\n0.359063\nMay\n56 days\n\n\n-28.36667\n38.40000\n2003-05-21\n2003-03-26\nNA\nMay\n56 days\n\n\n\n\n\n\nAbove, sight_date is the date in the sightings dataset sights and chl_date is the earlier date (it may be lagged) at which the chl-a data were extracted (i.e. after incorporating the lag).\nI am not too sure what to do with this output as there is actually no measured data associated with each observational record. The only thing of use really is that each row is one observation with an associated date and location. I assume that each row belongs with only one animal.\nIn order to create some observational data that are actually a bit more useful, I think it might be a good idea to create a column with the number of observations per day. The only way I can do this is to count the number of observations within a slightly larger spatial domain, and to do so, I regrid the observational data to a slightly courser resolution. So, at a resolution of, say, 0.2 × 0.2° latitude and longitude grid cells, I can count the number of observations within—now ‘observations’ are comprised of counts of point localities of individual observations for each day within these slightly expanded grid cells. This analysis is provided in the next file, 4_regrid_sights.html.\nConsequently, I don’t actually do anything with the end result of the calculations provided within this script."
  },
  {
    "objectID": "vignettes/prep_NOAA_OISST.html",
    "href": "vignettes/prep_NOAA_OISST.html",
    "title": "Downloading and Preparing NOAA OISST Data: ERDDAP",
    "section": "",
    "text": "This material also appears as a heatwaveR vignette."
  },
  {
    "objectID": "vignettes/prep_NOAA_OISST.html#overview",
    "href": "vignettes/prep_NOAA_OISST.html#overview",
    "title": "Downloading and Preparing NOAA OISST Data: ERDDAP",
    "section": "\n1 Overview",
    "text": "1 Overview\nIn this vignette we will see how to retrieve and prepare Reynolds optimally interpolated sea surface temperature (OISST) data for calculating marine heatwaves (MHWs). The OISST product is a global 1/4 degree gridded dataset of Advanced Very High Resolution Radiometer (AVHRR) derived SSTs at a daily resolution, starting on 1 September 1981. The source of the data is currently the NOAA NCDC.\nEach daily global file, when not compressed, is around 8.3 MB, so they add up to a large amount of data when a time series of the recommended 30 year minimum duration for the detection of MHWs is downloaded. If one were to download all of the data currently available it would exceed 100 GB of total disk space. It is therefore best practice to download only a subset of the data that matches one’s study area. Thanks to the rerddap package this is incredibly easy to do in R.\nShould one want to download the full global dataset, each daily global file is available in netCDF format and is roughly 1.6 MB. This means that one full year of global data will be roughly 600 MB, and the full dataset roughly 25 GB. This is however when the data are very compressed. If we were to attempt to load the entire uncompressed dataset into our memory at once it would take more than 200 GB of RAM. That is well beyond the scope of any current laptop so in the second half of this vignette we will see how to download the full OISST dataset before then seeing how we can load only a subset of the data into the R environment for use with further analyses.\nThis vignette may appear very long and complex but it has been written in an attempt to keep the process of downloading and working with satellite data as straight-forward and easy to follow as possible. Before we begin with all of the code etc. please note that for almost all applications it is only necessary to use the first method outlined below. For most users the second download method in this vignette can simply be skipped."
  },
  {
    "objectID": "vignettes/prep_NOAA_OISST.html#setup",
    "href": "vignettes/prep_NOAA_OISST.html#setup",
    "title": "Downloading and Preparing NOAA OISST Data: ERDDAP",
    "section": "\n2 Setup",
    "text": "2 Setup\nFor this vignette we will be accessing the NOAA OISST dataset on this ERDDAP server for the subsetted data, while the global data are indexed here. One may download the data on both servers manually by using the ERDDAP UI or clicking on each indexed file individually. But programming languages like R are designed to prevent us from needing to experience that sort of anguish. Below we will load the libraries we need in order to have R download all of the data that we may need. If any of the lines of code in the following chunk do not run it means that we will need to first install that package. Uncomment the line of code that would install the problem package and run it before trying to load the library again.\n\n# The packages we will need\n# install.packages(\"dplyr\")\n# install.packages(\"lubridate\")\n# install.packages(\"ggplot2\")\n# install.packages(\"tidync\")\n# install.packages(\"doParallel\")\n# install.packages(\"rerddap\")\n# install.packages(\"plyr\") # Note that this library should never be loaded, only installed\n\n# The packages we will use\nlibrary(dplyr) # A staple for modern data management in R\nlibrary(lubridate) # Useful functions for dealing with dates\nlibrary(ggplot2) # The preferred library for data visualisation\nlibrary(tidync) # For easily dealing with NetCDF data\nlibrary(rerddap) # For easily downloading subsets of data\nlibrary(doParallel) # For parallel processing\n\nWith our packages loaded we may now begin downloading and preparing our data for further use. Please use the table of contents on the right side of the screen to jump between the different download methods as desired. We will break each different method down into smaller steps in order to keep this process as clear as possible. Before we begin I need to stress that this is a very direct and unrestricted method for accessing these data and I urge responsibility in only downloading as much data as are necessary. Please do not download the entire dataset unless you have a specific need for it."
  },
  {
    "objectID": "vignettes/prep_NOAA_OISST.html#downloading-subsetted-data",
    "href": "vignettes/prep_NOAA_OISST.html#downloading-subsetted-data",
    "title": "Downloading and Preparing NOAA OISST Data: ERDDAP",
    "section": "\n3 Downloading subsetted data",
    "text": "3 Downloading subsetted data\n\n3.1 File information\nBefore we begin downloading the subsetted data for our study area we need to make sure that they are currently available on an ERDDAP server. The location of the NOAA OISST data has changed in the past so it should not be assumed that the current location will exist in perpetuity. Finding the server on which these data are located can be a cup game at times.\n\n# The information for the NOAA OISST data\nrerddap::info(datasetid = \"ncdcOisst21Agg_LonPM180\", url = \"https://coastwatch.pfeg.noaa.gov/erddap/\")\n\n# Note that there is also a version with lon values from 0 yo 360\nrerddap::info(datasetid = \"ncdcOisst21Agg\", url = \"https://coastwatch.pfeg.noaa.gov/erddap/\")\n\nWith our target dataset identified we may now begin the download with the griddap() function. While putting this vignette together however I noticed one little hiccup in the work flow. It seems that the ERDDAP server does not like it when one tries to access more than nine consecutive years of data in one request, regardless of the spatial extent being requested. So before we download our data we are going to make a wrapper function that helps us control the range of times we want to download. This will reduce the amount of redundant coding we would otherwise need to do.\n\n3.2 Download function\n\n# This function downloads and prepares data based on user provided start and end dates\nOISST_sub_dl &lt;- function(time_df){\n  OISST_dat &lt;- griddap(x = \"ncdcOisst21Agg_LonPM180\", \n                       url = \"https://coastwatch.pfeg.noaa.gov/erddap/\", \n                       time = c(time_df$start, time_df$end), \n                       zlev = c(0, 0),\n                       latitude = c(-40, -35),\n                       longitude = c(15, 21),\n                       fields = \"sst\")$data %&gt;% \n    mutate(time = as.Date(stringr::str_remove(time, \"T00:00:00Z\"))) %&gt;% \n    dplyr::rename(t = time, temp = sst) %&gt;% \n    select(lon, lat, t, temp) %&gt;% \n    na.omit()\n}\n\nIn the wrapper function above we see that we have chosen to download only the ‘sst’ data out of the several variables (‘fields’) available to us. We also see that we have chosen the spatial extent of latitude -40 to -35 and longitude 15 to 21. This a small window over some of the Agulhas Retroflection to the south west of South Africa. A larger area is not being chosen here simply due to the speed constraints of downloading the data and detecting the events therein. One may simply change the longitude and latitude values above as necessary to match the desired study area. The function will also be re-labelling the ‘time’ column as ‘t’, and the ‘sst’ column as ‘temp’. We do this so that they match the default column names that are expected for calculating MHWs and we won’t have to do any extra work later on.\nOne must note here that depending on the RAM available on one’s machine, it may not be possible to handle all of the data downloaded at once if they are very large (e.g. &gt; 5 GB). The discussion on the limitations of the R language due to its dependence on virtual memory is beyond the scope of this vignette, but if one limits one’s downloads to no more than several square pixels at a time that should be fine. Were one to try to download the whole Indian Ocean, for example, that may cause issues if being run on a laptop or computer of a similar power.\n\n3.3 Date range\nWith our wrapper function written we would now need to run it several times in order to grab all of the OISST data from 1982-01-01 to 2019-12-31. Even though each year of data for the extent used in this vignette is only ~360 KB, the server does not like it when more than 9 years of consecutive data are requested. The server will also end a users connection after ~17 individual files have been requested. Because we can’t download all of the data in one request, and we can’t download the data one year at a time, we will need to make requests for multiple batches of data. To accomplish this we will create a dataframe of start and end dates that will allow us to automate the entire download while meeting the aforementioned criteria.\n\n# Date download range by start and end dates per year\ndl_years &lt;- data.frame(date_index = 1:5,\n                       start = as.Date(c(\"1982-01-01\", \"1990-01-01\", \n                                         \"1998-01-01\", \"2006-01-01\", \"2014-01-01\")),\n                       end = as.Date(c(\"1989-12-31\", \"1997-12-31\", \n                                       \"2005-12-31\", \"2013-12-31\", \"2019-12-31\")))\n\n\n3.4 Download/prep data\nOne could also use the plyr suite of functions to automate the process of downloading and processing multiple files, but I’ve chosen here to stick with the tidyverse native approach. If the below chunk of code fails or times out, simply re-run it until all of the data have been downloaded.\nIt is worth pointing out here that these data are downloaded as cached files on the users computer by using the hoardr package. This means that if one runs the same command again, it will not re-download the data because it first looks in the folder where it has automatically cached the data for you and sees that it may simply draw the data from there. No need to change anything or write a second script for loading data.\n\n# Download all of the data with one nested request\n# The time this takes will vary greatly based on connection speed\nsystem.time(\n  OISST_data &lt;- dl_years %&gt;% \n    group_by(date_index) %&gt;% \n    group_modify(~OISST_sub_dl(.x)) %&gt;% \n    ungroup() %&gt;% \n    select(lon, lat, t, temp)\n) # 38 seconds, ~8 seconds per batch\n\nIf the above code chunk is giving errors it is likely due to one’s Internet connection timing out. There are also rare instances where the NOAA server is not responding due to an issue on their end. Any connection based issues may be resolved by simply waiting for a few minutes, or by ensuring a stable connection.\n\n3.5 Visualise data\nBefore we save our data for later use it is good practice to visualise them.\n\nOISST_data %&gt;% \n  filter(t == \"2019-12-01\") %&gt;% \n  ggplot(aes(x = lon, y = lat)) +\n  geom_tile(aes(fill = temp)) +\n  # borders() + # Activate this line to see the global map\n  scale_fill_viridis_c() +\n  coord_quickmap(expand = F) +\n  labs(x = NULL, y = NULL, fill = \"SST (°C)\") +\n  theme(legend.position = \"bottom\")\n\n\n3.6 Save data\nWith the data downloaded and prepared for further use (and a test visual run), all that’s left to do is save them.\n\n# Save the data as an .Rds file because it has a much better compression rate than .RData\nsaveRDS(OISST_data, file = \"~/Desktop/OISST_vignette.Rds\")\n\nNote above that I have chosen to save the file to my desktop. This is not normally where one (hopefully!) would save such a file. Rather one would be saving these data into the project folder out of which one is working. In the next vignette we will see how to detect MHWs in gridded data using the data downloaded here."
  },
  {
    "objectID": "vignettes/prep_NOAA_OISST.html#downloading-global-data",
    "href": "vignettes/prep_NOAA_OISST.html#downloading-global-data",
    "title": "Downloading and Preparing NOAA OISST Data: ERDDAP",
    "section": "\n4 Downloading global data",
    "text": "4 Downloading global data\nThe method for downloading and preparing NOAA OISST data outlined in the first half of this vignette should be considered best practice for all applications except those that specifically need to look at the entire globe. If one needs to download the global dataset then it is preferable to go straight to the source. Note that one may still download the full global dataset using the methods above by setting the lon/lat extent to be the full width and height of the globe. The method outlined below will download over 13,000 individual files. This makes dealing with individual files very easy, but agglomerating them into one file can be very time consuming.\n\n4.1 File information\nThe first step in downloading the full global dataset is to tell you computer where they are. There is an automated way to do this but it requires a couple of additional packages and we aim to keep this vignette as simple and direct as possible. For our purposes today we will manually create the URLs of the files we want to download.\n\n# First we tell R where the data are on the interwebs\nOISST_base_url &lt;- \"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/\"\n# Note that one may go to this URL in any web browser to manually inspect the files\n\n# Now we create a data.frame that contains all of the dates we want to download\n  # NB: In order to change the dates download changes the dates in the following line\nOISST_dates &lt;- data.frame(t = seq(as.Date(\"2019-12-01\"), as.Date(\"2019-12-31\"), by = \"day\"))\n\n# To finish up this step we add some text to those dates so they match the OISST file names\nOISST_files &lt;- OISST_dates %&gt;% \n  mutate(t_day = gsub(\"-\", \"\", t),\n         t_month = substr(t_day, 1, 6),\n         t_year = year(t),\n         file_name = paste0(OISST_base_url, t_month, \"/\", \"oisst-avhrr-v02r01.\", t_day ,\".nc\"))\n\n\n4.2 Download data\nNow that we have a dataframe that contains all of the URLs for the files we want to download we’ll create a function that will crawl through those URLs and download the files for us.\n\n# This function will go about downloading each day of data as a NetCDF file\n# Note that this will download files into a 'data/OISST' folder in the root directory\n  # If this folder does not exist it will create it\n  # If it does not automatically create the folder it will need to be done manually\n  # The folder that is created must be a new folder with no other files in it\n  # A possible bug with netCDF files in R is they won't load correctly from \n  # existing folders with other file types in them\n# This function will also check if the file has been previously downloaded\n  # If it has it will not download it again\nOISST_url_daily_dl &lt;- function(target_URL){\n  dir.create(\"~/data/OISST\", showWarnings = F)\n  file_name &lt;- paste0(\"~/data/OISST/\",sapply(strsplit(target_URL, split = \"/\"), \"[[\", 10))\n  if(!file.exists(file_name)) download.file(url = target_URL, method = \"libcurl\", destfile = file_name)\n}\n\n# The more cores used, the faster the data may be downloaded\n  # It is best practice to not use all of the cores on one's machine\n  # The laptop on which I am running this code has 8 cores, so I use 7 here\ndoParallel::registerDoParallel(cores = 7)\n\n# And with that we are clear for take off\nsystem.time(plyr::l_ply(OISST_files$file_name, .fun = OISST_url_daily_dl, .parallel = T)) # ~15 seconds\n\n# In roughly 15 seconds a user may have a full month of global data downloaded\n# This scales well into years and decades, and is much faster with more cores\n# Download speeds will also depend on the speed of the users internet connection\n\n\n4.3 Load data\nThe following code chunk contains the function we may use to load and prepare our OISST data for further use in R.\n\n# This function will load and subset daily data into one data.frame\n# Note that the subsetting by lon/lat is done before the data are loaded\n  # This means it will use much less RAM and is viable for use on most laptops\n  # Assuming one's study area is not too large\nOISST_load &lt;- function(file_name, lon1, lon2, lat1, lat2){\n      OISST_dat &lt;- tidync(file_name) %&gt;%\n        hyper_filter(lon = between(lon, lon1, lon2),\n                     lat = between(lat, lat1, lat2)) %&gt;% \n        hyper_tibble() %&gt;% \n        select(lon, lat, time, sst) %&gt;% \n        dplyr::rename(t = time, temp = sst) %&gt;% \n        mutate(t = as.Date(t, origin = \"1978-01-01\"))\n      return(OISST_dat)\n}\n\n# Locate the files that will be loaded\nOISST_files &lt;- dir(\"~/data/OISST\", full.names = T)\n\n# Load the data in parallel\nOISST_dat &lt;- plyr::ldply(.data = OISST_files, .fun = OISST_load, .parallel = T,\n                         lon1 = 270, lon2 = 320, lat1 = 30, lat2 = 50)\n\n# It should only take a few seconds to load one month of data depending on the size of the lon/lat extent chosen\n\nIn the code chunk above I have chosen the spatial extent of longitude 270 to 320 and latitude 30 to 50. This a window over the Atlantic Coast of North America. One may simply change the lon/lat values above as necessary to match the desired study area. The function also re-labels the ‘time’ column as ‘t’, and the ‘sst’ column as ‘temp’. We do this now so that they match the default column names that are expected for calculating MHWs so we won’t have to do any extra work later on.\nAgain, please note that trying to load too much data at once may be too much for the RAM on one’s machine. If running the above code causes one’s machine to hang, try loading a smaller subset of data. Or make friends with someone with a server sized machine.\n\n4.4 Visualise data\nIt is always good to visualise data early and often in any workflow. The code pipeline below shows how we can visualise a day of data from those we’ve loaded.\n\nOISST_dat %&gt;% \n  filter(t == \"2019-12-01\") %&gt;% \n  ggplot(aes(x = lon, y = lat)) +\n  geom_tile(aes(fill = temp)) +\n  scale_fill_viridis_c() +\n  coord_quickmap(expand = F) +\n  labs(x = NULL, y = NULL, fill = \"SST (°C)\") +\n  theme(legend.position = \"bottom\")\n\nIn the next vignette we will see how to detect MHWs in gridded data."
  },
  {
    "objectID": "vignettes/gridded_data.html",
    "href": "vignettes/gridded_data.html",
    "title": "Detecting Events in Gridded Data",
    "section": "",
    "text": "This vignette uses the data we acquired earlier in Downloading and Preparing NOAA OISST Data: ERDDAP. We will use these subsetted data for our example on how to detect MHWs in gridded data.\n\nlibrary(dplyr) # For basic data manipulation\nlibrary(ggplot2) # For visualising data\nlibrary(heatwaveR) # For detecting MHWs\nlibrary(tidync) # For easily dealing with NetCDF data\nlibrary(doParallel) # For parallel processing"
  },
  {
    "objectID": "vignettes/gridded_data.html#overview",
    "href": "vignettes/gridded_data.html#overview",
    "title": "Detecting Events in Gridded Data",
    "section": "",
    "text": "This vignette uses the data we acquired earlier in Downloading and Preparing NOAA OISST Data: ERDDAP. We will use these subsetted data for our example on how to detect MHWs in gridded data.\n\nlibrary(dplyr) # For basic data manipulation\nlibrary(ggplot2) # For visualising data\nlibrary(heatwaveR) # For detecting MHWs\nlibrary(tidync) # For easily dealing with NetCDF data\nlibrary(doParallel) # For parallel processing"
  },
  {
    "objectID": "vignettes/gridded_data.html#loading-data",
    "href": "vignettes/gridded_data.html#loading-data",
    "title": "Detecting Events in Gridded Data",
    "section": "\n2 Loading data",
    "text": "2 Loading data\nBecause we saved our data as an .Rds file, loading it into R is easy.\n\nOISST &lt;- readRDS(\"~/Desktop/OISST_vignette.Rds\")"
  },
  {
    "objectID": "vignettes/gridded_data.html#event-detection",
    "href": "vignettes/gridded_data.html#event-detection",
    "title": "Detecting Events in Gridded Data",
    "section": "\n3 Event detection",
    "text": "3 Event detection\n\n3.1 Two good choices: dplyr vs. plyr\n\nWhen we want to make the same calculation across multiple groups of data within one dataframe we have two good options available to us. The first is to make use of the map() suite of functions found in the purrr package, and now implemented in dplyr. This is a very fast tidyverse friendly approach to splitting up tasks. The other good option is to go back in time a bit and use the ddply() function from the plyr package. This is arguably a better approach as it allows us to very easily use multiple cores to detect the MHWs. The problem with this approach is that one must never load the plyr library directly as it has some fundamental inconsistencies with the tidyverse. We will see below how to perform these two different techniques without causing ourselves any headaches.\nIt is a little clumsy to use multiple functions at once with the two methods so we will combine the calculations we want to make into one wrapper function.\n\nevent_only &lt;- function(df){\n  # First calculate the climatologies\n  clim &lt;- ts2clm(data = df, climatologyPeriod = c(\"1982-01-01\", \"2011-01-01\"))\n  # Then the events\n  event &lt;- detect_event(data = clim)\n  # Return only the event metric dataframe of results\n  return(event$event)\n}\n\n\n3.1.1 The dplyr method\nThis method requires no special consideration and is performed just as any other friendly tidyverse code chunk would be.\n\nsystem.time(\n# First we start by choosing the 'OISST' dataframe\nMHW_dplyr &lt;- OISST %&gt;% \n  # Then we group the data by the 'lon' and 'lat' columns\n  group_by(lon, lat) %&gt;% \n  # Then we run our MHW detecting function on each group\n  group_modify(~event_only(.x))\n) # ~123 seconds\n\nRunning the above calculations with only one of the 2.8 GHz cores on a modern laptop took ~123 seconds. It must be noted however that a recent update to the dplyr package now allows it to interrogate one’s computer to determine how many cores it has at it’s disposal. It then uses one core at full capacity and the other cores usually at half capacity.\n\n3.1.2 The plyr technique\nThis method requires that we first tell our machine how many of its processor cores to give us for our calculation.\n\n# NB: One should never use ALL available cores, save at least 1 for other essential tasks\n# The computer I'm writing this vignette on has 8 cores, so I use 7 here\nregisterDoParallel(cores = 7)\n\n# Detect events\nsystem.time(\nMHW_plyr &lt;- plyr::ddply(.data = OISST, .variables = c(\"lon\", \"lat\"), .fun = event_only, .parallel = TRUE)\n) # 33 seconds\n\nThe plyr technique took 33 seconds using seven cores. This technique is not seven times faster because when using multiple cores there is a certain amount of loss in efficiency due to the computer needing to remember which results are meant to go where so that it can stitch everything back together again for you. This takes very little memory, but over large jobs it can start to become problematic. Occasionally ‘slippage’ can occur as well where an entire task can be forgotten. This is very rare but does happen. This is partly what makes dplyr a viable option as it does not have this problem. The other reason is that dplyr performs more efficient calculations than plyr. But what if we could have the best of both worlds?\n\n3.2 A harmonious third option\nAs one may see above, running these calculations on a very large (or even global) gridded dataset can quickly become very heavy. While running these calculations myself on the global OISST dataset I have found that the fastest option is to combine the two options above. In my workflow I have saved each longitude segment of the global OISST dataset as separate files and use the dplyr method on each individual file, while using the plyr method to be running the multiple calculations on as many files as my core limit will allow. One may not do this the other way around and use dplyr to run multiple plyr calculations at once. This will confuse your computer and likely cause a stack overflow. Which sounds more fun than it actually is… as I have had to learn.\nIn order to happily combine these two options into one we will need to convert the dplyr code we wrote above into it’s own wrapper function, which we will then call on a stack of files using the plyr technique. Before we do that we must first create the aforementioned stack of files.\n\nfor(i in 1:length(unique(OISST$lon))){\n  OISST_sub &lt;- OISST %&gt;% \n    filter(lon == unique(lon)[i])\n  saveRDS(object = OISST_sub, file = paste0(\"~/Desktop/OISST_lon_\",i,\".Rds\"))\n}\n\nThis may initially seem like an unnecessary extra step, but when one is working with time series data it is necessary to have all of the dates at a given pixel loaded at once. Unless one is working from a server/virtual machine/supercomputer this means that one will often not be able to comfortably hold an entire grid for a study area in memory at once. Having the data accessible as thin strips like this makes life easier. And as we see in the code chunk below it also (arguably) allows us to perform the most efficient calculations on our data.\n\n# The 'dplyr' wrapper function to pass to 'plyr'\ndplyr_wraper &lt;- function(file_name){\n  MHW_dplyr &lt;- readRDS(file_name) %&gt;% \n    group_by(lon, lat) %&gt;% \n    group_modify(~event_only(.x))\n}\n# Create a vector of the files we want to use\nOISST_files &lt;- dir(\"~/Desktop\", pattern = \"OISST_lon_*\", full.names = T)\n\n# Use 'plyr' technique to run 'dplyr' technique with multiple cores\nsystem.time(\nMHW_result &lt;- plyr::ldply(OISST_files, .fun = dplyr_wraper, .parallel = T)\n) # 31 seconds\n\n# Save for later use as desired\nsaveRDS(MHW_result, \"~/Desktop/MHW_result.Rds\")\n\nEven though this technique is not much faster computationally, it is much lighter on our memory (RAM) as it only loads one longitude slice of our data at a time. To maximise efficiency even further I would recommend writing out this full workflow in a stand-alone script and then running it using source() directly from an R terminal. The gain in speed here appears nominal, but as one scales this up the speed boost becomes apparent.\nAs mentioned above, recent changes to how dplyr interacts with one’s computer has perhaps slowed down the plyr + dplyr workflow shown here. It may be now that simply using plyr by itself is the better option. It depends on the number of cores and the amount of RAM that one has available."
  },
  {
    "objectID": "vignettes/gridded_data.html#case-study",
    "href": "vignettes/gridded_data.html#case-study",
    "title": "Detecting Events in Gridded Data",
    "section": "\n4 Case study",
    "text": "4 Case study\nBecause of human-induced climate change, we anticipate that extreme events will occur more frequently and that they will become greater in intensity. Here we investigate this hypothesis by using gridded SST data, which is the only way that we can assess if this trend is unfolding across large ocean regions. Using the gridded 0.25 degree Reynolds OISST, we will detect marine heatwaves (MHWs) around South Africa by applying the detect_event() function pixel-by-pixel to the data we downloaded in the previous vignette. After detecting the events, we will fit a generalised linear model (GLM) to each pixel to calculate rates of change in some MHW metrics, and then plot the estimated trends.\n\n4.1 Trend detection\nWith our MHW detected we will now look at how to fit some GLMs to the results in order to determine long-term trends in MHW occurrence.\nUp first we see how to calculate the number of events that occurred per pixel.\n\n# summarise the number of unique longitude, latitude and year combination:\nOISST_n &lt;- MHW_result %&gt;% \n  mutate(year = lubridate::year(date_start)) %&gt;% \n  group_by(lon, lat, year) %&gt;% \n  summarise(n = n(), .groups = \"drop\") %&gt;% \n  group_by(lon, lat) %&gt;%\n  tidyr::complete(year = c(1982:2019)) %&gt;% # Note that these dates may differ\n  mutate(n = ifelse(is.na(n), 0, n))\nhead(OISST_n)\n\nThen we specify the particulars of the GLM we are going to use.\n\nlin_fun &lt;- function(ev) {\n  mod1 &lt;- glm(n ~ year, family = poisson(link = \"log\"), data = ev)\n  # extract slope coefficient and its p-value\n  tr &lt;- data.frame(slope = summary(mod1)$coefficients[2,1],\n                   p = summary(mod1)$coefficients[2,4])\n  return(tr)\n}\n\nLastly we make the calculations.\n\nOISST_nTrend &lt;- plyr::ddply(OISST_n, c(\"lon\", \"lat\"), lin_fun, .parallel = T)\nOISST_nTrend$pval &lt;- cut(OISST_nTrend$p, breaks = c(0, 0.001, 0.01, 0.05, 1))\nhead(OISST_nTrend)\n\n\n4.2 Visualising the results\nLet’s finish this vignette by visualising the long-term trends in the annual occurrence of MHWs per pixel in the chosen study area. First we will grab the base global map from the maps package.\n\n# The base map\nmap_base &lt;- ggplot2::fortify(maps::map(fill = TRUE, plot = FALSE)) %&gt;% \n  dplyr::rename(lon = long)\n\nThen we will create two maps that we will stick together using ggpubr. The first map will show the slope of the count of events detected per year over time as shades of red, and the second map will show the significance (p-value) of these trends in shades of grey.\n\nmap_slope &lt;- ggplot(OISST_nTrend, aes(x = lon, y = lat)) +\n  geom_rect(size = 0.2, fill = NA,\n       aes(xmin = lon - 0.1, xmax = lon + 0.1, ymin = lat - 0.1, ymax = lat + 0.1,\n           colour = pval)) +\n  geom_raster(aes(fill = slope), interpolate = FALSE, alpha = 0.9) +\n  scale_fill_gradient2(name = \"count/year (slope)\", high = \"red\", mid = \"white\",\n                       low = \"darkblue\", midpoint = 0,\n                       guide = guide_colourbar(direction = \"horizontal\",\n                                               title.position = \"top\")) +\n  scale_colour_manual(breaks = c(\"(0,0.001]\", \"(0.001,0.01]\", \"(0.01,0.05]\", \"(0.05,1]\"),\n                      values = c(\"firebrick1\", \"firebrick2\", \"firebrick3\", \"white\"),\n                      name = \"p-value\", guide = FALSE) +\n  geom_polygon(data = map_base, aes(group = group), \n               colour = NA, fill = \"grey80\") +\n  coord_fixed(ratio = 1, xlim = c(13.0, 23.0), ylim = c(-33, -42), expand = TRUE) +\n  labs(x = \"\", y = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\nmap_p &lt;- ggplot(OISST_nTrend, aes(x = lon, y = lat)) +\n  geom_raster(aes(fill = pval), interpolate = FALSE) +\n  scale_fill_manual(breaks = c(\"(0,0.001]\", \"(0.001,0.01]\", \"(0.01,0.05]\",\n                               \"(0.05,0.1]\", \"(0.1,0.5]\", \"(0.5,1]\"),\n                    values = c(\"black\", \"grey20\", \"grey40\",\n                               \"grey80\", \"grey90\", \"white\"),\n                    name = \"p-value\",\n                    guide = guide_legend(direction = \"horizontal\",\n                                               title.position = \"top\")) +\n  geom_polygon(data = map_base, aes(group = group), \n               colour = NA, fill = \"grey80\") +\n  coord_fixed(ratio = 1, xlim = c(13.0, 23.0), ylim = c(-33, -42), expand = TRUE) +\n  labs(x = \"\", y = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\nmap_both &lt;- ggpubr::ggarrange(map_slope, map_p, align = \"hv\")\nmap_both\n\nFrom the figure above we may see that the entire study area shows significant (p&lt;= 0.05) increases in the count of MHWs per year. This is generally the case for the entire globe. Not shown here is the significant increase in the intensity of MHWs as well."
  },
  {
    "objectID": "vignettes/gridded_data_intro.html",
    "href": "vignettes/gridded_data_intro.html",
    "title": "Global Earth and Ocean Data",
    "section": "",
    "text": "The wealth of data about the global ocean we have access to today has gradually accumulated since 1784, which is the earliest date of the sea surface temperature data that populate the International Comprehensive Ocean Archive Network (ICOADS). Astounding improvements in data coverage and quality have resulted from the application of scientific principles and technological developments to the sounding the ocean depths, the pin-pointing of locational information, the measurement of variations in the physical properties of seawater, the gathering of ever-larger quantities of data in well-described datasets, and their processing, analysis, and interpretation. Current-day datasets come in various ‘flavours’ across a range of spatial, spectral, and temporal resolutions. Some, such as ICOADS, is comprised entirely of data sources, while others contain data obtained remotely, usually by instruments mounted on Earth-orbiting satellites, aeroplanes, or autonomous aerial vehicles. Many are blends of and satellite-derived sources. Such data may cover geophysical phenomena such as sea surface temperature or topography, or they may be maps of the ocean floor. Others may be about biological variables, such as ocean colour that relates to phytoplankton community properties.\nThe wealth of Earth and Ocean science datasets covers diverse topics such as climate, weather, geology, oceanography, and more. Some of the most important datasets include:\n\nThe Global Historical Climatology Network (GHCN) dataset, which provides temperature and precipitation data for thousands of weather stations around the world dating back to the late 1800s.\nThe Advanced Microwave Scanning Radiometer (AMSR) data, which provide information on global precipitation, sea surface temperature, and sea ice concentration.\nData from the SeaWiFS sensors, which provides information on global ocean color and chlorophyll concentrations, which can be used to study ocean productivity and the health of marine ecosystems.\nLandsat data, which provide detailed images of the Earth’s surface, including information on vegetation, land use, and other geographical features.\nThe MODIS Aqua and Terra datasets, which provide information on a wide variety of Earth’s features, including vegetation, land use, sea ice, and more, at a high spatial resolution.\nVarious National Oceanic and Atmospheric Administration (NOAA) datasets, which provide a wide range of data on the oceans, including sea surface temperature, currents, salinity, and more.\n\nThese datasets are important for researchers, policymakers, and the general public to better understand the Earth and its systems, and to aid in decision making and resource management.\nRegardless of how these data have been obtained or what they represent, a common feature is that they are stored digitally in common file formats that are described in a similar manner, and therefore can be accessed using a small collection of software tools."
  },
  {
    "objectID": "vignettes/gridded_data_intro.html#introduction-to-earth-and-ocean-science-datasets",
    "href": "vignettes/gridded_data_intro.html#introduction-to-earth-and-ocean-science-datasets",
    "title": "Global Earth and Ocean Data",
    "section": "",
    "text": "The wealth of data about the global ocean we have access to today has gradually accumulated since 1784, which is the earliest date of the sea surface temperature data that populate the International Comprehensive Ocean Archive Network (ICOADS). Astounding improvements in data coverage and quality have resulted from the application of scientific principles and technological developments to the sounding the ocean depths, the pin-pointing of locational information, the measurement of variations in the physical properties of seawater, the gathering of ever-larger quantities of data in well-described datasets, and their processing, analysis, and interpretation. Current-day datasets come in various ‘flavours’ across a range of spatial, spectral, and temporal resolutions. Some, such as ICOADS, is comprised entirely of data sources, while others contain data obtained remotely, usually by instruments mounted on Earth-orbiting satellites, aeroplanes, or autonomous aerial vehicles. Many are blends of and satellite-derived sources. Such data may cover geophysical phenomena such as sea surface temperature or topography, or they may be maps of the ocean floor. Others may be about biological variables, such as ocean colour that relates to phytoplankton community properties.\nThe wealth of Earth and Ocean science datasets covers diverse topics such as climate, weather, geology, oceanography, and more. Some of the most important datasets include:\n\nThe Global Historical Climatology Network (GHCN) dataset, which provides temperature and precipitation data for thousands of weather stations around the world dating back to the late 1800s.\nThe Advanced Microwave Scanning Radiometer (AMSR) data, which provide information on global precipitation, sea surface temperature, and sea ice concentration.\nData from the SeaWiFS sensors, which provides information on global ocean color and chlorophyll concentrations, which can be used to study ocean productivity and the health of marine ecosystems.\nLandsat data, which provide detailed images of the Earth’s surface, including information on vegetation, land use, and other geographical features.\nThe MODIS Aqua and Terra datasets, which provide information on a wide variety of Earth’s features, including vegetation, land use, sea ice, and more, at a high spatial resolution.\nVarious National Oceanic and Atmospheric Administration (NOAA) datasets, which provide a wide range of data on the oceans, including sea surface temperature, currents, salinity, and more.\n\nThese datasets are important for researchers, policymakers, and the general public to better understand the Earth and its systems, and to aid in decision making and resource management.\nRegardless of how these data have been obtained or what they represent, a common feature is that they are stored digitally in common file formats that are described in a similar manner, and therefore can be accessed using a small collection of software tools."
  },
  {
    "objectID": "vignettes/gridded_data_intro.html#levels-of-processing",
    "href": "vignettes/gridded_data_intro.html#levels-of-processing",
    "title": "Global Earth and Ocean Data",
    "section": "2 Levels of Processing",
    "text": "2 Levels of Processing\nThese datasets undergo a complex series of processing from then they are first captured by the instruments onboard Earth-orbiting satellites (Level 1) up to the stage where they are used by users (L3 and L4).\nL1, L2, L3, and L4 are different levels or stages of processing for gridded data products. These levels refer to the amount of processing that has been applied to the raw data before it is made available to users.\nL1 data products are the rawest form of data, and often consist of sensor measurements or observations. They have not undergone any processing or calibration and may contain errors or inconsistencies.\nL2 data products are an intermediate stage of processing, where some basic corrections and calibrations have been applied to the L1 data. This can include removing instrumental biases or correcting for sensor drift. This level of data is often used for initial analysis and quality control.\nL3 data products are created by processing L2 data, which has undergone some basic corrections and calibrations, converted to the geophysical variable of interest, and then gridded to a regular spatial resolution. This level of data is often used for operational applications and in the creation of higher-level data products. The data are usually gridded in time-series format, often daily, and cover a specific region or global coverage. This level of data is commonly used for monitoring and analysing the variability of physical and bio-geochemical ocean properties and for the creation of derived variables and indices.\nL4 data products are the highest level of processing, where L3 data are further processed to provide specific geophysical or environmental variables in a gap-free spatial and temporal format. This means that the data have to be interpollated to fill any spatial and temporal gaps. These gaps exist in the L3 data because some atmospheric (e.g. cloud cover) or ocean (e.g. sea glint reflectance) conditions can prevent the retrieval of information, or the quality of the data is deemed too low and the data are then discarded. Sometimes L4 data can undergo a blending of the measured data with the modelled or statistical representations of these measured geophysical phenomena in regions where, and during times when, direct measurements are absent in the gaps. L4 data are often used for long-term climate studies and other research applications. The data is usually gridded in a climatological format, covering a specific period of time, such as monthly or seasonal and covers a specific region or global coverage. This level of data is commonly used for climate studies, long-term variability and trend analysis, and model validation.\nIn summary, L1 data products are raw data, L2 data products are corrected and calibrated data, L3 data products are specific geophysical or environmental variables gridded data with some gaps in space and time, and L4 data products are gap free geophysical variables."
  },
  {
    "objectID": "vignettes/gridded_data_intro.html#about-this-module",
    "href": "vignettes/gridded_data_intro.html#about-this-module",
    "title": "Global Earth and Ocean Data",
    "section": "3 About this Module",
    "text": "3 About this Module\nIn this Module, we will:\n\nintroduce the mostly commonly format in which gridded data are stored,\nlook at some of the most commonly sites where these data are housed,\nprovide examples of accessing the data using\n\nthe python MOTU client (in R),\nOPeNDAP\nERDDAP\n\n\nOther download options are also available—notably via web interfaces, FTP, and WMS—but I’ll restrict the examples to those available within R ."
  },
  {
    "objectID": "vignettes/download_earthdata.html",
    "href": "vignettes/download_earthdata.html",
    "title": "wget download from NASA Earthdata",
    "section": "",
    "text": "Navigate to NASA’s EARTHDATA and follow the Find Data link:\n\n\n\nFigure 1: Navigate to the Find Data link.\n\n\nIf you have not already registered, select Register.\nScroll down a bit on the page you landed on when you selected Find Data, and select the Earthdata Search link:\n\n\n\nFigure 2: Go to the Earthdata Search page.\n\n\nOn the search page, enter a keyword for the data product you are interested in downloading (① — I searched for ‘chlorophyll’), select the processing levels of interest (②), and scroll down to the data product you want (③ – here I select “Aqua MODIS Global Mapped Chlorophyll (CHL) Data, version R2022.0”):\n\n\n\nFigure 3: Select the data product of interest by entering keywords and selecting from amongst various filter options.\n\n\nSelecting Option ③ in Figure 3 takes you to Figure 4. Here you will notice one file for each day in the observational period; as you can see, there are 19,782 ‘granules’ (as per 15 June 2023). There is an option to download each day using the\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit,\n  author = {Smit, AJ and Smit, AJ},\n  title = {Wget Download from {NASA} {Earthdata}},\n  url = {https://tangledbank.netlify.app/vignettes/download_earthdata.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A wget download from NASA Earthdata. https://tangledbank.netlify.app/vignettes/download_earthdata.html."
  },
  {
    "objectID": "vignettes/chl_ERDDAP.html",
    "href": "vignettes/chl_ERDDAP.html",
    "title": "Retrieving Chlorophyll-a Data from ERDDAP Servers",
    "section": "",
    "text": "# The packages we will use\nlibrary(tidyverse) # A staple for modern data management in R\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidync) # For easily dealing with NetCDF data\nlibrary(rerddap) # For easily downloading subsets of data\n\nRegistered S3 method overwritten by 'hoardr':\n  method           from\n  print.cache_info httr\n\nlibrary(doParallel) # For parallel processing\n\nLoading required package: foreach\n\nAttaching package: 'foreach'\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\nLoading required package: iterators\nLoading required package: parallel\nThis document is a more basic version of the tutorial Robert Schlegel wrote, and which is available on our GitHub site as a vignette of the heatwaveR package."
  },
  {
    "objectID": "vignettes/chl_ERDDAP.html#what-are-erddap-servers",
    "href": "vignettes/chl_ERDDAP.html#what-are-erddap-servers",
    "title": "Retrieving Chlorophyll-a Data from ERDDAP Servers",
    "section": "\n1 What are ERDDAP servers?",
    "text": "1 What are ERDDAP servers?\nAccording to the ERDDAP website, “ERDDAP is a data server that gives you a simple, consistent way to download subsets of scientific datasets in common file formats and make graphs and maps. This particular ERDDAP installation has oceanographic data (for example, data from satellites and buoys).”\nERDDAP allows us to conveniently access, subset, and download a multitude of gridded Earth system datasets maintained around the globe. Alternatives to ERDDAP include:\n\nUsing a python script on the command line.\nThe MOTU client (also python based).\n\nOPeNDAP.\n\nFTP.\n\nWMS.\n\nI will provide tutorial for each of these in due course."
  },
  {
    "objectID": "vignettes/chl_ERDDAP.html#erddap-sources",
    "href": "vignettes/chl_ERDDAP.html#erddap-sources",
    "title": "Retrieving Chlorophyll-a Data from ERDDAP Servers",
    "section": "\n2 ERDDAP sources",
    "text": "2 ERDDAP sources\nThe rerddap package provides a useful interface to ERDDAP servers via R. Internally, it uses the unix utility, curl. The package comes with a built-in list of links to widely used ERDDAP servers, which can be seen in a json file maintained by the package authors. Included are well-known servers such as:\n\nVarious CoastWatch Nodes\nNOAA’s National Centers for Environmental Information (NCEI)\nEuropean Marine Observation and Data Network (EMODnet) Physics ERDDAP\nRegional Ocean Modelling System\nFrench Research Institute for the Exploitation of the Sea (IFREMER)\nNOAA Pacific Marine Environmental Laboratory (PMEL)\n…and many more\n\nOther servers may be accessed if they are not listed here."
  },
  {
    "objectID": "vignettes/chl_ERDDAP.html#accessing-erddap-servers",
    "href": "vignettes/chl_ERDDAP.html#accessing-erddap-servers",
    "title": "Retrieving Chlorophyll-a Data from ERDDAP Servers",
    "section": "\n3 Accessing ERDDAP servers",
    "text": "3 Accessing ERDDAP servers\nLet us interrogate the list of servers known to the package. To find these servers, we use the servers() function:\n\n# ERDDAP servers?\nserv_list &lt;- servers()\n\nThis load the database of server names. There are four columns with useful information:\n\ncolnames(serv_list)\n\n[1] \"name\"       \"short_name\" \"url\"        \"public\"    \n\n\nThe content of the columns is reasonably self-explanatory.\nWe may be interested in some kind of variable contained somewhere in any of these servers. For example, the MODIS satellite platform provides gridded chlorophyll-a data, and so we can construct a search using the ed_search() function:\n\nwhich_chl &lt;- ed_search(query = \"chlorophyll-a\", which = \"griddap\")\n\n# voluminous output\n# head(which_chl)\n\nAbove you’ll notice the which = \"griddap\" argument. griddap is one of two kinds of ERDDAP data, the other being tabledap. griddap indicates that the data are gridded, that is, that Earth’s surface was ‘divided` (computationally) into grid cells, each with a well-defined geographical centre and N and W extent. These cells may vary in size from approx. 1 km latitude/longitude (0.01° × 0.01°) ’pixels’ to up to 2.5 × 2.5° pixels, or larger. Each of these grid cells (or pixels) is represented by some biogeophysical quantity, such as chlorophyll-a, sea surface temperature (SST), or wind speed (and many more). For each pixel a series of data across time might be available.\ntabledap (which = tabledap) on the other hand contains data that can be better represented as tables, such as station data (moorings, sites where coral reefs were continuously monitored for coral bleaching, etc.) particular to some points on Earth’s surface, but which do not systematically cover all of the land or ocean surface. These data can be seen as being discrete in space, but it may be continuous in time.\nI select dataset_id “erdMH1chla1day” which corresponds to the title “Chlorophyll-a, Aqua MODIS, NPP, L3SMI, Global, 4km, Science Quality, 2003-present (1 Day Composite)”. Note that I also select the version of the dataset in which longitudes west of the prime meridian run from -179.9792 to ~0. One can find this information inside the which_chl object and searching in the title column of the info dataframe contained within.\n\nView(which_chl[[\"info\"]])\n\nWe can obtain more information about the data using the browse() command, which opens some information (the meta-data) in a web browser:\n\nbrowse('erdMH1chla1day')\n\nWe can see that this dataset is griddap data. We can also use the info() function and now more concise but equally useful information is returned in the R console:\n\ninfo(\"erdMH1chla1day\")\n\n&lt;ERDDAP info&gt; erdMH1chla1day \n Base URL: https://upwell.pfeg.noaa.gov/erddap \n Dataset Type: griddap \n Dimensions (range):  \n     time: (2003-01-01T12:00:00Z, 2022-07-27T12:00:00Z) \n     latitude: (-89.97917, 89.97916) \n     longitude: (-179.9792, 179.9792) \n Variables:  \n     chlorophyll: \n         Units: mg m-3 \n\n\nThe convenience of ERDDAP is that we may specify various parameters to limit the data to download to a specific subset.\n\nlats = c(36.7950, 39.6790) # a region around the Azores\nlons = c(-31.5933, -23.6177)\ntime = c(\"2003-01-01\", \"2022-07-27\") # the full temporal extent\n\nNow we put together a function to download the files in CSV format.\n\n# this function downloads and prepares data based on user provided start and end dates\n# run once only, then save the downloaded data!\nchl_sub_dl &lt;- function(time_df) {\n  chl_dat &lt;- griddap(datasetx = \"erdMH1chla1day\", \n                     url = \"https://upwell.pfeg.noaa.gov/erddap/\", \n                     time = c(time_df$start, time_df$end),\n                     latitude = lats,\n                     longitude = lons,\n                     fields = \"chlorophyll\")$data %&gt;% \n    mutate(time = as.Date(stringr::str_remove(time, \"T00:00:00Z\")))\n}\n\nThe rationale for this script is provided in Downloading and Preparing NOAA OISST Data: ERDDAP. Basically, even though each year of data for the extent used in this vignette is not very large, the ERDDAP server does not like it when more than nine years of consecutive data are requested (at least, this was true several years ago, and I have not yet tried to establish of this limitation was removed). The server will also end a user’s connection after ~17 individual files have been requested. Because we can’t download all of the data in one request, and we can’t download the data one year at a time, we will need to make requests for multiple batches of data. To accomplish this we will create a dataframe of start and end dates that will allow us to automate the entire download while meeting the aforementioned criteria.\n\n# date download range by start and end dates per year\ndl_years &lt;- data.frame(date_index = 1:3,\n                       start = as.Date(c(\"2003-01-01\",\n                                         \"2010-01-01\",\n                                         \"2017-01-01\")),\n                       end = as.Date(c(\"2009-12-31\",\n                                       \"2016-12-31\",\n                                       \"2022-07-27\")))\n\nNow we may download all of the data with one nested request using some of the tidyverse’s functionality—we apply the function we made above to the dataframe of start and end dates. The time this takes will vary greatly based on connection speed:\n\nsystem.time(\n  chl_data &lt;- dl_years %&gt;% \n    group_by(date_index) %&gt;% \n    group_modify(~chl_sub_dl(.x)) %&gt;% \n    ungroup()\n) # 997.642 seconds, ~yy seconds per batch\n\nFinally, we save the data to a .Rdata file to avoid having to download it again:\n\nsave(chl_data, file = \"/data/MODIS_chl_data.Rdata\")\n\nOr we can download and save to disk as a netCDF (manually renamed to chl_data.nc afterwards):\n\ngriddap(datasetx = \"erdMH1chla1day\", \n        url = \"https://upwell.pfeg.noaa.gov/erddap/\", \n        time = time,\n        latitude = lats,\n        longitude = lons,\n        fields = \"chlorophyll\",\n        store = disk(path = getwd()))"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html",
    "href": "vignettes/elem_ts_methods.html",
    "title": "Wavelet analysis of diatom time series",
    "section": "",
    "text": "On this page I reproduce the analysis in the following paper:\nKirsten, K. L., Haberzettl, T., Wündsch, M., Frenzel, P., Meschner, S., Smit, A. J., … & Meadows, M. E. (2018). A multiproxy study of the ocean-atmospheric forcing and the impact of sea-level changes on the southern Cape coast, South Africa during the Holocene. Palaeogeography, Palaeoclimatology, Palaeoecology, 496, 282-291."
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#data-preparation",
    "href": "vignettes/elem_ts_methods.html#data-preparation",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n1.1 Data preparation",
    "text": "1.1 Data preparation\nWavelet analysis requires an evenly-spaced time series without missing values (NAs). To this end, we resampled the time series to the median sampling interval for the particular time series, i.e. a median interval of 2.4 years for the geochemistry data, and 38.3 to 39.2 years for the diatom and Principal Components (PC) series. This was accomplished with the linterp() function in the astrochron package. The individual time series are inconsistent in their length and the number of NAs and we therefore treated each time series independently.\nThe serial autocorrelation structure of the data was examined using the auto.arima() function of the forcast package. We noted that the time series have a first-order autoregressive correlation (AR1) structure, which is not uncommon in natural time series. In order to improve the detection of some of the higher frequency peaks, this serial autocorrelation was removed (i.e. ‘pre-whitened’) by using the prewhiteAR() function in the astrochron package. The result of this conditioning was that the residual error (aside from measurement error) that remained approaches white noise superimposed on the signal of interest. In the process, we also removed the linear trend from the data by applying a linear regression and taking the residuals, which became the new time series used in the subsequent analyses."
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#wavelet-decomposition",
    "href": "vignettes/elem_ts_methods.html#wavelet-decomposition",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n1.2 Wavelet decomposition",
    "text": "1.2 Wavelet decomposition\nWe apply the Morlet wavelet to decompose our time series into the time-frequency space (details given by Torrence and Compo, 1998; Murakami and Kawamura, 2001). Wavelet analysis is commonly used in time series or stratigraphic studies (Meyers, 1993; Prokoph & Barthelmes, 1996; Hosoda & Kawamura, 2004), as it allows us to examine the data sets’ temporal dynamics by identifying ‘regions’ of repetitive or regular behaviour based on its harmonic or oscillatory characteristics. In short, wavelet analysis locates the dominant modes of variability and represents these as a function of time. The advantage of wavelet transforms over other spectral decomposition methods, such as the Fourier transform, is that its allows us to identify geophysical features that might have variable rates over the duration of the study period, and it also permits us to locate multiple periodicities that may be present simultaneously – for an overview of wavelet analysis, see Lau & Weng (1995) and Torrence & Compo (1998). Here, a continuous wavelet transform was performed using the analyze.wavelet() function provided by the WaveletComp package. This function returns the wavelet power spectrum as well as p-values testing the null hypothesis that a period is not-significant at a certain time."
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#signal-reconstruction-and-bandpass-filtering",
    "href": "vignettes/elem_ts_methods.html#signal-reconstruction-and-bandpass-filtering",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n1.3 Signal reconstruction and bandpass filtering",
    "text": "1.3 Signal reconstruction and bandpass filtering\nWe then used the reconstruct() function to reassemble a time series from its wavelet properties. We permit only a narrow range of periodicities (analogous to the bandpass filter later on) to feed into the reconstruction. These periodicities are further selected by using only those at a power greater than 0.02 and at a significance level of less than 0.05. The resultant graphs are paired with graphs of the original (but interpolated, pre-whitened and detrended) data and provides confirmation that the wavelet analysis has indeed recovered the major modes in the time frequency domain that formed the signal in our stratigraphies.\nA more precise outcome than provided by the reconstruct method, above, was achieved by the application of bandpass filters. Bandpass filters allow signals through that fall within a certain “band” of frequencies while discriminating against signals that are present at other frequencies. We used a bandpass filter (in the astrochron package) within a tapered cosine window. We filtered the data to exclude everything below and above certain frequencies (i.e. localising specific peaks in the wavelet power spectra) and then superimposed these filtered bands onto the original data. Assurance about the periodic features of our time series was obtained in this manner. Furthermore, by selecting certain narrow bands from amongst the range of power spectra returned by wavelet analysis, bandpass filtering also allowed us to more closely evaluate which regions along the length of the time series were comprised of the major periodicities that were recovered."
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#interpolation",
    "href": "vignettes/elem_ts_methods.html#interpolation",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n2.1 Interpolation",
    "text": "2.1 Interpolation\n\n# read in the geochem data using a function from the data.table package\ngeochem.raw &lt;- fread(paste0(fpath, \"geochem.csv\"), sep = \";\")\n\n# first remove the rows with NAs\ngeochem &lt;- na.omit(geochem.raw)\n# function find the median time interval and interpolates time \n# series to it\ngeochem.int &lt;- linterp(geochem, verbose = FALSE, genplot = FALSE)\n# ...median interval of 2.4 years\n\n# now we do the same with the principal components and diatom data\ndiatoms &lt;- fread(paste0(fpath, \"diatoms.csv\"), sep = \";\")\n\n# create separate time series for each column, remove NAs if present\n# and then interpolate to median time interval\npc1 &lt;- dplyr::select(diatoms, age_cal_BP, PC1_marine) %&gt;% na.omit()\npc1.int &lt;- linterp(pc1, verbose = FALSE, genplot = FALSE) \n# ...median interval of 39.2\n\npc2 &lt;- dplyr::select(diatoms, age_cal_BP, PC2_moisture) %&gt;% na.omit()\npc2.int &lt;- linterp(pc2, verbose = FALSE, genplot = FALSE) \n# ...mediam interval of 39.2\n\nparalia &lt;- dplyr::select(diatoms, age_cal_BP, Paralia_sulcata) %&gt;% na.omit()\nparalia.int &lt;- linterp(paralia, verbose = FALSE, genplot = FALSE) \n# ...median interval of 38.3\n\nbenthics &lt;- dplyr::select(diatoms, age_cal_BP, Dilute_benthics) %&gt;% na.omit()\nbenthics.int &lt;- linterp(benthics, verbose = FALSE, genplot = FALSE) \n# ...median interval of 39\n\noffshore &lt;- dplyr::select(diatoms, age_cal_BP, Marine_offshore) %&gt;% na.omit()\noffshore.int &lt;- linterp(offshore, verbose = FALSE, genplot = FALSE) \n# ...median interval of 38.3\n\nThe geochem and PC/diatom data are different in terms of their sampling frequency and time series length, which has important implications for the frequency of the oscillations that can be detected. The sampling frequency will limit the minimum length of the wave period that can be detected. The geochem data with a sampling interval (dt) of 2.4 years lends itself to the detection of wave periods of no less than 2 * dt, i.e. 4.8 years. The PC/diatom data are courser grained, and 80 years is probably the best we can do as far as the minimum detectable wave period is concerned. Time series length (n) influences the maximum wave period that can be detected. Typically this limit is the floor(n/3) * dt. For the geochem data this is 2988 and for the diatom data it is 2400."
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#arima-and-pre-whitening",
    "href": "vignettes/elem_ts_methods.html#arima-and-pre-whitening",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n2.2 ARIMA and pre-whitening",
    "text": "2.2 ARIMA and pre-whitening\nI’ll continue the analysis using the geochem data, and later I’ll return to the diatom and PC time series. Now I am interested to know about the serial autocorrelation structure of the data. I use the forcast package’s auto.arima() function to automatically detect the Autoregressive Integrated Moving Averages (ARIMA) correlation structure. The outcome is printed below – we see that the geochem data have a ARIMA(1,1,3) correlation structure. The printout for the other data sets is not shown, but it is more-or-less the same. Knowing this is useful, because I need to remove this autocorrelation before I can continue. I’ll do this next.\n\n# check for autocorreltion using 'auto.arima()' in the 'forecast' package... \nauto.arima(geochem.int$Si_Al, max.p = 3, max.q = 3, stationary = FALSE, \n           seasonal = FALSE)\n\nSeries: geochem.int$Si_Al \nARIMA(1,1,3) \n\nCoefficients:\n         ar1      ma1      ma2     ma3\n      0.6267  -0.6858  -0.2771  0.0595\ns.e.  0.0571   0.0612   0.0201  0.0353\n\nsigma^2 = 0.9505:  log likelihood = -5203.26\nAIC=10416.52   AICc=10416.53   BIC=10447.64\n\n# ...yes, significant autocorrelation is present, i.e. ARIMA(1,1,3) in this case\n\nAbove I showed that the data are serially correlated. This is expected of time series. I should remove the autocorrelation before I do the wavelet analyses. One way to do this is to fit an ARIMA model and then continue with the rest of the workflow using the models’ residuals. Instead I will use the astrochron package’s prewhiteAR() function that does approximately the same, but just with a bit less fine control over how the model is specified. The result of this conditioning is that the only error (aside from measurement etc. error) that remains is white noise that’s superimposed on the signal of interest. A plot (below) also shows that the time serious is now detrended.\n\n# apply pre-whitening to the data; this effectively removes the above \n# autocorrelation structure and the residuals are then used for the remainder \n# of the analyses; this allows us to easily identify the embedded spectral \n# frequencies\ngeochem.int.w &lt;- prewhiteAR(geochem.int, order = 3, method = \"mle\", aic = TRUE,  \n                            genplot = FALSE, verbose = FALSE)\ncolnames(geochem.int.w) &lt;- c(\"age_cal_BP\",\"Si_Al\")\n\n# the diatom and PC data\npc1.int.w &lt;- prewhiteAR(pc1.int, order = 3, method = \"mle\", aic = TRUE,\n                        genplot = FALSE, verbose = FALSE)\ncolnames(pc1.int.w) &lt;- c(\"age_cal_BP\",\"pc1\")\npc2.int.w &lt;- prewhiteAR(pc2.int, order = 3, method = \"mle\", aic = TRUE,\n                        genplot = FALSE, verbose = FALSE)\ncolnames(pc2.int.w) &lt;- c(\"age_cal_BP\",\"pc2\")\nparalia.int.w &lt;- prewhiteAR(paralia.int, order = 3, method = \"mle\", aic = TRUE,\n                            genplot = FALSE, verbose = FALSE)\ncolnames(paralia.int.w) &lt;- c(\"age_cal_BP\",\"paralia\")\nbenthics.int.w &lt;- prewhiteAR(benthics.int, order = 3, method = \"mle\", aic = TRUE,\n                             genplot = FALSE, verbose = FALSE)\ncolnames(benthics.int.w) &lt;- c(\"age_cal_BP\",\"benthics\")\noffshore.int.w &lt;- prewhiteAR(offshore.int, order = 3, method = \"mle\", aic = TRUE,\n                             genplot = FALSE, verbose = FALSE)\ncolnames(offshore.int.w) &lt;- c(\"age_cal_BP\",\"offshore\")\n\nWhat effect has this pre-whitening had on the appearance of the time series? Producing a plot of the data before (raw), interpolated and pre-whitening clearly shows the effect for the geochem data:\n\nsource(paste0(fpath, \"custom_theme.R\"))\nlibrary(ggplot2)\npl1 &lt;- ggplot(geochem.raw, (aes(x = age_cal_BP, y = Si_Al))) +\n  geom_line(col = \"black\", size = 0.2) + xlab(\"Age (cal BP)\") + ylab(\"Si/Al\") +\n  ggtitle(\"a. Raw data\")\n\npl2 &lt;- ggplot(geochem.int, (aes(x = age_cal_BP, y = Si_Al))) +\n  geom_line(col = \"blue\", size = 0.2)  + xlab(\"Age (cal BP)\") + ylab(\"Si/Al\") +\n  ggtitle(\"b. Interpolated and gap-filled data\")\n\npl3 &lt;- ggplot(geochem.int.w, (aes(x = age_cal_BP, y = Si_Al))) +\n  geom_line(col = \"red\", size = 0.2)  + xlab(\"Age (cal BP)\") + ylab(\"Si/Al\") +\n  ggtitle(\"c. Pre-whitened and detrended\")\n\nlibrary(grid)\nlibrary(gridExtra)\ngrid.newpage()\npushViewport(viewport(layout = grid.layout(3, 1)))\nvplayout &lt;- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)\nprint(pl1, vp = vplayout(1,1))\nprint(pl2, vp = vplayout(2,1))\nprint(pl3, vp = vplayout(3,1))"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#wavelet-transformations",
    "href": "vignettes/elem_ts_methods.html#wavelet-transformations",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n2.3 Wavelet transformations",
    "text": "2.3 Wavelet transformations\nNext I do a wavelet analysis using the analyze.wavelet() function this lives in the WaveletComp package. I test the null hypothesis that there is no periodicity in the time series using p-values obtained from a simulation to indicate any significant periodicities. Then I plot the wavelet power spectrum of a single time series using the wt.image() function in the same package. The \\(y\\)-axis shows the Fourier periods and the bottom shows time step counts. I also draw contours to outline the areas of significant wavelet power. This is where to find the wave periods of events that are captured by the data. It seems as if most of the periodicities are &lt;50 years or so, but a weak period also occurs of 1,024 to 2,048 years around 6,000 to 8,000 years ago.\n\n# ts.plot(geochem.int.w$Si_Al)\n# using modified function to stop annoying default behaviour \n# (see inside 'functions.R')\nwl &lt;- analyze.wavelet_(geochem.int.w, \"Si_Al\", loess.span = 0, dt = 2.4, \n                      dj = 1/50, lowerPeriod = 6, make.pval = TRUE, n.sim = 50, \n                      method = \"white.noise\", verbose = FALSE)\n\n# plot the wavelets\nwt.image(wl, siglvl = 0.05, col.contour = \"black\", color.key = \"quantile\", \n         legend.params = list(lab = \"wavelet power levels\", label.digits = 2, \n                              shrink = 1.0), timelab = \"Years\", \n         periodlab = \"Period\", lwd = 1)"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#reconstruct",
    "href": "vignettes/elem_ts_methods.html#reconstruct",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n2.4 Reconstruct",
    "text": "2.4 Reconstruct\nI now use the reconstruct() function to reassemble a time series from its wavelet properties extracted from the data series just analysed. I permit only a narrow range of frequencies (analogous to the bandpass filter later on) to feed into the reconstruction. These frequencies are further selected by using only those at a power greater than 0.02 and at a significance level of less than 0.05. The graph shows quite a good reconstruction – the reconstructed time series matches the original (interpolated, whitened and detrended) one very nicely. This shows that the events the drive the Si/Al ratios occur at periodicities of less than 50 years (i.e. frequencies of &gt;0.02 per year).\n\n# using modified 'reconstruct' function to prevent plotting of sub-title\nreconstruct_(wl, plot.waves = FALSE, lwd = c(1.2, 0.8), legend.coords = \"bottomleft\",\n            only.coi = TRUE, lvl = 0.02, sel.lower = 6, sel.upper = 50,\n            col = c(\"black\",\"red\"), timelab = \"Years\", siglvl = 0.05,\n            legend.text = c(\"original (detrended)\", \"reconstructed\"),\n            verbose = FALSE)"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#bandpass-filters",
    "href": "vignettes/elem_ts_methods.html#bandpass-filters",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n2.5 Bandpass filters",
    "text": "2.5 Bandpass filters\nThe same outcome as above can be achieved using bandpass filters. Bandpass filters allow signals through that fall within a certain “band” of frequencies while discriminating against signals that are present at other frequencies. This particular bandpass filter (in the astrochron package) applies the filter within a tapered cosine window. Still using the geochem data, I filter the data to exclude everything below the low frequency of 0.2 (once every five years) and the high frequency of 0.02 (one in 50 years) and then I superimpose the filtered bands onto the original (interpolated, whitened and detrended) data. The signal that is permitted to pass through faithfully captures the frequency spectra present in the original data (the second of the two graphs is informative).\n\n# Using the pre-whitened data, apply band-pass filters using a \n# cosine-tapered window\n# note: this function was modified and it can be found in the file 'functions.R'\nbp1 &lt;- bandpass_(geochem.int.w, demean = FALSE, detrend = TRUE, padfac = 500, win = 2, \n                flow = 1/50, fhigh = 1/5, p = .1, verbose = FALSE, output = 1)\n\n\n\n\n\n\n\n\nstr(bp1)\n\n'data.frame':   3733 obs. of  2 variables:\n $ age_cal_BP: num  -45 -42.6 -40.2 -37.8 -35.4 ...\n $ Si_Al     : num  0.475 1.132 -0.232 -1.003 -0.238 ...\n\nht(bp1)\n\n\n\n\n\nage_cal_BP\nSi_Al\n\n\n\n1\n-45.0\n0.4753968\n\n\n2\n-42.6\n1.1315122\n\n\n3\n-40.2\n-0.2321325\n\n\n4\n-37.8\n-1.0029499\n\n\n5\n-35.4\n-0.2381605\n\n\n6\n-33.0\n0.1838719\n\n\n7\n-30.6\n-0.1033707\n\n\n3727\n8897.4\n-0.0196629\n\n\n3728\n8899.8\n-0.0552409\n\n\n3729\n8902.2\n0.4204507\n\n\n3730\n8904.6\n0.8785037\n\n\n3731\n8907.0\n-0.8718957\n\n\n3732\n8909.4\n-0.3550191\n\n\n3733\n8911.8\n-0.3612811\n\n\n\n\n\n\nWhat happens if we narrow the band to range from once in five years (0.2) to once in 10 years (0.1)? The resultant signal is still similar to the original series, but more so at 8,000 years and less so from 0 to ~6000 years.\n\nbp2 &lt;- bandpass_(geochem.int.w, demean = FALSE, detrend = TRUE, padfac = 500, win = 2,\n               flow = 1/10, fhigh = 1/5, p = .1, verbose = FALSE, output = 2)\n\n\n\n\n\n\n\nHigher frequencies (1/10 to 1/50) better match the earlier portions of the time series, as shown here. It seems that we need both frequency ranges (1/5 to 1/10 and 1/10 to 1/50) to permit the full set of frequencies through that’s necessary to shape the signals present in the original geochem series—this is in fact what the first of the bandpass figures, above, does.\n\nbp3 &lt;- bandpass_(geochem.int.w, demean = FALSE, detrend = TRUE, padfac = 500, win = 2, \n                 flow = 1/50, fhigh = 1/10, p = .1, verbose = FALSE, output = 2)\n\n\n\n\n\n\n\nWhat about frequencies higher than 0.02 (1/50)? As seen below, those frequencies carry very little (if any) of the signal that is necessary to construct the geochem data. This is the same result as the wavelet analysis and the reconstruction of the data based on the wavelet properties of the original data.\n\nbp4 &lt;- bandpass_(geochem.int.w, demean = FALSE, detrend = TRUE, padfac = 500, win = 2,\n                flow = 1/100, fhigh = 1/50, p = .1, verbose = FALSE, output = 2)"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#the-first-principal-components-axis",
    "href": "vignettes/elem_ts_methods.html#the-first-principal-components-axis",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n3.1 The first Principal Components axis",
    "text": "3.1 The first Principal Components axis\nFirst I do a wavelet analysis as before with the geochem data. The parameters that go into the equation are somewhat different to accommodate the different nature of these data.\n\n# ts.plot(pc1.int.w$pc1)\nw2 &lt;- analyze.wavelet_(pc1.int.w, \"pc1\", loess.span = 0, dt = 39.2, \n                      dj = 1/50, lowerPeriod = 40, make.pval = TRUE, n.sim = 50, \n                      method = \"white.noise\", verbose = FALSE)\n\nwt.image(w2, siglvl = 0.05, col.contour = \"black\", color.key = \"quantile\", \n         legend.params = list(lab = \"wavelet power levels\", label.digits = 2, \n                              shrink = 1.0), timelab = \"Years\", \n         periodlab = \"Period\", lwd = 1, plot.ridge = FALSE)\n\n\n\n\n\n\n\nAnd here are two bandpass filters applied to the data (note the frequencies specified in the equations):\n\npc1.bp &lt;- bandpass_(pc1.int.w, demean = FALSE, detrend = TRUE, padfac = 500, \n                    win = 2, flow = 1/550, fhigh = 1/50, p = .1, \n                    verbose = FALSE, output = 2)\n\n\n\n\n\n\n\n\npc1.2.bp &lt;- bandpass_(pc1.int.w, demean = FALSE, detrend = TRUE, padfac = 500, \n                      win = 2, flow = 1/550, fhigh = 1/250, p = .1, \n                      verbose = FALSE, output = 2)"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#the-second-principal-components-axis",
    "href": "vignettes/elem_ts_methods.html#the-second-principal-components-axis",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n3.2 The second Principal Components axis",
    "text": "3.2 The second Principal Components axis\nHere and further down I omit the bandpass filters. These can easily be done using the code provided.\n\n# ts.plot(pc2.int.w$pc2)\nw3 &lt;- analyze.wavelet_(pc2.int.w, \"pc2\", loess.span = 0, dt = 39.2, \n                      dj = 1/50, lowerPeriod = 40, make.pval = TRUE, n.sim = 50, \n                      method = \"white.noise\", verbose = FALSE)\n\n\nwt.image(w3, siglvl = 0.05, col.contour = \"black\", color.key = \"quantile\", \n         legend.params = list(lab = \"wavelet power levels\", label.digits = 2, \n                              shrink = 1.0), timelab = \"Years\", \n         periodlab = \"Period\", lwd = 1, plot.ridge = FALSE)"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#paralia-sulcata",
    "href": "vignettes/elem_ts_methods.html#paralia-sulcata",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n3.3 Paralia sulcata\n",
    "text": "3.3 Paralia sulcata\n\n\n# ts.plot(paralia.int.w$paralia)\nw3 &lt;- analyze.wavelet_(paralia.int.w, \"paralia\", loess.span = 0, dt = 39.2, \n                      dj = 1/50, lowerPeriod = 40, make.pval = TRUE, n.sim = 50, \n                      method = \"white.noise\", verbose = FALSE)\n\n\nwt.image(w3, siglvl = 0.05, col.contour = \"black\", color.key = \"quantile\", \n         legend.params = list(lab = \"wavelet power levels\", label.digits = 2, \n                              shrink = 1.0), timelab = \"Years\", \n         periodlab = \"Period\", lwd = 1, plot.ridge = FALSE)"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#dilute-benthics",
    "href": "vignettes/elem_ts_methods.html#dilute-benthics",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n3.4 Dilute benthics",
    "text": "3.4 Dilute benthics\n\n# ts.plot(benthics.int.w$benthics)\nw4 &lt;- analyze.wavelet_(benthics.int.w, \"benthics\", loess.span = 0, dt = 39.2, \n                      dj = 1/50, lowerPeriod = 40, make.pval = TRUE, n.sim = 50, \n                      method = \"white.noise\", verbose = FALSE)\n\n\nwt.image(w4, siglvl = 0.05, col.contour = \"black\", color.key = \"quantile\", \n         legend.params = list(lab = \"wavelet power levels\", label.digits = 2, \n                              shrink = 1.0), timelab = \"Years\", \n         periodlab = \"Period\", lwd = 1, plot.ridge = FALSE)"
  },
  {
    "objectID": "vignettes/elem_ts_methods.html#marine-offshore",
    "href": "vignettes/elem_ts_methods.html#marine-offshore",
    "title": "Wavelet analysis of diatom time series",
    "section": "\n3.5 Marine offshore",
    "text": "3.5 Marine offshore\n\n# ts.plot(offshore.int.w$offshore)\nw5 &lt;- analyze.wavelet_(offshore.int.w, \"offshore\", loess.span = 0, dt = 39.2, \n                      dj = 1/50, lowerPeriod = 40, make.pval = TRUE, n.sim = 50, \n                      method = \"white.noise\", verbose = FALSE)\n\n\nwt.image(w5, siglvl = 0.05, col.contour = \"black\", color.key = \"quantile\", \n         legend.params = list(lab = \"wavelet power levels\", label.digits = 2, \n                              shrink = 1.0), timelab = \"Years\", \n         periodlab = \"Period\", lwd = 1, plot.ridge = FALSE)"
  },
  {
    "objectID": "vignettes/alt_method.html",
    "href": "vignettes/alt_method.html",
    "title": "Downloading and Preparing NOAA OISST Data: python + CDO",
    "section": "",
    "text": "In this vignette we shall look at retrieving and processing the Reynolds Optimally Interpolated Sea Surface Temperature (OISST), which is a global data set of Advanced Very High Resolution Radiometer (AVHRR) derived SSTs at a daily resolution, starting on 1 September 1981. The source of the data is the Physical Oceanography Distributed Active Archive Centre (PODAAC).\nSeveral steps are involved:\n\nRetrieving the data using a python script\nUsing CDO to combine the daily files into an über netCDF\nExtracting the giant netCDF using tidync\n\nCreating longitude slices\nDetecting MHWs in each slice in parallel\n\n\n\nFigure 1. OISST data plotted on a Gnomonic Cubed Sphere projection thanks to Panoply—the similarity with certain religious iconography is purely coincidental.\n\nEach global, daily file is around 8.3Mb, so they add up to a large amount of data when a time series of at least 30 years duration is downloaded. A time series of at least 30 years is needed for heatwave detection. Currently I have 13,216 of these global files, and this amounts to ~108Gb of total disk space. Since not everyone will need all of these data, we shall subset the data using a python script prior to downloading them."
  },
  {
    "objectID": "vignettes/alt_method.html#overview",
    "href": "vignettes/alt_method.html#overview",
    "title": "Downloading and Preparing NOAA OISST Data: python + CDO",
    "section": "",
    "text": "In this vignette we shall look at retrieving and processing the Reynolds Optimally Interpolated Sea Surface Temperature (OISST), which is a global data set of Advanced Very High Resolution Radiometer (AVHRR) derived SSTs at a daily resolution, starting on 1 September 1981. The source of the data is the Physical Oceanography Distributed Active Archive Centre (PODAAC).\nSeveral steps are involved:\n\nRetrieving the data using a python script\nUsing CDO to combine the daily files into an über netCDF\nExtracting the giant netCDF using tidync\n\nCreating longitude slices\nDetecting MHWs in each slice in parallel\n\n\n\nFigure 1. OISST data plotted on a Gnomonic Cubed Sphere projection thanks to Panoply—the similarity with certain religious iconography is purely coincidental.\n\nEach global, daily file is around 8.3Mb, so they add up to a large amount of data when a time series of at least 30 years duration is downloaded. A time series of at least 30 years is needed for heatwave detection. Currently I have 13,216 of these global files, and this amounts to ~108Gb of total disk space. Since not everyone will need all of these data, we shall subset the data using a python script prior to downloading them."
  },
  {
    "objectID": "vignettes/alt_method.html#subsetting-using-a-python-script",
    "href": "vignettes/alt_method.html#subsetting-using-a-python-script",
    "title": "Downloading and Preparing NOAA OISST Data: python + CDO",
    "section": "\n2 Subsetting using a python script",
    "text": "2 Subsetting using a python script\nTo do the subsetting and bring the data to your local computer/server, you will need access to python &gt;3.7 with numpy. Make sure it is installed on your system and visible on the system PATH.\n&lt;&lt;&lt; — check python version — &gt;&gt;&gt;\nCreate a folder on your server where all the data will be received, below, for example, I use /Users/ajsmit/spatial/test/netCDF.\nInto this directory, copy the python script, subset_dataset.py (link). Remember to make the file executable by running chmod +x subset_dataset.py. I use MacOS X (or linux), so I’m not able to provide instructions if you use Windows. In a terminal, change to the directory that will receive the netCDF files, where the python script now lives. If python is in your system’s path, you should be able to execute the following command on the terminal/command line at the prompt &gt;:\n&gt; ./subset_dataset.py -s 19810901 -f 20171014 -b 5 45 -50 -12 -x AVHRR_OI-NCEI-L4-GLOB-v2.0\nEncapsulated by the above command are the following parameters:\n\nlong.min = 5\nlong.max = 45\nlat.min = -50\nlat.max = -12\nstart.date = 1981-09-01 (the OISST dataset starts here)\nend.date = 2022-02-28 (daily, new data are made available)\nshort.name = AVHRR_OI-NCEI-L4-GLOB-v2.0\n\nThe spatial extent is for a region around southern Africa that has both the Benguela and Agulhas Current in it; we select files starting in 1981-09-01 and going up to 2022-02-28. The short name is the name mentioned on the Reynolds OISST data website—substituting this name for any of the other SST datasets on that website should then permit the retrieval of other data sets (e.g. the MUR data’s short name is MUR-JPL-L4-GLOB-v4.1). This website seems to be down frequently, so try a couple of times if it does not work the first time.\nAdjust any of these parameters to taste in order to define the spatial extent and the time period as required by your study.\nIf everything works according to plan, a bunch of data will now be downloaded. This might take several hours. There will be one netCDF file for each day of the study period. In later steps we shall combine them into one netCDF file, and then do some further processing to extract the marine heatwaves."
  },
  {
    "objectID": "vignettes/alt_method.html#combine-daily-netcdfs-into-an-über-netcdf-using-cdo",
    "href": "vignettes/alt_method.html#combine-daily-netcdfs-into-an-über-netcdf-using-cdo",
    "title": "Downloading and Preparing NOAA OISST Data: python + CDO",
    "section": "\n3 Combine daily netCDFs into an über netCDF using CDO",
    "text": "3 Combine daily netCDFs into an über netCDF using CDO\nThe approach taken here sequentially slices the combined über netCDF and then detects marine heatwaves in parallel within each ‘slice.’\nTo make a giant netCDF, I use the Climate Data Operators (CDO) command in the terminal:\n&gt; cdo mergetime *.nc OISST_combined.nc\nThis can easily be applied to global data from 1 Sept 1981 to present. One can make a function in R to call cdo so as to do everything within an R script. Each new daily file can then be added to the über netCDF as it becomes available.\nAn advantage of working with this big netCDF is that subsetting and slicing are much easier and faster compared to working with individual files per each longitude slice.\nAnother advantage of going the combined netCDF route is that the resultant giant file is much smaller than either a db file or a series of RData files (e.g. one file per longitude slice). This is because netCDF has obvious advantages when it comes to storing array data, e.g. ~10 years worth of daily global files result in these file sizes:\n\nnetCDF: 30 Gb, or\ncsv and db file: ~97 Gb each"
  },
  {
    "objectID": "vignettes/alt_method.html#extract-sst-data-using-tidync-and-parallel-process-individual-slices",
    "href": "vignettes/alt_method.html#extract-sst-data-using-tidync-and-parallel-process-individual-slices",
    "title": "Downloading and Preparing NOAA OISST Data: python + CDO",
    "section": "\n4 Extract SST data using tidync, and parallel process individual ‘slices’",
    "text": "4 Extract SST data using tidync, and parallel process individual ‘slices’\nI use tidync::tidync() to sequentially select small slices that fit into memory and process each in parallel using plyr::ldapply(). Someday I’ll replace the plyr function with something newer as this package is sadly no longer maintained. Maybe one of the map() family of functions in the purrr package? The ‘width’ of a slice can be scaled with the amount of memory, and the subsequent parallel processing to detect the events within each slice scales with the number of CPU cores."
  },
  {
    "objectID": "vignettes/alt_method.html#apply-the-slice-and-detect-functions",
    "href": "vignettes/alt_method.html#apply-the-slice-and-detect-functions",
    "title": "Downloading and Preparing NOAA OISST Data: python + CDO",
    "section": "\n5 Apply the slice and detect functions",
    "text": "5 Apply the slice and detect functions\nHere is a set of steps that does the job for me:\n\n# Load packages -----------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(tidync)\nlibrary(data.table) # for the fast fwrite() function\nlibrary(heatwaveR)\nlibrary(doParallel)\nregisterDoParallel(cores = 14) # not using all 16\n\n# Define paths ------------------------------------------------------------\n\ndatadir &lt;- \"/Volumes/OceanData\"\noisst_file_dir &lt;- paste0(datadir, \"/test_files\")\nnc_file &lt;- paste0(oisst_file_dir, \"/OISST_combined.nc\")\nMHW_dir &lt;- datadir\n\n# Define various functions ------------------------------------------------\n\n# A load and slice function for the combined netCDF\nOISST_load &lt;- function(file_name, lon1, lon2) {\n  OISST_dat &lt;- tidync(file_name) %&gt;%\n    hyper_filter(lon = between(lon, lon1, lon2)) %&gt;%\n    hyper_tibble(select_var = \"sst\", force = TRUE, drop = TRUE) %&gt;%\n    select(-zlev) %&gt;%\n    dplyr::rename(t = time, temp = sst) %&gt;%\n    mutate(t = as.Date(t, origin = \"1978-01-01\"))\n  return(OISST_dat)\n  rm(OISST_dat)\n}\n\n# Rob's MHW detect function\nevent_only &lt;- function(df) {\n  # first calculate the climatologies\n  clim &lt;- ts2clm(data = df, climatologyPeriod = c(\"1991-01-01\", \"2020-12-31\"))\n  # then the events\n  event &lt;- detect_event(data = clim)\n  rm(clim)\n  # return only the event metric dataframe of results\n  return(event$event)\n  rm(event)\n}\n\n\n# Execute the code --------------------------------------------------------\n\n# Define the slices\n# 10° longitude slices seem to work fine on\n# my MacBook Pro with 64Gb RAM and 16 cores\nslice_df &lt;- tibble(lon1 = seq(0, 350, 10),\n                   lon2 = seq(10, 360, 10))\n\nsystem.time(\n  # extract slices sequentially\n  for (i in 1:nrow(slice_df)) {\n    cat(noquote(paste(\"Processing slice\", i, \"of\", nrow(slice_df),\n                      \"--&gt;\", slice_df$lon1[i], \"to\", slice_df$lon2[i], \"°E\\n\")))\n    cat(noquote(\"  &gt; 1. loading and slicing NetCDF\\n\"))\n    sst &lt;- OISST_load(nc_file, lon1 = slice_df$lon1[i], lon2 = slice_df$lon2[i])\n    # process each slice in parallel\n    cat(noquote(\"  &gt; 2. detecting marine heatwaves\\n\"))\n    MHW &lt;- plyr::ddply(.data = sst, .variables = c(\"lon\", \"lat\"),\n                       .fun = event_only, .parallel = TRUE)\n    rm(sst)\n    # save results to disk\n    cat(noquote(\"  &gt; 3. saving events to csv\\n\"))\n    fwrite(MHW, file = paste0(datadir, \"/MHW_slice_\", i, \"_\",\n                              slice_df$lon1[i], \"-\", slice_df$lon2[i], \".csv\"))\n    rm(MHW)\n    cat(noquote(\"SLICE DONE!\\n\"))\n    cat(sep=\"\\n\\n\")\n  }\n)\n\nPlease let me know if there are issues with the scripts, or if you have suggesations about how to improve them."
  },
  {
    "objectID": "vignettes/MHW_basic_detection.html",
    "href": "vignettes/MHW_basic_detection.html",
    "title": "Basic Detection and Visualisation of Events",
    "section": "",
    "text": "This material also appears as a heatwaveR vignette."
  },
  {
    "objectID": "vignettes/MHW_basic_detection.html#data",
    "href": "vignettes/MHW_basic_detection.html#data",
    "title": "Basic Detection and Visualisation of Events",
    "section": "\n1 Data",
    "text": "1 Data\nThe detect_event() function is the core of this package, and it expects to be fed the output of the second core function, ts2clm(). By default, ts2clm() wants to receive a two-column dataframe with one column labelled t containing all of the date values, and a second column temp containing all of the temperature values. Please note that the date format it expects is “YYYY-MM-DD”. For example, please see the top five rows of one of the datasets included with the heatwaveR package:\n\nhead(heatwaveR::sst_WA)\n\n\n\n\nt\ntemp\n\n\n\n1982-01-01\n20.94\n\n\n1982-01-02\n21.25\n\n\n1982-01-03\n21.38\n\n\n1982-01-04\n21.16\n\n\n1982-01-05\n21.26\n\n\n1982-01-06\n21.61\n\n\n\n\n\n\nIt is possible to use different column names other than t and temp with which to calculate events. Please see the help files for ts2clm() or detect_event() for a thorough explanation of how to do so.\nLoading ones data from a .csv file or other text based format is the easiest approach for the calculation of events, assuming one is not working with gridded data (e.g. NetCDF). Please see this vignette for a detailed walkthrough on using the functions in this package with gridded data."
  },
  {
    "objectID": "vignettes/MHW_basic_detection.html#calculating-marine-heatwaves-mhws",
    "href": "vignettes/MHW_basic_detection.html#calculating-marine-heatwaves-mhws",
    "title": "Basic Detection and Visualisation of Events",
    "section": "\n2 Calculating marine heatwaves (MHWs)",
    "text": "2 Calculating marine heatwaves (MHWs)\nHere are the ts2clm() and detect_event() function applied to the Western Australia test data included with this package (sst_WA), which are also discussed by Hobday et al. (2016):\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(heatwaveR)\n\n# Detect the events in a time series\nts &lt;- ts2clm(sst_WA, climatologyPeriod = c(\"1982-01-01\", \"2011-12-31\"))\nmhw &lt;- detect_event(ts)\n\n# View just a few metrics\nmhw$event %&gt;% \n  dplyr::ungroup() %&gt;%\n  dplyr::select(event_no, duration, date_start, date_peak, intensity_max, intensity_cumulative) %&gt;% \n  dplyr::arrange(-intensity_max) %&gt;% \n  head(5)\n\n\n\n\n\n\n\n\n\n\n\n\nevent_no\nduration\ndate_start\ndate_peak\nintensity_max\nintensity_cumulative\n\n\n\n52\n105\n2010-12-24\n2011-02-28\n6.5798\n293.2107\n\n\n41\n35\n2008-03-25\n2008-04-14\n3.8299\n79.3307\n\n\n29\n95\n1999-05-13\n1999-05-22\n3.6390\n240.2994\n\n\n60\n14\n2012-12-27\n2012-12-31\n3.4230\n32.2560\n\n\n59\n101\n2012-01-10\n2012-01-27\n3.3804\n214.0509"
  },
  {
    "objectID": "vignettes/MHW_basic_detection.html#visualising-marine-heatwaves-mhws",
    "href": "vignettes/MHW_basic_detection.html#visualising-marine-heatwaves-mhws",
    "title": "Basic Detection and Visualisation of Events",
    "section": "\n3 Visualising marine heatwaves (MHWs)",
    "text": "3 Visualising marine heatwaves (MHWs)\n\n3.1 Default MHW visuals\nOne may use event_line() and lolli_plot() directly on the output of detect_event() in order to visualise MHWs. Here are the functions being used to visualise the massive Western Australian heatwave of 2011:\n\nevent_line(mhw, spread = 180, metric = \"intensity_max\", \n           start_date = \"1982-01-01\", end_date = \"2014-12-31\")\n\nlolli_plot(mhw, metric = \"intensity_max\")\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\n\n\n3.2 Custom MHW visuals\nThe event_line() and lolli_plot() functions were designed to work directly on the list returned by detect_event(). If more control over the figures is required, it may be useful to create them in ggplot2 by stacking geoms. We specifically created two new ggplot2 geoms to reproduce the functionality of event_line() and lolli_plot(). These functions are more general in their functionality and can be used outside of the heatwaveR package, too. To apply them to MHWs and MCSs first requires that we access the climatology or event dataframes within the list that is produced by detect_event(). Here is how:\n\n# Select the region of the time series of interest\nmhw2 &lt;- mhw$climatology %&gt;% \n  slice(10580:10720)\n\nggplot(mhw2, aes(x = t, y = temp, y2 = thresh)) +\n  geom_flame() +\n  geom_text(aes(x = as.Date(\"2011-02-25\"), y = 25.8, label = \"the Destroyer\\nof Kelps\"))\n\nggplot(mhw$event, aes(x = date_start, y = intensity_max)) +\n  geom_lolli(colour = \"salmon\", colour_n = \"red\", n = 3) +\n  geom_text(colour = \"black\", aes(x = as.Date(\"2006-08-01\"), y = 5,\n                label = \"The marine heatwaves\\nTend to be left skewed in a\\nGiven time series\")) +\n  labs(y = expression(paste(\"Max. intensity [\", degree, \"C]\")), x = NULL)\n\n\n\nFigure 3: ?(caption)\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\n\n\n3.3 Spicy MHW visuals\nThe default output of these function may not be to your liking. If so, not to worry. As ggplot2 geoms, they are highly malleable. For example, if we were to choose to reproduce the format of the MHWs as seen in Hobday et al. (2016), the code would look something like this:\n\n# It is necessary to give geom_flame() at least one row on either side of \n# the event in order to calculate the polygon corners smoothly\nmhw_top &lt;- mhw2 %&gt;% \n  slice(5:111)\n\nggplot(data = mhw2, aes(x = t)) +\n  geom_flame(aes(y = temp, y2 = thresh, fill = \"all\"), show.legend = T) +\n  geom_flame(data = mhw_top, aes(y = temp, y2 = thresh, fill = \"top\"),  show.legend = T) +\n  geom_line(aes(y = temp, colour = \"temp\")) +\n  geom_line(aes(y = thresh, colour = \"thresh\"), size = 1.0) +\n  geom_line(aes(y = seas, colour = \"seas\"), size = 1.2) +\n  scale_colour_manual(name = \"Line Colour\",\n                      values = c(\"temp\" = \"black\", \n                                 \"thresh\" =  \"forestgreen\", \n                                 \"seas\" = \"grey80\")) +\n  scale_fill_manual(name = \"Event Colour\", \n                    values = c(\"all\" = \"salmon\", \n                               \"top\" = \"red\")) +\n  scale_x_date(date_labels = \"%b %Y\") +\n  guides(colour = guide_legend(override.aes = list(fill = NA))) +\n  labs(y = expression(paste(\"Temperature [\", degree, \"C]\")), x = NULL)\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nIt is also worth pointing out that when we use geom_flame() directly like this, but we don’t want to highlight events greater less than our standard five day length, allowing for a two day gap, we want to use the arguments n and n_gap respectively.\n\nmhw3 &lt;- mhw$climatology %&gt;% \n  slice(850:950)\n\nggplot(mhw3, aes(x = t, y = temp, y2 = thresh)) +\n  geom_flame(fill = \"black\", alpha = 0.5) +\n  # Note the use of n = 5 and n_gap = 2 below\n  geom_flame(n = 5, n_gap = 2, fill = \"red\", alpha = 0.5) +\n  ylim(c(22, 25)) +\n    geom_text(colour = \"black\", aes(x = as.Date(\"1984-05-16\"), y = 24.5,\n                label = \"heat\\n\\n\\n\\n\\nspike\"))\n\n\n\n\n\n\n\nShould we not wish to highlight any events with geom_lolli(), plot them with a colour other than the default, and use a different theme, it would look like this:\n\nggplot(mhw$event, aes(x = date_peak, y = intensity_max)) +\n  geom_lolli(colour = \"firebrick\") +\n  labs(x = \"Peak Date\", \n       y = expression(paste(\"Max. intensity [\", degree, \"C]\")), x = NULL) +\n  theme_linedraw()\n\n\n\nFigure 6: ?(caption)\n\n\n\n\nBecause these are simple ggplot2 geoms possibilities are nearly infinite."
  },
  {
    "objectID": "vignettes/MHW_basic_detection.html#calculating-marine-cold-spells-mcss",
    "href": "vignettes/MHW_basic_detection.html#calculating-marine-cold-spells-mcss",
    "title": "Basic Detection and Visualisation of Events",
    "section": "\n4 Calculating marine cold-spells (MCSs)",
    "text": "4 Calculating marine cold-spells (MCSs)\nThe calculation and visualisation of cold-spells is also provided for within this package. The data to be fed into the functions is the same as for MHWs. The main difference is that one is now calculating the 10th percentile threshold, rather than the 90th percentile threshold. Here are the top five cold-spells (cumulative intensity) detected in the OISST data for Western Australia:\n\n# First calculate the cold-spells\nts_10th &lt;- ts2clm(sst_WA, climatologyPeriod = c(\"1982-01-01\", \"2011-12-31\"), pctile = 10)\nmcs &lt;- detect_event(ts_10th, coldSpells = TRUE)\n\n# Then look at the top few events\nmcs$event %&gt;% \n  dplyr::ungroup() %&gt;%\n  dplyr::select(event_no, duration, date_start,\n                date_peak, intensity_mean, intensity_max, intensity_cumulative) %&gt;%\n  dplyr::arrange(intensity_cumulative) %&gt;% \n  head(5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nevent_no\nduration\ndate_start\ndate_peak\nintensity_mean\nintensity_max\nintensity_cumulative\n\n\n\n15\n76\n1990-04-13\n1990-05-11\n-2.5027\n-3.1883\n-190.2043\n\n\n49\n58\n2003-12-19\n2004-01-23\n-1.7341\n-2.5865\n-100.5806\n\n\n83\n41\n2020-04-26\n2020-05-25\n-2.3374\n-3.1433\n-95.8339\n\n\n64\n52\n2014-04-14\n2014-05-05\n-1.7824\n-2.5358\n-92.6844\n\n\n77\n46\n2018-07-24\n2018-08-02\n-1.8096\n-2.4311\n-83.2407"
  },
  {
    "objectID": "vignettes/MHW_basic_detection.html#visualising-marine-cold-spells-mcss",
    "href": "vignettes/MHW_basic_detection.html#visualising-marine-cold-spells-mcss",
    "title": "Basic Detection and Visualisation of Events",
    "section": "\n5 Visualising marine cold-spells (MCSs)",
    "text": "5 Visualising marine cold-spells (MCSs)\n\n5.1 Default MCS visuals\nThe default plots showing cold-spells look like this:\n\nevent_line(mcs, spread = 200, metric = \"intensity_cumulative\",\n           start_date = \"1982-01-01\", end_date = \"2014-12-31\")\n\nlolli_plot(mcs, metric = \"intensity_cumulative\", xaxis = \"event_no\")\n\n\n\nFigure 7: ?(caption)\n\n\n\n\n\nFigure 8: ?(caption)\n\n\n\n\n\nNote that one does not need to specify that MCSs are to be visualised, the functions are able to understand this on their own.\n\n5.2 Custom MCS visuals\nCold spell figures may be created as geoms in ggplot2, too:\n\n# Select the region of the time series of interest\nmcs2 &lt;- mcs$climatology %&gt;% \n  slice(2900:3190)\n\n# Note that one must specify a colour other than the default 'salmon'\nggplot(mcs2, aes(x = t, y = thresh, y2 = temp)) +\n  geom_flame(fill = \"steelblue3\")\n\nggplot(mcs$event, aes(x = date_start, y = intensity_max)) +\n  geom_lolli(colour = \"steelblue3\", colour_n = \"navy\", n = 3) +\n  labs(x = \"Start Date\",\n       y = expression(paste(\"Max. intensity [\", degree, \"C]\")))\n\n\n\nFigure 9: ?(caption)\n\n\n\n\n\nFigure 10: ?(caption)\n\n\n\n\n5.3 Minty MCS visuals\nAgain, because geom_flame() and geom_lolli() are simple ggplot2 geoms, one can go completely bananas with them:\n\nmcs_top &lt;- mcs2 %&gt;% \n  slice(125:202)\n\nggplot(data = mcs2, aes(x = t)) +\n  geom_flame(aes(y = thresh, y2 = temp, fill = \"all\"), show.legend = T) +\n  geom_flame(data = mcs_top, aes(y = thresh, y2 = temp, fill = \"top\"), show.legend = T) +\n  geom_line(aes(y = temp, colour = \"temp\")) +\n  geom_line(aes(y = thresh, colour = \"thresh\"), size = 1.0) +\n  geom_line(aes(y = seas, colour = \"seas\"), size = 1.2) +\n  scale_colour_manual(name = \"Line Colour\",\n                      values = c(\"temp\" = \"black\", \"thresh\" =  \"forestgreen\", \"seas\" = \"grey80\")) +\n  scale_fill_manual(name = \"Event Colour\", values = c(\"all\" = \"steelblue3\", \"top\" = \"navy\")) +\n  scale_x_date(date_labels = \"%b %Y\") +\n  guides(colour = guide_legend(override.aes = list(fill = NA))) +\n  labs(y = expression(paste(\"Temperature [\", degree, \"C]\")), x = NULL)\n\nggplot(mcs$event, aes(x = date_start, y = intensity_cumulative)) +\n  geom_lolli(colour = \"steelblue3\", colour_n = \"navy\", n = 7) +\n  labs( x = \"Start Date\", y = expression(paste(\"Cumulative intensity [days x \", degree, \"C]\")))\n\n\n\nFigure 11: ?(caption)\n\n\n\n\n\nFigure 12: ?(caption)"
  },
  {
    "objectID": "vignettes/MHW_basic_detection.html#interactive-visuals",
    "href": "vignettes/MHW_basic_detection.html#interactive-visuals",
    "title": "Basic Detection and Visualisation of Events",
    "section": "\n6 Interactive visuals",
    "text": "6 Interactive visuals\nAs of heatwaveR v0.3.6.9002, geom_flame() was also able to be used with plotly to allow for interactive MHW visuals. Unfortunately around December of 2020 the plotly packaged was orphaned and CRAN decided it didn’t want packages to include it as an imported package. Therefore as of v0.4.4.9005 heatwaveR no longer has built in support for using geom_flame() with plotly. It is however still possible with a bit of work and a simple working example is given below. It is not currently possible to use geom_lolli() with plotly. Rather one is advised to just create the dots and segments separately with geom_point() and geom_segment() respectively as these are already recognised by plotly.\nNote that the following code chunk is not run as it makes this vignette a bit too large.\n\n# Must load plotly library first\nlibrary(plotly)\n\n# Function needed for making geom_flame() work with plotly\ngeom2trace.GeomFlame &lt;- function (data,\n                                  params,\n                                  p) {\n  \n  x &lt;- y &lt;- y2 &lt;- NULL\n  \n  # Create data.frame for ease of use\n  data1 &lt;- data.frame(x = data[[\"x\"]],\n                      y = data[[\"y\"]],\n                      y2 = data[[\"y2\"]])\n  \n  # Grab parameters\n  n &lt;- params[[\"n\"]]\n  n_gap &lt;- params[[\"n_gap\"]]\n  \n  # Find events that meet minimum length requirement\n  data_event &lt;- heatwaveR::detect_event(data1, x = x, y = y,\n                                        seasClim = y,\n                                        threshClim = y2,\n                                        minDuration = n,\n                                        maxGap = n_gap,\n                                        protoEvents = T)\n  \n  # Detect spikes\n  data_event$screen &lt;- base::ifelse(data_event$threshCriterion == FALSE, FALSE,\n                                    ifelse(data_event$event == FALSE, TRUE, FALSE))\n  \n  # Screen out spikes\n  data1 &lt;- data1[data_event$screen != TRUE,]\n  \n  # Prepare to find the polygon corners\n  x1 &lt;- data1$y\n  x2 &lt;- data1$y2\n  \n  # # Find points where x1 is above x2.\n  above &lt;- x1 &gt; x2\n  above[above == TRUE] &lt;- 1\n  above[is.na(above)] &lt;- 0\n  \n  # Points always intersect when above=TRUE, then FALSE or reverse\n  intersect.points &lt;- which(diff(above) != 0)\n  \n  # Find the slopes for each line segment.\n  x1.slopes &lt;- x1[intersect.points + 1] - x1[intersect.points]\n  x2.slopes &lt;- x2[intersect.points + 1] - x2[intersect.points]\n  \n  # # Find the intersection for each segment.\n  x.points &lt;- intersect.points + ((x2[intersect.points] - x1[intersect.points]) / (x1.slopes - x2.slopes))\n  y.points &lt;- x1[intersect.points] + (x1.slopes * (x.points - intersect.points))\n  \n  # Coerce x.points to the same scale as x\n  x_gap &lt;- data1$x[2] - data1$x[1]\n  x.points &lt;- data1$x[intersect.points] + (x_gap*(x.points - intersect.points))\n  \n  # Create new data frame and merge to introduce new rows of data\n  data2 &lt;- data.frame(y = c(data1$y, y.points), x = c(data1$x, x.points))\n  data2 &lt;- data2[order(data2$x),]\n  data3 &lt;- base::merge(data1, data2, by = c(\"x\",\"y\"), all.y = T)\n  data3$y2[is.na(data3$y2)] &lt;- data3$y[is.na(data3$y2)]\n  \n  # Remove missing values for better plotting\n  data3$y[data3$y &lt; data3$y2] &lt;- NA\n  missing_pos &lt;- !stats::complete.cases(data3[c(\"x\", \"y\", \"y2\")])\n  ids &lt;- cumsum(missing_pos) + 1\n  ids[missing_pos] &lt;- NA\n  \n  # Get the correct positions\n  positions &lt;- data.frame(x = c(data3$x, rev(data3$x)),\n                          y = c(data3$y, rev(data3$y2)),\n                          ids = c(ids, rev(ids)))\n  \n  # Convert to a format geom2trace is happy with\n  positions &lt;- plotly::group2NA(positions, groupNames = \"ids\")\n  positions &lt;- positions[stats::complete.cases(positions$ids),]\n  positions &lt;- dplyr::left_join(positions, data[,-c(2,3)], by = \"x\")\n  if(length(stats::complete.cases(positions$PANEL)) &gt; 1) \n    positions$PANEL &lt;- positions$PANEL[stats::complete.cases(positions$PANEL)][1]\n  if(length(stats::complete.cases(positions$group)) &gt; 1) \n    positions$group &lt;- positions$group[stats::complete.cases(positions$group)][1]\n  \n  # Run the plotly polygon code\n  if(length(unique(positions$PANEL)) == 1){\n    getFromNamespace(\"geom2trace.GeomPolygon\", asNamespace(\"plotly\"))(positions)\n  } else{\n    return()\n  }\n}\n\n# Time series\nts_res &lt;- heatwaveR::ts2clm(data = heatwaveR::sst_WA,\n                            climatologyPeriod = c(\"1982-01-01\", \"2011-12-31\"))\nts_res_sub &lt;- ts_res[10500:10800,]\n\n# Flame Figure\np &lt;- ggplot(data = ts_res_sub, aes(x = t, y = temp)) +\n  heatwaveR::geom_flame(aes(y2 = thresh), n = 5, n_gap = 2) +\n  geom_line(aes(y = temp)) +\n  geom_line(aes(y = seas), colour = \"green\") +\n  geom_line(aes(y = thresh), colour = \"red\") +\n  labs(x = \"\", y = \"Temperature (°C)\")\n\n# Create interactive visuals\nggplotly(p)"
  },
  {
    "objectID": "vignettes/regridding.html",
    "href": "vignettes/regridding.html",
    "title": "Regridding gridded data",
    "section": "",
    "text": "library(tidyverse) # A staple of modern data processing in R\nlibrary(tidync) # For easily dealing with NetCDF data\nlibrary(data.table)\nlibrary(rerddap) # For easily downloading subsets of data\nlibrary(lubridate)\nlibrary(reticulate)\nlibrary(doParallel) # For parallel processing\nFirst, I define the Benguela region and time extent of interest:\nlats &lt;- c(-37.5, -20)\nlons &lt;- c(15, 20)\ntime &lt;- c(\"2021-01-01\", \"2021-12-31\")\nThen I find some data."
  },
  {
    "objectID": "vignettes/regridding.html#viirs-chlorophyll-a-data",
    "href": "vignettes/regridding.html#viirs-chlorophyll-a-data",
    "title": "Regridding gridded data",
    "section": "\n1 VIIRS chlorophyll-a data",
    "text": "1 VIIRS chlorophyll-a data\n\nwhich_chl &lt;- ed_search(query = \"Chlorophyll-a\", which = \"griddap\")\n\nI select the VIIRS chl-a data. These data start in 2012 and it has a spatial resolution of ~4km lat/lon. The data, “VIIRSN, Suomi-NPP, Level-3 SMI, NASA, Global, 4km, Chlorophyll a, OCI Algorithm, R2018, 2012-present, Daily,” were retrieved from here.\n\nbrowse(\"erdVH2018chla1day\")\n\nAnd now I download it for the region and time period specified earlier:\n\nchl &lt;- griddap(datasetx = \"erdVH2018chla1day\", \n               url = \"https://upwell.pfeg.noaa.gov/erddap/\", \n               time = c(time[1], time[2]),\n               latitude = lats,\n               longitude = lons,\n               fields = \"all\")$data %&gt;% \n  mutate(time = as.Date(stringr::str_remove(time, \"T00:00:00Z\"))) |&gt; \n  as.data.table()"
  },
  {
    "objectID": "vignettes/regridding.html#era5-reanalysis-wind-data",
    "href": "vignettes/regridding.html#era5-reanalysis-wind-data",
    "title": "Regridding gridded data",
    "section": "\n2 ERA5 reanalysis wind data",
    "text": "2 ERA5 reanalysis wind data\nI use the “ERA5 hourly data on single levels from 1940 to present” data, which come in at an hourly resolution for the whole world, starting in 1940. The spatial resolution is an unimpressive 0.25° × 0.25° lat/lon, far coarser than the chl-a data.\nThe ERA5 reanalysis wind data (u and v components) were downloaded from Copernicus using a python script, which can be generated on the website using the “Show API request” option after selecting the variables and spatio-temporal ranges of interest:\n\nimport cdsapi\n\nc = cdsapi.Client()\n\nc.retrieve(\n    'reanalysis-era5-single-levels',\n    {\n        'product_type': 'reanalysis',\n        'variable': [\n            '10m_u_component_of_wind', '10m_v_component_of_wind',\n        ],\n        'year': '2021',\n        'month': [\n            '01', '02', '03',\n            '04', '05', '06',\n            '07', '08', '09',\n            '10', '11', '12',\n        ],\n        'day': [\n            '01', '02', '03',\n            '04', '05', '06',\n            '07', '08', '09',\n            '10', '11', '12',\n            '13', '14', '15',\n            '16', '17', '18',\n            '19', '20', '21',\n            '22', '23', '24',\n            '25', '26', '27',\n            '28', '29', '30',\n            '31',\n        ],\n        'time': [\n            '00:00', '01:00', '02:00',\n            '03:00', '04:00', '05:00',\n            '06:00', '07:00', '08:00',\n            '09:00', '10:00', '11:00',\n            '12:00', '13:00', '14:00',\n            '15:00', '16:00', '17:00',\n            '18:00', '19:00', '20:00',\n            '21:00', '22:00', '23:00',\n        ],\n        'area': [\n            -20, 15, -37.5,\n            20,\n        ],\n        'format': 'netcdf',\n    },\n    'download.nc')\n\nThe end product of the python download is a 52.3 Mb netCDF file, which I will now load and process to produce the daily temperature values to match the daily resolution of the VIIRS chl-a data:\n\n# time units: hours since 1900-01-01 00:00:00.0\norigin &lt;- as.POSIXct(\"1900-01-01 00:00:00\", tz = \"UTC\")\n\nncFile &lt;- \"/Volumes/OceanData/ERA5/ERA5_2021_Benguela.nc\"\n# ncFile &lt;- \"~/Downloads/ERA5_2021_Benguela.nc\"\n\nera5 &lt;- tidync(ncFile) |&gt;\nhyper_tibble() %&gt;%\nmutate(time = floor_date(time * 3600 + origin, \"day\")) |&gt;\nreframe(u10 = mean(u10),\nv10 = mean(v10),\n.by = c(time, longitude, latitude)) |&gt; \nas.data.table()\n\nNote that I coerce the data to a date.table object since the regridding step (Option 1) uses"
  },
  {
    "objectID": "vignettes/regridding.html#regridding",
    "href": "vignettes/regridding.html#regridding",
    "title": "Regridding gridded data",
    "section": "\n3 Regridding",
    "text": "3 Regridding\nLet’s check out their respective spatial resolutions:\n\nhead(unique(chl$longitude))\n\n[1] 14.97917 15.02084 15.06251 15.10417 15.14584 15.18751\n\nsort(unique(era5$longitude))\n\n [1] 15.00 15.25 15.50 15.75 16.00 16.25 16.50 16.75 17.00 17.25 17.50 17.75\n[13] 18.00 18.25 18.50 18.75 19.00 19.25 19.50 19.75 20.00\n\nhead(unique(chl$latitude))\n\n[1] -19.97917 -20.02084 -20.06250 -20.10417 -20.14584 -20.18750\n\nhead(sort(unique(era5$latitude)))\n\n[1] -37.50 -37.25 -37.00 -36.75 -36.50 -36.25\n\n\nThere is a huge difference. It is seldom a good idea to go from a low resolution like 25 km to a higher resolution like 4 km, but if you insist, you can do it with bi-linear interpolation. Here we will degrade the 4 km product to match the 25 km resolution of the wind data.\n\n3.1 Option 1\nDefine a new output grid. This will be the coarsest resolution one from ERA5:\n\nlon.out &lt;- unique(era5$longitude)\nlat.out &lt;- sort(unique(era5$latitude))\n\nUsing the metR package and its Interpolate() function, interpolate the data to the new coarser resolution grid. I show two approaches: i) a method shown in the function’s help file that uses data.table, and ii) a dplyr (tidyverse) method using the new reframe() function. What reframe() does when used within a dplyr data pipe is reveal the column names, which can then be given to the function of interest in the usual way; it then returns a dataframe or tibble of arbitrary length. reframe() also accommodates the grouping structure within the function itself through the use of the .by = argument (i.e. no need for an a priori group_by()), making the syntax not dissimilar to that of data.table’s. A few years ago the data.table approach would have been faster, but it seems the new versions of dplyr have undergone some significant speed improvements. See the results of the system.time() function:\n\nlibrary(metR)\n\n# using the data.table method\nsystem.time(\n  interp_chl &lt;- chl[, Interpolate(chla ~ longitude + latitude, lon.out, lat.out), by = time]\n)\n\n   user  system elapsed \n  5.955   0.823   5.662 \n\nhead(interp_chl)\n\n         time longitude latitude  chla\n       &lt;Date&gt;     &lt;num&gt;    &lt;num&gt; &lt;num&gt;\n1: 2021-01-01     15.00    -37.5    NA\n2: 2021-01-01     15.25    -37.5    NA\n3: 2021-01-01     15.50    -37.5    NA\n4: 2021-01-01     15.75    -37.5    NA\n5: 2021-01-01     16.00    -37.5    NA\n6: 2021-01-01     16.25    -37.5    NA\n\n# using dplyr\nsystem.time(\n  interp_chl &lt;- chl |&gt; \n    reframe(Interpolate(chla ~ longitude + latitude,\n                        x.out = lon.out, y.out = lat.out), .by = time) |&gt; \n    as_tibble()\n) \n\n   user  system elapsed \n  6.008   0.892   6.086 \n\ninterp_chl\n\n# A tibble: 544,215 × 4\n   time       longitude latitude  chla\n   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 2021-01-01      15      -37.5    NA\n 2 2021-01-01      15.2    -37.5    NA\n 3 2021-01-01      15.5    -37.5    NA\n 4 2021-01-01      15.8    -37.5    NA\n 5 2021-01-01      16      -37.5    NA\n 6 2021-01-01      16.2    -37.5    NA\n 7 2021-01-01      16.5    -37.5    NA\n 8 2021-01-01      16.8    -37.5    NA\n 9 2021-01-01      17      -37.5    NA\n10 2021-01-01      17.2    -37.5    NA\n# ℹ 544,205 more rows\n\n\nNote that daily chl-a data are very gappy and hence there are many NAs in the interpolated dataset.\nLet’s verify that the output grids are now the same:\n\nhead(unique(interp_chl$longitude))\n\n[1] 15.00 15.25 15.50 15.75 16.00 16.25\n\nhead(unique(era5$longitude))\n\n[1] 15.00 15.25 15.50 15.75 16.00 16.25\n\nhead(unique(interp_chl$latitude))\n\n[1] -37.50 -37.25 -37.00 -36.75 -36.50 -36.25\n\nhead(sort(unique(era5$latitude)))\n\n[1] -37.50 -37.25 -37.00 -36.75 -36.50 -36.25\n\n\nThese approaches seem to work… ‘work’ as in they don’t fail. I have not tested the output data to see if the results are believable; for example, how does the Interpolate() function handle missing values? I am unsure.\n\n3.2 Option 2\nAnother commonly used function for interpolation lives in the akima package. The advantage of this package is that it can do spline interpolation in addition to linear interpolation (Interpolate() only does linear interpolation). The disadvantage is that is really does not like NAs. This is how it would work:\n\n# Assuming 'chl' is your data.frame and 'time', 'longitude', 'latitude' and 'chla' are your columns\nlibrary(akima)\n\nchl |&gt; \n  reframe(interp_chl = interp(longitude, latitude, chla,\n                              xo = lon.out, yo = lat.out))\n\n\n3.3 Option 3\nThe next regridding option is done entirely within a spatial data framework. Traditionally we used the raster package, but this has been phased out in favour of sf (Simple Features for R), stars (Spatiotemporal Arrays: Raster and Vector Data Cubes), and terra (Spatial Data Analysis). Here I shall use sf and stars. Refer to Spatial Data Science for information about these spatial methods; specifically, see Chapter 7, Introduction to sf and stars.\nR spatial packages are experiencing a rapid evolution and the learning curve might be steep. I think, however, that it’s well worth one’s time as a host of spatial mapping options become available, bringing R closer in functionality to GIS. I am still learning all the various features myself and I am exploring options for integrating the spatial functionality into our marine heatwave workflows.\nMake stars objects from the gridded data. Let’s start with the chl-a data first:\n\nlibrary(stars)\nlibrary(sf)\n\n# EPSG:4326\n# WGS 84 -- WGS84 - World Geodetic System 1984, used in GPS\nchl_st &lt;- chl |&gt;\n  st_as_stars(dims = c(\"longitude\", \"latitude\", \"time\"),\n              raster = \"chlorophyll\") |&gt;\n  sf::st_set_crs(4326) |&gt; \n  st_warp(crs = st_crs(4326))\n\nHere are a few interrogation methods to see info about the data’s spatial extent:\n\nprint(chl_st)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n            Min.   1st Qu.    Median      Mean  3rd Qu.     Max.  NA's\nchla  0.09295794 0.1467107 0.2234708 0.5681996 0.375212 7.863433 99166\ndimension(s):\n     from  to     offset      delta refsys x/y\nx       1 121    14.9583  0.0416985 WGS 84 [x]\ny       1 422   -19.9583 -0.0416985 WGS 84 [y]\ntime    1 365 2021-01-01     1 days   Date    \n\ndim(chl_st)\n\n   x    y time \n 121  422  365 \n\nst_dimensions(chl_st)\n\n     from  to     offset      delta refsys x/y\nx       1 121    14.9583  0.0416985 WGS 84 [x]\ny       1 422   -19.9583 -0.0416985 WGS 84 [y]\ntime    1 365 2021-01-01     1 days   Date    \n\nst_bbox(chl_st)\n\n     xmin      ymin      xmax      ymax \n 14.95834 -37.55509  20.00385 -19.95834 \n\n\nWe can also plot the stars data directly; here I plot the data on the 34th day in the time series:\n\n# visualise a time step (day 34):\nplot(chl_st[, , , 34])\n\n\n\n\nNext we also need to get the ERA5 data into a stars format, and we print out some spatial info and make a basic map:\n\nera5_st &lt;- era5 |&gt;\n  st_as_stars(dims = c(\"longitude\", \"latitude\", \"time\"),\n              raster = c(\"u10\", \"v10\")) |&gt;\n  sf::st_set_crs(4326) |&gt; \n  st_warp(crs = st_crs(4326))\nprint(era5_st)\n\nstars object with 3 dimensions and 2 attributes\nattribute(s):\n          Min.   1st Qu.     Median        Mean  3rd Qu.    Max. NA's\nu10  -17.47988 -2.111889 -0.1112329 0.009983922 1.863335 17.6071 7665\nv10  -11.88497 -1.194172  0.7711142 1.171964005 3.200816 15.3441 7665\ndimension(s):\n     from  to         offset     delta  refsys x/y\nx       1  21         14.875  0.250205  WGS 84 [x]\ny       1  72        -19.875 -0.250205  WGS 84 [y]\ntime    1 365 2021-01-01 UTC    1 days POSIXct    \n\nst_bbox(era5_st)\n\n     xmin      ymin      xmax      ymax \n 14.87500 -37.88974  20.12930 -19.87500 \n\nplot(era5_st[\"u10\", , , 34])\n\n\n\n\nAll of this was to bring us to a point where we can do the actual regridding. This is done with the same st_warp() function. Previously it was used to make the data conform to a specific coordinate reference system (CRS) but here I use it to perform the regridding:\n\nchl_st_regrid &lt;- st_warp(src = chl_st, dest = era5_st)\n\nLet us see if it worked as advertised:\n\nprint(chl_st_regrid)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n            Min.   1st Qu.    Median      Mean   3rd Qu.     Max.   NA's\nchla  0.04547131 0.1944372 0.3054729 0.8114727 0.6934659 93.51793 488906\ndimension(s):\n     from  to     offset     delta refsys x/y\nx       1  21     14.875  0.250205 WGS 84 [x]\ny       1  72    -19.875 -0.250205 WGS 84 [y]\ntime    1 365 2021-01-01    1 days   Date    \n\nst_bbox(chl_st_regrid)\n\n     xmin      ymin      xmax      ymax \n 14.87500 -37.88974  20.12930 -19.87500 \n\nplot(chl_st_regrid[, , , 34])\n\n\n\n\nConvert the stars object back to a tibble if necessary:\n\nchl_st_regrid_df &lt;- as_tibble(chl_st_regrid, xy = TRUE)\nchl_st_regrid_df\n\n# A tibble: 551,880 × 4\n       x     y time        chla\n   &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1  15.0 -20.0 2021-01-01    NA\n 2  15.3 -20.0 2021-01-01    NA\n 3  15.5 -20.0 2021-01-01    NA\n 4  15.8 -20.0 2021-01-01    NA\n 5  16.0 -20.0 2021-01-01    NA\n 6  16.3 -20.0 2021-01-01    NA\n 7  16.5 -20.0 2021-01-01    NA\n 8  16.8 -20.0 2021-01-01    NA\n 9  17.0 -20.0 2021-01-01    NA\n10  17.3 -20.0 2021-01-01    NA\n# ℹ 551,870 more rows\n\n\nThat’s it, folks!\nIf you have any suggestions about how to do regridding or make the spatial functions more user-friendly in a marine heatwave analysis workflow, please let me know."
  },
  {
    "objectID": "vignettes/chl_sightings.html",
    "href": "vignettes/chl_sightings.html",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(gganimate)\nlibrary(raster)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(maptools)\nlibrary(elevatr)\nlibrary(RANN) # for the nearest neighbour search\nlibrary(ggpubr)\nlibrary(gt) # for nice tables\n\nsource(\"../R/map_theme.R\")\n\n\n\nFigure 1: The Azores region in the whales sightings study.\n\nThis analysis uses the Azores chlorophyll-a data downloaded in Retrieving Chlorophyll-a Data from ERDDAP Servers."
  },
  {
    "objectID": "vignettes/chl_sightings.html#load-packages",
    "href": "vignettes/chl_sightings.html#load-packages",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(gganimate)\nlibrary(raster)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(maptools)\nlibrary(elevatr)\nlibrary(RANN) # for the nearest neighbour search\nlibrary(ggpubr)\nlibrary(gt) # for nice tables\n\nsource(\"../R/map_theme.R\")\n\n\n\nFigure 1: The Azores region in the whales sightings study.\n\nThis analysis uses the Azores chlorophyll-a data downloaded in Retrieving Chlorophyll-a Data from ERDDAP Servers."
  },
  {
    "objectID": "vignettes/chl_sightings.html#background",
    "href": "vignettes/chl_sightings.html#background",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n2 Background",
    "text": "2 Background\nThe region around the Azores is characterised by relatively higher nutrient availability in a ‘sea’ of otherwise oligotrophic conditions, and hence they are of major interest as biodiversity hotspots. The enhanced productivity results from high mesoscale activity (often measured as Eddy Kinetic Energy, EKE) and the presence (and interaction with) undersea topographic features (Santos et al. 2013). The enhanced chlorophyll-a biomass in the region results in it being an important foraging area for whales en route to areas further north in the Atlantic (González Garcı́a et al. 2018).\nWe expect seasonal variation to provide a strongly signal in the region, with typical autumn/winter to spring chl-a blooms. All satellite ocean colour products show the same general pattern with the highest pigment concentrations during spring months and the lowest during summer. SST also shows a seasonal trend, with highest SST during summer and the lowest during winter."
  },
  {
    "objectID": "vignettes/chl_sightings.html#ruis-objectives",
    "href": "vignettes/chl_sightings.html#ruis-objectives",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n3 Rui’s objectives",
    "text": "3 Rui’s objectives\nOur main objective is to interpret if Azorean waters are a migratory corridor for the four main baleen whale’s species sighted in the Azores (blue, Balaenoptera musculus; fin, B. physalus; sei, B. borealis; humpback whale, Megaptera novaeangliae), during their migration from breeding to feeding areas, and vice-versa. This leads to other main questions:\n\nHow long the target species use the study area during migration? –&gt; Unlikely achievable without IDs.\nAre they returning in different years? –&gt; Probably not possible without IDs of individual whales.\nHow does the intensity and timing of the spring bloom influence the migration? –&gt; Timing question already addressed, but we can probably improve. Question about intensity vs sightings can be done using a regression-type approach."
  },
  {
    "objectID": "vignettes/chl_sightings.html#questions",
    "href": "vignettes/chl_sightings.html#questions",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n4 Questions",
    "text": "4 Questions\n\n\nDo whales track temporal appearance of chl-a max? [Yes] What is the lag? [Table 1]\n\nIs there a correlation in time between whale presence and lagged co-located chl-a conc.?\nIs the correlation between whale presence and chl-a conc spatially fixed at i) Pico and Faial and ii) São Miguel, or\ndo max aggregations at localities located near i) Pico and Faial and ii) São Miguel coincide with the chl-a max there? I.e. is there a spatial association?\n\n\nIs there some association between chl-a biomass and the number of whales visiting the region? Look at inter-annual variation.\nShow the inverse association between chl-a biomass and SST.\nGonzález Garcı́a et al. (2018) identify Mean Kinetic Energy (MKE), meridional and zonal transport components, eddies, bathymetry (depth), slope, nett primary productivity, distance from coast and wind at multiple spatial and temporal scales as influencing whale sightings.\nIs there a difference in habitat suitability between the north of the islands, the south, or around the sea mounts? –&gt; Do a multivariate analyses of all variables (see González Garcı́a et al. (2018)), with data classified a priori into the subsets representing the three regions."
  },
  {
    "objectID": "vignettes/chl_sightings.html#load-the-whale-sighting-data",
    "href": "vignettes/chl_sightings.html#load-the-whale-sighting-data",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n5 Load the whale sighting data",
    "text": "5 Load the whale sighting data\n\nsights &lt;- read_csv(\n  \"../data/occurences_sampled.Mn.txt\",\n  show_col_types = FALSE\n)\n\n# make a column with year only\nsights &lt;- sights |&gt;\n  mutate(year = year(date),\n         month = month(date, label = TRUE),\n         week = week(date),\n         yday = yday(date))"
  },
  {
    "objectID": "vignettes/chl_sightings.html#explore-the-whale-sighting-data",
    "href": "vignettes/chl_sightings.html#explore-the-whale-sighting-data",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n6 Explore the whale sighting data",
    "text": "6 Explore the whale sighting data\nThe time span of the study:\n\nrange(sights$date)\n\n[1] \"1996-09-01 00:00:00 UTC\" \"2022-05-07 15:41:51 UTC\"\n\n\n\nyrs &lt;- max(year(sights$date)) - min(year(sights$date))\n\nThe longitudinal and latitudinal range:\n\nrange(sights$lat)\n\n[1] 37.04499 39.42898\n\nrange(sights$lon)\n\n[1] -31.29517 -23.86771\n\n\n\nmedian(sights$lat)\n\n[1] 38.3807\n\nmedian(sights$lon)\n\n[1] -28.2625\n\n\nThere is one outlier which I will remove:\n\nsights &lt;- sights |&gt; \n  filter(lat &gt; min(lat))\n\nFrom here I define the spatial extent for the study region as:\n\n# the extent of the full regional map\n# a region around the Azores\n\nymin &lt;- min(sights$lat) - 0.25; ymax &lt;- max(sights$lat) + 0.25\nxmin &lt;- min(sights$lon) - 0.25; xmax &lt;- max(sights$lon) + 0.25\n\nsights_bbox &lt;- st_bbox(c(xmin = xmin, xmax = xmax, ymax = ymax, ymin = ymin),\n                       crs = CRS)\n\narea_sf &lt;- st_as_sfc(sights_bbox)\n\n# EPSG:4326\n# WGS 84 -- WGS84 - World Geodetic System 1984, used in GPS\nst_crs(area_sf) = 4326\n\nThe bounding box for the study region is:\n\nsights_bbox\n\n     xmin      ymin      xmax      ymax \n-31.54517  36.81231 -23.61771  39.67898 \n\n\nStart preparing all the map layers by loading the Natural Earth data for the continent outlines:\n\n# Get countries\nworld_ne &lt;- ne_countries(\n  scale = \"large\",\n  returnclass = \"sf\"\n)\nclass(world_ne)\n\n[1] \"sf\"         \"data.frame\"\n\n\nA first stab plot of the region shows:\n\nggplot(data = world_ne) +\n  geom_sf(col = \"black\", fill = \"black\", linewidth = 0.4) +\n  coord_sf(xlim = c(xmin, xmax),\n           ylim = c(ymin, ymax),\n           expand = FALSE) +\n  labs(x = NULL, y = NULL) +\n  theme_map()\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\nFigure 2: The Azores Islands.\n\n\n\nNext we get a DEM of the region using elevatr:\n\ndem &lt;- elevatr::get_elev_raster(locations = area_sf, z = 7, \n                                src = \"srtm15plus\",\n                                clip = \"bbox\")\n\n# keep for later and to prevent having to download each time\nsave(dem, file = \"../data/azores.dem\")\n\n\n# re-use previously downloaded dem\nload(\"../data/azores.dem\")\n\n# make dataframe from DEM raster\ndem_df &lt;- as.data.frame(dem, xy = TRUE, na.rm = TRUE)\ncolnames(dem_df)[3] &lt;- \"layer\""
  },
  {
    "objectID": "vignettes/chl_sightings.html#mapping-and-plotting-observations",
    "href": "vignettes/chl_sightings.html#mapping-and-plotting-observations",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n7 Mapping and plotting observations",
    "text": "7 Mapping and plotting observations\nLet us add all the map layer together:\n\nthe bathymetry for the region,\nthe land area polygons around the above-water regions, and\nall the whale sighting data.\n\n\n# make a colourmap\nlibrary(cmocean)\ncmap &lt;- cmocean(\"topo\")\ncols &lt;- cmap(51) # for bathy/topo\ncols3 &lt;- rainbow(33) # for observations\n\n# create the layered graph\nggplot() +\n  geom_raster(data = dem_df, aes(x = x, y = y, fill = layer)) +\n  geom_sf(data = world_ne, col = \"white\", fill = NA, linewidth = 0.4) +\n  scale_fill_gradientn(colours = cols,\n                       values = scales::rescale(c(min(dem_df$layer), 0,\n                                                  max(dem_df$layer))),\n                       breaks = c(-4000, -2000, -1000, 0, 500, 1000, 2000),\n                       name = \"Elevation /\\nDepth (m)\") +\n  geom_point(data = sights, aes(x = lon, y = lat, colour = year(date)),\n             size = 0.5, shape = 4, alpha = 1) +\n  scale_color_gradientn(colours = cols3,\n                        name = \"Year\") +\n  coord_sf(xlim = c(xmin, xmax),\n           ylim = c(ymin, ymax),\n           expand = FALSE) +\n  labs(x = NULL, y = NULL) +\n  theme_map()\n\n\n\nFigure 3: A view of the aggregated whale sigtings over the period 1989-08-20 to 2022-05-07.\n\n\n\nAre there distribution differences across years? Difficult to see in the above figure, so I create an animation of pooled annual observations across years:\n\np &lt;- ggplot() +\n  geom_raster(data = dem_df, aes(x = x, y = y, fill = layer)) +\n  geom_sf(data = world_ne, col = \"white\", fill = NA, linewidth = 0.4) +\n  scale_fill_gradientn(colours = cols,\n                       values = scales::rescale(c(min(dem_df$layer), 0,\n                                                  max(dem_df$layer))),\n                       breaks = c(-4000, -2000, -1000, 0, 500, 1000, 2000),\n                       name = \"Elevation /\\nDepth (m)\") +\n  geom_point(data = sights, aes(x = lon, y = lat),\n             colour = \"red\", size = 0.8, shape = 1, alpha = 1) +\n  coord_sf(xlim = c(xmin, xmax),\n           ylim = c(ymin, ymax),\n           expand = FALSE) +\n  theme_map() +\n  labs(title = 'Year: {floor(frame_time)}', x = NULL, y = NULL) +\n  transition_time(year) +\n  ease_aes('linear')\n\ngganimate::animate(p, fps = 2, nframes = 1 * yrs, device = \"svg\")\n\ngganimate::anim_save(\"../data/sightings_anim.gif\")\n\n\n\nFigure 4: Animation of yearly whale sightings."
  },
  {
    "objectID": "vignettes/chl_sightings.html#annual-chl-a-climatology",
    "href": "vignettes/chl_sightings.html#annual-chl-a-climatology",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n8 Annual chl-a climatology",
    "text": "8 Annual chl-a climatology\n\nchlDir &lt;- \"/Users/ajsmit/Documents/R/R_in_Ocean_Science/_development/ERDDAP/\"\nload(paste0(chlDir, \"MODIS_chl_data.Rdata\"))\n\n# Create a column of weeks\nchl_data &lt;- chl_data |&gt; \n  mutate(year = year(time),\n         month = month(time, label = TRUE),\n         week = week(time),\n         yday = yday(time))\n\n\nlibrary(palr)\npal &lt;- chl_pal(palette = TRUE)\n\nchl_data |&gt; \n  group_by(longitude, latitude) |&gt; \n  summarise(med_chlorophyll = median(chlorophyll, na.rm = TRUE),\n            .groups = \"drop\") |&gt; \n  ggplot() + \n  geom_tile(aes(x = longitude, y = latitude, fill = med_chlorophyll)) +\n  geom_sf(data = world_ne, col = \"white\", fill = \"black\", linewidth = 0.4) +\n  scale_fill_gradientn(\n    colours = pal$cols,\n    trans = \"log\",\n    breaks = c(0.1, 1, 3, 9)\n  ) +\n  guides(fill = guide_colourbar(title = \"Chl-a [mg/L]\",\n                                title.position = \"top\",\n                                direction = \"horizontal\",\n                                barwidth = 8)) +\n  scale_x_continuous(breaks = seq(-30.5, -25, 2.75)) +\n  scale_y_continuous(breaks = seq(37, 39.5, 1.25)) +\n  coord_sf(xlim = c(xmin, xmax),\n           ylim = c(ymin, ymax),\n           expand = FALSE) +\n  labs(title = \"MODIS Aqua annual chlorophyll-a climatology\",\n       x = NULL, y = NULL) +\n  theme_map() +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 5: Median chl-a concentrations over the period 2003-01-01 to 2022-07-27."
  },
  {
    "objectID": "vignettes/chl_sightings.html#monthly-chl-a-climatology",
    "href": "vignettes/chl_sightings.html#monthly-chl-a-climatology",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n9 Monthly chl-a climatology",
    "text": "9 Monthly chl-a climatology\nPlots for the full regional extent:\n\nchl_data |&gt; \n  mutate(month = month(time, label = TRUE)) |&gt; \n  group_by(longitude, latitude, month) |&gt; \n  summarise(med_chlorophyll = median(chlorophyll, na.rm = TRUE),\n            .groups = \"drop\") |&gt; \n  ggplot() + \n  geom_tile(aes(x = longitude, y = latitude, fill = med_chlorophyll)) +\n  geom_sf(data = world_ne, col = \"white\", fill = \"grey50\", linewidth = 0.4) +\n  scale_fill_gradientn(\n    colours = pal$cols,\n    trans = \"log\",\n    breaks = c(0.05, 0.1, 1),\n    limits = c(0.05, 1)\n  ) +\n  guides(fill = guide_colourbar(title = \"Chl-a [mg/m3]\",\n                                title.position = \"top\",\n                                direction = \"horizontal\",\n                                barwidth = 8)) +\n  scale_x_continuous(breaks = c(-30.5, -25)) +\n  scale_y_continuous(breaks = c(37, 39.5)) +\n  coord_sf(xlim = c(xmin, xmax),\n           ylim = c(ymin, ymax),\n           expand = FALSE) +\n  labs(title = \"MODIS Aqua seasonal chlorophyll-a climatology\",\n       x = NULL, y = NULL) +\n  theme_map() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(vars(month), ncol = 3)\n\n\n\nFigure 6: Monthly climatological median chl-a concentrations over the period 2003-01-01 to 2022-07-27.\n\n\n\nI also plot the seasonal profile for the central (Ilha do Faial, Ilha do Pico, São Jorge, Graciosa, Ilha Terceira) and eastern-most (Ilha do São Miguel, São Pedro) island groups. The respective bounding boxes are:\n\n# center group\nc_lonmin &lt;- -29\nc_lonmax &lt;- -27.5\nc_latmin &lt;- 38\nc_latmax &lt;- 39\n\n# eastern group\ne_lonmin &lt;- -26\ne_lonmax &lt;- -25\ne_latmin &lt;- 37.5\ne_latmax &lt;- 38\n\n\nchl_data |&gt;\n  filter(between(longitude, c_lonmin, c_lonmax),\n         between(latitude, c_latmin, c_latmax)) |&gt;\n  mutate(month = month(time, label = TRUE)) |&gt;\n  group_by(longitude, latitude, month) |&gt;\n  summarise(med_chlorophyll = median(chlorophyll, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  ggplot() +\n  geom_tile(aes(x = longitude, y = latitude, fill = med_chlorophyll)) +\n  geom_point(data = sights, aes(x = lon, y = lat),\n             colour = \"black\", shape = 4, size = 0.5) +\n  geom_sf(\n    data = world_ne,\n    col = \"white\",\n    fill = \"grey50\",\n    linewidth = 0.4\n  ) +\n  scale_fill_gradientn(\n    colours = pal$cols,\n    trans = \"log\",\n    breaks = c(0.05, 0.1, 1),\n    limits = c(0.05, 1)\n  ) +\n  guides(\n    fill = guide_colourbar(\n      title = \"Chl-a [mg/m3]\",\n      title.position = \"top\",\n      direction = \"horizontal\",\n      barwidth = 8\n    )\n  ) +\n  scale_x_continuous(breaks = c(-28.6, -27.8)) +\n  scale_y_continuous(breaks = c(38.2, 38.8)) +\n  coord_sf(\n    xlim = c(c_lonmin, c_lonmax),\n    ylim = c(c_latmin, c_latmax),\n    expand = FALSE\n  ) +\n  labs(title = \"Seasonal chl-a climatology\",\n       subtitle = \"Central group\",\n       x = NULL, y = NULL) +\n  theme_map() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(vars(month), ncol = 3)\n\n\n\nFigure 7: Monthly climatological median chl-a concentrations over the period 2003-01-01 to 2022-07-27 for the central island group comprised of Ilha do Faial, Ilha do Pico, São Jorge, Graciosa, and Ilha Terceira.\n\n\n\n\nchl_data |&gt;\n  filter(between(longitude, e_lonmin, e_lonmax),\n         between(latitude, e_latmin, e_latmax)) |&gt;\n  mutate(month = month(time, label = TRUE)) |&gt;\n  group_by(longitude, latitude, month) |&gt;\n  summarise(med_chlorophyll = median(chlorophyll, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  ggplot() +\n  geom_tile(aes(x = longitude, y = latitude, fill = med_chlorophyll)) +\n  geom_point(\n    data = sights,\n    aes(x = lon, y = lat),\n    colour = \"black\",\n    shape = 4,\n    size = 0.5\n  ) +\n  geom_sf(\n    data = world_ne,\n    col = \"white\",\n    fill = \"grey50\",\n    linewidth = 0.4\n  ) +\n  scale_fill_gradientn(\n    colours = pal$cols,\n    trans = \"log\",\n    breaks = c(0.05, 0.1, 1),\n    limits = c(0.05, 1)\n  ) +\n  guides(\n    fill = guide_colourbar(\n      title = \"Chl-a [mg/m3]\",\n      title.position = \"top\",\n      direction = \"horizontal\",\n      barwidth = 8\n    )\n  ) +\n  scale_x_continuous(breaks = c(-25.8, -25.2)) +\n  scale_y_continuous(breaks = c(37.6, 37.9)) +\n  coord_sf(\n    xlim = c(e_lonmin, e_lonmax),\n    ylim = c(e_latmin, e_latmax),\n    expand = FALSE\n  ) +\n  labs(\n    title = \"Seasonal chl-a climatology\",\n    subtitle = \"Eastern group\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_map() +\n  theme(\n    legend.position = \"bottom\",\n  ) +\n  facet_wrap(vars(month), ncol = 3)\n\n\n\nFigure 8: Monthly climatological median chl-a concentrations over the period 2003-01-01 to 2022-07-27 for the eastern island group comprised of Ilha do São Miguel and São Pedro."
  },
  {
    "objectID": "vignettes/chl_sightings.html#climatology-of-whale-sightings",
    "href": "vignettes/chl_sightings.html#climatology-of-whale-sightings",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n10 ‘Climatology’ of whale sightings",
    "text": "10 ‘Climatology’ of whale sightings\nI create a histogram of Julian days (day of the year), which summarises the times during the year across the observational record when most sightings are observed for the full region, the central group, and the eastern group.\nI also want to create a plot of cl-a concentration with time for the full extent, the central group, and the eastern group. All of these plots will then be displayed in an intuitive manner so that the time of highest chl-a concentration can be displayed next to the timing of whale sightings.\n\n# make labels to use along the x-axis in stead of yday\nfig_labels &lt;-\n  data.frame(date = seq.Date(\n    from = as.Date(\"2020-01-01\"),\n    to = as.Date(\"2020-12-01\"),\n    by = \"2 month\"\n  ))\nfig_labels &lt;- fig_labels |&gt; \n  mutate(yday = yday(date),\n         month = month(date, label = TRUE))\n\n# plot the sightings histograms\na &lt;- sights |&gt;\n  ggplot(aes(x = yday)) +\n  stat_bin(geom = \"step\",\n           binwidth = 14,\n           colour = \"navy\",\n           linewidth = 0.75) + \n  scale_x_continuous(breaks = fig_labels$yday,\n                     labels = fig_labels$month) +\n  labs(x = NULL, y = \"Number of sightings\",\n       title = \"Full region\")\n\nb &lt;- sights |&gt;\n  filter(between(lon, c_lonmin, c_lonmax),\n         between(lat, c_latmin, c_latmax)) |&gt;\n  ggplot(aes(x = yday)) +\n  stat_bin(geom = \"step\",\n           binwidth = 14,\n           colour = \"darkcyan\",\n           linewidth = 0.75) + \n  scale_x_continuous(breaks = fig_labels$yday,\n                     labels = fig_labels$month) +\n  labs(x = NULL, y = \"Number of sightings\",\n       title = \"Central group\")\n\nc &lt;- sights |&gt; \n  filter(between(lon, e_lonmin, e_lonmax),\n         between(lat, e_latmin, e_latmax)) |&gt;\n  ggplot(aes(x = yday)) + \n  scale_x_continuous(breaks = fig_labels$yday,\n                     labels = fig_labels$month) +\n  stat_bin(geom = \"step\",\n           binwidth = 14,\n           colour = \"indianred3\",\n           linewidth = 0.75) +\n  labs(x = NULL, y = \"Number of sightings\",\n       title = \"Eastern group\")\n\n# calculate a cyclic rolling mean with a window width of 30 days\n# for the chlorophyll-a data\nw_width &lt;- 30\nsmooth_fun &lt;- function(data) {\n  chl &lt;- data |&gt;\n    group_by(yday) |&gt;\n    summarise(med_chl = median(chlorophyll, na.rm = TRUE),\n            .groups = \"drop\")\n  \n  chl_pad &lt;-\n    data.frame(yday = rbind(tail(chl[, 1], w_width/2),\n                            chl[, 1],\n                            head(chl[, 1], w_width/2)),\n               chlorophyll = rbind(tail(chl[, -1], w_width/2),\n                                   chl[, -1],\n                                   head(chl[, -1], w_width/2)))\n\n  chl_out &lt;- chl_pad |&gt;\n    mutate(s_chl = RcppRoll::roll_mean(\n      med_chl,\n      n = w_width,\n      fill = NA,\n      align = \"center\"\n    ))\n  return(chl_out[(w_width/2 + 1):(nrow(chl_out) - w_width/2), ])\n}\n\nfull &lt;- smooth_fun(chl_data)\n\ncenter_group &lt;- chl_data |&gt;\n  filter(between(longitude, c_lonmin, c_lonmax),\n         between(latitude, c_latmin, c_latmax)) |&gt; \n  smooth_fun()\n\neast_group &lt;- chl_data |&gt;\n  filter(between(longitude, e_lonmin, e_lonmax),\n         between(latitude, e_latmin, e_latmax)) |&gt; \n  smooth_fun()\n\n# plot the chlorophyll-a data\nd &lt;- ggplot(full, aes(x = yday)) +\n  geom_line(aes(y = med_chl), colour = \"navy\") +\n  geom_line(\n    aes(y = s_chl),\n    colour = \"yellow\",\n    alpha = 0.9,\n    linewidth = 0.7\n  ) + \n  scale_x_continuous(breaks = fig_labels$yday,\n                     labels = fig_labels$month) +\n  labs(title = \"Full region\",\n       x = NULL,\n       y = expression(Chl-a~(mg.m^-3)))\n\ne &lt;- ggplot(center_group, aes(x = yday)) +\n  geom_line(aes(y = med_chl), colour = \"darkcyan\") +\n  geom_line(\n    aes(y = s_chl),\n    colour = \"yellow\",\n    alpha = 0.9,\n    linewidth = 0.7\n  ) + \n  scale_x_continuous(breaks = fig_labels$yday,\n                     labels = fig_labels$month) +\n  labs(title = \"Central group\",\n       x = NULL,\n       y = expression(Chl-a~(mg.m^-3)))\n\nf &lt;- ggplot(east_group, aes(x = yday)) +\n  geom_line(aes(y = med_chl), colour = \"indianred3\") +\n  geom_line(\n    aes(y = s_chl),\n    colour = \"yellow\",\n    alpha = 0.9,\n    linewidth = 0.7\n  ) + \n  scale_x_continuous(breaks = fig_labels$yday,\n                     labels = fig_labels$month) +\n  labs(title = \"Eastern group\",\n       x = NULL,\n       y = expression(Chl-a~(mg.m^-3)))\n    \nggarrange(d, e, f, a, b, c, align = \"hv\",\n          nrow = 2, ncol = 3,\n          labels = \"AUTO\")\n\n\n\nFigure 9: The annual course of sightings (D-F) and the corresponding onset of the chlorophyll-a maximum (A-C).\n\n\n\nWhat is the timing of the peak sightings and chl-a maximum? Visser et al. (2011) found that peak abundances of the blue Balaenoptera musculus, fin B. physalus, humpback Megaptera novaeangliae and sei whale B. borealis occurred from April to May, tracking the onset of the spring bloom by 13 to 16 wk (depending on species). The lag period accounts for the development of the whales’ zooplankton prey, which is positioned at a trophic position intermediate between phytoplankton and whales. Similar findings were obtained in this study (Table 1), although no distinction was made between cetacean species.\n\n\n\n\n\n\n\n\nTable 1: Timing of the chlorophyll-a maximum and the peak day of sightings\n    \n\nChlorophyll-a maximum and sightings timing are day of the year, and for lag it is the number of days between the chlorophyll-a maximum and peak observations\n    \n\n\n\nExtent\n      \n        Day of the year\n      \n      Lag,(days)\n    \n\nChlorophyll-a\n      Sightings\n    \n\n\n\nFull extent\n72\n126\n54\n\n\nCentral group\n81\n126\n45\n\n\nEastern group\n77\n140\n63"
  },
  {
    "objectID": "vignettes/chl_sightings.html#finding-the-nearest-chl-a-pixels-to-the-whale-sightings",
    "href": "vignettes/chl_sightings.html#finding-the-nearest-chl-a-pixels-to-the-whale-sightings",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n11 Finding the nearest chl-a pixels to the whale sightings",
    "text": "11 Finding the nearest chl-a pixels to the whale sightings\nThis analysis is continued in Spatial Localisation, Subsetting, and Aggregation of the Chlorophyll-a Data."
  },
  {
    "objectID": "vignettes/chl_sightings.html#references",
    "href": "vignettes/chl_sightings.html#references",
    "title": "Plotting the Whale Sightings and Chlorophyll-a Concentrations",
    "section": "\n12 References",
    "text": "12 References\n\n\nGonzález Garcı́a L, Pierce GJ, Autret E, Torres-Palenzuela JM (2018) Multi-scale habitat preference analyses for azorean blue whales. PLoS One 13:e0201786.\n\n\nSantos M, Moita M, Bashmachnikov I, Menezes G, Carmo V, Loureiro C, Mendonça A, Silva A, Martins A (2013) Phytoplankton variability and oceanographic conditions at condor seamount, azores (NE atlantic). Deep Sea Research Part II: Topical Studies in Oceanography 98:52–62.\n\n\nVisser F, Hartman KL, Pierce GJ, Valavanis VD, Huisman J (2011) Timing of migratory baleen whales at the azores in relation to the north atlantic spring bloom. Marine Ecology Progress Series 440:267–279."
  },
  {
    "objectID": "vignettes/MHW_MCS_horizonplots.html",
    "href": "vignettes/MHW_MCS_horizonplots.html",
    "title": "Event horizon plots",
    "section": "",
    "text": "Horizon plots provide a snooty, impactful approach for showing patterns in time series. Because they can be set up to highlight events that occur at certain thresholds, they can be used to show the extreme temperature thresholds as per Hobday et al. (2018) and as shown in Robert Schlegel’s post.\nHorizon plots are a type of visualisation technique used to display time series data, particularly when there are multiple overlapping series or when the data have a wide range of values. They are an extension of the traditional line plot and are particularly useful when dealing with large datasets with numerous data points or when trying to visualise data with both large and small variations in value.\nIn a horizon plot, the data are first divided into bands or layers, which can be either equally spaced or defined by the user. Each layer represents a specific range of data values. The layers are then colour-coded, with the intensity of the colour corresponding to the magnitude of the physical quantity represented by the data within each layer. Next, the layers are collapsed, or overlaid, on top of each other to create a single, compact visualisation.\nThe primary advantage of horizon plots is that they can display a large amount of data in a small space, making it easier to identify trends, patterns, and extremes. By using colour and layering, horizon plots can reveal variations in the data that might be difficult to discern in other types of plots. Additionally, they can provide a clearer view of multiple time series when they are overlapping or have different magnitudes.\n\nHowever, standard horizon plots can also be challenging to interpret for those unfamiliar with the technique. The layering and colour-coding can sometimes make it difficult to determine the exact values of the data points, especially when there are many overlapping layers. These graphs are therefore recommended as a first stab view into the patterns contained within the data, and more a fit-for-purpose plots such as an event_lines() is necessary when the deeper insight into the extreme event metrics is required.\nThe purpose of this vignette is to take inspiration from horizon plots and to create a figure that can be used to visualise extreme events along a long (~40 yr) time series of data. I call them event horizon plots. At this stage I have not been able to create a visual that is a beautiful as the horizon plots of the ggHoriPlot package: that would require modification of the horizon plot geom as it accepts constant thresholds (cut points) whereas heatwaveR works with daily-varying thresholds and categories. So, event horizon plots do not collopse the layers as standard horizon plots do, and they have a time varying baseline. They realy are only a compressed view of normal event lines and geom_flame() (as per heatwaveR). In the end, the idea is not too far different from standard horizon plots. They are the same but different."
  },
  {
    "objectID": "vignettes/MHW_MCS_horizonplots.html#calculate-extreme-events",
    "href": "vignettes/MHW_MCS_horizonplots.html#calculate-extreme-events",
    "title": "Event horizon plots",
    "section": "Calculate extreme events",
    "text": "Calculate extreme events\nLoad the packages and the data. The data are the for a region off Northwest Africa in the Canary Current System. The spatial extent of the data is displayed below.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(ggHoriPlot)\nlibrary(heatwaveR)\nlibrary(ggthemes)\nlibrary(doParallel) # For parallel processing\n\nsource(\"../R/extreme_event_horizon.R\")\n\nDefine a colour scheme for the figure:\n\n# Set line colours\nlineColCat &lt;- c(\n  \"Daily\" = \"grey40\",\n  \"Climatology\" = \"darkseagreen2\",\n  \"Threshold (90)\" = \"red3\",\n  \"Threshold (10)\" = \"blue3\"\n  )\n\n# Set category fill colours\nfillColCat &lt;- c(\n  \"+ Extreme\" = \"#2d0000\",\n  \"+ Severe\" = \"#9e0000\",\n  \"+ Strong\" = \"#ff6900\",\n  \"+ Moderate\" = \"#ffc866\",\n  \"- Moderate\" = \"#C7ECF2\",\n  \"- Strong\" = \"#85B7CC\",\n  \"- Severe\" = \"#4A6A94\",\n  \"- Extreme\" = \"#111433\"\n  )\n\nPrepare the data: I use the built-in sst_WA dataset with heatwaveR:\n\nevents &lt;- thresh_fun(sst_WA)\n\n\nhorizon_plot(events, title = \"Extreme temperature timeline, Western Australia\")"
  },
  {
    "objectID": "vignettes/heatwaveR_issues.html",
    "href": "vignettes/heatwaveR_issues.html",
    "title": "heatwaveR issues",
    "section": "",
    "text": "# devtools::install_github(\"robwschlegel/heatwaveR\")\nlibrary(tidyverse)\nlibrary(heatwaveR)\n# session_info()\n\n\n# heatwaves\nclm1 &lt;- ts2clm(sst_WA, climatologyPeriod = c(\"1982-01-01\", \"2011-12-31\"))\n\nApplying detect_event(..., protoEvent = FALSE) (the default) returns protoEvents as part of the climatology even if they were not requested. The protoEvents must only be returned when protoEvents = TRUE.\n\n# heatwaves\nev1 &lt;- detect_event(clm1)\n\nWhen detect_event(..., protoEvent = TRUE), a climatology with the protoEvents is returned without the associated detected events (expected behaviour upheld).\n\n# heatwaves\nev2 &lt;- detect_event(clm1, protoEvents = TRUE)\n\nBelow, swithing on climatology = TRUE returns the events in a long dataframe, but it is not actually a climatology. From the helpfile:\n\n“If set to TRUE, this function will return a list of two dataframes, same as detect_event. The first dataframe climatology, contains the same information as found in detect_event, but with the addition of the daily intensity (anomaly above seasonal doy threshold) and category values.”\n\nBelow, the ‘climatology’ returned by category() is a truncated climatology since dates and associated temperatures on the dates when events were not detected are not present. The climatology returned by detect_event() has ALL the data, from the first day of the raw data time series right through to the last. Either the help file must be updated, or the full climatology must be returned.\n\ncat1 &lt;- category(ev1, name = \"WA\", climatology = TRUE)\n\nThere is an issue with detect_event(..., categories = TRUE) when fed a climatology created from a temperature time series which does not have the standard name, temp:\n\nsst &lt;- sst_Med |&gt; \n  rename(temperature = temp)\n\nclm2 &lt;- ts2clm(sst, y = temperature, climatologyPeriod = c(\"1982-01-01\", \"2011-12-31\"))\n\n# events are named and seasons are present, but columns `p_moderate`, `p_strong`, `p_severe` and `p_extreme` are empty\nev3 &lt;- detect_event(clm2, y = temperature, categories = TRUE)\n\n# here the expected behaviour the above columns is observed\nev4 &lt;- detect_event(clm2, y = temperature)\ncat2 &lt;- category(ev4, y = temperature)\n\n# using the climatology that has the name `temp` works fine when the categories are requested as part of the `detect_event()` function\nev5 &lt;- detect_event(clm1, categories = TRUE)\n\nLastly, for the S = TRUE switch in category(), what happens when the data straddle the equator? Also, there is no way to specify the S or N hemispheres when categories are requested as part of detect_event() so this should probably be added.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit,\n  author = {Smit, AJ and Smit, AJ},\n  title = {heatwaveR Issues},\n  date = {},\n  url = {https://tangledbank.netlify.app/vignettes/heatwaveR_issues.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A heatwaveR issues. https://tangledbank.netlify.app/vignettes/heatwaveR_issues.html."
  },
  {
    "objectID": "assessments/Intro-R_CA_Summative Tasks.html",
    "href": "assessments/Intro-R_CA_Summative Tasks.html",
    "title": "BCB744: Summative End-of-Intro-R Task",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/Intro-R_CA_Summative Tasks.html#honesty-pledge",
    "href": "assessments/Intro-R_CA_Summative Tasks.html#honesty-pledge",
    "title": "BCB744: Summative End-of-Intro-R Task",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/Intro-R_CA_Summative Tasks.html#format-and-mode-of-submission",
    "href": "assessments/Intro-R_CA_Summative Tasks.html#format-and-mode-of-submission",
    "title": "BCB744: Summative End-of-Intro-R Task",
    "section": "Format and mode of submission",
    "text": "Format and mode of submission\nThis Assignment requires submission as both a Quarto (.qmd) file and the knitted .html product. You are welcome to copy any text from here to use as headers or other pieces of informative explanation to use in your Assignment."
  },
  {
    "objectID": "assessments/Intro-R_CA_Summative Tasks.html#style-and-organisation",
    "href": "assessments/Intro-R_CA_Summative Tasks.html#style-and-organisation",
    "title": "BCB744: Summative End-of-Intro-R Task",
    "section": "Style and organisation",
    "text": "Style and organisation\nAs part of the assessment, we will look for a variety of features, including, but not limited to the following:\n\nContent:\n\nQuestions answered in order\nA written explanation of approach included for each question\nAppropriate formatting of text, for example, fonts not larger than necessary, headings used properly, etc. Be sensible and tasteful.\n\n\nCode formatting:\n\nUse Tidyverse code and style conventions\n\nNo more than ~80 characters of code per line (pay particular attention to the comments)\nApplication of R code conventions, e.g. spaces around &lt;-, after #, after ,, etc.\nNew line for each dplyr function (lines end in %&gt;%) or ggplot layer (lines end in +)\nProper indentation of pipes and ggplot() layers\nAll chunks labelled without spaces\nNo unwanted / commented out code left behind in the document\n\n\nFigures:\n\nSensible use of themes / colours\nPublication quality\nInformative and complete titles, axes labels, legends, etc.\nNo redundant features or aesthetics"
  },
  {
    "objectID": "assessments/Intro-R_CA_Summative Tasks.html#packages",
    "href": "assessments/Intro-R_CA_Summative Tasks.html#packages",
    "title": "BCB744: Summative End-of-Intro-R Task",
    "section": "Packages",
    "text": "Packages\nFor this assignment, you will have to install the nycflights13 package. The package contains the dataset flights and some associated meta-data, all of which you need to complete the questions below. You will also need tidyverse and ggpubr\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(nycflights13)"
  },
  {
    "objectID": "assessments/Intro-R_CA_Summative Tasks.html#questions",
    "href": "assessments/Intro-R_CA_Summative Tasks.html#questions",
    "title": "BCB744: Summative End-of-Intro-R Task",
    "section": "Questions",
    "text": "Questions\nQuestion 1\nInsert Task G which can be found here.\nQuestion 2\nWhat are the 10 most common destinations for flights from NYC airports in 2013, and what is the total distance travelled to each of these airports? Make a 2-panel figure and display these data graphically.\n\ntab1 &lt;- flights %&gt;%\n  group_by(dest) %&gt;%  \n  summarise(n = n(),\n            total_distance = sum(distance)) %&gt;% \n  arrange(desc(n)) %&gt;%\n  slice(1:10)\n\nplt1 &lt;- ggplot(tab1) +\n  geom_col(aes(x = dest, y = n), fill = \"grey90\", alpha = 0.7,\n           colour = \"indianred\") +\n  theme_minimal() +\n  labs(x = \"Airport code\",\n       y = \"Number of\\ndestimations\")\nplt2 &lt;- ggplot(tab1) +\n  geom_col(aes(x = dest, y = total_distance * 0.621371), # convert to km\n           fill = \"grey90\", alpha = 0.7, colour = \"indianred\") +\n  theme_minimal() +\n  labs(x = \"Airport code\",\n       y = \"Total\\ndistance (km)\")\nggarrange(plt1, plt2, ncol = 1, labels = \"AUTO\")\n\n\n\nThe figure for Question 2.\n\n\n\nQuestion 3\nWhich airlines have the most flights departing from NYC airports in 2013? Make a table that lists these in descending order of frequency and shows the number of flights for each airline. In your table, list the names of the airlines as well. Hint: you can use the airlines dataset to look up the airline name based on carrier code.\n\npopular_destinations &lt;- flights %&gt;%\n  count(carrier) %&gt;%\n  arrange(desc(n)) %&gt;%\n  inner_join(airlines, by = \"carrier\") %&gt;% \n  as_tibble()\nhead(popular_destinations, n = 16)\n\n# A tibble: 16 × 3\n   carrier     n name                       \n   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;                      \n 1 UA      58665 United Air Lines Inc.      \n 2 B6      54635 JetBlue Airways            \n 3 EV      54173 ExpressJet Airlines Inc.   \n 4 DL      48110 Delta Air Lines Inc.       \n 5 AA      32729 American Airlines Inc.     \n 6 MQ      26397 Envoy Air                  \n 7 US      20536 US Airways Inc.            \n 8 9E      18460 Endeavor Air Inc.          \n 9 WN      12275 Southwest Airlines Co.     \n10 VX       5162 Virgin America             \n11 FL       3260 AirTran Airways Corporation\n12 AS        714 Alaska Airlines Inc.       \n13 F9        685 Frontier Airlines Inc.     \n14 YV        601 Mesa Airlines Inc.         \n15 HA        342 Hawaiian Airlines Inc.     \n16 OO         32 SkyWest Airlines Inc.      \n\n\n\nQuestion 4\nConsider only flights that have non-missing arrival delay information. Your answer should include the name of the carrier in addition to the carrier code and the values asked.\n\nWhich carrier had the highest mean arrival delay?\nWhich carrier had the lowest mean arrival delay?\n\nMake sure that your answer includes the name of the carrier and the calculated mean (±SD) delay times, and use a sensible number of decimal digits.\n\nflights %&gt;%\n  filter(!is.na(arr_delay)) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(mean_arr_delay = round(mean(arr_delay), 1),\n            sd_arr_delay = round(sd(arr_delay), 1)) %&gt;%\n  arrange(desc(mean_arr_delay)) %&gt;%\n  inner_join(airlines, by = \"carrier\") %&gt;%\n  slice(c(1, n())) %&gt;% \n  as_tibble()\n\n# A tibble: 2 × 4\n  carrier mean_arr_delay sd_arr_delay name                  \n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                 \n1 F9                21.9         61.6 Frontier Airlines Inc.\n2 AS                -9.9         36.5 Alaska Airlines Inc.  \n\n\nThe longest arrival delay was on Frontier Airlines that, on average, arrived 21.9 ± 61.5 (mean ± SD) minutes late. On the contrary, Alaska Airlines typically arrived earlier than anticipated by 9.9 ± 36.5 (mean ± SD) minutes.\nQuestion 5\nWhat were the mean values for the weather variables at the origin airport on the top 10 days with the highest departure delays? Contrast this with a similar view on the 10 days with the lowest departure delays. Your table(s) should include the names of origin airports, the dates with the highest (lowest) departure delays, and the mean (±SD) weather variables on these days.\nCan you make any inferences about the effect of weather conditions on flight delays? Are there any problems with this analysis, and how might you improve this analysis for a clearer view of the effect of weather conditions on the ability of flights to depart on time?\n\ntop_delay &lt;- bind_rows(\n    flights %&gt;% slice_max(order_by = dep_delay, n = 10),\n    flights %&gt;% slice_min(order_by = dep_delay, n = 10),\n) %&gt;% \n  select(carrier, flight, tailnum, dep_delay, month, day, origin) %&gt;%\n  inner_join(weather, by = c(\"origin\", \"month\", \"day\")) %&gt;% \n  pivot_longer(cols = temp:visib,\n               names_to = \"weather_var\",\n               values_to = \"value\") %&gt;% \n  na.omit() %&gt;% \n  group_by(carrier, flight, tailnum, dep_delay, weather_var) %&gt;% \n  summarise(mean_weather_var = round(mean(value, na.rm = TRUE), ),\n            sd_weather_var = round(sd(value, na.rm = TRUE), 1)) %&gt;% \n  unite(\"mean_sd\", mean_weather_var:sd_weather_var, sep = \" ± \") %&gt;% \n  arrange(desc(dep_delay)) %&gt;% \n  pivot_wider(names_from = weather_var, values_from = mean_sd) %&gt;% \n  as_tibble()\ntop_delay\n\n# A tibble: 22 × 13\n   carrier flight tailnum dep_delay dewp     humid   precip pressure temp  visib\n   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n 1 HA          51 N384HA       1301 35 ± 2.7 74 ± 1… 0 ± 0  1026 ± … 43 ±… 10 ±…\n 2 MQ        3535 N504MQ       1137 53 ± 4.5 57 ± 1… 0 ± 0  1014 ± … 71 ±… 10 ±…\n 3 MQ        3695 N517MQ       1126 24 ± 2.8 48 ± 1… 0 ± 0  1031 ± … 44 ±… 10 ±…\n 4 AA         177 N338AA       1014 57 ± 2.1 73 ± 1… 0 ± 0  1018 ± 1 66 ±… 10 ±…\n 5 MQ        3075 N665MQ       1005 73 ± 1.2 83 ± 1… 0 ± 0  1013 ± … 79 ±… 9 ± 1\n 6 DL        2391 N959DL        960 47 ± 2.8 68 ± 1… 0 ± 0  1013 ± … 58 ±… 10 ±…\n 7 DL        2119 N927DA        911 17 ± 5.2 50 ± 14 0 ± 0  1020 ± 4 35 ±… 10 ±…\n 8 DL        2007 N3762Y        899 70 ± 1.1 83 ± 9  0 ± 0  1008 ± … 76 ±… 9 ± …\n 9 DL        2047 N6716C        898 72 ± 1.4 84 ± 3… 0 ± 0  1012 ± … 77 ±… 9 ± …\n10 AA         172 N5DMAA        896 51 ± 3.9 93 ± 6… 0 ± 0  1016 ± … 54 ±… 3 ± …\n# ℹ 12 more rows\n# ℹ 3 more variables: wind_dir &lt;chr&gt;, wind_speed &lt;chr&gt;, wind_gust &lt;chr&gt;\n\n\nNothing obvious I can see about the effect of weather variables in affecting the departure delay. We would need to do some multivariate stats to assess.\nQuestion 6\nPartition each day into four equal time intervals, e.g. 00:01-06:00, 06:01-12:00, 12:01-18:00, and 18:01-00:00.\n\nAt each time interval, what is the proportion of flights delayed at departure? Illustrate your finding in a figure.\n\n\n# Create time of day variable\nflights_tod &lt;- flights %&gt;%\n  mutate(time_of_day = case_when(\n    sched_dep_time &gt;= 001  & sched_dep_time &lt;= 600  ~ \"00:01-06:00\",\n    sched_dep_time &gt;= 601  & sched_dep_time &lt;= 1200 ~ \"06:01-12:00\",\n    sched_dep_time &gt;= 1201 & sched_dep_time &lt;= 1800 ~ \"12:01-18:00\",\n    sched_dep_time &gt;= 1801                          ~ \"18:01-00:00\"\n  )) %&gt;% \n  mutate(day_of_week = wday(time_hour))\n\n# Find proportion of delayed flights for each time of day\ndelay_time &lt;- flights_tod %&gt;%\n  filter(!is.na(dep_delay)) %&gt;%\n  mutate(dep_delayed = ifelse(dep_delay &gt; 0, \"delayed\", \"ontime\")) %&gt;%\n  count(time_of_day, dep_delayed) %&gt;%\n  group_by(time_of_day) %&gt;%\n  mutate(prop_delayed = n / sum(n)) %&gt;%\n  filter(dep_delayed == \"delayed\") %&gt;%\n  arrange(prop_delayed) %&gt;% \n  as_tibble()\nhead(delay_time)\n\n# A tibble: 4 × 4\n  time_of_day dep_delayed     n prop_delayed\n  &lt;chr&gt;       &lt;chr&gt;       &lt;int&gt;        &lt;dbl&gt;\n1 00:01-06:00 delayed      1819        0.207\n2 06:01-12:00 delayed     32466        0.260\n3 12:01-18:00 delayed     58325        0.463\n4 18:01-00:00 delayed     35822        0.520\n\n\n\nggplot(delay_time, aes(x = time_of_day, y = prop_delayed)) +\n  geom_col(fill = \"grey90\", alpha = 0.7, colour = \"indianred\") +\n  theme_minimal() +\n  labs(x = \"Time of day\",\n       y = \"Proportion of\\nflights delayed\")\n\n\n\nThe figure for Question 6a.\n\n\n\nAbout 21% of flights are delayed between midnight and 6:00, 26% are delayed between 6:00-12:00, 46% delays between 12:00-18:00, and 52% delays between 18:00pm and midnight. As the day progresses, the better the chance is of there being a delay.\n\nBased on your analysis, does the chance of being delayed change throughout the day?\n\nSee answer to Q.6a.\n\nFor each weekday (1-7) aggregated over 2013, which of the time intervals has the most flights? Create a figure to show your finding.\n\n\nflights_tod %&gt;% \n  group_by(time_of_day, day_of_week) %&gt;% \n  summarise(n_flights = n()) %&gt;% \n  ggplot(aes(x = day_of_week, y = n_flights)) +\n    geom_col(fill = \"grey90\", alpha = 0.7, colour = \"indianred\") +\n    theme_minimal() +\n    labs(x = \"Time of day\",\n         y = \"Number of\\nflights\") +\n    facet_wrap(~time_of_day)\n\n\n\nThe figure for Question 6c.\n\n\n\nMidnight to 6-am has the fewest flights, regardless of what day of the week we are looking at – although Sundays (day 1) and Saturday (day 7) have slightly fewer flights. Similarly, Sun?Sat have slightly fewer flights between 6-am to 6-pm, although the total number of flights are much higher (closer to 20000 flights per day). Evenings from 6-pm to midnight the flights decrease in numbers, and Saturdays have fewer flights during this time than other days.\nQuestion 7\nFind the 10 planes that spend the longest time (cumulatively) in the air.\n\nFor each model, what are the cumulative and mean flight times? In this table, also mention their type, manufacturer, model, number of engines, and speed.\n\n\ncum_flights &lt;- flights %&gt;%\n  group_by(tailnum) %&gt;% \n  summarise(cum_air_time = sum(air_time),\n            mean_air_time = round(mean(air_time, na.rm = TRUE), 1)) %&gt;% \n  slice_max(order_by = cum_air_time, n = 10) %&gt;% \n  inner_join(planes, by = \"tailnum\") %&gt;%\n  select(tailnum, cum_air_time, mean_air_time, type, manufacturer, model, engines, speed) %&gt;% \n  as_tibble()\nhead(cum_flights)\n\n# A tibble: 6 × 8\n  tailnum cum_air_time mean_air_time type       manufacturer model engines speed\n  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n1 N502UA         97320          340. Fixed win… BOEING       757-…       2    NA\n2 N512UA         95943          341. Fixed win… BOEING       757-…       2    NA\n3 N505UA         95591          339  Fixed win… BOEING       757-…       2    NA\n4 N557UA         87371          337. Fixed win… BOEING       757-…       2    NA\n5 N518UA         80772          341. Fixed win… BOEING       757-…       2    NA\n6 N508UA         79998          336. Fixed win… BOEING       757-…       2    NA\n\n\n\nCreate a table that lists, for each air-plane identified in (a.), each flight (and associated destination) that it undertook during 2013.\n\n\nflight_dest &lt;- flights %&gt;% \n  filter(tailnum %in% cum_flights$tailnum) %&gt;% \n  select(tailnum, origin, dest, time_hour) %&gt;% \n  as_tibble()\nhead(flight_dest)\n\n# A tibble: 6 × 4\n  tailnum origin dest  time_hour          \n  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;dttm&gt;             \n1 N518UA  JFK    LAX   2013-01-01 11:00:00\n2 N502UA  JFK    SFO   2013-01-01 14:00:00\n3 N512UA  JFK    LAX   2013-01-01 15:00:00\n4 N557UA  JFK    SFO   2013-01-01 17:00:00\n5 N508UA  JFK    SFO   2013-01-01 18:00:00\n6 N591JB  JFK    PSE   2013-01-01 23:00:00\n\n\n\nSummarise all the in formation in (b.) on a map of the USA. Use lines to connect departure and destination locations (each labelled). Different facets in the figure must be used for each of the 10 planes. You can use the alpha value in ggplot2 such that the colour intensity of overlapping flight lines is proportional to the number of flights taken along the path. For bonus marks, ensure that the curvature of Earth is indicated in the flight lines. Hint: such lines would display as curves, not straight lines.\n\n\n# to be done in due course...\n\nQuestion 8\nLimit this analysis to only the coldest three winter and warmest three summer months (show evidence for how this is decided). For each of these two seasons, create a visualisation to explore if there is a relationship between the mean daily departure delay and the mean daily temperature. Be as economical with your code as possible.\nDiscuss your answer.\n\nseas &lt;- weather %&gt;% \n  group_by(month) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = TRUE), 1)) %&gt;% \n  as_tibble()\nseas\n\n# A tibble: 12 × 2\n   month mean_temp\n   &lt;int&gt;     &lt;dbl&gt;\n 1     1      35.6\n 2     2      34.3\n 3     3      39.9\n 4     4      51.7\n 5     5      61.8\n 6     6      72.2\n 7     7      80.1\n 8     8      74.5\n 9     9      67.4\n10    10      60.1\n11    11      45  \n12    12      38.4\n\nggplot(seas, aes(x = month, y = (mean_temp - 32)*5/9)) +\n  geom_col(fill = \"grey90\", alpha = 0.7, colour = \"indianred\") +\n  scale_x_continuous(breaks = seq(2, 12, by = 2)) +\n  theme_minimal() +\n  labs(x = \"Month\",\n       y = \"Mean temp. (°C)\")\n\n\n\n\nThe coldest months are December, January, and February. The warmest time of year is during June, July, August.\n\nflights %&gt;% \n  filter(month %in% c(12, 1, 2, 6, 7, 8)) %&gt;% \n  inner_join(weather, by = c(\"origin\", \"month\", \"day\")) %&gt;% \n  group_by(month) %&gt;% \n  summarise(mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n            mean_temp = round(mean(temp, na.rm = TRUE), 1)) %&gt;% \n  mutate(seas = c(\"winter\", \"winter\", \"summer\", \"summer\", \"summer\", \"winter\")) %&gt;% \n  ggplot(aes(x = (mean_temp - 32)*5/9, y = mean_dep_delay)) +\n    geom_point(aes(col = seas)) +\n  theme_minimal() +\n  labs(x = \"Mean temp. (°C)\", y = \"Mean delay (min)\")\n\n\n\n\nIt seems that, in general, shorter delays are experienced during winter months. To fully assess the effect of weather variables on delays, a more detailed statistical analysis will be required."
  },
  {
    "objectID": "assessments/Intro-R_CA_Summative Tasks.html#submission-instructions",
    "href": "assessments/Intro-R_CA_Summative Tasks.html#submission-instructions",
    "title": "BCB744: Summative End-of-Intro-R Task",
    "section": "Submission instructions",
    "text": "Submission instructions\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit your .qmd and .html files wherein you provide answers to these Questions by no later than 1 March 2023 at 23:59.\nLabel the files as follows:\n\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Final.qmd, and\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Final.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Tasks on the Google Form when ready."
  },
  {
    "objectID": "assessments/Intro-R_CA_Assessment.html",
    "href": "assessments/Intro-R_CA_Assessment.html",
    "title": "BCB744: End-of-Intro-R Assessment",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/Intro-R_CA_Assessment.html#honesty-pledge",
    "href": "assessments/Intro-R_CA_Assessment.html#honesty-pledge",
    "title": "BCB744: End-of-Intro-R Assessment",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/Intro-R_CA_Assessment.html#format-and-mode-of-submission",
    "href": "assessments/Intro-R_CA_Assessment.html#format-and-mode-of-submission",
    "title": "BCB744: End-of-Intro-R Assessment",
    "section": "Format and mode of submission",
    "text": "Format and mode of submission\nThis Assignment requires submission as both a Quarto (.qmd) file and the knitted .html product. You are welcome to copy any text from here to use as headers or other pieces of informative explanation to use in your Assignment."
  },
  {
    "objectID": "assessments/Intro-R_CA_Assessment.html#style-and-organisation",
    "href": "assessments/Intro-R_CA_Assessment.html#style-and-organisation",
    "title": "BCB744: End-of-Intro-R Assessment",
    "section": "Style and organisation",
    "text": "Style and organisation\nAs part of the assessment, we will look for a variety of features, including, but not limited to the following:\n\nContent:\n\nQuestions answered in order\nA written explanation of approach included for each question\nAppropriate formatting of text, for example, fonts not larger than necessary, headings used properly, etc. Be sensible and tasteful.\n\n\nCode formatting:\n\nUse Tidyverse code\nNo more than ~80 characters of code per line (pay particular attention to the comments)\nApplication of R code conventions, e.g. spaces around &lt;-, after #, after ,, etc.\nNew line for each dplyr function (lines end in %&gt;%) or ggplot layer (lines end in +)\nProper indentation of pipes and ggplot() layers\nAll chunks labelled without spaces\nNo unwanted / commented out code left behind in the document\n\n\nFigures:\n\nSensible use of themes / colours\nPublication quality\nInformative and complete titles, axes labels, legends, etc.\nNo redundant features or aesthetics"
  },
  {
    "objectID": "assessments/Intro-R_CA_Assessment.html#questions",
    "href": "assessments/Intro-R_CA_Assessment.html#questions",
    "title": "BCB744: End-of-Intro-R Assessment",
    "section": "Questions",
    "text": "Questions\nQuestion 1\nThe shells.csv data\n\nProduce a tidy dataset from the data contained in shells.csv.\nFor each species, relate two measurement variables within the dataset to one-another and represent the relationship with a straight line.\nFor each species, concisely produce histograms for each of the measurement variables.\nUse the colorspace package and assign interesting colours to your graphs (all graphs above).\nUse the ggthemr package and assign interesting themes to your graphs (all graphs above).\nQuestion 2\nHead Dimensions in Brothers\nThe boot::frets data: The data consist of measurements of the length and breadth of the heads of pairs of adult brothers in 25 randomly sampled families. All measurements are expressed in millimetres.\nPlease consult the dataset’s help file (i.e., load the package boot package and type ?frets on the command line).\n\nCreate a tidy dataset from the frets data.\nDemonstrate the most concise way for displaying both brother’s data on one set of axes.\nApply your own unique theme modification to the graph in order to produce a publication-worthy figure.\nQuestion 3\nResults from an Experiment on Plant Growth\nThe datasets::PlantGrowth data: Results from an experiment to compare yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions.\n\nConcisely present the results of the plant growth experiment as graphs:\n\na scatterplot with individual weight datapoints as a function of group\n\na box and whisker plot showing each group (on one set of axes)\na bar plot with associated SD for each group (on one set of axes)\n\n\nQuestion 4\nStudent’s Sleep Data\nThe datasets::sleep data: Data which show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients.\n\nGraphically display these data in two different ways.\nQuestion 5\nEnglish Narrative for Some Code\n\nProvide an English description for what the following lines of code does.\n\nListing 1\n\nthe_data &lt;- some_data %&gt;%\n  mutate(yr = year(date),\n         mo = month(date)) %&gt;% \n  group_by(country, yr) %&gt;% \n  summarise(med_chl = mean(chl, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\nggplot(the_data, aes(x = yr, y = med_chl)) +\n  geom_line(aes(group = country), colour = \"blue3\") +\n  facet_wrap(~country, nrow = 3) +\n  labs(x = \"Year\", y = \"Chlorophyll-a (mg/m3)\",\n       title = \"Chlorophyll-a concentration\")\n\nListing 2\n\nlibrary(ggforce)\nggplot(iris, aes(Petal.Length, Petal.Width, colour = Species)) +\n    geom_point() +\n    facet_zoom(x = Species == \"versicolor\")\n\nListing 3\n\nset.seed(13)\nmy_data = data.frame(\n        gender = factor(rep(c(\"F\", \"M\"), each=200)),\n        length = c(rnorm(200, 55), rnorm(200, 58)))\nhead(my_data)\n\nggplot(my_data, aes(x = gender, y = length)) +\n  geom_boxplot(aes(fill = gender))\n\nggplot(my_data, aes(x = gender, y = length)) +\n  geom_violin()\n\nggplot(my_data, aes(x = gender, y = length)) +\n  geom_dotplot(stackdir = \"center\", binaxis = \"y\", dotsize = 0.5)\n\nQuestion 6\nCreate panels of plots\n\nFor this exercise, you’ll be expected to accomplish Parts 1, 2 and 3 before producing the final output in Part 4.\nConsiderations:\n\ntake care to use the most appropriate geom considering the nature of the data\ncreatively modify the graph’s appearance (but remain sensible and be cognisant of which aesthetics are suitable for publications!)\n\n\n\nPart 1\nThe datasets::AirPassengers data\nMonthly totals of international airline passengers, 1949 to 1960.\n\nlibrary(tidyverse)\nair &lt;- datasets::AirPassengers\n\n# just plot the time series object directly\nplot(air)\n\n\n\n# or make a dataframe and plot with ggplot\nair_df &lt;- data.frame(date = seq.Date(as.Date(\"1949-01-01\"), as.Date(\"1960-12-01\"), by = \"month\"),\n                     number = as.numeric(datasets::AirPassengers))\nggplot(air_df, aes(x = date, y = number)) +\n  geom_line() +\n  xlab(\"Date\") + ylab(\"Number of passengers\") +\n  ggtitle(\"Monthly mean number of passengers\")\n\n\n\n\nConstruct a figure showing the annual number of airline passengers (±SE) from 1949-1960.\n\nair_df |&gt; \n  mutate(year = year(date)) |&gt; \n  group_by(year) |&gt; \n  summarise(mean_number = round(mean(number), 2),\n            SE = round(sd(number)/sqrt(n()), 2),\n            .groups = \"drop\") |&gt; \n  ggplot(aes(x = year, y = mean_number)) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_number - SE, ymax = mean_number + SE),\n                width = 0.1) +\n  xlab(\"Date\") + ylab(\"Number of passengers\") +\n  ggtitle(\"Annual mean number of passengers\")\n\n\n\n\nPart 2\nThe datasets::Loblolly and the datasets::Orange data\nThese are some data collected from two kinds of trees at different ages.\nDevise a figure with a two-panel 2 x 1 (rows x columns) layout showing:\n\nthe relationship between age and height independently for each seed source for the Loblolly data\nthe relationship between age and circumference for each tree\n\n\npines &lt;- datasets::Loblolly\n\n# age vs height\nplt1 &lt;- ggplot(pines, aes(x = age, y = height, colour = Seed)) +\n  geom_line(aes(colour = Seed)) +\n  ylab(\"Height (ft)\") + xlab(\"Age (yr)\")\n  \norange &lt;- datasets::Orange\n  \nplt2 &lt;- ggplot(orange, aes(x = age, y = circumference, colour = Tree)) +\n  geom_line(aes(colour = Tree)) +\n  ylab(\"Circumference (mm)\") + xlab(\"Age (days)\")\n\nggpubr::ggarrange(plt1, plt2, nrow = 2, labels = \"AUTO\")\n\n\n\n\nPart 3\nYour ‘own’ data\n\nFind your own dataset (one that has not been used in this Assessment or earlier in the BCB744 module) and create a pair of faceted figures of your choice.\nProvide an explanation of what you aim to show, and what the figure ultimately tells you.\n\n\n## Make own assessment about validity of this graph\n\nPart 4\nThe last steps\n\nAssemble all graphs (Parts 1-3) into a 2 x 2 layout using a suitable function provided by an appropriate R package. Note that only three of the four facets will be occupied by the figures you created in Parts 1-3.\n\n\n# Again, this is self evident.\n\nQuestion 7\nThe datasets::UKDriverDeaths and datasets::Seatbelts datasets\nThese datasets are meant to be used together—UKDriverDeaths has the same data as is provided in the variable drivers in seatbelts, but it also provides information about the temporal structure of the Seatbelts dataset. You will have to devise a way to use this temporal information in your analysis.\n\nProduce a dataframe that combines the temporal information provided in UKDriverDeaths with the other information in Seatbelts.\nProduce a faceted graph (using facet_wrap(), placing drivers, front, rear, and VanKilled in facets) showing a timeline of monthly means of deaths (means taken across years) whilst distinguishing between the two levels of law.\nWhat do you conclude from your analysis?\n\n\ndeaths &lt;- datasets::UKDriverDeaths\nseatbelts &lt;- datasets::Seatbelts\n\n# checking the data (not necessary)\nplot(deaths)\n\n\n\nplot(seatbelts)\n\n\n\n# make the combined dataframe\ndf &lt;- data.frame(date = seq.Date(as.Date(\"1969-01-01\"), as.Date(\"1984-12-01\"), by = \"month\"),\n                 as.matrix(seatbelts))\n\n# make long dataframe\ndf |&gt; \n  select(-DriversKilled, -kms, -PetrolPrice) |&gt; \n  pivot_longer(cols = c(drivers, front, rear, VanKilled),\n               names_to = \"type\",\n               values_to = \"killed\") |&gt; \n  mutate(year = year(date)) |&gt; \n  group_by(year, type, law) |&gt; \n  summarise(mean_killed = round(mean(killed), 2), .groups = \"drop\") |&gt; \n  ggplot(aes(x = year, y = mean_killed)) +\n    geom_line(colour = \"grey70\") +\n    geom_point(aes(colour = as.factor(law)), size = 0.3) +\n    facet_wrap(~type, ncol = 2, scales = \"free\") +\n    guides(colour = guide_legend(\"Law\")) +\n    theme_minimal() +\n    labs(x = \"Year\", y = \"Mean deaths (count)\",\n         title = \"UK Car Driver and Passenger Deaths\",\n         subtitle = \"Jan 1969 to Dec 1984\")\n\n\n\n\nThe data indicate that mandatory the seatbelt law, introduced on January 31, 1983, appears to have reduced the number of deaths resulting from car crashes for drivers and front passengers of passenger cars, and for the drivers of vans. However, there was no apparent reduction in deaths for rear passengers, likely because the law requiring them to wear seatbelts did not come into effect until 1991.\nIt should be noted, however, that the interpretation of these results is not entirely clear. From 1969 to December 1982, there was a consistent reduction in deaths, which may have been due to other advancements in car safety features and road safety legislation. Additionally, only two data points are available following the introduction of mandatory seatbelt laws in 1983, and more data are needed to determine whether this reduction was statistically significant. Nevertheless, it is worth noting that the reduction did prove to be statistically significant."
  },
  {
    "objectID": "assessments/Intro-R_CA_Assessment.html#submission-instructions",
    "href": "assessments/Intro-R_CA_Assessment.html#submission-instructions",
    "title": "BCB744: End-of-Intro-R Assessment",
    "section": "Submission instructions",
    "text": "Submission instructions\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit your .qmd and .html files wherein you provide answers to these Questions by no later than 4 March 2023 at 16:00.\nLabel the files as follows:\n\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Intro_R_Assessment.qmd, and\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Intro_R_Assessment.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Tasks on the Google Form when ready."
  },
  {
    "objectID": "assessments/Final_Integrative_Assessment_2023.html",
    "href": "assessments/Final_Integrative_Assessment_2023.html",
    "title": "BCB744 (BioStatistics): Final Integrative Assessment, 2 May 2023",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/Final_Integrative_Assessment_2023.html#honesty-pledge",
    "href": "assessments/Final_Integrative_Assessment_2023.html#honesty-pledge",
    "title": "BCB744 (BioStatistics): Final Integrative Assessment, 2 May 2023",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/Final_Integrative_Assessment_2023.html#instructions",
    "href": "assessments/Final_Integrative_Assessment_2023.html#instructions",
    "title": "BCB744 (BioStatistics): Final Integrative Assessment, 2 May 2023",
    "section": "Instructions",
    "text": "Instructions\nPlease note the following instructions. Failing to comply with them in full will result in a loss of marks.\n\nQUARTO –&gt; HTML Submit your assessment answers as an .html file compiled from your Quarto document. Produce fully annotated reports, including the meta-information at the top (name, date, purpose, etc.). Provide ample commentary explaining the purpose of the various tests/sections as necessary.\nTESTING OF ASSUMPTIONS For all questions, make sure that when formal inferential statistics are required, each is preceded by the appropriate tests for the assumptions, i.e., state the assumptions, state the statistical procedure for testing the assumptions and mention their corresponding \\(H_{0}\\). If a graphical approach is used to test assumptions, explain the principle behind the approach. Explain the findings emerging from the test of assumptions, and justify your selection of the appropriate inferential test (e.g. t-test, ANOVA, etc.) that you will use.\nSTATE HYPOTHESES When inferential statistics are required, please provide the full \\(H_{0}\\) and \\(H_{A}\\), and conclude the analysis with a statement of which is accepted or rejected.\nGRAPHICAL SUPPORT All descriptive and inferential statistics must be supported by the appropriate figures of the results.\n\nPRESENTATION OF ANSWERS For each question, the answer must be written up in the format of a mini-paper under the section headings Introduction, Methods, Results, Discussion, and References. I don’t expect that each answer would be more than 2-3 pages, although there are no lower or upper limits.\n\nIn this exam, you are required to provide any additional comments and supporting information related to methods, results, assumptions, and statistical explorations in a separate Preamble section, which will not be read as part of the mini-paper (i.e. it contains the ‘behinds-the-scenes’ workflows that do not make it into the paper). This section should contain any preliminary analyses, figures, tables, outputs, or any other relevant information not directly related to the formal hypothesis tests. Please ensure to include the Preamble section prior to the Introduction section in your submission.\nThe Introduction serves to provide background information, establish the context and relevance of the research, and clearly state the research question or hypothesis being investigated.\nThe Methods section will clearly outline only the statistical methods followed, e.g. which statistical tests were selected, how assumptions were tested, and a mention of any special data analyses that may have proceeded the statistical tests (if any). Typically, the focus here is only on the inferential statistics, not the EDA.\nIn the Results section you will focus only on the results around the hypotheses as stated in the Introduction. Although tests for assumptions also have statistical tests, they do not have to be mentioned in the Results.\nThe Discussion is where you will interpret and contextualise the findings, exploring their implications, limitations, and potential future directions within the broader scientific landscape around the topic. You can include up to five relevant papers across the Methods and Discussion sections.\nA combined References section in the end after all the questions can contain all the references.\n\n\nFORMATTING Pay attention to formatting. Some marks will be allocated to the appearance of the script, including considerations of aspects of the tidiness of the file, the use of the appropriate headings, and adherence to code conventions (e.g. spacing etc.).\nMARK ALLOCATION Please see the Introduction Page for an explanation of the assessment approach that will be applied to these questions.\n\nSubmit the .html file wherein you provide answers to Questions 1–3 by no later than 08:00, Monday, 8 May. Label the script as follows:\nBCB744_&lt;Name&gt;_&lt;Surname&gt;_Final_Integrative_Assessment.html, e.g.\nBCB744_AJ_Smit_Final_Integrative_Assessment.html.\nUpload your .html files onto Google Forms (to be provided)."
  },
  {
    "objectID": "assessments/Final_Integrative_Assessment_2023.html#question-1",
    "href": "assessments/Final_Integrative_Assessment_2023.html#question-1",
    "title": "BCB744 (BioStatistics): Final Integrative Assessment, 2 May 2023",
    "section": "Question 1",
    "text": "Question 1\nElephant growth data\nDescription Data on 288 African elephants that lived through droughts in the first two years of life.\nFormat A data set with 288 observations on the following 3 variables:\n\n\nAge: Age (in years)\n\nHeight: Shoulder height (in cm)\n\nSex: F, female; M, male\n\nBackground to the study can be found in the paper by Lee et al (2013).\nThe basic research question is whether there are sex-specific effects on growth of elephants.\nThe most basic answer is either “yes, there is a sex-specific effect” or “no, there is no sex-specific effect”. A substantive and statistically correct analysis addressing this most basic question will earn you 65%—note that a mark of 65% requires adhering to ALL requirements as per the instructions in the preamble. To get a mark of approaching 100% for this question will require additional analyses that demonstrate your own initiative towards achieving deeper insight into the biology of the species."
  },
  {
    "objectID": "assessments/Final_Integrative_Assessment_2023.html#question-2",
    "href": "assessments/Final_Integrative_Assessment_2023.html#question-2",
    "title": "BCB744 (BioStatistics): Final Integrative Assessment, 2 May 2023",
    "section": "Question 2",
    "text": "Question 2\nFor this question, you will obtain data on the effects of biochar on growth and elemental content of four crops, carrot, lettuce, soybean and sweetcorn from a US EPA website. Please also consult the two papers cited there as some useful hints regarding the data analysis are available, which you might decide to heed (or not). You’ll certainly want to read the papers for background to the studies.\nThe purpose of your work here is to focus on the plant yield and the three nutrients that you deem are most important in affecting human nutrition (the case for which must make in the Introduction section). Your analysis will allow you to make recommendations for about:\n\nwhether or not there are differences between crops regarding the best biochar treatments to apply, and\nto offer insight about how to best optimise the biochar application specifically for each crop with the aim to provide the best balance of human-benefitting nutrients produced and the biomass attained at the end of the growth period."
  },
  {
    "objectID": "assessments/Final_Integrative_Assessment_2023.html#question-3",
    "href": "assessments/Final_Integrative_Assessment_2023.html#question-3",
    "title": "BCB744 (BioStatistics): Final Integrative Assessment, 2 May 2023",
    "section": "Question 3",
    "text": "Question 3\nMiscellaneous datasets are provided. Analyse each use the statistical test most appropriate for the respective datasets.\nFor this question, it will suffice to simply state the hypotheses in the Introduction and explain the findings and reach a conclusion in the Discussion. No need for referencing, contextualising, discussing, etc. The Methods and Results sections must be complete and detailed, however.\na. Spruce Moth Traps\n\nResponse: number of spruce moths found in trap after 48 hours\n\nFactor 1: Location of trap in tree (top branches, middle branches, lower branches, ground)\nFactor 2: Type of lure in trap (scent, sugar, chemical)\nb. Apple Orchard Experiment\n\nFive types of root-stock were used in an apple orchard grafting experiment. The following data represent the extension growth (cm) after four years.\n\nX1: extension growth for type I\nX2: extension growth for type II\nX3: extension growth for type III\nX4: extension growth for type IV\nX5: extension growth for type V\nc. Birds’ Bones and Living Habits\n\nThis dataset represent several ecological bird groups and measurments of various bones in their bodies.\nGroups:\n\nSW: Swimming Birds\nW: Wading Birds\nT: Terrestrial Birds\nR: Raptors\nP: Scansorial Birds\nSO: Singing Birds\n\nMeasurements of bones (mm):\n\nLength and Diameter of Humerus\nLength and Diameter of Ulna\nLength and Diameter of Femur\nLength and Diameter of Tibiotarsus\nLength and Diameter of Tarsometatarsus\nd. The urine dataset\nThis dataset is in the boot package and can be loaded as boot::urine. See the helpfile for an explanation of what’s inside."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html",
    "href": "assessments/BCB744_Summative_Task_2_2023.html",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#honesty-pledge",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#honesty-pledge",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "",
    "text": "This assignment requires that you work as an individual and not share your code, results, or discussion with your peers. Penalties and disciplinary action will apply if you are found cheating.\n\n\n\n\n\n\nAcknowledgement of the Pledge\n\n\n\nCopy the statement, below, into your document and replace the underscores with your name acknowledging adherence to the UWC’s Honesty Pledge.\nI, ____________, hereby state that I have not communicated with or gained information in any way from my peers and that all work is my own."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#instructions",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#instructions",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Instructions",
    "text": "Instructions\nPlease note the following instructions. Failing to comply with them in full will result in a loss of marks.\n\nQUARTO –&gt; HTML Submit your assessment answers as an .html file compiled from your Quarto document. Produce fully annotated reports, including the meta-information at the top (name, date, purpose, etc.). Provide ample commentary explaining the purpose of the various tests/sections as necessary.\nTESTING OF ASSUMPTIONS For all questions, make sure that when formal inferential statistics are required, each is preceded by the appropriate tests for the assumptions, i.e., state the assumptions, state the statistical procedure for testing the assumptions and mention their corresponding \\(H_{0}\\). If a graphical approach is used to test assumptions, explain the principle behind the approach. Explain the findings emerging from the test of assumptions, and justify your selection of the appropriate inferential test (e.g. t-test, ANOVA, etc.) that you will use.\nSTATE HYPOTHESES When inferential statistics are required, please provide the full \\(H_{0}\\) and \\(H_{A}\\), and conclude the analysis with a statement of which is accepted or rejected.\nGRAPHICAL SUPPORT All descriptive and inferential statistics must be supported by the appropriate figures of the results.\nSTATEMENT OF RESULTS Make sure that the textual statement of the final result is written exactly as required for it to be published in a journal article. Please consult a journal if you don’t know how.\nFORMATTING Pay attention to formatting. Some marks will be allocated to the appearance of the script, including considerations of aspects of the tidiness of the file, the use of the appropriate headings, and adherence to code conventions (e.g. spacing etc.).\nMARK ALLOCATION Please see the Introduction Page for an explanation of the assessment approach that will be applied to these questions.\n\nSubmit the .html file wherein you provide answers to Questions 1–7 by no later than 19:00 today. Label the script as follows:\nBCB744_&lt;Name&gt;_&lt;Surname&gt;_Summative_Task_2.html, e.g.\nBCB744_AJ_Smit_Summative_Task_2.html.\nUpload your .html files onto Google Forms."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#question-1",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#question-1",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Question 1",
    "text": "Question 1\nChromosomal effects of mercury-contaminated fish consumption\nThese data reside in package coin, dataset mercuryfish. The dataframe contains the mercury level in blood, the proportion of cells with abnormalities, and the proportion of cells with chromosome aberrations in consumers of mercury-contaminated fish and a control group. Please see the dataset’s help file for more information.\nAnalyse the dataset and answer the following questions:\n\nDoes the presence of methyl-mercury in a diet containing fish result in a higher proportion of cellular abnormalities?\nDoes the concentration of mercury in the blood influence the proportion of cells with abnormalities, and does this differ between the control and exposed groups?\nIs there a relationship between the variables abnormal and ccells? This will have to be for the control and exposed groups, noting that an interaction effect might be present."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#question-2",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#question-2",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Question 2",
    "text": "Question 2\nMalignant glioma pilot study\nPackage coin, dataset glioma: A non-randomized pilot study on malignant glioma patients with pretargeted adjuvant radioimmunotherapy using yttrium-90-biotin.\n\nDo sex and group interact to affect survival time (time)?\nDo age and histology interact to affect survival time (time)?\nShow a full graphical exploration of the data. Are there any other remaining patterns visible in the data that should be explored statistically? Study your results, select the most promising and insightful question that remains, and do the analysis."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#question-3",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#question-3",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Question 3",
    "text": "Question 3\nRisk factors associated with low infant birth weight\nPackage MASS, dataset birthwt: A dataset about the risk factors associated with low infant birth mass collected at Baystate Medical Center, Springfield, Mass. during 1986.\nState three hypotheses and test them. Make sure one of the tests makes use of the 95% confidence interval approach rather than a formal inferential methodology."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#question-4",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#question-4",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Question 4",
    "text": "Question 4\nThe LungCapData.csv data\n\n\nUsing the Lung Capacity data provided, please calculate the 95% CIs for the LungCap variable as a function of:\n\nGender\nSmoke\nCaesarean\n\n\n\n\nlungs &lt;- read.csv(\"../data/LungCapData.csv\", sep = \"\\t\")\n\nlibrary(rcompanion)\n\n(gender_ci &lt;- groupwiseMean(LungCap ~ Gender, data = lungs, conf = 0.95, digits = 3))\n\n  Gender   n Mean Conf.level Trad.lower Trad.upper\n1 female 358 7.41       0.95       7.14       7.67\n2   male 367 8.31       0.95       8.03       8.58\n\n(smoke_ci &lt;- groupwiseMean(LungCap ~ Smoke, data = lungs, conf = 0.95, digits = 3))\n\n  Smoke   n Mean Conf.level Trad.lower Trad.upper\n1    no 648 7.77       0.95       7.56       7.98\n2   yes  77 8.65       0.95       8.22       9.07\n\n(caesarean_ci &lt;- groupwiseMean(LungCap ~ Caesarean, data = lungs, conf = 0.95, digits = 3))\n\n  Caesarean   n Mean Conf.level Trad.lower Trad.upper\n1        no 561 7.83       0.95       7.61       8.05\n2       yes 164 7.97       0.95       7.56       8.38\n\n\n\nCreate a graph of the mean ± 95% CIs and determine if there are statistical differences in LungCap between the levels of Gender, Smoke, and Caesarean. Do the same using inferential statistics. Are your findings the same using these two approaches?\n\n\nplt1 &lt;- ggplot(gender_ci, aes(x = Gender, y = Mean)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper), width = 0.2) +\n  ylab(\"Mean lung capacity\")\n\nplt2 &lt;- ggplot(smoke_ci, aes(x = Smoke, y = Mean)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper), width = 0.2) +\n  ylab(\"Mean lung capacity\")\n\nplt3 &lt;- ggplot(caesarean_ci, aes(x = Caesarean, y = Mean)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper), width = 0.2) +\n  ylab(\"Mean lung capacity\")\n\nggarrange(plt1, plt2, plt3, ncol = 3, labels = \"AUTO\")\n\n\n\n\n\nProduce all the associated tests for assumptions—i.e. the assumptions to be met when deciding whether to use your choice of inferential test or its non-parametric counterpart.\n\n\ntwo_assum &lt;- function(x) {\n  x_var &lt;- var(x)\n  x_norm &lt;- as.numeric(shapiro.test(x)[2])\n  result &lt;- c(x_var, x_norm)\n  return(result)\n}\n\nlungs %&gt;% \n  group_by(Gender) %&gt;% \n  summarise(LungCap_var = round(two_assum(LungCap)[1], 3),\n            LungCap_norm = round(two_assum(LungCap)[2], 3))\n\n# A tibble: 2 × 3\n  Gender LungCap_var LungCap_norm\n  &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 female        6.58        0.002\n2 male          7.2         0.073\n\nlungs %&gt;% \n  group_by(Smoke) %&gt;% \n  summarise(LungCap_var = round(two_assum(LungCap)[1], 3),\n            LungCap_norm = round(two_assum(LungCap)[2], 3))\n\n# A tibble: 2 × 3\n  Smoke LungCap_var LungCap_norm\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 no           7.43        0.008\n2 yes          3.54        0.622\n\nlungs %&gt;% \n  group_by(Caesarean) %&gt;% \n  summarise(LungCap_var = round(two_assum(LungCap)[1], 3),\n            LungCap_norm = round(two_assum(LungCap)[2], 3))\n\n# A tibble: 2 × 3\n  Caesarean LungCap_var LungCap_norm\n  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 no               7.13        0.004\n2 yes              6.97        0.554\n\n# It would be best to continue with a Wilcoxon test\n\n\nCreate a combined tidy dataframe (observe tidy principles) with the estimates for the 95% CI for the LungCap data (LungCap as a function of Gender), estimated using both the traditional and bootstrapping approaches. Create a plot comprising two panels (one for the traditional estimates, one for the bootstrapped estimates) of the mean, median, scatter of raw data points, and the upper and lower 95% CI.\n\n\ngroupwiseMean(LungCap ~ Gender, data = lungs, conf = 0.95, digits = 3, normal = TRUE) |&gt; \n  pivot_longer(cols = Trad.lower:Normal.upper,\n               names_to = \"type\", values_to = \"CI\") |&gt; \n  separate(col = type, into = c(\"type\", \"direction\")) |&gt; \n  pivot_wider(names_from = direction, values_from = CI) |&gt; \n  ggplot(aes(x = Gender, y = Mean)) +\n    geom_jitter(data = lungs, aes(x = Gender, y = LungCap, colour = Smoke),\n                width = 0.1, alpha = 0.2) +\n    geom_point(colour = \"black\") +\n    geom_errorbar(aes(ymin = lower, ymax = upper),\n                  width = 0.2, colour = \"black\") +\n    geom_point(data = lungs, aes(x = Gender, y = median(LungCap)),\n               colour = \"red\", shape = \"X\") +\n    facet_wrap(~type) +\n    ylab(\"Mean lung capacity\")\n\n\n\n\n\nUndertake a statistical analysis that incorporates both the effect of Age and one of the categorical variables on LungCap. What new insight does this provide?\n\n\n# focus only on males\nlungs |&gt; \n  filter(Gender == \"male\") |&gt; \n  group_by(Smoke) |&gt; \n  summarise(LungCap_var = round(two_assum(LungCap)[1], 3),\n            LungCap_norm = round(two_assum(LungCap)[2], 3))\n\n# A tibble: 2 × 3\n  Smoke LungCap_var LungCap_norm\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 no           7.52        0.132\n2 yes          2.94        0.565\n\n# above we see that within males, the subgroups based on whether or not\n# they smoke are normally distributed in both instances\n\nmod1 &lt;- lm(LungCap ~ Smoke * Age, data = lungs[lungs$Gender == \"male\", ])\nsummary(mod1)\n\n\nCall:\nlm(formula = LungCap ~ Smoke * Age, data = lungs[lungs$Gender == \n    \"male\", ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5903 -0.9875  0.0920  1.0286  3.7097 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.44681    0.25191   5.743 1.96e-08 ***\nSmokeyes      2.57844    1.48497   1.736   0.0833 .  \nAge           0.56613    0.01996  28.367  &lt; 2e-16 ***\nSmokeyes:Age -0.21021    0.09924  -2.118   0.0348 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.484 on 363 degrees of freedom\nMultiple R-squared:  0.6968,    Adjusted R-squared:  0.6943 \nF-statistic: 278.1 on 3 and 363 DF,  p-value: &lt; 2.2e-16\n\n# lung capacity of males is affected by age (disregarding effect of smoke),\n# lung capacity is not affected by smoke (disregarding effect of age), but\n# there is a significant interaction between them, i.e. the effect of age \n# is more pronounced in non-smoker than it is in smokers...\n\nlungs[lungs$Gender == \"male\", ] |&gt; \n  ggplot(aes(x = Age, y = LungCap)) +\n    geom_point() +\n    geom_smooth(method = \"lm\") +\n    facet_wrap(~Smoke)\n\n\n\n# the figure shows the interaction effect: in non-smokers their lung capacity\n# increases more rapidly with age, whereas in smokers, the development of lung\n# capacity with age seems to be stunted."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#question-5",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#question-5",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Question 5",
    "text": "Question 5\nThe air quality data\nPackage datasets, dataset airquality. These are daily air quality measurements in New York, May to September 1973. See the help file for details.\n\nWhich two of the four response variables are best correlated with each other?"
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#question-6",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#question-6",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Question 6",
    "text": "Question 6\nThe shells.csv data\nThis dataset contains measurements of shell widths and lengths of the left and right valves of two species of mussels, Aulacomya sp. and Choromytilus sp. Length and width measurements are presented in mm.\nFully analyse this dataset."
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#question-7",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#question-7",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "Question 7",
    "text": "Question 7\nThe fertiliser_crop_data.csv data\nThe data represent an experiment designed to test whether or not fertiliser type and the density of planting have an effect on the yield of wheat. The dataset contains the following variables:\n\nFinal yield (kg per acre)—make sure to convert this to the most suitable SI unit before continuing with your analysis\nType of fertiliser (fertiliser type A, B, or C)\nPlanting density (1 = low density, 2 = high density)\nBlock in the field (north, east, south, west)\n\nFully analyse this dataset.\n\nfert &lt;- read.csv(\"../data/fertiliser_crop_data.csv\")\n\n# convert to SI units\nfert &lt;- fert |&gt; \n  mutate(mass = mass / 0.40468564224)\n\n# are assumptions met? note that I also calculate the mean +/- SD here\nfert %&gt;% \n  group_by(density) %&gt;% \n  summarise(mean = mean(mass),\n            SD = sd(mass),\n            mass_var = round(two_assum(mass)[1], 3),\n            mass_norm = round(two_assum(mass)[2], 3))\n\n# A tibble: 2 × 5\n  density   mean    SD mass_var mass_norm\n    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1       1 11889.  40.8    1668.     0.469\n2       2 11920.  43.3    1877.     0.529\n\nfert %&gt;% \n  group_by(block) %&gt;% \n  summarise(mean = mean(mass),\n            SD = sd(mass),\n            mass_var = round(two_assum(mass)[1], 3),\n            mass_norm = round(two_assum(mass)[2], 3))\n\n# A tibble: 4 × 5\n  block   mean    SD mass_var mass_norm\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 east  11925.  43.4    1882.     0.422\n2 north 11894.  42.2    1781.     0.77 \n3 south 11884.  39.7    1578.     0.212\n4 west  11915.  43.7    1906.     0.21 \n\nfert %&gt;% \n  group_by(fertilizer) %&gt;% \n  summarise(mean = mean(mass),\n            SD = sd(mass),\n            mass_var = round(two_assum(mass)[1], 3),\n            mass_norm = round(two_assum(mass)[2], 3))\n\n# A tibble: 3 × 5\n  fertilizer   mean    SD mass_var mass_norm\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 A          11887.  46.1    2122.     0.774\n2 B          11899.  38.6    1490.     0.887\n3 C          11927.  40.3    1623.     0.254\n\n# yes, all assumptions check out, proceed with normal paramatric stats\n\n# do an ANOVA and look at main effects first\naov1 &lt;- aov(mass ~ density + block + fertilizer, data = fert)\nsummary(aov1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndensity      1  23164   23164  15.224 0.000184 ***\nblock        2   2199    1099   0.723 0.488329    \nfertilizer   2  27444   13722   9.018 0.000269 ***\nResiduals   90 136940    1522                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# the block effect is not significant but density and fertilizer are\n\n# let's check if the fertilizer type interacts with density\naov2 &lt;- aov(mass ~ density * fertilizer, data = fert)\nsummary(aov2)\n\n                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndensity             1  23164   23164  15.195 0.000186 ***\nfertilizer          2  27444   13722   9.001 0.000273 ***\ndensity:fertilizer  2   1935     967   0.635 0.532500    \nResiduals          90 137203    1524                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# no interaction effect is present, so the fertilizer has the same\n# effect regardless of at which planting density it is applied\n\n# lets see which planting fertilizer results in the greatest mass\n\nTukeyHSD(aov2, which = \"fertilizer\", ordered = TRUE)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n    factor levels have been ordered\n\nFit: aov(formula = mass ~ density * fertilizer, data = fert)\n\n$fertilizer\n        diff        lwr      upr     p adj\nB-A 11.84752 -11.414312 35.10935 0.4482026\nC-A 40.29177  17.029945 63.55360 0.0002393\nC-B 28.44426   5.182428 51.70609 0.0123951\n\nTukeyHSD(aov2, which = \"fertilizer\", ordered = TRUE)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n    factor levels have been ordered\n\nFit: aov(formula = mass ~ density * fertilizer, data = fert)\n\n$fertilizer\n        diff        lwr      upr     p adj\nB-A 11.84752 -11.414312 35.10935 0.4482026\nC-A 40.29177  17.029945 63.55360 0.0002393\nC-B 28.44426   5.182428 51.70609 0.0123951\n\nplot(TukeyHSD(aov2, which = \"fertilizer\", ordered = TRUE))\n\n\n\n# here we can see that the mass of crop produced by fertilizer C is the\n# greatest, significantly more so compared to both A and B; the effect\n# of fertilizer B is no different than that of A\n# \n# the second planting density also yields a greater mass per ha\n# \n# make sure the results are written up as appropriate for a journal,\n# so indicate the d.f., S.S., and p-value"
  },
  {
    "objectID": "assessments/BCB744_Summative_Task_2_2023.html#the-end",
    "href": "assessments/BCB744_Summative_Task_2_2023.html#the-end",
    "title": "BCB744 (BioStatistics): Summative Task 2, 12 April 2023",
    "section": "The end",
    "text": "The end\nSubmit the .html file wherein you provide answers to Questions 1–7 by no later than 19:00 today. Label the script as follows:\nBCB744_&lt;Name&gt;_&lt;Surname&gt;_Summative_Task_2.html, e.g.\nBCB744_AJ_Smit_Summative_Task_2.html.\nUpload your .html files onto Google Forms."
  },
  {
    "objectID": "resources/ecology_resources_web.html",
    "href": "resources/ecology_resources_web.html",
    "title": "Quantitative Ecology Resources",
    "section": "",
    "text": "AUTHOR\nTITLE\n\n\n\n\nDavid Zelený\nAnalysis of Community Ecology Data in R\n\n\nMike Palmer\nOrdination Methods for Ecologists\n\n\nButtigieg and Ramette (2014)\nGUide to STatistical Analysis in Microbial Ecology (GUSTA ME)\n\n\nGreenacre and Primicerio\nMultivariate Analysis of Ecological Data\n\n\n\n\n\n\n\nButtigieg PL, Ramette A (2014) A guide to statistical analysis in microbial ecology: A community-focused, living review of multivariate data analyses. FEMS microbiology ecology 90:543–550."
  },
  {
    "objectID": "resources/ecology_resources_web.html#references",
    "href": "resources/ecology_resources_web.html#references",
    "title": "Quantitative Ecology Resources",
    "section": "",
    "text": "Buttigieg PL, Ramette A (2014) A guide to statistical analysis in microbial ecology: A community-focused, living review of multivariate data analyses. FEMS microbiology ecology 90:543–550."
  },
  {
    "objectID": "resources/general_resources_web.html",
    "href": "resources/general_resources_web.html",
    "title": "R Resources",
    "section": "",
    "text": "1 Web resources about R, RStudio, R Markdown, and Quarto\n\n\n\nAUTHOR\nTITLE\n\n\n\n\nABOUT R and the Tidyverse\n\n\n\nChester Ismay and Albert Y. Kim\nA ModernDive into R and the Tidyverse\n\n\nGarrett Grolemund\nHands-On Programming with R\n\n\nHadley Wickham\nR for Data Science\n\n\nHadley Wickham\nR for Data Science (2e)\n\n\nFrank E Harrell Jr\nR Workflow\n\n\nWright et al.\nTidyverse Skills for Data Science\n\n\nRoger D Peng\nR Programming for Data Science\n\n\nAbout ggplot2\n\n\n\nHadley Wickham et al.\nggplot2: Elegant Graphics for Data Analysis\n\n\nWinston Change\nR Graphics Cookbook, 2nd edition\n\n\nABOUT R Markdown\n\n\n\nRStudio\nR Markdown\n\n\nRStudio\nR Markdown cheatsheet\n\n\nGarrett Grolemund\nIntroduction to R Markdown\n\n\nIvan Millanes\nR Markdown Tips\n\n\nABOUT Quarto\n\n\n\nDario Radečić\nR Quarto Tutorial\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {R {Resources}},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/resources/general_resources_web.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) R Resources. https://tangledbank.netlify.app/resources/general_resources_web.html."
  },
  {
    "objectID": "resources/spatial_resources_web.html",
    "href": "resources/spatial_resources_web.html",
    "title": "Spatial R Resources",
    "section": "",
    "text": "1 Web resources about R for Spatial Applications\n\n\n\nAUTHOR\nTITLE\n\n\n\n\nSpatial R\n\n\n\nEdzer Pebesma\nSimple Features for R\n\n\nEdzer Pebesma, Roger Bivand\nSpatial Data Science with applications in R\n\n\nRobin Lovelace et al.\nGeocomputation with R\n\n\nManuel Gimond\nIntro to GIS and Spatial Analysis\n\n\nWasser et al.\nIntroduction to Geospatial Raster and Vector Data with R\n\n\nTaro Mieno\nR as GIS for Economists\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ and AJ Smit, Prof.},\n  title = {Spatial {R} {Resources}},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/resources/spatial_resources_web.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, AJ Smit Prof (2021) Spatial R Resources. https://tangledbank.netlify.app/resources/spatial_resources_web.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Tangled Bank",
    "section": "",
    "text": "“It is interesting to contemplate a tangled bank, clothed with many plants of many kinds, with birds singing on the bushes, with various insects flitting about, and with worms crawling through the damp earth, and to reflect that these elaborately constructed forms, so different from each other, and dependent upon each other in so complex a manner, have all been produced by laws acting around us. These laws, taken in the largest sense, being Growth with reproduction; Inheritance which is almost implied by reproduction; Variability from the indirect and direct action of the conditions of life, and from use and disuse; a Ratio of Increase so high as to lead to a Struggle for Life, and as a consequence to Natural Selection, entailing Divergence of Character and the Extinction of less improved forms. Thus, from the war of nature, from famine and death, the most exalted object which we are capable of conceiving, namely, the production of the higher animals, directly follows. There is grandeur in this view of life, with its several powers, having been originally breathed by the Creator1 into a few forms or into one; and that, whilst this planet has gone circling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being evolved.”\n— Charles Darwin, Origin of Species, 1859"
  },
  {
    "objectID": "index.html#untangling-the-tangled-bank",
    "href": "index.html#untangling-the-tangled-bank",
    "title": "The Tangled Bank",
    "section": "Untangling the Tangled Bank",
    "text": "Untangling the Tangled Bank\nCharles Darwin’s The Tangled Bank describes the complexity and interconnectedness of ecosystems and how different species compete and coexist within them. “The Tangled Bank” is a metaphor that refers to the idea that the relationships between species in an ecosystem are intricate and overlapping, like a bank of plants and associated species in a natural environment that is densely woven together.\nDarwin used this metaphor to explain how changes in one species can have ripple effects throughout an ecosystem, affecting other species and altering the entire ecosystem. He argued that the survival and evolution of a species are intimately connected to the conditions and interactions within its environment. In particular, he stressed the importance of natural selection as the driving force behind the evolution of species, where those that are best adapted to their environment are more likely to survive and pass on their traits to the next generation.\nToday, data about the world and the diversity of life in it are ubiquitous. This vast amount and complexity of data can be overwhelming, and it is often challenging to see the connections between different data sources and how they affect one another. Like a natural ecosystem that richly interacts on all fronts, scientists dealing with environmental, ecological, and biological data must understand the interconnectedness of the different data sources, analytical methods, and models. The ‘Tangled Bank’ metaphor extends to explaining how insights from diverse data sources can interact with one another and equip us to arrive at deep knowledge about our world. The ever-growing array of analytical methods, which in themselves may be intricately related, drives the ‘data2 –&gt; information3 –&gt; knowledge4 –&gt; wisdom5’ (DIKW) pipeline, and helps us to untangle the Tangled Bank.\nIn a changing and increasingly fragile world, Darwin’s Tangled Bank is untangling and losing its structural and functional integrity. However, the complexity of data about our world and the depths of information gained is becoming increasing complex. The DIKW pipeline is crucial for guiding our interactions with the natural systems. By transforming data into wisdom, we can:\n\nDevelop more sustainable policies and practices that consider the long-term implications of our actions on the environment.\nRaise public awareness about the importance of preserving natural resources and ecosystems for future generations.\nImplement early warning systems and adaptive management strategies to address environmental challenges such as climate change, habitat loss, and pollution.\nFoster innovation in green technologies and promote environmentally responsible behavior among individuals and organisations.\n\nUltimately, the DIKW pipeline helps us to make wiser decisions that protect and preserve the natural systems, ensuring a healthy and sustainable world for present and future generations."
  },
  {
    "objectID": "index.html#on-these-pages",
    "href": "index.html#on-these-pages",
    "title": "The Tangled Bank",
    "section": "On these pages",
    "text": "On these pages\nThis website offers material in support of several modules taught at the Biological and Conservation Biology Department, University of the Western Cape. The courses are taught by AJ Smit and are rich in R content. They include:\n\nPlant Ecophysiology (BDC223)—to be developed\nBiogeography and Global Ecology (BDC334)\nIntroduction to R and Biostatistics (BCB744)—in development\nQuantitative Ecology (BCB743)\n\nIn addition to the taught material, there are vignettes with some R tricks I have learned over the years, including examples of how to analyse oceanographic and Earth datasets."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "The Tangled Bank",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTalking about the compatibility of science and religion, Darwin said “I cannot pretend to throw the least light on such abstruse problems. The mystery of the beginning of all things is insoluble by us; and I for one must be content to remain an Agnostic.” The source of this insight in Darwin’s religious views is, Darwin, Charles (1958), Barlow, Nora (ed.), The Autobiography of Charles Darwin 1809–1882. With the original omissions restored. Edited and with appendix and notes by his granddaughter Nora Barlow↩︎\nData refer to raw, unprocessed numerical representations (facts) about the world. Because of applying the process of scientific enquiry, we can make a case for data being the closest approximation and reflection of reality. In the context of natural systems, this could include measurements of temperature, precipitation, air quality, populations, communities, or any other quantifiable aspect of the environment. It is typically presented in a structured or unstructured format and lacks context or meaning.↩︎\nInformation refers to data that have been processed, organised, and structured to provide context and meaning. It can be thought of as the result of adding structure and interpretation to raw data. In this stage, data are put into context, such as trends in climate change, loss of biodiversity, or increasing pollution levels. This helps us understand the state of the natural systems and identify potential problems.↩︎\nKnowledge is the next level of abstraction and refers to a deeper understanding or insight that is gained from information. It is the result of synthesising information to draw conclusions or make predictions. Through the process of knowledge generation we gain a deeper understanding of the underlying patterns, relationships, and principles. This stage involves continuing to use scientific methods, our and other’s expertise, and past experiences to make sense of the information. For instance, understanding the factors driving climate change or the consequences of deforestation on ecosystems. In other words, knowledge is what we gain when we apply meaning and context to information, and we use it to make informed decisions or take actions.↩︎\nWisdom is the final stage in the DIKW pipeline, and the highest level of abstraction possible. Here we apply knowledge to make informed, ethical, and sustainable decisions about how we interact with the natural systems that make up our planet, Earth. This stage involves critical thinking, foresight, and an understanding of the complex interdependencies within the environment. Wisdom allows us to make choices that balance our needs with the long-term health and resilience of the planet.↩︎"
  },
  {
    "objectID": "slides/BCB744_Summative_Task_2_BioStats_2023.html",
    "href": "slides/BCB744_Summative_Task_2_BioStats_2023.html",
    "title": "BCB744 (BioStats): Summative Task 2",
    "section": "",
    "text": "Please see the file ‘fertiliser_crop_data.csv’ for this dataset. The data represent an experiment designed to test whether or not fertiliser type and the density of planting have an effect on the yield of wheat. The dataset contains the following variables:\nSee the ‘shells.csv’ file. This dataset contains measurements of shell widths and lengths of the left and right valves of two species of mussels, sp. and sp. Length and width measurements are presented in mm.\nThese data are in ‘health.csv’. Inside the file are several columns, but the ones that are relevant to this question are:\nPackage datasets, dataset airquality. These are daily air quality measurements in New York, May to September 1973. See the help file for details.\nThe file ‘crickets.csv’ contains data for some crickets whose chirp rate was measured at several temperatures. The temperature was measured in °F, but please make sure you do all the calculations using °C instead.\nThe file ‘SST.csv’ contains sea surface temperatures for Port Nolloth and Muizenberg in °C. The data are from 1 January 2010 to 31 December 2011.\nHint: The lubridate package (and others) offers convenient ways to work with time series (i.e. in this case coding a variable for month).\nThat’s all, Folks!\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2023,\n  author = {Smit, AJ and Smit, AJ},\n  title = {BCB744 {(BioStats):} {Summative} {Task} 2},\n  date = {2023-04-11},\n  url = {https://tangledbank.netlify.app/slides/BCB744_Summative_Task_2_BioStats_2023.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2023) BCB744 (BioStats): Summative Task 2. https://tangledbank.netlify.app/slides/BCB744_Summative_Task_2_BioStats_2023.html."
  },
  {
    "objectID": "slides/BCB743_11-nmds.html",
    "href": "slides/BCB743_11-nmds.html",
    "title": "nMDS",
    "section": "",
    "text": "Submit a R script wherein you provide answers to Questions 1–2 by no later than 8:00 tomorrow. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Assignment_7.R, e.g. BCB743_AJ_Smit_Assignment_7.R.\nRefer to the non-Metric Multidimensional Scaling lecture material to see the questions in context.\n\n\n0.1 Assignment 7 Questions\n\n\nUsing two unconstrained ordination techniques of your choice, analyse the mite data in the vegan package. Provide a brief description and discussion of what you have found, and produce the R code.\nUsing two unconstrained ordination techniques of your choice (not already used in 1, above) analyse the dune data in the vegan package. Provide a brief description and discussion of what you have found, and produce the R code.\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2020,\n  author = {Smit, AJ and Smit, AJ},\n  title = {nMDS},\n  date = {2020-06-28},\n  url = {https://tangledbank.netlify.app/slides/BCB743_11-nmds.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2020) nMDS. https://tangledbank.netlify.app/slides/BCB743_11-nmds.html."
  },
  {
    "objectID": "slides/BCB743_05-spp_dissimilarity.html",
    "href": "slides/BCB743_05-spp_dissimilarity.html",
    "title": "Species dissimilarities",
    "section": "",
    "text": "Submit a R script wherein you provide answers to Questions 1–9 by no later than 8:00 tomorrow. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Assignment_2.R, e.g. BCB743_AJ_Smit_Assignment_2.R.\nRefer to the Species Dissimilarity lecture material for the question context.\n\n\n0.1 Assignment 2 Questions\n\nQuestion 1: Look at the dataset and explain its structure in words.\nQuestion 2: Would we use Bray-Curtis or Jaccard dissimilarities?\nQuestion 3: Apply the calculation.\nQuestion 4: Explain the meaning of the results in broad terms.\n\n\n\nQuestion 5: Examine it more closely: what general pattern comes out?\nQuestion 6: Plot this pattern (hint, it is best seen in the 1st column of the dissimilarity matrix).\nQuestion 7: What explanation can you offer for this pattern?\nQuestion 8: Using the decostand() function, create presence/absence data, and apply the appropriate vegdist() function to obtain a suitable dissimilarity matrix.\nQuestion 9: Create another plot and explain the pattern.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2020,\n  author = {Smit, AJ and Smit, AJ},\n  title = {Species Dissimilarities},\n  date = {2020-06-28},\n  url = {https://tangledbank.netlify.app/slides/BCB743_05-spp_dissimilarity.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2020) Species dissimilarities. https://tangledbank.netlify.app/slides/BCB743_05-spp_dissimilarity.html."
  },
  {
    "objectID": "slides/BCB743_08-pca_sdg.html",
    "href": "slides/BCB743_08-pca_sdg.html",
    "title": "PCA WHO SDGs",
    "section": "",
    "text": "Submit a Rmarkdown script wherein you provide answers to Questions 1–5, and provide the associated compiled html output. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Assignment_5.R, e.g. BCB743_AJ_Smit_Assignment_5.R.\nNote that these questions also cover the Cluster Analysis lecture. Refer to the Principal Component Analysis SDG and the Cluster Analysis lecture material to see the questions in context.\nThe deadline for this submission is Monday 1 August 2022.\n\n\n0.1 Assignment 5 Questions\n\nQuestion 1: Explain the code section-by-section in long-form text. Include also the reasoning/rationale behind each section.\nPlease see the Cluster Analysis section for additional questions.\nQuestion 2: What happens if we use pam() to create four, five, or even six clusters?\nQuestion 3: In your reasoned opinion, what would be the optimal number of clusters to use?\n\n\n\nQuestion 4: Repeat the analysis using either kmeans() or hclust(), and feel free to use the factoextra helper functions and visualisations. Are the results markedly different? Which clustering approach do you wish to proceed with—i.e., pam(), hclust() or kmeans()?\nQuestion 5: Describe the patterns that you observe at the end of your ordination and final cluster selection (i.e. based on the optimal number of clusters and whichever cluster technique you deem most appropriate). How does South Africa fare in terms of attaining SDGs? Contrast with some key countries of your choice to make your points. Label the key countries that you refer to in your text by updating the code accordingly. Continue to explain these patterns in terms of the global socio-political/socio-economic landscape. Provide a discourse about possible explanations for the patterns observed globally and regionally.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2020,\n  author = {Smit, AJ and Smit, AJ},\n  title = {PCA {WHO} {SDGs}},\n  date = {2020-06-28},\n  url = {https://tangledbank.netlify.app/slides/BCB743_08-pca_sdg.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2020) PCA WHO SDGs. https://tangledbank.netlify.app/slides/BCB743_08-pca_sdg.html."
  },
  {
    "objectID": "slides/BCB743_06-correlations.html",
    "href": "slides/BCB743_06-correlations.html",
    "title": "Correlations & associations",
    "section": "",
    "text": "Submit a R script wherein you provide answers to Questions 1–8 by no later than 8:00 tomorrow. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Assignment_3.R, e.g. BCB743_AJ_Smit_Assignment_3.R.\nRefer to the Correlations & Associations lecture material to see the questions in context.\n\n\n0.1 Assignment 3 Questions\n\nQuestion 1: Create a plot of pairwise correlations.\nQuestion 2: Name to two top positive and two top negative statistically-significant correlations.\nQuestion 3: For each, discuss the mechanism behind the relationships. Why do these relationships exist?\nQuestion 4: Why do we need to transpose the data?\n\n\n\nQuestion 5: What are the properties of a transposed species table?\nQuestion 6: What are the properties of an association matrix? How do these properties differ from that of a i) species dissmilarity matrix and from a ii) correlation matrix?\nQuestion 7: What is the difference between spp_assoc1 and spp_assoc2? Is the information contained in each markedly different from the other?\nQuestion 8: Explain the kind of insight we are able to glean from a species association matrix.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2020,\n  author = {Smit, AJ and Smit, AJ},\n  title = {Correlations \\& Associations},\n  date = {2020-06-28},\n  url = {https://tangledbank.netlify.app/slides/BCB743_06-correlations.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2020) Correlations & associations. https://tangledbank.netlify.app/slides/BCB743_06-correlations.html."
  },
  {
    "objectID": "slides/BCB743_09-ca.html",
    "href": "slides/BCB743_09-ca.html",
    "title": "CA",
    "section": "",
    "text": "Submit a R script wherein you provide answers to Questions 1–3 by no later than 8:00 tomorrow. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Assignment_6.R, e.g. BCB743_AJ_Smit_Assignment_6.R.\nRefer to the Correspondence Analysis lecture material to see the questions in context.\n\n\n0.1 Assignment 6 Questions\n\nQuestion 1. How would you explain the patterns seen in the four panels of the above figure?\nQuestion 2. Apply approaches taken from the analysis shown immediately above to these datasets: 1. bird communities along elevation gradient in Yushan Mountain, Taiwan; 2. alpine plant communities in Aravo, France.\nQuestion 3. Discuss the patterns observed, and explain the ordination diagrams with particular reference to how the species are influenced by the major environmental drivers.\nQuestion 4 (Bonus) For bonus marks that could earn you 120/100, please see if you can recreate the bottom right figure for the species ‘Cogo’ using ggplot2. This will require digging deep into the ordination and ordisurf objects, creating dataframes for each layer of data, and assembling the graph manually.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2020,\n  author = {Smit, AJ and Smit, AJ},\n  title = {CA},\n  date = {2020-06-28},\n  url = {https://tangledbank.netlify.app/slides/BCB743_09-ca.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2020) CA. https://tangledbank.netlify.app/slides/BCB743_09-ca.html."
  },
  {
    "objectID": "slides/BCB743_02-biodiversity.html",
    "href": "slides/BCB743_02-biodiversity.html",
    "title": "Biodiversity",
    "section": "",
    "text": "Submit a R script wherein you provide answers to Questions 1–9 by no later than 8:00 tomorrow. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Assignment_1.R, e.g. BCB743_AJ_Smit_Assignment_1.R.\nRefer to the Biodiversity lecture material for the question context.\n\n\n0.1 Assignment 1 Questions\n\nQuestion 1: Why is the matrix square, and what determines the number of rows/columns?\nQuestion 2: What is the meaning of the diagonal?\nQuestion 3: What is the meaning of the non-diagonal elements?\nQuestion 4: Take the data in row 1 and create a line graph that shows these values as a function of section number.\nQuestion 5: Provide a mechanistic (ecological) explanation for why this figure takes the shape that it does.\n\n\n\nQuestion 6: Why is there a difference between the two?\nQuestion 7: Which is correct?\nQuestion 8: Plot species turnover as a function of Section number, and provide a mechanistic exaplanation for the pattern observed.\nQuestion 9: Based on an assessment of literature on the topic, provide a discussion of nestedness-resultant β-diversity. Use either a marine or terrestrial example to explain this mode of structuring biodiversity.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2020,\n  author = {Smit, AJ and Smit, AJ},\n  title = {Biodiversity},\n  date = {2020-06-28},\n  url = {https://tangledbank.netlify.app/slides/BCB743_02-biodiversity.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2020) Biodiversity. https://tangledbank.netlify.app/slides/BCB743_02-biodiversity.html."
  },
  {
    "objectID": "slides/BCB743_08-pca.html",
    "href": "slides/BCB743_08-pca.html",
    "title": "PCA",
    "section": "",
    "text": "Submit a R script wherein you provide answers to Questions 1–8 by no later than 8:00 tomorrow. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Assignment_4.R, e.g. BCB743_AJ_Smit_Assignment_4.R.\nRefer to the Principal Component Analysis lecture material to see the questions in context.\n\n\n0.1 Assignment 4 Questions\n\nQuestion 1: With reference to the sampling design (i.e. position of sample sites along the length of the river), provide mechanistics/ecological reasons for the strongly correlated environmental variables shown above in the pairwise correlation diagram. You might have to create additional spatial maps of scaled variables (as immediately above) to support your answer.\nQuestion 2: Provide a summary of the main findings of the Doubs River fish community structure study, focusing in this instance mainly on the environmental drivers.\nQuestion 3: Why can a PCA, or any ordination for that matter, not explain all of the variation in a dataset? In other words, why is it best to only use the first few Principal Components for insight into the drivers of variability? What is ‘explained’ by the remaining PC axes?\n\n\n\nQuestion 4: Replicate the analysis shown above on the environmental data included with these datasets: 1. bird communities along elevation gradient in Yushan Mountain, Taiwan; 2. alpine plant communities in Aravo, France.\nQuestion 5: Discuss the patterns observed: 1. explain the ordination diagram with particular reference to the major patterns shown; 2. provide a mechanistic explanation for the existence of the patterns seen with respect to elevation/altitude; and 3. if there are significant positive or negative correlations between the environmental variables, provide mechanistic reasons for how they came about.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2020,\n  author = {Smit, AJ and Smit, AJ},\n  title = {PCA},\n  date = {2020-06-28},\n  url = {https://tangledbank.netlify.app/slides/BCB743_08-pca.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A (2020) PCA. https://tangledbank.netlify.app/slides/BCB743_08-pca.html."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The aim of these workshops is to guide you through the basics of using R via RStudio for analysis of environmental and biological data. The workshop is laid out so it begins simply and slowly to impart the basics of using R. It then gathers pace, so that by the end we are doing intermediate level analyses. The first course, Introduction to R, is ideal for people new to R or who have limited experience. This workshop is not comprehensive, but is necessarily selective. We are not hardcore statisticians, but rather ecologists who have an interest in statistics, and use R frequently. Our emphasis is thus on the steps required to analyse and visualise data in R, rather than focusing on the statistical theory.\nThe next installment is Biostatistics. Here we emphasise the various classes of data, distributions, hypothesis tests, and some basic inferential statistics. We draw in the learnings of Intro R as we manipulate our data and make various visualisations in support of the hypotheses being tested.\nThe third workshop, Quantitative Ecology, is about community ecology and not so much about population ecology. Community ecology underpins the vast fields of biodiversity and biogeography, and concerns spatial scales from squares of meters to all of Earth. We can look at historical, contemporary, and future processes that have been implicated in shaping the distribution of life on our planet. Two main groups of multivariate statistics, ‘classifications’ and ‘ordinations’, take central position in the modern-day ecologist’s toolbox. This final workshop will explore these ideas as we work with biodiversity data across various scales time, space and complexity.\nDon’t worry if you feel overwhelmed and do not follow everything at any time during the three Workshops; that is totally natural with learning a new and powerful program. Remember that you have the notes and material to go through the exercises later at your own pace; we will also be walking the room during sessions and breaks so that we can answer questions one-on-one. We hope that this Workshop gives you the confidence to start incorporating R into your daily workflow, and if you are already a user, we hope that it will expose you to some new ways of doing things.\nFinally, keep in mind that we are self-taught when it comes to R. Our methods will work, but you will learn as you gain more experience with programming that there are many ways to get the right answer or to accomplish the same task.\n\n1 About this website\n\nflowchart LR\n  A[qmd] --&gt; B(Knitr)\n  A[qmd] --&gt; C(Jupyter)\n  B(Knitr) --&gt; D[md]\n  C(Jupyter) --&gt; D[md]\n  D[md] --&gt; E(pandoc)\n  E(pandoc) --&gt; F(HTML)\n  E(pandoc) --&gt; G(PDF)\n  E(pandoc) --&gt; H(Word)\n  E(pandoc) --&gt; I{and others}\n\n\n\n\nflowchart LR\n  A[qmd] --&gt; B(Knitr)\n  A[qmd] --&gt; C(Jupyter)\n  B(Knitr) --&gt; D[md]\n  C(Jupyter) --&gt; D[md]\n  D[md] --&gt; E(pandoc)\n  E(pandoc) --&gt; F(HTML)\n  E(pandoc) --&gt; G(PDF)\n  E(pandoc) --&gt; H(Word)\n  E(pandoc) --&gt; I{and others}\n\n\nFigure 1: How Quarto orchestrates rendering of documents: start with a qmd file, use the Knitr or Jupyter engine to perform the computations and convert it to an md file, then use Pandoc to convert to various file formats including HTML, PDF, and Word.\n\n\n\n\n…\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit,\n  author = {Smit, AJ},\n  title = {About},\n  url = {https://tangledbank.netlify.app/about.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A About. https://tangledbank.netlify.app/about.html."
  },
  {
    "objectID": "BCB743/BCB743_index.html",
    "href": "BCB743/BCB743_index.html",
    "title": "BCB743: Quantitative Ecology",
    "section": "",
    "text": "“We have become, by the power of a glorious evolutionary accident called intelligence, the stewards of life’s continuity on earth. We did not ask for this role, but we cannot abjure it. We may not be suited to it, but here we are.”\n— Stephen J. Gould\nWelcome to the pages for BCB743 Quantitative Ecology. This page provides the syllabus and teaching policies for the module, and it serves is a starting point accessing all the theory, instruction, and data."
  },
  {
    "objectID": "BCB743/BCB743_index.html#reading",
    "href": "BCB743/BCB743_index.html#reading",
    "title": "BCB743: Quantitative Ecology",
    "section": "8.1 Reading",
    "text": "8.1 Reading\nYou are expected to read additional material in support of the content covered in class and on this website.\nA compulsory reference is Numerical Ecology with R by Daniel Borcard, François Gillet and Pierre Legendre. Much of the class’ content and many of the examples (and code) that I use have been adapted from this source. It should be considered the ‘gold standard’ reference for Quantitative Ecology.\nA second highly recommended text is the book Tree Diversity Analysis by Roeland Kindt and Richard Coe.\nI can also recommend a few websites with excellent content, such as:\n\nDavid Zelený’s Analysis of Community Ecology Data in R\nMike Palmer’s Ordination Methods for Ecologists\nGUide to STatistical Analysis in Microbial Ecology (GUSTA ME)\n\nNote that the URLs with links to additional reading that appear with the worked-through example code should not be seen as optional. They are there for a reason and should be consulted even though I might not necessarily refer to each of them in class.\nUse these materials liberally.\nShould you want to download the source code for the BCB743 (and BCB744 website), you may find it on  GitHub."
  },
  {
    "objectID": "BCB743/BCB743_index.html#datasets-used-in-this-module",
    "href": "BCB743/BCB743_index.html#datasets-used-in-this-module",
    "title": "BCB743: Quantitative Ecology",
    "section": "8.2 Datasets used in this module",
    "text": "8.2 Datasets used in this module\nNote that the links provided might not necessarily lead to the vegan help page.\n\n\n\n\nDataset\nSource\n\n\n\n\n1\nVegetation and Environment in Dutch Dune Meadows\nvegan\n\n\n2\nOribatid Mite Data with Explanatory Variables\nvegan\n\n\n3\nThe Doubs River Data\nNumerical Ecology with R\n\n\n4\nThe Barro Colorado Island Tree Counts\nvegan\n\n\n5\nJohn Bolton, Rob Anderson, and Herre Stegenga’s Seaweed Data\nSmit et al., 2017\n\n\n6\nSerge Mayombo’s Diatoms Data\nMayombo et al., 2019\n\n\n7\nWorld Health Organization Sustainable Development Goals Data\nWHO"
  },
  {
    "objectID": "BCB743/BCB743_index.html#continuous-assessment",
    "href": "BCB743/BCB743_index.html#continuous-assessment",
    "title": "BCB743: Quantitative Ecology",
    "section": "16.1 Continuous Assessment",
    "text": "16.1 Continuous Assessment\nQuantitative Ecology is about working with real-world datasets. To this end, a series of mini-projects (Tasks) involving real data are a required part of the BCB743. You may work alone or with a single partner on all projects (in which case you will share the same mark). All Tasks are due by the date and time indicated, and due care must be taken that they are submitted as instructed, i.e. paying attention to naming conventions and the format of the files submitted—typically this will be in Quarto format (.qmd) and the knitted output (I prefer .html).\nThe Continuous Assessment is comprised of the material indicated in the Syllabus by the  icon. There are seven smaller assessments (each weighted 0.1) and one slightly more demanding Integrative Assessment (weighted 0.3) at the end of the module.\n\n16.1.1 Tasks\nWhen assessing the tasks, we will pay attention to the following criteria:\n\nContent (10%):\n\nQuestions answered in order\nAnnotations (meta-data in file header, commnets about code, ideas, and approach)\n\nCode formatting and correctness (45%):\n\nApplication of R code conventions, e.g. spaces around &lt;-, after #, after ,, etc.\nNew line for each dplyr function (lines end in %&gt;%) or ggplot layer (lines end in +)\n\nFigures (45%):\n\nSensible use of themes / colours\nPublication quality\nInformative and complete titles, axes labels, legends, etc.\n\n\n\n\n16.1.2 Integrative Assessment\nThe above-mentioned Assessments will be graded with the following in expectations in mind:\n\nContent (20%):\n\nQuestions answered in order\nA written explanation of approach included for each question\nAppropriate formatting of text, for example, fonts not larger than necessary, headings used properly, etc. Be sensible and tasteful.\n\nCode formatting, structure, and correctness (50%):\n\nUse Tidyverse code\nNo more than ~80 characters of code per line (pay particular attention to the comments)\nApplication of R code conventions, e.g. spaces around &lt;-, after #, after ,, etc.\nNew line for each dplyr function (lines end in %&gt;%) or ggplot layer (lines end in +)\nProper indentation of pipes and ggplot() layers\nAll chunks labelled without spaces\nNo unwanted / commented out code left behind in the document\n\nFigures (30%):\n\nSensible use of themes / colours\nPublication quality\nInformative and complete titles, axes labels, legends, etc.\nNo redundant features or aesthetics\n\n\nRandom quizzes will not form part of the CA for BCB743."
  },
  {
    "objectID": "BCB743/BCB743_index.html#final-assessment-exam",
    "href": "BCB743/BCB743_index.html#final-assessment-exam",
    "title": "BCB743: Quantitative Ecology",
    "section": "16.2 Final Assessment (Exam)",
    "text": "16.2 Final Assessment (Exam)\nThe Final Assessment will be a 48-hour affair that you can do in the comfort of your home. It will involve the analysis of real world data, and it is expected that the submission includes 1) statements of aims, objectives, and hypotheses; 2) the full and detailed methods followed by analyses together with all code, 3) full reporting of results in a manner suited for peer reviewed publications; 4) graphical support highlighting the patterns observed (again with the code), and 5) a discussion if and when required. The allocation of marks to the various sections is:\n\nAims, objectives, and hypotheses: 5%\nMethods and analyses: 45%\nResults: 20%\nGraphs: 15%\nDicsussion: 15%"
  },
  {
    "objectID": "BCB743/BCB743_index.html#submission-of-assignments-and-exams",
    "href": "BCB743/BCB743_index.html#submission-of-assignments-and-exams",
    "title": "BCB743: Quantitative Ecology",
    "section": "16.3 Submission of assignments and exams",
    "text": "16.3 Submission of assignments and exams\nA statement such as the one below accompanies every assignment—pay attention, as failing to observe this instruction may result in a loss of marks (i.e. if an assignment remains ungraded because the owner of the material cannot be identified):\nSubmit a R script wherein you provide answers to Questions 1–9 by no later than 8:00 tomorrow. Label the script as follows (e.g.): BCB743_AJ_Smit_Assignment_2.R."
  },
  {
    "objectID": "BCB743/BCB743_index.html#late-submission-of-ca",
    "href": "BCB743/BCB743_index.html#late-submission-of-ca",
    "title": "BCB743: Quantitative Ecology",
    "section": "16.4 Late submission of CA",
    "text": "16.4 Late submission of CA\nLate assignments will be penalised 10% per day and will not be accepted more than 48 hours late, unless evidence such as a doctor’s note, a death certificate, or another documented emergency can be provided. If you know in advance that a submission will be late, please discuss this and seek prior approval. This policy is based on the idea that in order to learn how to translate your human thoughts into computer language (coding) you should be working with them at multiple times each week—ideally daily. Time has been allocated in class for working on assignments and students are expected to continue to work on the assignments outside of class. Successfully completing (and passing) this module requires that you finish assignments based on what we have covered in class by the following class period. Work diligently from the onset so that even if something unexpected happens at the last minute you should already be close to done. This approach also allows rapid feedback to be provided to you, which can only be accomplished by returning assignments quickly and punctually."
  },
  {
    "objectID": "BCB743/BCB743_index.html#help-via-bcb744-and-bcb743-issues-on-github",
    "href": "BCB743/BCB743_index.html#help-via-bcb744-and-bcb743-issues-on-github",
    "title": "BCB743: Quantitative Ecology",
    "section": "17.1 Help via BCB744 and BCB743 Issues on GitHub",
    "text": "17.1 Help via BCB744 and BCB743 Issues on GitHub\nAll discussion for the BCB744 and BCB743 workshops will be held in the Issues of this repository. Please post all content-related questions there, and use email only for personal matters. Note that this is a public repository, so be professional in your writing here (grammar, etc.).\nTo start a new thread, create a New issue. Tag your peers using their handle—@ajsmit, for example—to get their attention.\nOnce a question has been answered, the issue will be closed, so lots of good answers might end up in closed issues. Don’t forget to look there when looking for answers—you can use the Search feature on this repository to find answers that might have been offered by the same or similar problem experienced by someone else in the past.\nGuidelines for posting questions:\n\nFirst search existing issues (open or closed) for answers. If the question has already been answered, you’re done! If there is an open issue, feel free to contribute to it. Or feel free to open a closed issue if you believe the answer is not satisfactory.\nGive your issue an informative title.\n\nGood: “Error: could not find function”ggplot””\nBad: “My code does not work!” Note that you can edit an issue’s title after it’s been posted.\n\nFormat your questions nicely using markdown and code formatting. Preview your issue prior to posting.\nAs I explained above, your peers and I will more sympathetic to your cause if you can show all the things you have tried as you, yourself, tried to fix the issue first.\nInclude code and example data so the person trying to help you have something to work with (and which results in the error, perhaps)\nWhere appropriate, provide links to specific files, or even lines within them, in the body of your issue. This will help your peers understand your question. Note that only the teaching team will have access to private repos.\n(Optional) Tag someone or some group of people. Start by typing their GitHub username prefixed with the @ symbol. Of course this supposes that each of you have a GitHub account and username.\nHit Submit new issue when you’re ready to post."
  },
  {
    "objectID": "BCB743/assessments/Task_B.html",
    "href": "BCB743/assessments/Task_B.html",
    "title": "5. Correlations and Associations",
    "section": "",
    "text": "Using the Doubs River environmental data, create a plot of pairwise correlations.\nName to two top positive and two top negative statistically-significant correlations.\nFor each, discuss the mechanism behind the relationships. Why do these relationships exist?\nWhy do we need to transpose the data?\nWhat are the properties of a transposed species table?\nWhat are the properties of an association matrix? How do these properties differ from that of a i) species dissimilarity matrix and from a ii) correlation matrix?\nWhat is the difference between spp_assoc1 and spp_assoc2? Is the information contained in each markedly different from the other?\nExplain the kind of insight we are able to glean from a species association matrix.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–8 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_B.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_B.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_B.html#task-b",
    "href": "BCB743/assessments/Task_B.html#task-b",
    "title": "5. Correlations and Associations",
    "section": "",
    "text": "Using the Doubs River environmental data, create a plot of pairwise correlations.\nName to two top positive and two top negative statistically-significant correlations.\nFor each, discuss the mechanism behind the relationships. Why do these relationships exist?\nWhy do we need to transpose the data?\nWhat are the properties of a transposed species table?\nWhat are the properties of an association matrix? How do these properties differ from that of a i) species dissimilarity matrix and from a ii) correlation matrix?\nWhat is the difference between spp_assoc1 and spp_assoc2? Is the information contained in each markedly different from the other?\nExplain the kind of insight we are able to glean from a species association matrix.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–8 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_B.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_B.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_C.html",
    "href": "BCB743/assessments/Task_C.html",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "",
    "text": "With reference to the sampling design (i.e. position of sample sites along the length of the river), provide mechanistic/ecological reasons for the strongly correlated environmental variables shown above in the pairwise correlation diagram. You might have to create additional spatial maps of scaled variables (as immediately above) to support your answer.\nProvide a summary of the main findings of the Doubs River fish community structure study, focusing in this instance mainly on the environmental drivers.\nWhy can a PCA, or any ordination for that matter, not explain all of the variation in a dataset? In other words, why is it best to only use the first few Principal Components for insight into the drivers of variability? What is ‘explained’ by the remaining PC axes?\nReplicate the analysis shown above on the environmental data included with these datasets:\n\nbird communities along elevation gradient in Yushan Mountain, Taiwan;\nalpine plant communities in Aravo, France.\n\nDiscuss the patterns observed:\n\nexplain the ordination diagram with particular reference to the major patterns shown;\nprovide a mechanistic explanation for the existence of the patterns seen with respect to elevation/altitude; and\nif there are significant positive or negative correlations between the environmental variables, provide mechanistic reasons for how they came about.\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–5 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_C.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_C.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_C.html#task-c",
    "href": "BCB743/assessments/Task_C.html#task-c",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "",
    "text": "With reference to the sampling design (i.e. position of sample sites along the length of the river), provide mechanistic/ecological reasons for the strongly correlated environmental variables shown above in the pairwise correlation diagram. You might have to create additional spatial maps of scaled variables (as immediately above) to support your answer.\nProvide a summary of the main findings of the Doubs River fish community structure study, focusing in this instance mainly on the environmental drivers.\nWhy can a PCA, or any ordination for that matter, not explain all of the variation in a dataset? In other words, why is it best to only use the first few Principal Components for insight into the drivers of variability? What is ‘explained’ by the remaining PC axes?\nReplicate the analysis shown above on the environmental data included with these datasets:\n\nbird communities along elevation gradient in Yushan Mountain, Taiwan;\nalpine plant communities in Aravo, France.\n\nDiscuss the patterns observed:\n\nexplain the ordination diagram with particular reference to the major patterns shown;\nprovide a mechanistic explanation for the existence of the patterns seen with respect to elevation/altitude; and\nif there are significant positive or negative correlations between the environmental variables, provide mechanistic reasons for how they came about.\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–5 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_C.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_C.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_A.html",
    "href": "BCB743/assessments/Task_A.html",
    "title": "1. Introduction",
    "section": "",
    "text": "Ecologists use diverse data sources, including open data repositories and data obtained through field campaigns, to address questions about ecological communities across various scales of time and space. Discuss how different data types can be accessed, integrated, and utilised in studying ecological patterns and processes. Your considered and detailed response should:\n\nDefine and differentiate open data and field campaign data in the context of ecological studies.\nDevelop a typology for the types ecologists access to advance the theory of ecological communities.\nProvide specific examples of databases or data repositories for open and field campaign data.\nDiscuss the strengths and limitations of the main classes of data types.\nDescribe methods for integrating these data sources and why this is beneficial.\nProvide specific examples of studies or scenarios where open and field campaign data have been used to enhance our understanding of ecological communities across different spatial and temporal scales.\nReflect on the role of technology, including advancements in data analytics and computational models, in accessing, analysing, and integrating these data sources.\n\nYour essay should demonstrate a comprehensive understanding of the use of data in ecology and how it can be harnessed to yield valuable insights into the dynamics and patterns of ecological communities across different scales."
  },
  {
    "objectID": "BCB743/assessments/Task_A.html#task-a",
    "href": "BCB743/assessments/Task_A.html#task-a",
    "title": "1. Introduction",
    "section": "",
    "text": "Ecologists use diverse data sources, including open data repositories and data obtained through field campaigns, to address questions about ecological communities across various scales of time and space. Discuss how different data types can be accessed, integrated, and utilised in studying ecological patterns and processes. Your considered and detailed response should:\n\nDefine and differentiate open data and field campaign data in the context of ecological studies.\nDevelop a typology for the types ecologists access to advance the theory of ecological communities.\nProvide specific examples of databases or data repositories for open and field campaign data.\nDiscuss the strengths and limitations of the main classes of data types.\nDescribe methods for integrating these data sources and why this is beneficial.\nProvide specific examples of studies or scenarios where open and field campaign data have been used to enhance our understanding of ecological communities across different spatial and temporal scales.\nReflect on the role of technology, including advancements in data analytics and computational models, in accessing, analysing, and integrating these data sources.\n\nYour essay should demonstrate a comprehensive understanding of the use of data in ecology and how it can be harnessed to yield valuable insights into the dynamics and patterns of ecological communities across different scales."
  },
  {
    "objectID": "BCB743/assessments/Task_A.html#additional-instructions",
    "href": "BCB743/assessments/Task_A.html#additional-instructions",
    "title": "1. Introduction",
    "section": "2 Additional instructions",
    "text": "2 Additional instructions\nYour goal is to provide a professional, well-structured, and information-rich document. Here, professional applies to presentation (formatting and appearance) and content (quality of content, narrative, and language).\nThe maximum length of the essay should not exceed 10 pages, excluding references.\n\n2.1 Presentation\nThe points below will address some of the presentation aspects:\n\nUse citations and footnotes (including bibliography files and the automatic generation of bibliographies)—through collective effort by the class, you will learn how these tools work. To this end, 10% of the marks will assess your ability to use these facilities to their fullest extent.\nThe typology of data types can be presented as flow diagrams. Should you wish to include diagrams, please use Mermaid Diagrams within Quarto. Again, the combined class effort will quickly bring you towards grasping and understanding the concepts.\nShould diagrams not be to your liking or not suited to your specific requirement, markdown tables might be a better option for presenting structured information.\nFigures can also be inserted into Quarto documents if needs be.\nTo ensure easy use, provide your html file as a self-contained document by inserting the necessary options in your Quarto document’s YAML header.\n\n\n\n2.2 Content\nTo arrive at professional content (emphasis on the language component of professional content), please feel free to use ChatGPT. However, I want to see two versions of your document:\n\nthe first version will show the essay in your own words, prior to applying ChatGPT polish\nthe second version will have the grammatical, punctuation, grammar, and language checked by ChatGPT\n\nFor example, consider the following text produced by a student:\n\nThin and elongated structures are essential for plants, this structures increase the surface area of the plants. Having a large surface area ensures that the mass f the plant is exposed to the sun and the environment around it. The plants will have large sa/v ratios, the plants will be dominated with physiological processes thus increasing their of photosynthesis because the ability oof diffusion is much faster.\n\nIn ChatGPT, construct the following prompt:\n\nCorrect the language and grammar: “Thin and elongated structures are essential for plants, this structures increase the surface area of the plants. Having a large surface area ensures that the mass f the plant is exposed to the sun and the environment around it. The plants will have large sa/v ratios, the plants will be dominated with physiological processes thus increasing their of photosynthesis because the ability oof diffusion is much faster.”\n\nIt will return the improved text:\n\nThin and elongated structures are vital for plants, as these structures augment the plant’s surface area. A larger surface area ensures that a substantial portion of the plant mass is exposed to the sun and its surrounding environment. As a result, plants will exhibit large surface area to volume (SA/V) ratios. Such plants will be characterized by dominant physiological processes, thereby enhancing their photosynthesis rates due to faster diffusion capabilities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit an essay structured under well-defined and logical headings by Tuesday, 27 June, by no later than 08:00.\nProvide your essay as a professional Quarto-generated html file.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_A.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_A.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_D.html",
    "href": "BCB743/assessments/Task_D.html",
    "title": "8c. PCA of WHO SDGs and 13. Cluster Analysis",
    "section": "",
    "text": "Question 1 refers to PCA SDG.\n\nExplain the code section-by-section in long-form text. Include also the reasoning/rationale behind each section.\n\nQuestions 2–5 refer to Cluster Analysis.\n\nWhat happens if we use pam() to create four, five, or even six clusters?\nIn your reasoned opinion, what would be the optimal number of clusters to use?\nRepeat the analysis using either kmeans() or hclust(), and feel free to use the factoextra helper functions and visualisations. Are the results markedly different? Which clustering approach do you wish to proceed with—i.e., pam(), hclust() or kmeans()?\nDescribe the patterns that you observe at the end of your ordination and final cluster selection (i.e. based on the optimal number of clusters and whichever cluster technique you deem most appropriate). How does South Africa fare in terms of attaining SDGs? Contrast with some key countries of your choice to make your points. Label the key countries that you refer to in your text by updating the code accordingly. Continue to explain these patterns in terms of the global socio-political/socio-economic landscape. Provide a discourse about possible explanations for the patterns observed globally and regionally.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–5 by no later than 8:00 the day after the Cluster Analysis lecture.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_D.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_D.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_D.html#task-d",
    "href": "BCB743/assessments/Task_D.html#task-d",
    "title": "8c. PCA of WHO SDGs and 13. Cluster Analysis",
    "section": "",
    "text": "Question 1 refers to PCA SDG.\n\nExplain the code section-by-section in long-form text. Include also the reasoning/rationale behind each section.\n\nQuestions 2–5 refer to Cluster Analysis.\n\nWhat happens if we use pam() to create four, five, or even six clusters?\nIn your reasoned opinion, what would be the optimal number of clusters to use?\nRepeat the analysis using either kmeans() or hclust(), and feel free to use the factoextra helper functions and visualisations. Are the results markedly different? Which clustering approach do you wish to proceed with—i.e., pam(), hclust() or kmeans()?\nDescribe the patterns that you observe at the end of your ordination and final cluster selection (i.e. based on the optimal number of clusters and whichever cluster technique you deem most appropriate). How does South Africa fare in terms of attaining SDGs? Contrast with some key countries of your choice to make your points. Label the key countries that you refer to in your text by updating the code accordingly. Continue to explain these patterns in terms of the global socio-political/socio-economic landscape. Provide a discourse about possible explanations for the patterns observed globally and regionally.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–5 by no later than 8:00 the day after the Cluster Analysis lecture.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_D.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_D.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_E.html",
    "href": "BCB743/assessments/Task_E.html",
    "title": "9. Correspondence Analysis (CA)",
    "section": "",
    "text": "How would you explain the patterns seen in the last four panels in the Correspondence Analysis Chapter?\nApply approaches taken from the analysis in the Correspondence Analysis Chapter to these datasets:\n\nbird communities along elevation gradient in Yushan Mountain, Taiwan;\nalpine plant communities in Aravo, France.\n\nDiscuss the patterns observed, and explain the ordination diagrams with particular reference to how the species are influenced by the major environmental drivers.\n(Bonus) For bonus marks that could earn you 120/100, please see if you can recreate the bottom right figure for the species ‘Cogo’ using ggplot2. This will require digging deep into the ordination and ordisurf objects, creating dataframes for each layer of data, and assembling the graph manually.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–4 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_E.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_E.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_E.html#task-e",
    "href": "BCB743/assessments/Task_E.html#task-e",
    "title": "9. Correspondence Analysis (CA)",
    "section": "",
    "text": "How would you explain the patterns seen in the last four panels in the Correspondence Analysis Chapter?\nApply approaches taken from the analysis in the Correspondence Analysis Chapter to these datasets:\n\nbird communities along elevation gradient in Yushan Mountain, Taiwan;\nalpine plant communities in Aravo, France.\n\nDiscuss the patterns observed, and explain the ordination diagrams with particular reference to how the species are influenced by the major environmental drivers.\n(Bonus) For bonus marks that could earn you 120/100, please see if you can recreate the bottom right figure for the species ‘Cogo’ using ggplot2. This will require digging deep into the ordination and ordisurf objects, creating dataframes for each layer of data, and assembling the graph manually.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–4 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_E.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_E.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_F.html",
    "href": "BCB743/assessments/Task_F.html",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "",
    "text": "Using two unconstrained ordination techniques of your choice, analyse the mite data in the vegan package. Provide a brief description and discussion of what you have found, and produce the R code.\nUsing two unconstrained ordination techniques of your choice (not already used in 1, above) analyse the dune data in the vegan package. Provide a brief description and discussion of what you have found, and produce the R code.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–2 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_F.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_F.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/Task_F.html#task-f",
    "href": "BCB743/assessments/Task_F.html#task-f",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "",
    "text": "Using two unconstrained ordination techniques of your choice, analyse the mite data in the vegan package. Provide a brief description and discussion of what you have found, and produce the R code.\nUsing two unconstrained ordination techniques of your choice (not already used in 1, above) analyse the dune data in the vegan package. Provide a brief description and discussion of what you have found, and produce the R code.\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to Questions 1–2 by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated Quarto/html files which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions.\nPlease label the Quarto and resulting HTML files as follows:\n\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_F.qmd, and\nBCB743_&lt;first_name&gt;_&lt;last_name&gt;_Task_F.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Labs on iKamva when ready."
  },
  {
    "objectID": "BCB743/assessments/BCB743_intgrative_assignment.html",
    "href": "BCB743/assessments/BCB743_intgrative_assignment.html",
    "title": "BCB743",
    "section": "",
    "text": "In the light of all the possible analyses of the Doubs River study (i.e. the earlier PCA and CA analyses as well as the nMDS, RDA, CCA, and clustering techniques in the coming week), provide a full analysis of the Doubs River fish community structure study, focusing on:\n\nthe environmental drivers,\nthe fish community composition, and\nan integrative view of the environmental structuring of the fish community."
  },
  {
    "objectID": "BCB743/assessments/BCB743_intgrative_assignment.html#the-assignment",
    "href": "BCB743/assessments/BCB743_intgrative_assignment.html#the-assignment",
    "title": "BCB743",
    "section": "",
    "text": "In the light of all the possible analyses of the Doubs River study (i.e. the earlier PCA and CA analyses as well as the nMDS, RDA, CCA, and clustering techniques in the coming week), provide a full analysis of the Doubs River fish community structure study, focusing on:\n\nthe environmental drivers,\nthe fish community composition, and\nan integrative view of the environmental structuring of the fish community."
  },
  {
    "objectID": "BCB743/assessments/BCB743_intgrative_assignment.html#additional-information",
    "href": "BCB743/assessments/BCB743_intgrative_assignment.html#additional-information",
    "title": "BCB743",
    "section": "2 Additional Information",
    "text": "2 Additional Information\nYou are welcome to suggest your own analyses, as necessary, to support the approaches already taken in the module. Your analysis must include one or several ordination techniques (with a justification for why they were selected), as well as a clustering approach. The more novelty you bring to the analysis the better for your marks.\nCritically discuss your findings in the context of the work initially done by Verneaux et al. (2003). Note that a critical discussion necessitates looking at all major findings of Verneaux et al. (2003) in the light of what your own analyses tell you. In doing so, you must support your own reasoning for agreeing or disagreeing by providing substantiating rational reasoning.\nA completely novel (and correct) data and theoretical analysis can earn you marks in excess of 100%."
  },
  {
    "objectID": "BCB743/assessments/BCB743_intgrative_assignment.html#instructions",
    "href": "BCB743/assessments/BCB743_intgrative_assignment.html#instructions",
    "title": "BCB743",
    "section": "3 Instructions",
    "text": "3 Instructions\nSubmit a R markdown script and the knitted output for the Integrative Assignment by no later than 11:59 on Friday 4 August 2023. Label the script as follows: BCB743_&lt;Name&gt;_&lt;Surname&gt;_Integrative_Assignment.qmd and BCB743_&lt;Name&gt;_&lt;Surname&gt;_Integrative_Assignment.html (or use MS Word format).\nUnlike previous work in the module, this assignment will be submitted as a professionally formatted MS Word document that follows the author guidelines of South African Journal of Botany.\nYour analysis must be structured as follows: Introduction (with Aims and Objectives), Methods, Results, Discussion, References (minimum 10 references, all of which were published after 2003).\nThe page limit for the full body of work must not exceed 20 pages (minimum 13 pages), with the Figures/Tables not occupying more than 25% of the total page count. In order to professionally arrange the figures (multiple figures per page, and liberal use of subplots), please make use of the ggarrange package."
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html",
    "href": "BCB743/12-constrained_ordination.html",
    "title": "12. Constrained Ordination",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nSlides\nConstrained ordination lecture slides\n💾 BCB743_12_constrained_ordination.pdf\n\n\nReading\nSmit et al. (2017)\n💾 Smit_et_al_2017.pdf\n\n\n\nSupp. to Smit et al. (2017)\n💾 Smit_the_seaweed_data.pdf\n\n\nData\nThe seaweed environmental data\n💾 SeaweedEnv.RData\n\n\n\nThe seaweed species data\n💾 SeaweedSpp.csv\n\n\n\nThe bioregions\n💾 bioregions.csv\n\n\n\nThe seaweed coastal section coordinates\n💾 SeaweedSites.csv"
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#introduction",
    "href": "BCB743/12-constrained_ordination.html#introduction",
    "title": "12. Constrained Ordination",
    "section": "\n1 Introduction",
    "text": "1 Introduction\nUp to now we have applied unconstrained ordination, or indirect gradient analyses. The lecture slides mention several constrained ordinations and provide some theory for three of them, viz. Redundancy Analysis (RDA), Canonical Correspondence Analysis (CCA), and distance-based Redundancy Analysis (db-RDA). These ordinations form the topic of this Chapter. Constrained ordination is sometimes called ‘direct gradient analysis’ or ‘canonical’ ordination.\nConstrained ordination is used to extract and summarise the variation in a set of response variables (species data in the case of ecology) that can be explained by some explanatory variables (‘constraints’), such as measurements of environmental properties at the places where the species data were collected from. These analyses relate two (or more) matrices to one-another—one of them with the species table within which the community structure is sought, and the other an explanatory matrix of environmental conditions (or traits, etc.) that are thought to explain the community patterns. The ecologist is then also able to apply a confirmatory analysis, i.e., methods are available to test the statistical significance of the relationships between explanatory variables and the resultant species composition. This is not possible with unconstrained ordination, and hence unconstrained ordination is not actually a statistical methodology. Note that the confirmation relates to the fact that there is some kind of relationship between the matrices, NOT that the ecological process ACTUALLY exists (although it hints at a good likelihood that it does—but a careful scientist will use this as a starting point for hypothesis generation and design experimental confirmation of the causal relationship hinted at by the confirmation).\nWe will consider three constrained ordination techniques:\n\nRDA is a direct gradient analysis that highlights linear relationships between components of response variables, i.e. variables that are ‘redundant’ with (i.e. ‘explained’ by) a set of predictors. RDA is an extension of a PCA with a multiple linear regression. The same constraints inherent in a PCA present themselves in an RDA. Use vegan’s rda() to perform an RDA.\nCCA is the extension of a CA with multiple regression, and is therefore also based on 𝝌2-metric (dissimilarities). We do not have a choice of specifying which dissimilarity meric to use. CCA performs best when species distribution follows a unimodal model. Use vegan’s cca() to perform an RDA.\ndb-RDA can be viewed as the extension of a PCoA with multiple regressions. As with a PCoA, we also benefit from being able to specify any dissimilarity matrix as input, and hence this approach is more versatile compared to RDA or CCA. I prefer the db-RDA implemented in vegan’s capscale(). The help file states: “Distance-based redundancy analysis (dbRDA) is an ordination method similar to Redundancy Analysis (rda), but it allows non-Euclidean dissimilarity indices, such as Manhattan or Bray–Curtis distance.”"
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#the-seaweed-dataset",
    "href": "BCB743/12-constrained_ordination.html#the-seaweed-dataset",
    "title": "12. Constrained Ordination",
    "section": "\n2 The seaweed dataset",
    "text": "2 The seaweed dataset\nFor this example we will use the seaweed data of Smit et al. (2017); please make sure that you read it! An additional file describing the background to the data is available at the link above (see The_seaweed_data.pdf).\nI use two data sets. The first, \\(Y\\) (in the file seaweeds.csv), comprises distribution records of 847 macroalgal species within each of 58 × 50 km-long sections of the South African coast (updated from Bolton and Stegenga (2002)). This represents ca. 90% of the known seaweed flora of South Africa, but excludes some very small and/or very rare species for which data are insufficient. The data are from verifiable literature sources and John Bolton and Rob Anderson’s own collections, assembled from information collected by teams of phycologists over three decades (Bolton 1986; Stegenga et al. 1997; Bolton and Stegenga 2002; De Clerck et al. 2005).\nThe second, \\(E\\) (in env.csv), is a dataset of in situ coastal seawater temperatures (Smit et al. 2013) derived from daily measurements over up to 40 years."
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#set-up-the-analysis-environment",
    "href": "BCB743/12-constrained_ordination.html#set-up-the-analysis-environment",
    "title": "12. Constrained Ordination",
    "section": "\n3 Set-up the analysis environment",
    "text": "3 Set-up the analysis environment\n\nlibrary(tidyverse)\nlibrary(betapart)\nlibrary(vegan)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(gridBase)\n\nLoad the seaweed data:\n\nspp &lt;- read.csv(\"../data/seaweed/SeaweedSpp.csv\")\nspp &lt;- dplyr::select(spp, -1)\ndim(spp)\n\n[1]  58 847"
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#set-up-the-data",
    "href": "BCB743/12-constrained_ordination.html#set-up-the-data",
    "title": "12. Constrained Ordination",
    "section": "\n4 Set-up the data",
    "text": "4 Set-up the data\nThe first step involves the species table (\\(Y\\)). First I compute the Sørensen dissimilarity, which I then decompose into ‘nestedness-resultant’ (\\(\\beta_\\text{sne}\\)) and ‘turnover’ (\\(\\beta_\\text{sim}\\)) components using the betapart.core() and betapart.pair() functions of the betapart package (Baselga et al. 2018). These are placed into the matrices \\(Y1\\) and \\(Y2\\). It is not necessary to decompose into \\(Y1\\) and \\(Y2\\), but I do so here because I want to focus on the turnover component without a nestedness-resultant influence. Optionally, I can apply a CA, PCoA, or nMDS on \\(Y\\) to find the major patterns in the community data. The formal analysis will use the species data in a distance-based redundancy analyses (db-RDA as per vegan’s capscale() function) by coupling it with \\(E\\).\n\nY.core &lt;- betapart.core(spp) \nY.pair &lt;- beta.pair(Y.core, index.family = \"sor\")\n\n# Let Y1 be the turnover component (beta-sim):\nY1 &lt;- as.matrix(Y.pair$beta.sim)\n\nIt is now necessary to load the environmental data and some setup files that partition the 58 coastal sections (and the species and environmental data that fall within these sections) into bioregions.\nThe thermal (environmental) data contain various variables, but in the analysis I use only some of them. These data were obtained from many sites along the South African coast, but using interpolation (not included here) I calculated the thermal properties for each of the coastal sections for which seaweed data are available. Consequently we have a data frame with 58 rows and a column for each of the thermal metrics.\n\nload(\"../data/seaweed/SeaweedEnv.RData\")\ndim(env)\n\n[1] 58 18\n\n\nNote that they have the same number of rows as the seaweed data.\nI select only some of the thermal variables; the rest are collinear with some of the ones I import:\n\nE1 &lt;- dplyr::select(env, febMean, febRange, febSD, augMean,\n                    augRange, augSD, annMean, annRange, annSD)\n\nNext I calculate z-scores:\n\nE1 &lt;- decostand(E1, method = \"standardize\")\n\nFour bioregions are recognised for South Africa by Bolton and Anderson (2004) (the variable called bolton), namely the Benguela Marine Province (BMP; coastal sections 1–17), the Benguela-Agulhas Transition Zone (B-ATZ; 18–22), the Agulhas Marine Province (AMP; 19–43/44) and the East Coast Transition Zone (ECTZ; 44/45–58). My plotting functions partition the data into the bioregions and colour code the figures accordingly so I can see regional patterns in \\(\\beta\\)-diversity emerging.\n\nbioreg &lt;- read.csv(\"../data/seaweed/bioregions.csv\")\nhead(bioreg)\n\n  spal.prov spal.ecoreg lombard bolton\n1       BMP          NE   NamBR    BMP\n2       BMP          NE   NamBR    BMP\n3       BMP          NE   NamBR    BMP\n4       BMP          NE   NamBR    BMP\n5       BMP          NE   NamBR    BMP\n6       BMP          NE   NamBR    BMP\n\n\nLoad the geographic coordinates for the coastal sections:\n\nsites &lt;- read.csv(\"../data/seaweed/SeaweedSites.csv\")\nsites &lt;- sites[, c(2, 1)]\nhead(sites)\n\n  Longitude  Latitude\n1  16.72429 -28.98450\n2  16.94238 -29.38053\n3  17.08194 -29.83253\n4  17.25928 -30.26426\n5  17.47638 -30.67874\n6  17.72167 -31.08580\n\ndim(sites)\n\n[1] 58  2\n\n\nAgain, we have 58 rows of data for both the coastal section coordinates and the bioregions. You may omit the dataset with spatial coordinates as it is not actually used further below. Can you think of ways in which to use this dataset to graphically represent the spatial distribution of some environmental or biodiversity data?"
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#start-the-db-rda",
    "href": "BCB743/12-constrained_ordination.html#start-the-db-rda",
    "title": "12. Constrained Ordination",
    "section": "\n5 Start the db-RDA",
    "text": "5 Start the db-RDA\nI test the niche difference mechanism as the primary species compositional assembly process operating along South African shores. I suggest that the thermal gradient along the coast provides a suite of abiotic (thermal) conditions from which species can select based on their physiological tolerances, and hence this will structure \\(\\beta\\)-diversity. For this mechanism to function one would assume that all species have equal access to all sections along this stretch of coast, thus following ‘Beijerinck’s Law’ that everything is everywhere but the environment selects (Sauer 1991).\nI do a RDA involving all the thermal variables in \\(E1\\) (the ‘global analysis’ resulting in the full model, rda_full). Analysis shown for \\(Y1\\):\n\n# fit the full model:\nrda_full &lt;- capscale(Y1 ~., E1)\nrda_full\n\nCall: capscale(formula = Y1 ~ febMean + febRange + febSD + augMean +\naugRange + augSD + annMean + annRange + annSD, data = E1)\n\n               Inertia Proportion Rank\nTotal          7.52344    1.00000     \nConstrained    6.86398    0.91235    8\nUnconstrained  1.02840    0.13669   28\nImaginary     -0.36895   -0.04904   24\nInertia is squared Unknown distance \nSome constraints or conditions were aliased because they were redundant\n\nEigenvalues for constrained axes:\n CAP1  CAP2  CAP3  CAP4  CAP5  CAP6  CAP7  CAP8 \n5.620 1.155 0.074 0.006 0.004 0.003 0.001 0.001 \n\nEigenvalues for unconstrained axes:\n  MDS1   MDS2   MDS3   MDS4   MDS5   MDS6   MDS7   MDS8 \n0.5768 0.1687 0.1096 0.0413 0.0322 0.0243 0.0179 0.0103 \n(Showing 8 of 28 unconstrained eigenvalues)\n\n\n\n# summary(rda_full)\n# notice that the species scores are missing\n# refer to PCoA for why\n\nSpecies information is lost during the calculation of the dissimilarity matrix, but if the original matrix of species composition is available, the species scores can be added back into the ordination diagram as weighted means of site scores in which case they occur or as vectors fitted onto the ordination space.\nIs the fit significant? I run a permutation test to check:\n\nanova(rda_full, parallel = 4) # ... yes!\n\nPermutation test for capscale under reduced model\nPermutation: free\nNumber of permutations: 999\n\nModel: capscale(formula = Y1 ~ febMean + febRange + febSD + augMean + augRange + augSD + annMean + annRange + annSD, data = E1)\n         Df SumOfSqs      F Pr(&gt;F)    \nModel     8   6.8640 40.881  0.001 ***\nResidual 49   1.0284                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the fit is significant (the environmental variables capture the variation seen in the species data), I compute the adjusted \\(R^{2}\\):\n\nrda_full_R2 &lt;- RsquareAdj(rda_full)$adj.r.squared\nround(rda_full_R2, 2)\n\n[1] 0.9\n\n\nThe inertia accounted for by constraints:\n\nround(sum(rda_full$CCA$eig), 2)\n\n[1] 6.86\n\n\nThe remaining (unconstrained) inertia:\n\nround(sum(rda_full$CA$eig), 2)\n\n[1] 1.03\n\n\nThe total inertia:\n\nround(rda_full$tot.chi, 2)\n\n[1] 7.52\n\n\nWhat is the proportion of variation explained by the full set environmental variables?\n\nround(sum(rda_full$CCA$eig) / rda_full$tot.chi * 100, 2) # in %\n\n[1] 91.23\n\n\nI check for collinearity using variance inflation factors (VIF), and retain a subset of non-collinear variables to include in the ‘reduced’ or ‘final’ model. A common rule is that values over 10 indicate redundant constraints. I run the VIF procedure iteratively, each time removing the highest VIF and examining the remaining ones until these are mostly below 10.\nFirst on the full model:\n\nvif.cca(rda_full)\n\n   febMean   febRange      febSD    augMean   augRange      augSD    annMean \n 91.129700   6.775959   7.734436  73.090382   8.486631  12.118914 233.400746 \n  annRange      annSD \n        NA   5.396343 \n\n\nDrop annMean:\n\nE2 &lt;- dplyr::select(E1, -annMean)\nrda_sel1 &lt;- capscale(Y1 ~., E2)\nvif.cca(rda_sel1)\n\n  febMean  febRange     febSD   augMean  augRange     augSD  annRange     annSD \n24.996152  6.149245  7.160637 17.717936  8.066340 10.726117        NA  5.396275 \n\n\nDrop febMean:\n\nE3 &lt;- dplyr::select(E2, -febMean)\nrda_sel2 &lt;- capscale(Y1 ~., E3)\nvif.cca(rda_sel2)\n\n febRange     febSD   augMean  augRange     augSD  annRange     annSD \n 6.149245  7.160637  1.619233  8.066340 10.726117  5.529971  5.396275 \n\n\nWe select E3 as the variables to construct the final model (rda_final) from.\nNote: you can switch to the formula interface within capscale() and specify the variables to use on the right-hand side of the formula (as shown but not executed). You will (obviously) no longer analyse only the turnover component of \\(\\beta\\)-diversity as you’ll be using the raw spp data that encapsulate both nestedness-resultant and turnover processes, but the upshot of this is that you’ll now have species scores. Run this bit of code by yourself and see what the outcome is (the ordiplot is affected, as well as the \\(R^{2}\\), number of significant reduced axes, etc.).\n\nrda_final &lt;- rda_sel2\n# rda_final &lt;- capscale(spp ~ febRange + febSD + augMean + augRange + augSD + annRange + annSD, data = E3, distance = \"jaccard\")\n\nWe calculate the significance of the model, the variance explained by all the constraints (in \\(E3\\)) in the final model, as well as the \\(R^{2}\\):\n\n# is the fit significant?\nanova(rda_final, parallel = 4) # ... yes!\n\nPermutation test for capscale under reduced model\nPermutation: free\nNumber of permutations: 999\n\nModel: capscale(formula = Y1 ~ febRange + febSD + augMean + augRange + augSD + annRange + annSD, data = E3)\n         Df SumOfSqs      F Pr(&gt;F)    \nModel     7   6.8251 45.675  0.001 ***\nResidual 50   1.0673                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhich axes are significant?\n\nanova(rda_final, by = \"axis\", parallel = 4) # ... yes!\n\nPermutation test for capscale under reduced model\nForward tests for axes\nPermutation: free\nNumber of permutations: 999\n\nModel: capscale(formula = Y1 ~ febRange + febSD + augMean + augRange + augSD + annRange + annSD, data = E3)\n         Df SumOfSqs        F Pr(&gt;F)    \nCAP1      1   5.6179 263.1786  0.001 ***\nCAP2      1   1.1242  52.6665  0.001 ***\nCAP3      1   0.0725   3.3978  0.368    \nCAP4      1   0.0050   0.2320  1.000    \nCAP5      1   0.0027   0.1274  1.000    \nCAP6      1   0.0013   0.0632  1.000    \nCAP7      1   0.0013   0.0622  1.000    \nResidual 50   1.0673                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nExtract the significant variables in \\(E3\\) that are influential in the final model as influencers of seaweed community differences amongsth coastal sections:\n\n(rda_final_axis_test &lt;- anova(rda_final, by = \"terms\", parallel = 4))\n\nPermutation test for capscale under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nModel: capscale(formula = Y1 ~ febRange + febSD + augMean + augRange + augSD + annRange + annSD, data = E3)\n         Df SumOfSqs        F Pr(&gt;F)    \nfebRange  1   1.0962  51.3541  0.001 ***\nfebSD     1   0.1850   8.6653  0.001 ***\naugMean   1   5.3815 252.1023  0.001 ***\naugRange  1   0.0903   4.2286  0.018 *  \naugSD     1   0.0212   0.9918  0.359    \nannRange  1   0.0196   0.9191  0.376    \nannSD     1   0.0313   1.4666  0.219    \nResidual 50   1.0673                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant variables are:\n\nrda_final_ax &lt;- which(rda_final_axis_test[, 4] &lt; 0.05)\nrda_final_sign_ax &lt;- colnames(E3[,rda_final_ax])\nrda_final_sign_ax\n\n[1] \"febRange\" \"febSD\"    \"augMean\"  \"augRange\"\n\n\nThe adjusted \\(R^{2}\\) for the constraints:\n\nround(rda_final_R2 &lt;- RsquareAdj(rda_final)$adj.r.squared, 2) # %\n\n[1] 0.89\n\n\nThe variance explained by reduced (final) model:\n\nround(sum(rda_final$CCA$eig) / rda_final$tot.chi * 100, 2)\n\n[1] 90.72\n\n\nThe biplot scores for constraining variables:\n\nscores(rda_final, display = \"bp\", choices = c(1:2))\n\n                CAP1        CAP2\nfebRange -0.17951109 -0.90202271\nfebSD    -0.08259676 -0.50969451\naugMean   0.98516406  0.15657181\naugRange  0.03499429 -0.14728687\naugSD    -0.01944793 -0.07441746\nannRange  0.41313533 -0.18050396\nannSD     0.20412535 -0.56833391\nattr(,\"const\")\n[1] 4.550643\n\n\nThese biplot scores will mark the position of the termini of the arrows that indicate the direction and strength of the constraining variables."
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#ordination-diagrams",
    "href": "BCB743/12-constrained_ordination.html#ordination-diagrams",
    "title": "12. Constrained Ordination",
    "section": "\n6 Ordination diagrams",
    "text": "6 Ordination diagrams\nThis code recreates Figure 2a in Smit et al. (2017):\n\n# use scaling = 1 or scaling = 2 for site and species scaling, respectively\nrda_final_scrs &lt;- scores(rda_final, display = c(\"sp\", \"wa\", \"lc\", \"bp\"))\n# see ?plot.cca for insight into the use of lc vs wa scores\n# below I splot the wa (site) scores rather than lc (constraints) scores\nsite_scores &lt;- data.frame(rda_final_scrs$site) # the wa scores\nsite_scores$bioreg &lt;- bioreg$bolton\nsite_scores$section &lt;- seq(1:58)\n\nbiplot_scores &lt;- data.frame(rda_final_scrs$biplot)\nbiplot_scores$labels &lt;- rownames(biplot_scores)\nbiplot_scores_sign &lt;- biplot_scores[biplot_scores$labels %in% rda_final_sign_ax,]\n\nggplot(data = site_scores, aes(x = CAP1, y = CAP2, colour = bioreg)) +\n  geom_point(size = 5.0, shape = 24, fill = \"white\") +\n  geom_text(aes(label = section), size = 3.0, col = \"black\") +\n  geom_label(data = biplot_scores_sign,\n             aes(CAP1, CAP2, label = rownames(biplot_scores_sign)),\n             color = \"black\") +\n  geom_segment(data = biplot_scores_sign,\n               aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"),\n               color = \"lightseagreen\", alpha = 1, size = 0.7) +\n  xlab(\"CAP1\") + ylab(\"CAP2\") +\n  ggtitle(expression(paste(\"Significant thermal variables and \", beta[sim]))) +\n  theme_grey() +\n  theme(panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        aspect.ratio = 0.8)\n\n\n\n\nNote that in Smit et al. (2017, Fig. 2a) I plot the linear constraints (lc scores) rather than the site scores (wa scores). The fact that the positioning of the site scores in ordination space in the figure, above, represents a crude map of South Africa corresponding with geographical coordinates (N-E-S-W) is coincidental (yet it can be logically explained). The coenoclines and gradients are clearly discernible, and the west to east numbering of sites and transitioning of one bioregon into the next are obvious. This map-like arrangement of sites disappears when lc scores are used, but the interpretation of how the thermal drivers structure seaweed biodiversity remains the same."
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#dealing-with-factor-variables",
    "href": "BCB743/12-constrained_ordination.html#dealing-with-factor-variables",
    "title": "12. Constrained Ordination",
    "section": "\n7 Dealing with factor variables",
    "text": "7 Dealing with factor variables\n\nE4 &lt;- E3\n# append the bioregs after the thermal vars\nE4$bioreg &lt;- bioreg$bolton\nhead(E4)\n\n     febRange      febSD   augMean    augRange        augSD   annRange\n1 -0.04433865 -0.2713395 -1.376511 -0.47349787 -0.409770805 -1.0460150\n2 -0.14318268 -0.1083868 -1.433925 -0.06998551 -0.096490845 -0.7657214\n3 -0.39321619 -0.1719978 -1.526950  0.02484832  0.009053095 -0.6311568\n4 -0.60199306 -0.3120605 -1.579735 -0.05076148  0.005487982 -0.5861267\n5 -0.64081940 -0.4095900 -1.546420 -0.09833845  0.041336545 -0.8799034\n6 -0.55083241 -0.4294142 -1.458642 -0.11132528  0.095589389 -1.2225182\n      annSD bioreg\n1 -1.636461    BMP\n2 -1.307622    BMP\n3 -1.143445    BMP\n4 -1.074994    BMP\n5 -1.200028    BMP\n6 -1.343088    BMP\n\nrda_cat &lt;- capscale(Y1 ~., E4)\nplot(rda_cat)\n\n\n\n\nThe default plot works okay and shows all necessary info, but the various pieces (site, species, and centroid scores) are not clearly discernable. Plot the class (factor) centroids in ggplot():\n\n# also extractthe factor centroids for the bioregions\nrda_cat_scrs &lt;- scores(rda_cat, display = c(\"sp\", \"wa\", \"lc\", \"bp\", \"cn\"))\nsite_scores &lt;- data.frame(rda_cat_scrs$site) # the wa scores\nsite_scores$bioreg &lt;- bioreg$bolton\nsite_scores$section &lt;- seq(1:58)\n\nbiplot_scores &lt;- data.frame(rda_cat_scrs$biplot)\nbiplot_scores$labels &lt;- rownames(biplot_scores)\nbiplot_scores_sign &lt;- biplot_scores[biplot_scores$labels %in% rda_final_sign_ax,]\n\nbioreg_centroids &lt;- data.frame(rda_cat_scrs$centroids)\nbioreg_centroids$labels &lt;- rownames(bioreg_centroids)\n\nggplot(data = site_scores, aes(CAP1, CAP2, colour = bioreg)) +\n  geom_point(size = 5.0, shape = 24, fill = \"white\") +\n  geom_text(aes(label = section), size = 3.0, col = \"black\") +\n  geom_label(data = biplot_scores_sign,\n             aes(CAP1, CAP2, label = rownames(biplot_scores_sign)),\n             color = \"black\") +\n  geom_segment(data = biplot_scores_sign,\n               aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n               arrow = arrow(length = unit(0.2, \"cm\"), type = \"closed\"),\n               color = \"lightseagreen\", alpha = 1, size = 0.7) +\n  geom_label(data = bioreg_centroids,\n             aes(x = CAP1, y = CAP2,\n                 label = labels), size = 4.0,\n             col = \"black\", fill = \"yellow\") +\n  xlab(\"CAP1\") + ylab(\"CAP2\") +\n  ggtitle(expression(paste(\"Significant thermal variables and \", beta[sim]))) +\n  theme_grey() +\n  theme(panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        aspect.ratio = 0.8)"
  },
  {
    "objectID": "BCB743/12-constrained_ordination.html#references",
    "href": "BCB743/12-constrained_ordination.html#references",
    "title": "12. Constrained Ordination",
    "section": "\n8 References",
    "text": "8 References\nBaselga, A., Orme, D., Villeger, S., De Bortoli, J., and Leprieur, F. (2017). betapart: Partitioning Beta Diversity into Turnover and Nestedness Components. R package version 1.4-1. Available online at: https://CRAN.R-project.org/package=betapart\nBolton, J. J. (1986). Marine phytogeography of the Benguela upwelling region on the west coast of southern Africa: A temperature dependent approach. Botanica Marina 29, 251–256.\nBolton, J. J., and Anderson, R. J. (2004). “Marine vegetation,” in Vegetation of Southern Africa, eds R. M. Cowling, D. M. Richardson, and S. M. Pierce, (Cambridge, UK: Cambridge University Press), 348–370.\nBolton, J. J., and Stegenga, H. (2002). Seaweed species diversity in South Africa. South African Journal of Marine Science 24, 9–18.\nDe Clerck, O., Bolton, J. J., Anderson, R. J., and Coppejans, E. (2005). Guide to the seaweeds of KwaZulu- Natal. Scripta Botanica Belgica 33, 294 pp.\nSauer, J. D. (1988). Plant migration: The dynamics of geographic patterning in seed plant species. University of California Press.\nSmit, A. J., Bolton, J. J., and Anderson, R. J. (2017). Seaweeds in two oceans: beta-diversity. Frontiers in Marine Science, 4, 404.\nStegenga, H., Bolton, J. J., and Anderson, R. J. (1997). Seaweeds of the South African west coast. Contributions of the Bolus Herbarium 18, 3–637."
  },
  {
    "objectID": "BCB743/10-PCoA.html",
    "href": "BCB743/10-PCoA.html",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nSlides\nCA lecture slides\n💾 BCB743_10_PCoA.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\n\n\n\nThe seaweed environmental data\n💾 SeaweedEnv.RData\n\n\n\nThe seaweed bioregion classification\n\n💾 bioregions.csv.\nRather than using raw data as in a CA, PCoA takes a (dis)similarity matrix as input; in other words, any of the dissimilarities calculated by vegan’s vegdist() function can be used, which is great as they are well-suited to species data. If dissimilarities are Euclidean distances, then PCoA is equal to PCA. Another thing that makes a PCoA more useful is that (dis)similarity matrices calculated from quantitative, semi-quantitative, qualitative, and mixed variables can be handled.\nPCoA scaling takes a set of dissimilarities and returns a set of points such that when plotted in 2D or 3D space the distances between the points are approximately equal to the dissimilarities. In other words, it tries to represent species dissimilarities as Euclidean distances."
  },
  {
    "objectID": "BCB743/10-PCoA.html#set-up-the-analysis-environment",
    "href": "BCB743/10-PCoA.html#set-up-the-analysis-environment",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "\n1 Set-up the analysis environment",
    "text": "1 Set-up the analysis environment\n\nlibrary(tidyverse)\nlibrary(vegan)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/\""
  },
  {
    "objectID": "BCB743/10-PCoA.html#the-doubs-river-data",
    "href": "BCB743/10-PCoA.html#the-doubs-river-data",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "\n2 The Doubs River data",
    "text": "2 The Doubs River data\nWe continue to use the species data:\n\nload(paste0(root, \"NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\"))\n# remove the 8th row because it sums to zero\nspe &lt;- dplyr::slice(spe, -8)"
  },
  {
    "objectID": "BCB743/10-PCoA.html#calculate-a-suitable-dissimilarity-matrix",
    "href": "BCB743/10-PCoA.html#calculate-a-suitable-dissimilarity-matrix",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "\n3 Calculate a suitable dissimilarity matrix",
    "text": "3 Calculate a suitable dissimilarity matrix\nYou may or may not want to calculate a dissimilarity index upfront (see below). Here I calculate the Bray-Curtis dissimilarity which is appropriate for abundance data:\n\nspe_bray &lt;- vegdist(spe)"
  },
  {
    "objectID": "BCB743/10-PCoA.html#do-the-pcoa",
    "href": "BCB743/10-PCoA.html#do-the-pcoa",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "\n4 Do the PCoA",
    "text": "4 Do the PCoA\n\nThe book Numerical Ecology in R uses a built-in function cmdscale() or the function pcoa() in ape for its PCoA calculation. The vegan function capscale() can also be used for PCoA, and this is the approach I take here. The ‘CAP’ in capscale() stands for ‘Canonical Analysis of Principal Coordinates’. capscale() works differently from rda() or cca() in that we can only specify the input via a formula interface. See ?capscale for information. To run a PCoA without constraints we use 1 on the righthand side of the formula, with the dissimilarity matrix on the left. Here is how, and i give three options for doing the analysis:\n\n4.1 Option 1—supply a precalculated dissimilarity matrix\n\n# spe_pcoa &lt;- cmdscale(spe_bray, k = nrow(spe) - 1, eig = TRUE)\nspe_pcoa &lt;- capscale(spe_bray ~ 1)\nspe_pcoa\n\nCall: capscale(formula = spe_bray ~ 1)\n\n              Inertia Rank\nTotal          6.7621     \nUnconstrained  7.0583   17\nImaginary     -0.2963   11\nInertia is squared Bray distance \n\nEigenvalues for unconstrained axes:\n MDS1  MDS2  MDS3  MDS4  MDS5  MDS6  MDS7  MDS8 \n3.695 1.098 0.710 0.415 0.305 0.192 0.157 0.132 \n(Showing 8 of 17 unconstrained eigenvalues)\n\n\nWhen we do a summary() of the output we see that the results are similar to that of PCA and CA, but the Species scores are missing because information about original variables (species) are not available. This is due to the fact that in this instance input into capscale() was the square (site × site) dissimilarity matrix produced from the species table, not the raw species table itself. Here is the output:\n\nsummary(spe_pcoa)\n\n\nCall:\ncapscale(formula = spe_bray ~ 1) \n\nPartitioning of squared Bray distance:\n              Inertia Proportion\nTotal           7.058          1\nUnconstrained   7.058          1\n\nEigenvalues, and their contribution to the squared Bray distance \n\nImportance of components:\n                        MDS1   MDS2   MDS3    MDS4    MDS5    MDS6    MDS7\nEigenvalue            3.6953 1.0985 0.7105 0.41497 0.30456 0.19179 0.15697\nProportion Explained  0.5235 0.1556 0.1007 0.05879 0.04315 0.02717 0.02224\nCumulative Proportion 0.5235 0.6792 0.7798 0.83862 0.88177 0.90894 0.93118\n                         MDS8    MDS9   MDS10    MDS11    MDS12   MDS13\nEigenvalue            0.13191 0.12943 0.08668 0.046158 0.038645 0.02746\nProportion Explained  0.01869 0.01834 0.01228 0.006539 0.005475 0.00389\nCumulative Proportion 0.94987 0.96820 0.98048 0.987023 0.992498 0.99639\n                         MDS14    MDS15     MDS16     MDS17\nEigenvalue            0.013065 0.007088 0.0040395 0.0013006\nProportion Explained  0.001851 0.001004 0.0005723 0.0001843\nCumulative Proportion 0.998239 0.999243 0.9998157 1.0000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  3.70945 \n\n\nSpecies scores\n\n      MDS1 MDS2 MDS3 MDS4 MDS5 MDS6\nDim1                               \nDim2                               \nDim3                               \nDim4                               \nDim5                               \nDim6                               \nDim7                               \nDim8                               \nDim9                               \nDim10                              \nDim11                              \nDim12                              \nDim13                              \nDim14                              \nDim15                              \nDim16                              \nDim17                              \n\n\nSite scores (weighted sums of species scores)\n\n       MDS1     MDS2     MDS3      MDS4     MDS5      MDS6\n1  -0.61833  0.47814 -2.68647 -0.008979  0.75432  0.287801\n2  -0.97007 -0.18831 -0.50647 -0.747940 -0.24523  0.552569\n3  -0.91095 -0.26725 -0.11086 -0.269204  0.59014  0.495686\n4  -0.61035 -0.23286  0.24092 -0.369601  0.87393 -0.003802\n5   0.12850 -0.36697  0.27123 -1.021930  1.00975 -1.813872\n6  -0.46630  0.07903  0.51793 -0.574116  1.09111  0.175131\n7  -0.85286  0.04856  0.08616 -0.345653  0.43990  0.118891\n8  -0.05527  0.49066  0.83491 -2.293328 -1.49393 -0.171486\n9  -0.56550  0.13337  0.88932 -0.692163  0.27050  0.509773\n10 -0.84664  0.25300 -0.67671  0.118437 -0.50196 -0.383681\n11 -0.91233 -0.04658  0.02921  0.193696 -0.79675 -0.020530\n12 -0.91757 -0.35596 -0.34803  0.860325 -1.07274 -0.143379\n13 -0.72310 -0.24814  0.33714  1.141654 -0.35503  0.107426\n14 -0.47027 -0.50795  0.57853  0.697808 -0.30231 -0.444267\n15 -0.14181 -0.55761  0.65846  0.872479  0.53048 -0.528434\n16  0.07838 -0.46361  0.58811  0.933646 -0.41813  0.177637\n17  0.27874 -0.35778  0.64505  0.700157 -0.43702  0.119531\n18  0.44306 -0.36281  0.68911  0.265521  0.52313  0.656374\n19  0.77381 -0.35859  0.09935 -0.155020 -0.01502  0.452721\n20  0.84592 -0.39536 -0.21418 -0.074459  0.11215  0.152485\n21  0.89761 -0.44152 -0.41707 -0.197719 -0.11940 -0.333680\n22  0.22493  2.33322  0.02727  0.445114 -0.84537 -1.488871\n23  0.61758  1.47945  0.13437  0.181825 -0.49401  2.022706\n24  0.43613  1.64121  0.48842  0.751905  1.60371 -0.426384\n25  0.83875  0.01829 -0.02959 -0.091501  0.36656  0.905801\n26  0.89258 -0.32152 -0.38210 -0.156756 -0.04914  0.197682\n27  0.89317 -0.37681 -0.47566 -0.274907 -0.19793 -0.007757\n28  0.79006 -0.61545 -0.54558  0.100449 -0.58864 -0.580587\n29  0.92215 -0.48985 -0.72275  0.010259 -0.23308 -0.585482\n\n\n\n4.2 Option 2—supply the raw data to capscale()\n\nWe can provide the raw species table instead and request that capscale() calculates the required dissimilarity indices by automagically calling vegdist(). The advantage of this approach is that it adds species scores as weighted sums of (residual) community matrix, whereas only providing the pre-calculated dissimilarity matrix provides no fixed method for adding species scores. I advocate providing a raw species table to capscale() to retain the species information. This avoids many problems later on, such as having to calculate the weighted species scores ourselves.\n\nspe_pcoa &lt;- capscale(spe ~ 1, distance = \"bray\")\nspe_pcoa\n\nCall: capscale(formula = spe ~ 1, distance = \"bray\")\n\n              Inertia Rank\nTotal          6.7621     \nUnconstrained  7.0583   17\nImaginary     -0.2963   11\nInertia is squared Bray distance \nSpecies scores projected from 'spe' \n\nEigenvalues for unconstrained axes:\n MDS1  MDS2  MDS3  MDS4  MDS5  MDS6  MDS7  MDS8 \n3.695 1.098 0.710 0.415 0.305 0.192 0.157 0.132 \n(Showing 8 of 17 unconstrained eigenvalues)\n\n\nsummary() now produces a familiar and more complete output:\n\nsummary(spe_pcoa)\n\n\nCall:\ncapscale(formula = spe ~ 1, distance = \"bray\") \n\nPartitioning of squared Bray distance:\n              Inertia Proportion\nTotal           7.058          1\nUnconstrained   7.058          1\n\nEigenvalues, and their contribution to the squared Bray distance \n\nImportance of components:\n                        MDS1   MDS2   MDS3    MDS4    MDS5    MDS6    MDS7\nEigenvalue            3.6953 1.0985 0.7105 0.41497 0.30456 0.19179 0.15697\nProportion Explained  0.5235 0.1556 0.1007 0.05879 0.04315 0.02717 0.02224\nCumulative Proportion 0.5235 0.6792 0.7798 0.83862 0.88177 0.90894 0.93118\n                         MDS8    MDS9   MDS10    MDS11    MDS12   MDS13\nEigenvalue            0.13191 0.12943 0.08668 0.046158 0.038645 0.02746\nProportion Explained  0.01869 0.01834 0.01228 0.006539 0.005475 0.00389\nCumulative Proportion 0.94987 0.96820 0.98048 0.987023 0.992498 0.99639\n                         MDS14    MDS15     MDS16     MDS17\nEigenvalue            0.013065 0.007088 0.0040395 0.0013006\nProportion Explained  0.001851 0.001004 0.0005723 0.0001843\nCumulative Proportion 0.998239 0.999243 0.9998157 1.0000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  3.70945 \n\n\nSpecies scores\n\n        MDS1     MDS2     MDS3      MDS4     MDS5       MDS6\nCogo -0.1806 -0.12166  0.12009  0.318345 -0.20896 -0.0628447\nSatr -0.8126 -0.25562 -0.09466  0.189816  0.03958 -0.0205182\nPhph -0.7065 -0.36074  0.40967  0.148161  0.04256  0.0044487\nBabl -0.4844 -0.40185  0.63024 -0.015441  0.20424  0.0984599\nThth -0.1828 -0.11133  0.04616  0.308605 -0.30549 -0.0404440\nTeso -0.1110 -0.21839  0.23781  0.435940 -0.10805 -0.0970663\nChna  0.2912 -0.13557 -0.04331  0.028119 -0.06114  0.0808072\nPato  0.2740 -0.29530  0.13661  0.259197 -0.04269 -0.0170839\nLele  0.3080 -0.37006  0.20073  0.081605  0.23066 -0.3057908\nSqce  0.3846 -0.15729  0.17876 -0.240323 -0.28958 -0.0902284\nBaba  0.5774 -0.45316 -0.07739  0.191930 -0.18527 -0.0243514\nAlbi  0.3698 -0.29397 -0.01089  0.179049 -0.14466 -0.0098897\nGogo  0.6896 -0.38070 -0.05647  0.064552  0.13137 -0.0511734\nEslu  0.4650 -0.33541 -0.14921 -0.092236  0.17152 -0.2225910\nPefl  0.4301 -0.36860 -0.08225 -0.056223  0.07008 -0.2628469\nRham  0.5761 -0.34405 -0.23487  0.010991 -0.16096 -0.0231124\nLegi  0.4925 -0.25771 -0.20414  0.019508 -0.14985  0.0094055\nScer  0.3579 -0.16837 -0.16318 -0.069449  0.05846 -0.1482742\nCyca  0.4281 -0.27220 -0.20946  0.020683 -0.11845 -0.0814711\nTiti  0.6089 -0.39474 -0.13588 -0.190483  0.07140 -0.1149085\nAbbr  0.4992 -0.27219 -0.30006 -0.055196 -0.10996 -0.0667655\nIcme  0.3542 -0.20318 -0.27335 -0.035675 -0.13817 -0.1136177\nGyce  0.6967 -0.22339 -0.32474 -0.034739 -0.11643  0.0801622\nRuru  0.8579 -0.30169 -0.02533 -0.341579 -0.09109 -0.1013461\nBlbj  0.5909 -0.27102 -0.33322 -0.074650 -0.14314  0.0005328\nAlal  0.9276 -0.01657 -0.20241  0.112213 -0.10829  0.2386945\nAnan  0.4853 -0.27931 -0.24261 -0.007294 -0.14271 -0.0349343\n\n\nSite scores (weighted sums of species scores)\n\n       MDS1     MDS2     MDS3      MDS4     MDS5      MDS6\n1  -0.61833  0.47814 -2.68647 -0.008979  0.75432  0.287801\n2  -0.97007 -0.18831 -0.50647 -0.747940 -0.24523  0.552569\n3  -0.91095 -0.26725 -0.11086 -0.269204  0.59014  0.495686\n4  -0.61035 -0.23286  0.24092 -0.369601  0.87393 -0.003802\n5   0.12850 -0.36697  0.27123 -1.021930  1.00975 -1.813872\n6  -0.46630  0.07903  0.51793 -0.574116  1.09111  0.175131\n7  -0.85286  0.04856  0.08616 -0.345653  0.43990  0.118891\n8  -0.05527  0.49066  0.83491 -2.293328 -1.49393 -0.171486\n9  -0.56550  0.13337  0.88932 -0.692163  0.27050  0.509773\n10 -0.84664  0.25300 -0.67671  0.118437 -0.50196 -0.383681\n11 -0.91233 -0.04658  0.02921  0.193696 -0.79675 -0.020530\n12 -0.91757 -0.35596 -0.34803  0.860325 -1.07274 -0.143379\n13 -0.72310 -0.24814  0.33714  1.141654 -0.35503  0.107426\n14 -0.47027 -0.50795  0.57853  0.697808 -0.30231 -0.444267\n15 -0.14181 -0.55761  0.65846  0.872479  0.53048 -0.528434\n16  0.07838 -0.46361  0.58811  0.933646 -0.41813  0.177637\n17  0.27874 -0.35778  0.64505  0.700157 -0.43702  0.119531\n18  0.44306 -0.36281  0.68911  0.265521  0.52313  0.656374\n19  0.77381 -0.35859  0.09935 -0.155020 -0.01502  0.452721\n20  0.84592 -0.39536 -0.21418 -0.074459  0.11215  0.152485\n21  0.89761 -0.44152 -0.41707 -0.197719 -0.11940 -0.333680\n22  0.22493  2.33322  0.02727  0.445114 -0.84537 -1.488871\n23  0.61758  1.47945  0.13437  0.181825 -0.49401  2.022706\n24  0.43613  1.64121  0.48842  0.751905  1.60371 -0.426384\n25  0.83875  0.01829 -0.02959 -0.091501  0.36656  0.905801\n26  0.89258 -0.32152 -0.38210 -0.156756 -0.04914  0.197682\n27  0.89317 -0.37681 -0.47566 -0.274907 -0.19793 -0.007757\n28  0.79006 -0.61545 -0.54558  0.100449 -0.58864 -0.580587\n29  0.92215 -0.48985 -0.72275  0.010259 -0.23308 -0.585482\n\n\n\n4.3 Option 3—use pre-made dissimilarity matrix and add species back using sppscores()\n\nAnother approach to add back the species information into the ordination object produced by supplying the pre-made dissimialrity matrix to capscale():\n\nspe_pcoa &lt;- capscale(spe_bray ~ 1)\nsppscores(spe_pcoa) &lt;- spe\n\nsummary(spe_pcoa)\n\n\nCall:\ncapscale(formula = spe_bray ~ 1) \n\nPartitioning of squared Bray distance:\n              Inertia Proportion\nTotal           7.058          1\nUnconstrained   7.058          1\n\nEigenvalues, and their contribution to the squared Bray distance \n\nImportance of components:\n                        MDS1   MDS2   MDS3    MDS4    MDS5    MDS6    MDS7\nEigenvalue            3.6953 1.0985 0.7105 0.41497 0.30456 0.19179 0.15697\nProportion Explained  0.5235 0.1556 0.1007 0.05879 0.04315 0.02717 0.02224\nCumulative Proportion 0.5235 0.6792 0.7798 0.83862 0.88177 0.90894 0.93118\n                         MDS8    MDS9   MDS10    MDS11    MDS12   MDS13\nEigenvalue            0.13191 0.12943 0.08668 0.046158 0.038645 0.02746\nProportion Explained  0.01869 0.01834 0.01228 0.006539 0.005475 0.00389\nCumulative Proportion 0.94987 0.96820 0.98048 0.987023 0.992498 0.99639\n                         MDS14    MDS15     MDS16     MDS17\nEigenvalue            0.013065 0.007088 0.0040395 0.0013006\nProportion Explained  0.001851 0.001004 0.0005723 0.0001843\nCumulative Proportion 0.998239 0.999243 0.9998157 1.0000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  3.70945 \n\n\nSpecies scores\n\n        MDS1     MDS2     MDS3      MDS4     MDS5       MDS6\nCogo -0.1806 -0.12166  0.12009  0.318345 -0.20896 -0.0628447\nSatr -0.8126 -0.25562 -0.09466  0.189816  0.03958 -0.0205182\nPhph -0.7065 -0.36074  0.40967  0.148161  0.04256  0.0044487\nBabl -0.4844 -0.40185  0.63024 -0.015441  0.20424  0.0984599\nThth -0.1828 -0.11133  0.04616  0.308605 -0.30549 -0.0404440\nTeso -0.1110 -0.21839  0.23781  0.435940 -0.10805 -0.0970663\nChna  0.2912 -0.13557 -0.04331  0.028119 -0.06114  0.0808072\nPato  0.2740 -0.29530  0.13661  0.259197 -0.04269 -0.0170839\nLele  0.3080 -0.37006  0.20073  0.081605  0.23066 -0.3057908\nSqce  0.3846 -0.15729  0.17876 -0.240323 -0.28958 -0.0902284\nBaba  0.5774 -0.45316 -0.07739  0.191930 -0.18527 -0.0243514\nAlbi  0.3698 -0.29397 -0.01089  0.179049 -0.14466 -0.0098897\nGogo  0.6896 -0.38070 -0.05647  0.064552  0.13137 -0.0511734\nEslu  0.4650 -0.33541 -0.14921 -0.092236  0.17152 -0.2225910\nPefl  0.4301 -0.36860 -0.08225 -0.056223  0.07008 -0.2628469\nRham  0.5761 -0.34405 -0.23487  0.010991 -0.16096 -0.0231124\nLegi  0.4925 -0.25771 -0.20414  0.019508 -0.14985  0.0094055\nScer  0.3579 -0.16837 -0.16318 -0.069449  0.05846 -0.1482742\nCyca  0.4281 -0.27220 -0.20946  0.020683 -0.11845 -0.0814711\nTiti  0.6089 -0.39474 -0.13588 -0.190483  0.07140 -0.1149085\nAbbr  0.4992 -0.27219 -0.30006 -0.055196 -0.10996 -0.0667655\nIcme  0.3542 -0.20318 -0.27335 -0.035675 -0.13817 -0.1136177\nGyce  0.6967 -0.22339 -0.32474 -0.034739 -0.11643  0.0801622\nRuru  0.8579 -0.30169 -0.02533 -0.341579 -0.09109 -0.1013461\nBlbj  0.5909 -0.27102 -0.33322 -0.074650 -0.14314  0.0005328\nAlal  0.9276 -0.01657 -0.20241  0.112213 -0.10829  0.2386945\nAnan  0.4853 -0.27931 -0.24261 -0.007294 -0.14271 -0.0349343\n\n\nSite scores (weighted sums of species scores)\n\n       MDS1     MDS2     MDS3      MDS4     MDS5      MDS6\n1  -0.61833  0.47814 -2.68647 -0.008979  0.75432  0.287801\n2  -0.97007 -0.18831 -0.50647 -0.747940 -0.24523  0.552569\n3  -0.91095 -0.26725 -0.11086 -0.269204  0.59014  0.495686\n4  -0.61035 -0.23286  0.24092 -0.369601  0.87393 -0.003802\n5   0.12850 -0.36697  0.27123 -1.021930  1.00975 -1.813872\n6  -0.46630  0.07903  0.51793 -0.574116  1.09111  0.175131\n7  -0.85286  0.04856  0.08616 -0.345653  0.43990  0.118891\n8  -0.05527  0.49066  0.83491 -2.293328 -1.49393 -0.171486\n9  -0.56550  0.13337  0.88932 -0.692163  0.27050  0.509773\n10 -0.84664  0.25300 -0.67671  0.118437 -0.50196 -0.383681\n11 -0.91233 -0.04658  0.02921  0.193696 -0.79675 -0.020530\n12 -0.91757 -0.35596 -0.34803  0.860325 -1.07274 -0.143379\n13 -0.72310 -0.24814  0.33714  1.141654 -0.35503  0.107426\n14 -0.47027 -0.50795  0.57853  0.697808 -0.30231 -0.444267\n15 -0.14181 -0.55761  0.65846  0.872479  0.53048 -0.528434\n16  0.07838 -0.46361  0.58811  0.933646 -0.41813  0.177637\n17  0.27874 -0.35778  0.64505  0.700157 -0.43702  0.119531\n18  0.44306 -0.36281  0.68911  0.265521  0.52313  0.656374\n19  0.77381 -0.35859  0.09935 -0.155020 -0.01502  0.452721\n20  0.84592 -0.39536 -0.21418 -0.074459  0.11215  0.152485\n21  0.89761 -0.44152 -0.41707 -0.197719 -0.11940 -0.333680\n22  0.22493  2.33322  0.02727  0.445114 -0.84537 -1.488871\n23  0.61758  1.47945  0.13437  0.181825 -0.49401  2.022706\n24  0.43613  1.64121  0.48842  0.751905  1.60371 -0.426384\n25  0.83875  0.01829 -0.02959 -0.091501  0.36656  0.905801\n26  0.89258 -0.32152 -0.38210 -0.156756 -0.04914  0.197682\n27  0.89317 -0.37681 -0.47566 -0.274907 -0.19793 -0.007757\n28  0.79006 -0.61545 -0.54558  0.100449 -0.58864 -0.580587\n29  0.92215 -0.48985 -0.72275  0.010259 -0.23308 -0.585482\n\n\n\nWe can unpack what is inside the results, and there we can see that we can access the eigenvalues as we did for PCA and CA:\n\nstr(spe_pcoa) # not shown due to length of output\n\nThe percentage inertia explained by the first three axes is therefore:\n\nround(sum(spe_pcoa$CA$eig[1:3]) / sum(spe_pcoa$CA$eig) * 100, 2)\n\n[1] 77.98\n\n\nSee Numerical Ecology in R (pp. 140 to 145) for information about the interpretation of a PCoA and the ordination diagrams shown below."
  },
  {
    "objectID": "BCB743/10-PCoA.html#ordination-diagrams",
    "href": "BCB743/10-PCoA.html#ordination-diagrams",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "\n5 Ordination diagrams",
    "text": "5 Ordination diagrams\nWe create the ordination diagrammes as before:\n\nplot(spe_pcoa, scaling = 1, main = \"PCoA fish abundances - biplot scaling 1\")\n\n\n\nplot(spe_pcoa, scaling = 2, main = \"PCoA fish abundances - biplot scaling 2\")\n\n\n\n\nScaling 1 and scaling 2 is the same as in CA.\nThe plots above work okay, but we can improve them. Note that you can also apply these improvements to PCA and CA ordinations. Let us build plots from scratch:\n\npl1 &lt;- ordiplot(spe_pcoa, type = \"none\", scaling = 1,\n                main = \"PCoA fish abundances - biplot scaling 1\")\npoints(pl1, \"sites\", pch = 21, cex = 1.75, col = \"grey80\", bg = \"grey80\")\npoints(pl1, \"species\", pch = 21, col = \"turquoise\", arrows = TRUE)\ntext(pl1, \"species\", col = \"blue4\", cex = 0.9)\ntext(pl1, \"sites\", col = \"red4\", cex = 0.9)\n\n\n\npl2 &lt;- ordiplot(spe_pcoa, type = \"none\", scaling = 2,\n                main = \"PCoA fish abundances - biplot scaling 2\")\npoints(pl2, \"sites\", pch = 21, cex = 1.75, col = \"grey80\", bg = \"grey80\")\npoints(pl2, \"species\", pch = 21, col = \"turquoise\", arrows = TRUE)\ntext(pl2, \"species\", col = \"blue4\", cex = 0.9)\ntext(pl2, \"sites\", col = \"red4\", cex = 0.9)\n\n\n\n\nWe can also fit response surfaces using ordisurf():\n\ndev.off()\n\nnull device \n          1 \n\nrequire('viridis')\npalette(viridis(8))\npar(mar = c(4, 4, 0.9, 0.5) + .1, mfrow = c(2, 2))\nwith(spe, tmp &lt;- ordisurf(spe_pcoa ~ Satr, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Satr\"))\nabline(h = 0, v = 0, lty = 3)\nwith(spe, tmp &lt;- ordisurf(spe_pcoa ~ Scer, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Scer\"))\nabline(h = 0, v = 0, lty = 3)\nwith(spe, tmp &lt;- ordisurf(spe_pcoa ~ Teso, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Teso\"))\nabline(h = 0, v = 0, lty = 3)\nwith(spe, tmp &lt;- ordisurf(spe_pcoa ~ Cogo, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Cogo\"))\nabline(h = 0, v = 0, lty = 3)\n\nenv &lt;- dplyr::slice(env, -8)\n\n(spe_pcoa_env &lt;- envfit(spe_pcoa, env, scaling = 2)) \n\n\n***VECTORS\n\n        MDS1     MDS2     r2 Pr(&gt;r)    \ndfs  0.99710  0.07609 0.7210  0.001 ***\nele -0.99807 -0.06208 0.5659  0.001 ***\nslo -0.92225  0.38660 0.1078  0.113    \ndis  0.99746 -0.07129 0.5324  0.003 ** \npH  -0.42673 -0.90438 0.0480  0.505    \nhar  0.98804  0.15417 0.2769  0.024 *  \npho  0.45343  0.89129 0.6912  0.001 ***\nnit  0.86338  0.50456 0.6117  0.001 ***\namm  0.42719  0.90416 0.7076  0.001 ***\noxy -0.76847 -0.63989 0.7639  0.001 ***\nbod  0.43152  0.90210 0.8561  0.001 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\nplot(spe_pcoa_env, col = \"grey40\")\nplot(spe_pcoa_env, p.max = 0.05, col = \"red\")"
  },
  {
    "objectID": "BCB743/10-PCoA.html#handling-mixed-variable-types",
    "href": "BCB743/10-PCoA.html#handling-mixed-variable-types",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "\n6 Handling mixed variable types",
    "text": "6 Handling mixed variable types\nTo handle mixed variable types (numerical, nominal, ordinal, binary) we can use the Gower distance. We do not use vegan for this, but rather the daisy() function in cluster.\nI construct a environmental dataset that contains some mixed variables by column binding a dataset of seawater temperatures and a bioregional classificationn of the 58 coastal sections (the seaweed datasets):\n\nbioreg &lt;- read.csv(paste0(root, \"seaweed/bioregions.csv\"), header = TRUE)\nload(paste0(root, \"seaweed/SeaweedEnv.RData\"))\nE &lt;- cbind(bioreg, env) %&gt;% \n  mutate(spal.prov = factor(spal.prov),\n         spal.ecoreg = factor(spal.ecoreg),\n         lombard = factor(lombard),\n         bolton = factor(bolton))\nhead(E)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspal.prov\nspal.ecoreg\nlombard\nbolton\nfebMean\nfebMax\nfebMed\nfebX95\nfebRange\naugMean\naugMin\naugMed\naugX5\naugRange\nannMean\nannSD\nannRange\nfebSD\naugSD\nannChl\naugChl\nfebChl\n\n\n\nBMP\nNE\nNamBR\nBMP\n13.00117\n18.72044\n12.66004\n16.80969\n6.070326\n11.75228\n9.812431\n11.82838\n10.12598\n2.502092\n12.33503\n1.255298\n1.2488912\n1.625917\n0.7665420\n2.623040\n11.070480\n8.884580\n\n\nBMP\nNE\nNamBR\nBMP\n13.37950\n18.61897\n13.18389\n17.07242\n5.889300\n11.57731\n9.739288\n11.61312\n10.08165\n2.973370\n12.38795\n1.401646\n1.8021850\n1.753863\n0.8969112\n4.903870\n8.760170\n8.401560\n\n\nBMP\nNE\nNamBR\nBMP\n13.36163\n17.86458\n13.23187\n16.61114\n5.431383\n11.29382\n9.619388\n11.26842\n10.01617\n3.084130\n12.24332\n1.474712\n2.0678127\n1.703917\n0.9408326\n3.723187\n8.356506\n6.718254\n\n\nBMP\nNE\nNamBR\nBMP\n13.28966\n17.12073\n13.10284\n16.12137\n5.049024\n11.13296\n9.567049\n11.02333\n10.03277\n2.995822\n12.15410\n1.505176\n2.1567012\n1.593944\n0.9393490\n4.165980\n4.164904\n3.727157\n\n\nBMP\nNE\nNamBR\nBMP\n12.81128\n16.37829\n12.40032\n15.53240\n4.977916\n11.23448\n9.624302\n10.99935\n10.17375\n2.940255\n11.94613\n1.449530\n1.5767921\n1.517366\n0.9542671\n8.020257\n8.765154\n8.786165\n\n\nBMP\nNE\nNamBR\nBMP\n12.40247\n15.96730\n11.75096\n15.21999\n5.142721\n11.50199\n9.757004\n11.15880\n10.38581\n2.925088\n11.83773\n1.385862\n0.9004776\n1.501801\n0.9768441\n12.882601\n7.591975\n9.160030\n\n\n\n\n\nstr(E)\n\n'data.frame':   58 obs. of  22 variables:\n $ spal.prov  : Factor w/ 2 levels \"AMP\",\"BMP\": 2 2 2 2 2 2 2 2 2 2 ...\n $ spal.ecoreg: Factor w/ 2 levels \"ABE\",\"NE\": 2 2 2 2 2 2 2 2 2 2 ...\n $ lombard    : Factor w/ 4 levels \"ABR\",\"NamBR\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ bolton     : Factor w/ 4 levels \"AMP\",\"B-ATZ\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ febMean    : num  13 13.4 13.4 13.3 12.8 ...\n $ febMax     : num  18.7 18.6 17.9 17.1 16.4 ...\n $ febMed     : num  12.7 13.2 13.2 13.1 12.4 ...\n $ febX95     : num  16.8 17.1 16.6 16.1 15.5 ...\n $ febRange   : num  6.07 5.89 5.43 5.05 4.98 ...\n $ augMean    : num  11.8 11.6 11.3 11.1 11.2 ...\n $ augMin     : num  9.81 9.74 9.62 9.57 9.62 ...\n $ augMed     : num  11.8 11.6 11.3 11 11 ...\n $ augX5      : num  10.1 10.1 10 10 10.2 ...\n $ augRange   : num  2.5 2.97 3.08 3 2.94 ...\n $ annMean    : num  12.3 12.4 12.2 12.2 11.9 ...\n $ annSD      : num  1.26 1.4 1.47 1.51 1.45 ...\n $ annRange   : num  1.25 1.8 2.07 2.16 1.58 ...\n $ febSD      : num  1.63 1.75 1.7 1.59 1.52 ...\n $ augSD      : num  0.767 0.897 0.941 0.939 0.954 ...\n $ annChl     : num  2.62 4.9 3.72 4.17 8.02 ...\n $ augChl     : num  11.07 8.76 8.36 4.16 8.77 ...\n $ febChl     : num  8.88 8.4 6.72 3.73 8.79 ...\n\n\nNow we calculate the Gower distances and proceed with the PCoA as before:\n\nlibrary(cluster)\n\n# cannot use mixed var  types\n# E_gower &lt;- vegdist(E, method = \"gower\") \n\n# can handle mixed var types... use instead of vegdist() gower dissimilarity\nE_gower &lt;- daisy(E, metric = \"gower\") \n\nsummary(E_gower)\n\n1653 dissimilarities, summarized :\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.006058 0.181880 0.344160 0.321890 0.443730 0.724140 \nMetric :  mixed ;  Types = N, N, N, N, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I \nNumber of objects : 58\n\nE_mat &lt;- as.matrix(E_gower)\nE_mat[1:5, 1:5]\n\n           1          2          3          4          5\n1 0.00000000 0.03893923 0.05586573 0.09502716 0.06723042\n2 0.03893923 0.00000000 0.02753347 0.06586077 0.04354363\n3 0.05586573 0.02753347 0.00000000 0.04284923 0.04719106\n4 0.09502716 0.06586077 0.04284923 0.00000000 0.06826655\n5 0.06723042 0.04354363 0.04719106 0.06826655 0.00000000\n\nE_pcoa &lt;- capscale(E_mat ~ 1)\n\n# sadly this means that the names in the Spcies scores are now missing\nsummary(E_pcoa)\n\n\nCall:\ncapscale(formula = E_mat ~ 1) \n\nPartitioning of squared Unknown distance:\n              Inertia Proportion\nTotal            3.93          1\nUnconstrained    3.93          1\n\nEigenvalues, and their contribution to the squared Unknown distance \n\nImportance of components:\n                        MDS1   MDS2    MDS3    MDS4    MDS5    MDS6    MDS7\nEigenvalue            2.2311 1.0530 0.16179 0.11635 0.08678 0.05990 0.05202\nProportion Explained  0.5678 0.2680 0.04117 0.02961 0.02208 0.01524 0.01324\nCumulative Proportion 0.5678 0.8358 0.87693 0.90654 0.92863 0.94387 0.95711\n                         MDS8     MDS9    MDS10    MDS11    MDS12    MDS13\nEigenvalue            0.04274 0.023523 0.019153 0.016634 0.013035 0.010275\nProportion Explained  0.01088 0.005986 0.004874 0.004233 0.003317 0.002615\nCumulative Proportion 0.96799 0.973971 0.978845 0.983079 0.986396 0.989011\n                         MDS14    MDS15    MDS16    MDS17    MDS18     MDS19\nEigenvalue            0.008646 0.006938 0.005866 0.005202 0.004133 0.0033437\nProportion Explained  0.002200 0.001766 0.001493 0.001324 0.001052 0.0008509\nCumulative Proportion 0.991211 0.992976 0.994469 0.995793 0.996845 0.9976959\n                          MDS20     MDS21     MDS22    MDS23     MDS24\nEigenvalue            0.0029563 0.0020816 0.0012713 0.001041 0.0007704\nProportion Explained  0.0007523 0.0005297 0.0003235 0.000265 0.0001961\nCumulative Proportion 0.9984482 0.9989779 0.9993015 0.999567 0.9997626\n                          MDS25     MDS26     MDS27\nEigenvalue            3.718e-04 2.940e-04 2.671e-04\nProportion Explained  9.463e-05 7.482e-05 6.798e-05\nCumulative Proportion 9.999e-01 9.999e-01 1.000e+00\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  3.796338 \n\n\nSpecies scores\n\n      MDS1 MDS2 MDS3 MDS4 MDS5 MDS6\nDim1                               \nDim2                               \nDim3                               \nDim4                               \nDim5                               \nDim6                               \nDim7                               \nDim8                               \nDim9                               \nDim10                              \nDim11                              \nDim12                              \nDim13                              \nDim14                              \nDim15                              \nDim16                              \nDim17                              \nDim18                              \nDim19                              \nDim20                              \nDim21                              \nDim22                              \nDim23                              \nDim24                              \nDim25                              \nDim26                              \nDim27                              \n\n\nSite scores (weighted sums of species scores)\n\n       MDS1     MDS2      MDS3     MDS4     MDS5      MDS6\n1  -0.79013  0.38166 -0.173768  0.57039  0.79344  0.512755\n2  -0.76314  0.33143 -0.008178  0.36578  0.49007  0.621427\n3  -0.73738  0.36386  0.018401  0.31933  0.88666  0.701070\n4  -0.66926  0.41635  0.127623  0.54880  1.10858  0.163911\n5  -0.84837  0.39572 -0.244658  0.34424  0.08223  0.499518\n6  -0.90828  0.39897 -0.218817  0.22404 -0.41377  0.356537\n7  -0.90231  0.37991 -0.145482  0.31160 -0.28184  0.383509\n8  -0.80923  0.30610 -0.096777  0.31097 -0.25116  0.324438\n9  -0.78734  0.25171  0.946921 -0.57939 -0.46261  0.515513\n10 -0.58270  0.23053 -0.359670 -0.16402 -0.58738  0.476902\n11 -0.71442  0.36185 -0.290318  0.24433  0.32709 -0.815638\n12 -0.36490  0.21178 -0.911007 -1.22613  0.31925 -0.727051\n13 -0.50439  0.26245 -0.607388 -0.81865 -0.28943 -0.620622\n14 -0.76437  0.30494  0.121796 -0.35247 -0.43075 -0.863628\n15 -0.70889  0.27643  0.196195 -1.09528 -0.58859 -0.388155\n16 -0.42903  0.37140  0.405564 -0.24548  0.99327 -1.848863\n17 -0.19534 -0.59676 -0.343509 -1.05198 -1.14453  0.120568\n18 -0.15666 -0.55425 -0.348442 -0.59526 -1.34702 -0.141789\n19 -0.29043 -0.48242 -0.016081  0.73776 -1.02015 -0.580959\n20 -0.17266 -0.42598  0.198563  0.72814 -0.32142 -0.374266\n21  0.02952 -0.37952  0.313661  0.26835 -0.11044 -0.892234\n22  0.13369 -0.49261 -0.119274 -0.06670  0.47179 -0.370090\n23  0.18214 -0.54008 -0.376209 -0.23140  0.67179 -0.191136\n24  0.12733 -0.60704 -0.613916 -0.48898  0.47331  0.681587\n25  0.17306 -0.62154 -0.233602 -0.40928  0.71488  0.092160\n26  0.17564 -0.57969 -0.192100 -0.10102  0.60207 -0.277074\n27  0.20567 -0.54798 -0.469060 -0.63026  0.42854  0.478831\n28  0.24018 -0.57062 -0.712613 -0.31035  0.50948  0.554681\n29  0.13718 -0.68364 -0.056174 -0.58700  0.31416  0.518307\n30  0.09218 -0.84749  0.998863 -0.61359  0.69165  0.234583\n31  0.08014 -0.71330  0.525886 -0.29457  0.28891  0.204325\n32  0.03536 -0.64566 -0.137076  0.53299 -0.13679  0.338186\n33  0.17778 -0.56634  0.010451  0.47620  0.08372 -0.527797\n34  0.25394 -0.56727 -1.000972  0.75481 -0.73685  0.189328\n35  0.17217 -0.47071  0.144925  0.38725  0.16740 -0.454451\n36  0.29306 -0.47783 -0.555480  0.20440  0.07281 -0.007826\n37  0.18878 -0.45133 -0.175750  0.43333 -0.30453  0.070481\n38  0.09653 -0.48113  0.236846  0.80401 -0.22543 -0.096429\n39  0.07680 -0.48304  0.407342  0.75242  0.01863 -0.030696\n40  0.12648 -0.44195  0.313853  0.39177 -0.08224  0.082093\n41  0.11951 -0.40261  0.587823  0.53382 -0.21173 -0.265631\n42  0.14108  0.12726  1.095648  0.47804 -0.52415 -0.177298\n43  0.22753  0.07281  0.978696  0.04637 -0.32799 -0.125217\n44  0.33066  0.05554  0.934078 -0.35062 -0.11169  0.205007\n45  0.44087  0.35208  0.898520 -0.64323 -0.04816  0.522269\n46  0.50897  0.45962  0.894705 -0.60946 -0.09667  0.504342\n47  0.51160  0.52903  0.810327 -0.34116 -0.06552  0.317509\n48  0.47022  0.48660  0.188222  0.07088  0.17153  0.027351\n49  0.54995  0.56383  0.013332 -0.25436  0.08897  0.018211\n50  0.56927  0.58447 -0.141587  0.29694  0.03737 -0.108186\n51  0.56577  0.58955 -0.133604  0.36199  0.02937 -0.067712\n52  0.51035  0.54246 -0.223984  0.26709 -0.01226  0.372927\n53  0.47042  0.53945 -0.509707  0.18650 -0.61067  1.100291\n54  0.55955  0.57260 -0.161884 -0.13318  0.12517  0.088924\n55  0.65060  0.65080 -0.210964 -0.08426  0.09171 -0.178201\n56  0.74001  0.70556 -0.353213  0.12330 -0.05337 -0.328759\n57  0.82761  0.75710 -0.516312  0.13168 -0.12845 -0.393144\n58  0.90767  0.79695 -0.710665  0.07055 -0.12824 -0.424688\n\n\nWe can extract the various kinds of scores for manual plotting.\n\n\n\n\n\n\nIntegrative Assignment\n\n\n\nAn integrative assignment needs to be submitted a week after the conclusion of this module. Please refer to the Integrative Assignment exercises and start working towards completing the various analyses."
  },
  {
    "objectID": "BCB743/10-PCoA.html#references",
    "href": "BCB743/10-PCoA.html#references",
    "title": "10. Principal Coordinate Analysis (PCoA)",
    "section": "\n7 References",
    "text": "7 References"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html",
    "href": "BCB743/08-PCA_SDG.html",
    "title": "8c. PCA of WHO SDGs",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\nData\nThe WHO data\n💾 WHO.zip\nThe United Nations adopted an agenda for sustainable development and lists 17 development goals to achieve by 2030. These are called the Sustainable Development Goals (SDGs). The World Health Organization assembles a collection of indicators to track how countries are progressing towards these goals so as to achieve “a world free of poverty, hunger, disease and want” (WHO).\nThis is an ordination analysis of the SDG 3, “Good Health and Well-Being.”"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#load-packages",
    "href": "BCB743/08-PCA_SDG.html#load-packages",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n1 Load packages",
    "text": "1 Load packages\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(missMDA) # to impute missing values\nlibrary(ggcorrplot) # for the correlations\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/WHO/\""
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#define-and-load-the-data",
    "href": "BCB743/08-PCA_SDG.html#define-and-load-the-data",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n2 Define and load the data",
    "text": "2 Define and load the data\nNote The combined data and SDG descriptors are in the zip file. They are called SDG_complete.csv and SDG_description.csv, respectively. There is no need to work through the entire process below; you can simply start with loading the combined data. See the section Scale and centre the data and do the PCA, below.\nSDG 1.a Domestic general government health expenditure (GGHE-D) as percentage of general government expenditure (GGE) (%)\n\n# define base location of data files\n\nSDG1.a &lt;- read.csv(paste0(root, \"WHO_SDG1.a_domestic_health_expenditure.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG1.a\")\n\nSDG 3.1 Maternal mortality ratio (per 100 000 live births)\n\nSDG3.1_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.1_maternal_mort.csv\")) %&gt;%\n  filter(Period == 2016,\n         Indicator == \"Maternal mortality ratio (per 100 000 live births)\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.1_1\")\n\nSDG 3.1 Births attended by skilled health personnel (%)\n\nSDG3.1_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.1_skilled_births.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.1_2\")\n\nSDG 3.2 Number of neonatal deaths (Child mortality)\n\nSDG3.2_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.2_neonatal_deaths.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.2_1\")\n\nSDG 3.2 Number of under-five deaths (Child mortality)\n\nSDG3.2_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.2_under_5_deaths.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.2_2\")\n\nSDG 3.2 Number of infant deaths (Child mortality)\n\nSDG3.2_3 &lt;- read.csv(paste0(root, \"WHO_SDG3.2_infant_deaths.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.2_3\")\n\nSDG 3.3 New HIV infections (per 1000 uninfected population)\n\nSDG3.3_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.3_new_HIV_infections.csv\")) %&gt;%\n  filter(Period == 2015,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.3_1\")\n\nSDG 3.3 Incidence of tuberculosis (per 100 000 population per year)\n\nSDG3.3_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.3_TB.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.3_2\")\n\nSDG 3.3 Malaria incidence (per 1 000 population at risk)\n\nSDG3.3_3 &lt;- read.csv(paste0(root, \"WHO_SDG3.3_malaria.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.3_3\")\n\nSDG 3.3 Hepatitis B surface antigen (HBsAg) prevalence among children under 5 years\n\nSDG3.3_4 &lt;- read.csv(paste0(root, \"WHO_SDG3.3_hepatitis_B.csv\")) %&gt;%\n  filter(Period == 2015) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.3_4\")\n\nSDG 3.3 Reported number of people requiring interventions against NTDs\n\nSDG3.3_5 &lt;- read.csv(paste0(root, \"WHO_SDG3.3_NCD_interventions.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.3_5\")\n\nSDG 3.4 Adult mortality rate (probability of dying between 15 and 60 years per 1000 population)\n\nSDG3.4_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.4_adult_death_prob.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.4_1\")\n\nSDG 3.4 Number of deaths attributed to non-communicable diseases, by type of disease and sex\n\nSDG3.4_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.4_NCD_by_cause.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\",\n         Dim2 == \"Diabetes mellitus\") %&gt;%\n  mutate(Indicator = Dim2) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.4_2\")\n\nSDG3.4_3 &lt;- read.csv(paste0(root, \"WHO_SDG3.4_NCD_by_cause.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\",\n         Dim2 == \"Cardiovascular diseases\") %&gt;%\n  mutate(Indicator = Dim2) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.4_3\")\n\nSDG3.4_4 &lt;- read.csv(paste0(root, \"WHO_SDG3.4_NCD_by_cause.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\",\n         Dim2 == \"Respiratory diseases\") %&gt;%\n  mutate(Indicator = Dim2) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.4_4\")\n\nSDG 3.4 Crude suicide rates (per 100 000 population) (SDG 3.4.2)\n\nSDG3.4_5 &lt;- read.csv(paste0(root, \"WHO_SDG3.4_suicides.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.4_5\")\n\nSDG3.4 Total NCD Deaths (in thousands)\n\nSDG3.4_6 &lt;- read.csv(paste0(root, \"WHO_SDG3.4_NCD_data_total.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.4_6\")\n\nSDG 3.5 Alcohol, total per capita (15+) consumption (in litres of pure alcohol) (SDG Indicator 3.5.2)\n\nSDG3.5 &lt;- read.csv(paste0(root, \"WHO_SDG3.5_alcohol_consumption.csv\")) %&gt;%\n  filter(Period == 2015,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.5\")\n\nSDG 3.6 Estimated road traffic death rate (per 100 000 population)\n\nSDG3.6 &lt;- read.csv(paste0(root, \"WHO_SDG3.6_traffic_deaths_prop.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.6\")\n\nSDG 3.7 Adolescent birth rate (per 1000 women aged 15-19 years)\n\nSDG3.7 &lt;- read.csv(paste0(root, \"WHO_SDG3.7_adolescent_births.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.7\")\n\nSDG 3.8 UHC Index of service coverage (SCI)\n\nSDG3.8_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.8_UHC_data_availability.csv\")) %&gt;%\n  filter(Period == \"2013-2017\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.8_1\")\n\nSDG 3.8 Data availability for UHC index of essential service coverage (%)\n\nSDG3.8_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.8_UHC_index_of_service_coverage.csv\")) %&gt;%\n  filter(Period == 2017) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.8_2\")\n\nSDG 3.8 Population with household expenditures on health greater than 10% of total household expenditure or income (SDG 3.8.2) (%)\nNot used for some reason.\n\n# SDG3.8_3 &lt;- read.csv(paste0(root, \"WHO_SDG3.8_UHC_percent_of_expenditure_1.csv\")) %&gt;%\n#   filter(Period == 2016) %&gt;%\n#   select(Indicator, ParentLocation, Location, FactValueNumeric)\n\nSDG 3.8 Population with household expenditures on health greater than 25% of total household expenditure or income ( SDG indicator 3.8.2) (%)\nNot used for some reason.\n\n# SDG3.8_4 &lt;- read.csv(paste0(root, \"WHO_SDG3.8_UHC_percent_of_expenditure_2.csv\")) %&gt;%\n#   filter(Period == 2016) %&gt;%\n#   select(Indicator, ParentLocation, Location, FactValueNumeric)\n\nSDG 3.9 Poison control and unintentional poisoning\n\nSDG3.9_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.9_unintentional_poisoning_prop.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.9_1\")\n\nSDG 3.9 Indicator 3.9.1: Mortality rate attributed to household and ambient air pollution (per 100 000 population)\nData in a format that’s not easy to use.\n\n# SDG3.9_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.9_ambient_air_pollution.csv\")) %&gt;%\n#   filter(Period == 2016,\n#          Dim1 == \"Both sexes\") %&gt;%\n#   select(Indicator, ParentLocation, Location, FactValueNumeric)\n\nSDG 3.9 Mortality rate attributed to exposure to unsafe WASH services (per 100 000 population) (SDG 3.9.2)\n\nSDG3.9_3 &lt;- read.csv(paste0(root, \"WHO_SDG3.9_WASH_mortalities.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.9_3\")\n\nSDG 16.1 Estimates of rate of homicides (per 100 000 population)\n\nSDG16.1 &lt;- read.csv(paste0(root, \"WHO_SDG16.1_homicides.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG16.1\")\n\nSDG 3.a Prevalence of current tobacco use among persons aged 15 years and older (age-standardized rate)\n\nSDG3.a &lt;- read.csv(paste0(root, \"WHO_SDG3.a_tobacco_control.csv\")) %&gt;%\n  filter(Period == 2016,\n         Dim1 == \"Both sexes\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.a\")\n\nSDG 3.b Total net official development assistance to medical research and basic health sectors per capita (US$), by recipient country\n\nSDG3.b_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.b_dev_assistence_for_med_research.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.b_1\")\n\nSDG 3.b Measles-containing-vaccine second-dose (MCV2) immunization coverage by the nationally recommended age (%)\n\nSDG3.b_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.b_measles_vaccine.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.b_2\")\n\nSDG 3.b Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n\nSDG3.b_3 &lt;- read.csv(paste0(root, \"WHO_SDG3.b_diphtheria_vaccine.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.b_3\")\n\nSDG 3.b Pneumococcal conjugate vaccines (PCV3) immunization coverage among 1-year-olds (%)\n\nSDG3.b_4 &lt;- read.csv(paste0(root, \"WHO_SDG3.b_pneumococcal_vaccine.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.b_4\")\n\nSDG 3.b Girls aged 15 years old that received the recommended doses of HPV vaccine\n\nSDG3.b_5 &lt;- read.csv(paste0(root, \"WHO_SDG3.b_HPV_vaccine.csv\")) %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.b_5\")\n\nSDG 3.b Proportion of health facilities with a core set of relevant essential medicines available and affordable on a sustainable basis\nFull data not available.\nSDG 3.c SDG Target 3.c | Health workforce: Substantially increase health financing and the recruitment, development, training and retention of the health workforce in developing countries, especially in least developed countries and small island developing States\n\nSDG3.c_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.c_health_workforce.csv\"))  %&gt;%\n  filter(Period == 2016,\n         Indicator == \"Medical doctors (per 10,000)\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.c_1\")\n\nSDG3.c_2 &lt;- read.csv(paste0(root, \"WHO_SDG3.c_health_workforce.csv\"))  %&gt;%\n  filter(Period == 2016,\n         Indicator == \"Nursing and midwifery personnel (per 10,000)\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.c_2\")\n\nSDG3.c_3 &lt;- read.csv(paste0(root, \"WHO_SDG3.c_health_workforce.csv\"))  %&gt;%\n  filter(Period == 2016,\n         Indicator == \"Dentists (per 10,000)\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.c_3\")\n\nSDG3.c_4 &lt;- read.csv(paste0(root, \"WHO_SDG3.c_health_workforce.csv\"))  %&gt;%\n  filter(Period == 2016,\n         Indicator == \"Pharmacists  (per 10,000)\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.c_4\")\n\nSDG 3.d SDG Target 3.d | National and global health risks: Strengthen the capacity of all countries, in particular developing countries, for early warning, risk reduction and management of national and global health risks\nData not available.\nSDG 3.d Average of 13 International Health Regulations core capacity scores, SPAR version\n\nSDG3.d_1 &lt;- read.csv(paste0(root, \"WHO_SDG3.d_health_risks.csv\"))  %&gt;%\n  filter(Period == 2016) %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"SDG3.d_1\")\n\nOther Life expectancy at birth (years)\n\nother_1 &lt;- read.csv(paste0(root, \"WHO_Other_life_expectancy.csv\")) %&gt;%\n  filter(Period == 2015,\n         Dim1 == \"Both sexes\",\n         Indicator == \"Life expectancy at birth (years)\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"other_1\")\n\nOther Life expectancy at age 60 (years)\n\nother_2 &lt;- read.csv(paste0(root, \"WHO_Other_life_expectancy.csv\")) %&gt;%\n  filter(Period == 2015,\n         Dim1 == \"Both sexes\",\n         Indicator == \"Life expectancy at age 60 (years)\") %&gt;%\n  select(Indicator, ParentLocation, Location, FactValueNumeric) %&gt;%\n  mutate(SDG = \"other_2\")"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#rbind-the-data",
    "href": "BCB743/08-PCA_SDG.html#rbind-the-data",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n3 rbind the data",
    "text": "3 rbind the data\n\nhealth_ls = sapply(.GlobalEnv, is.data.frame) \nhealth &lt;- do.call(rbind, mget(names(health_ls)[health_ls]))"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#create-list-of-sdgs-used",
    "href": "BCB743/08-PCA_SDG.html#create-list-of-sdgs-used",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n4 Create list of SDGs used",
    "text": "4 Create list of SDGs used\n\nunique(health[, c(5, 1)])\n# ...not shown\n# write_csv(unique(health[, c(5, 1)]), file = paste0(root, \"SDG_description.csv\"))"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#pivot-wider",
    "href": "BCB743/08-PCA_SDG.html#pivot-wider",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n5 Pivot wider",
    "text": "5 Pivot wider\n\nhealth_wide &lt;- health %&gt;%\n  arrange(Location) %&gt;%\n  select(-Indicator) %&gt;%\n  pivot_wider(names_from = SDG, values_from = FactValueNumeric) %&gt;%\n  as_tibble()\nhealth_wide &lt;- health_wide[2:nrow(health_wide), -3]"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#add-world-population-data",
    "href": "BCB743/08-PCA_SDG.html#add-world-population-data",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n6 Add world population data",
    "text": "6 Add world population data\n\npopl &lt;- read_csv(paste0(root, \"WHO_population.csv\")) %&gt;%\n  filter(Year == 2016) %&gt;%\n  rename(popl_size = `Population (in thousands) total`,\n         Location = Country) %&gt;%\n  select(Location, popl_size) %&gt;%\n  mutate(popl_size = as.numeric(gsub(\"[[:space:]]\", \"\", popl_size)) * 1000)\n\nhealth_wide &lt;- health_wide %&gt;%\n  left_join(popl)"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#express-some-variables-to-unit-of-population-size",
    "href": "BCB743/08-PCA_SDG.html#express-some-variables-to-unit-of-population-size",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n7 Express some variables to unit of population size",
    "text": "7 Express some variables to unit of population size\n\nhealth_wide &lt;- health_wide %&gt;%\n  mutate(SDG3.4_4 = SDG3.4_4 / popl_size * 100000,\n         SDG3.4_3 = SDG3.4_3 / popl_size * 100000,\n         SDG3.4_2 = SDG3.4_2 / popl_size * 100000,\n         SDG3.4_6 = SDG3.4_6 / 100,\n         SDG3.2_2 = SDG3.2_2 / popl_size * 100000,\n         SDG3.2_3 = SDG3.2_3 / popl_size * 100000,\n         SDG3.2_1 = SDG3.2_1 / popl_size * 100000)"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#histograms-of-missing-values-and-correlations",
    "href": "BCB743/08-PCA_SDG.html#histograms-of-missing-values-and-correlations",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n8 Histograms of missing values, and correlations",
    "text": "8 Histograms of missing values, and correlations\n\n# calculate histograms\nhealth_wide$na_count &lt;- apply(health_wide[, 3:(ncol(health_wide) - 1)], 1,\n                              function(x) sum(is.na(x)))\nhist(health_wide$na_count, breaks = 14, plot = TRUE)\n\n\n\n\n\n# remove rows where there are more than 10 NAs\nhealth_wide &lt;- health_wide %&gt;%\n  dplyr::filter(na_count &lt;= 10) %&gt;%\n  dplyr::select(-na_count)\n\n\n# calculate pairwise correlations\ncorr &lt;- round(cor(health_wide[, 3:(ncol(health_wide) - 1)]), 1)\n\n# visualization of the correlation matrix\nggcorrplot(corr, type = 'upper', outline.col = \"grey60\",\n           colors = c(\"#1679a1\", \"white\", \"#f8766d\"),\n           lab = TRUE)\n\n\n\n\nSome of the variables are multicollinear. See Graham (2003) for a discussion of collieanrity in ecological data and how to deal with it."
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#impute-remaining-nas",
    "href": "BCB743/08-PCA_SDG.html#impute-remaining-nas",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n9 Impute remaining NAs",
    "text": "9 Impute remaining NAs\nThere are still many remaining NAs, and I impute them with the imputePCA() method in the missMDA package (see Dray and Josse 2015).\n\nhealth_wide_complete &lt;- imputePCA(health_wide[, 3:(ncol(health_wide) - 1)])$completeObs\n\n# save for later use\n# SGD_data &lt;- cbind(health_wide[, 1:2], health_wide_complete)\n# write_csv(SGD_data, file = paste0(root, \"SDG_complete.csv\"))"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#scale-and-center-the-data-and-do-the-pca",
    "href": "BCB743/08-PCA_SDG.html#scale-and-center-the-data-and-do-the-pca",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n10 Scale and center the data and do the PCA",
    "text": "10 Scale and center the data and do the PCA\nNote The analysis can proceed from here from the SDG_complete.csv and SDG_description.csv files.\n\nhealth_wide_complete_std &lt;- decostand(health_wide_complete,\n                                      method = \"standardize\")\nhealth_pca &lt;- rda(health_wide_complete_std)\nhealth_pca\n\nCall: rda(X = health_wide_complete_std)\n\n              Inertia Rank\nTotal              37     \nUnconstrained      37   37\nInertia is variance \n\nEigenvalues for unconstrained axes:\n   PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8 \n17.349  3.211  1.967  1.654  1.526  1.357  1.025  0.825 \n(Showing 8 of 37 unconstrained eigenvalues)\n\n# summary(health_pca)"
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#graphical-displays",
    "href": "BCB743/08-PCA_SDG.html#graphical-displays",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n11 Graphical displays",
    "text": "11 Graphical displays\nMake figure using the vegan biplot.rda() function:\n\nbiplot(health_pca, scaling = 1, main = \"PCA scaling 1\", choices = c(1, 2))\n\n\n\nbiplot(health_pca, scaling = 2, main = \"PCA scaling 2\", choices = c(1, 2))\n\n\n\n\nAssemble the ordination plot using the vegan component functions:\n\npl1 &lt;- ordiplot(health_pca, type = \"none\", scaling = 1, main = \"PCA WHO/SDG\")\npoints(pl1, \"sites\", pch = 21, cex = 1.0, col = \"grey20\", bg = \"grey80\")\npoints(pl1, \"species\", pch = 21, col = \"turquoise\", arrows = TRUE)\ntext(pl1, \"species\", col = \"blue4\", cex = 0.9)\n\n\n\n# text(pl1, \"sites\", col = \"red4\", cex = 0.9)\n\npl2 &lt;- ordiplot(health_pca, type = \"none\", scaling = 2, main = \"PCA WHO/SDG\")\npoints(pl2, \"sites\", pch = 21, cex = 1.75, col = \"grey80\", bg = \"grey80\")\npoints(pl2, \"species\", pch = 21, col = \"turquoise\", arrows = TRUE)\ntext(pl2, \"species\", col = \"blue4\", cex = 0.9)\ntext(pl2, \"sites\", col = \"red4\", cex = 0.9)\n\n\n\n\nYet another way to make an ordination plot. Notice how I use ggplot() to assemble the figure from pre-assembled dataframes containing the species (SDGs) and site (countries) scores. In the respecitve dataframes I also include appropriate labels that can be used to colour-code the ParentLocation (major groupings of countries).\n\nsite_scores &lt;- tibble(ParentLocation = health_wide$ParentLocation,\n                      Location = health_wide$Location)\nsite_scores &lt;- tibble(cbind(site_scores, scores(health_pca, display = \"sites\", choices = c(1:7))))\nspecies_scores &lt;- data.frame(scores(health_pca, display = \"species\", choices = c(1:7)))\nspecies_scores$species &lt;- rownames(species_scores)\nspecies_scores &lt;- tibble(species_scores)\n\nggplot(data = site_scores, aes(x = PC1, y = PC2)) +\n  geom_point(aes(col = ParentLocation)) +\n  geom_segment(data = species_scores, \n               aes(x = 0, y = 0, xend = PC1, yend = PC2),\n               arrow = arrow(length = unit(0.4, \"cm\"), type = \"closed\"), \n               color = \"lightseagreen\", alpha = 1, size = 0.3) +\n  geom_text(data = species_scores, \n            aes(x = PC1, y = PC2, label = species),\n            color = \"black\") +\n  xlab(\"PC1\") + ylab(\"PC2\") + \n  ggtitle(\"WHO SDGs, Scaling 2\")\n\n\n\n\nThere seems to be separate groups of colours (ParentLocation). Certain countries come out together in this ordination. This analysis will benefit from a cluster analyses of some kind."
  },
  {
    "objectID": "BCB743/08-PCA_SDG.html#references",
    "href": "BCB743/08-PCA_SDG.html#references",
    "title": "8c. PCA of WHO SDGs",
    "section": "\n12 References",
    "text": "12 References\n\n\nDray S, Josse J (2015) Principal component analysis with missing values: A comparative survey of methods. Plant Ecology 216:657–667.\n\n\nGraham MH (2003) Confronting multicollinearity in ecological multiple regression. Ecology 84:2809–2815."
  },
  {
    "objectID": "BCB743/13-cluster_analysis.html",
    "href": "BCB743/13-cluster_analysis.html",
    "title": "13. Cluster Analysis",
    "section": "",
    "text": "“There are two types of people in the world: 1) those who extrapolate from incomplete data.”\n– Anon."
  },
  {
    "objectID": "BCB743/13-cluster_analysis.html#introduction",
    "href": "BCB743/13-cluster_analysis.html#introduction",
    "title": "13. Cluster Analysis",
    "section": "\n1 Introduction",
    "text": "1 Introduction\nWe have seen that the WHO/SDG data seem to form neat groupings of countries within their respective parent locations. In this exercise we will apply a cluster analysis called ‘Partitioning Around Medoids’ to these data. Whereas ordination attempts to display the presence and influence of gradients, clustering tries to place our samples into a certain number of discrete units or clusters. The goal of the clustering algorithms is to produce groups (clusters) such that dissimilarities between objects within these groups are smaller than those between them.\nMy reading of the ecological literature suggests that cluster analysis is far less common than ordination, unless you’re an ecologist with conservationist tendencies. If this is a true observation, why would it be? This is also the reason why I spend less time in this module on cluster analysis, but it is nevertheless a tool that you should be familiar with. Sometimes clustering techniques are combined with ordinations (particularly PCA), in which case they can be quite powerful and insightful.\nBroadly speaking, clustering algorithms can be divided into ‘hierarchical agglomerative classification’ and non-hierarchical classification (e.g. K-means). Numerical Ecology in R provides more information about the various kinds of classifications and makes the following distinctions of classification methods: ‘sequential or simultaneous,’ ‘agglomerative or divisive,’ ‘monothetic versus polythetic,’ ‘hierarchical versus non-hierarchical methods,’ ‘probabilistic versus non-probabilistic,’ and ‘fuzzy’ methods. Regardless of how one classifies the classification algorithms, they are well-represented in R. The workhorse cluster analysis package in R is, strangely, called cluster. The function we will use in this example is called pam() but several other functions are also available, most notably ‘Agglomerative Nesting (Hierarchical Clustering)’ called by agnes(), ‘DIvisive ANAlysis Clustering’ by diana(), and ‘Fuzzy Analysis Clustering’ by fanny(). The kmeans() and hclust() functions in base R are also available and frequently used by ecologists. Of course, there is also the old faithful TWINSPAN which has been ported to R that might be of interest still, and IndVal, which is a modern replacement for TWINSPAN. All of the cluster analyses functions come with their own plotting methods, and you should become familiar with them.\nThe package factoextra provides useful helper functions for cluster analysis, and also provides clustering functions that can be used in lieu of the ones mentioned above.\nFor examples of clustering, please refer to:\n\nNumerical Ecology in R, pp. 53-62. Later pages in the Cluster Analysis chapter go deeper into clustering and you should read over it for a broad overview. For the purpose of this module, we will focus on 4.3 Hierarchical Clustering and 4.4 Agglomerative Clustering.\nA Kaggle challenge with examples of both Hierarchical Clustering and K-means Clustering.\nThe iris dataset is an excellent dataset to practice cluster analysis on; in fact, cluster analysis examples of this dataset are common on the internet.\n\nLet’s explore the WHO/SDG dataset using the pam() function."
  },
  {
    "objectID": "BCB743/13-cluster_analysis.html#set-up-the-analysis-environment",
    "href": "BCB743/13-cluster_analysis.html#set-up-the-analysis-environment",
    "title": "13. Cluster Analysis",
    "section": "\n2 Set-up the analysis environment",
    "text": "2 Set-up the analysis environment\n\nlibrary(tidyverse) \nlibrary(cluster)\nlibrary(ggcorrplot)\nlibrary(factoextra)\nlibrary(vegan)\nlibrary(ggpubr)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/\""
  },
  {
    "objectID": "BCB743/13-cluster_analysis.html#load-the-sdg-data",
    "href": "BCB743/13-cluster_analysis.html#load-the-sdg-data",
    "title": "13. Cluster Analysis",
    "section": "\n3 Load the SDG data",
    "text": "3 Load the SDG data\nI load the combined dataset that already had their missing values imputed (as per the PCA example).\n\nSDGs &lt;- read_csv(paste0(root, \"WHO/SDG_complete.csv\"))\nSDGs[1:5, 1:8]\n\n# A tibble: 5 × 8\n  ParentLocation       Location other_1 other_2 SDG1.a SDG16.1 SDG3.1_1 SDG3.2_1\n  &lt;chr&gt;                &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Eastern Mediterrane… Afghani…    61.6    15.6   2.14    9.02      673   135.  \n2 Europe               Albania     77.8    21.1   9.62    3.78       16     7.55\n3 Africa               Algeria     76.5    21.8  10.7     1.66      113    38.0 \n4 Africa               Angola      61.7    16.7   5.43    9.82      246   125.  \n5 Americas             Antigua…    76.1    20.4  11.6     2.42       43     5.94\n\n\nThe parent locations:\n\nunique(SDGs$ParentLocation)\n\n[1] \"Eastern Mediterranean\" \"Europe\"                \"Africa\"               \n[4] \"Americas\"              \"Western Pacific\"       \"South-East Asia\"      \n\n\nThe number of countries:\n\nlength(SDGs$Location)\n\n[1] 176\n\n\nAs is often the case with measured variables, we can start our exploration with a correlation analysis to see the extent to which correlation between variable pairs is present:\n\n# a correalation matrix\ncorr &lt;- round(cor(SDGs[3:ncol(SDGs)]), 1)\nggcorrplot(corr, type = 'upper', outline.col = \"white\", \n           colors = c(\"navy\", \"white\", \"#FC4E07\"), \n           lab = TRUE)\n\n\n\n\nWe might decide to remove collinear variables. A useful approach to use here might be to look at the strongest loadings along the significant reduced axes in a PCA and exclude the others, or find the ones most strongly correlated as seen in the biplots—how you do this can be rationalised on a case-by-case basis. I proceed with the full dataset, but this is not ideal.\nWe need to standardise first to account for the different measurement scales of the variables. We can calculate Euclidian distances before running pam(), but it can also be specified within the function call. We do the latter:\n\nSDGs_std &lt;- decostand(SDGs[3:ncol(SDGs)], method = \"standardize\")\n# SDGs_euc &lt;- vegdist(SDGs_std, method = \"euclidian\")\nrownames(SDGs_std) &lt;- SDGs$Location # carry location names into output\n\nThe frustrating thing with cluster analysis, which often confuses novice users, is that there is often an expectation that the clustering alorithm decides for the user how many clusters to use. However, this is a misconception that must be overcome. Although some numerical guidance can be obtained through ‘silhouette,’ ‘within cluster sum of squares’ or ‘elbow’ analysis, and ‘gap statistic’, in my experience they are no substitute for the power of human reasoning. Let us see what the factoextra package function fviz_nbclust() tell us about how many group to use:\n\n# using silhouette analysis\nplt1 &lt;- fviz_nbclust(SDGs_std, cluster::pam, method = \"silhouette\") + \n  theme_grey()\n\n# total within cluster sum of square / elbow analysis\nplt2 &lt;- fviz_nbclust(SDGs_std, cluster::pam, method = \"wss\") + \n  theme_grey()\n\n# gap statistics\nplt3 &lt;- fviz_nbclust(SDGs_std, cluster::pam, method = \"gap_stat\") + \n  theme_grey()\n\nggarrange(plt1, plt2, plt3, nrow = 3)\n\n\n\n\nEven with the supposedly objective assessment of what the optimal number of clusters should be, we see that each method still provides a different result. Much better to proceed with expert knowledge about the nature of the data and the intent of the study. Let us proceed with three clusters as I think two clusters are insufficient for our purpose.\n\nSDGs_pam &lt;- pam(SDGs_std, metric = \"euclidean\", k = 3)\n\nfviz_cluster(SDGs_pam, geom = \"point\", ellipse.type = \"convex\",\n             palette = c(\"#FC4E07\", \"violetred3\", \"deepskyblue3\"),\n             ellipse.alpha = 0.05) +\n  geom_text(aes(label = SDGs$Location), size = 2.5)\n\n\n\n\nWe cannot clearly see where SA is, so let’s create a clearer plot:\n\n# scale SA bigger for plotting\nSDGs &lt;- SDGs |&gt; \n  mutate(col_vec = ifelse(Location == \"South Africa\", \"black\", \"grey50\"),\n         scale_vec = ifelse(Location == \"South Africa\", 3.5, 2.5))\n\nfviz_cluster(SDGs_pam, geom = \"point\", ellipse.type = \"convex\",\n             palette = c(\"#FC4E07\", \"violetred3\", \"deepskyblue3\"),\n             ellipse.alpha = 0.05, pointsize = 2.0) +\n  geom_text(aes(label = SDGs$Location), size = SDGs$scale_vec, col = SDGs$col_vec)\n\n\n\n\nNote that pam(), unlike hierarchical or agglomerative clustering, does not produce a dendrogram and the usual way to graphically present the cluster arrangement is to create a scatter plot similar to an ordination diagramme (but it is NOT an ordination diagram).\nSame as above, but showing a star plot and numbers indicating the countries (their row numbers in SDGs):\n\nfviz_cluster(SDGs_pam, palette = c(\"#FC4E07\", \"violetred3\", \"deepskyblue3\"),\n             ellipse.type = \"euclid\", star.plot = TRUE, repel = TRUE,\n             pointsize = SDGs$scale_vec * 0.8) + # SA plotted slightly bigger\n  theme_grey()\n\n\n\n\nDo a silhouette analysis to check cluster fidelity:\n\nfviz_silhouette(SDGs_pam, palette = c(\"#FC4E07\", \"violetred3\", \"deepskyblue3\"),\n                ggtheme = theme_grey())\n\n  cluster size ave.sil.width\n1       1   46          0.27\n2       2   78          0.09\n3       3   52          0.27\n\n\n\n\n\nOnce happy with the number of clusters, find the median value for each cluster:\n\nSDGs_centroids &lt;- SDGs |&gt; \n  mutate(cluster = SDGs_pam$clustering) |&gt; \n  group_by(cluster) |&gt; \n  summarise_at(vars(other_1:SDG3.b_5), median, na.rm = TRUE)\nSDGs_centroids\n\n# A tibble: 3 × 39\n  cluster other_1 other_2 SDG1.a SDG16.1 SDG3.1_1 SDG3.2_1 SDG3.2_2 SDG3.2_3\n    &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1       1    62.4    16.7   5.43    8.88      396    90.0    214.     152.  \n2       2    73.2    19.6   9.64    4.4        60    19.8     33.9     28.1 \n3       3    80.4    23.2  13.3     1.28        7     2.78     4.73     4.00\n# ℹ 30 more variables: SDG3.3_1 &lt;dbl&gt;, SDG3.3_2 &lt;dbl&gt;, SDG3.3_3 &lt;dbl&gt;,\n#   SDG3.3_4 &lt;dbl&gt;, SDG3.3_5 &lt;dbl&gt;, SDG3.4_1 &lt;dbl&gt;, SDG3.4_2 &lt;dbl&gt;,\n#   SDG3.4_3 &lt;dbl&gt;, SDG3.4_4 &lt;dbl&gt;, SDG3.4_5 &lt;dbl&gt;, SDG3.4_6 &lt;dbl&gt;,\n#   SDG3.5 &lt;dbl&gt;, SDG3.6 &lt;dbl&gt;, SDG3.8_1 &lt;dbl&gt;, SDG3.8_2 &lt;dbl&gt;, SDG3.9_1 &lt;dbl&gt;,\n#   SDG3.9_3 &lt;dbl&gt;, SDG3.b_1 &lt;dbl&gt;, SDG3.b_2 &lt;dbl&gt;, SDG3.b_3 &lt;dbl&gt;,\n#   SDG3.b_4 &lt;dbl&gt;, SDG3.c_1 &lt;dbl&gt;, SDG3.c_2 &lt;dbl&gt;, SDG3.c_3 &lt;dbl&gt;,\n#   SDG3.c_4 &lt;dbl&gt;, SDG3.d_1 &lt;dbl&gt;, SDG3.7 &lt;dbl&gt;, SDG3.a &lt;dbl&gt;, …\n\n\npam() can also provide the most representative example countries of each cluster. Note that the values inside are very different from that produced when we calculated the medians because medoids report the standardised data:\n\nSDGs_pam$medoids\n\n             other_1     other_2     SDG1.a     SDG16.1   SDG3.1_1   SDG3.2_1\nTogo      -1.3082283 -1.04437853 -1.1903643  0.06808945  1.1324894  1.3017843\nNicaragua  0.3737688  0.08872107  1.3796506  0.08533933 -0.2323864 -0.2586716\nCzechia    0.8823411  0.61551298  0.8833042 -0.64737012 -0.6765494 -0.8435034\n            SDG3.2_2   SDG3.2_3   SDG3.3_1   SDG3.3_2   SDG3.3_3   SDG3.3_4\nTogo       1.4102734  1.3592849  0.1576964 -0.4579343  2.4882232  1.2835322\nNicaragua -0.3767895 -0.3446689 -0.3154270 -0.4579343 -0.1580909 -0.6637395\nCzechia   -0.7133630 -0.7745694 -0.1376945 -0.7136379 -0.9480541 -0.5125539\n             SDG3.3_5   SDG3.4_1   SDG3.4_2   SDG3.4_3   SDG3.4_4    SDG3.4_5\nTogo      -0.06030043  1.2520369 -0.4762540 -0.6960758 -0.6952419 -0.07418662\nNicaragua -0.16897721 -0.1601567  0.4796550 -0.5087310 -0.2996050 -0.62139368\nCzechia   -0.18762124 -0.9262184  0.1254489  1.1464418  0.3787484  0.33180571\n            SDG3.4_6     SDG3.5      SDG3.6   SDG3.8_1   SDG3.8_2   SDG3.9_1\nTogo      -0.2449637 -0.8481419  1.32097641 -0.2415269 -1.4304958  0.7715301\nNicaragua -0.2418110 -0.2133414 -0.08575199 -0.2415269  0.5462165 -0.6816096\nCzechia   -0.1503081  2.0328759 -1.14835854  0.2417061  0.7438877 -0.2935552\n            SDG3.9_3    SDG3.b_1   SDG3.b_2   SDG3.b_3    SDG3.b_4   SDG3.c_1\nTogo       1.5236560  0.17567787 -1.0409470 -0.5061518 -0.02793898 -1.1831072\nNicaragua -0.4682756 -0.03942639  0.2371043  0.7038002  0.94197637 -0.6102974\nCzechia   -0.5724488 -0.30874491  0.7352539  0.5525562  0.25560867  1.1612648\n            SDG3.c_2   SDG3.c_3   SDG3.c_4   SDG3.d_1     SDG3.7      SDG3.a\nTogo      -0.9983214 -1.1842216 -1.0940540 -1.0278904  0.8082986 -1.53587553\nNicaragua -0.6958462  0.3067516  0.3246320  0.7660773 -0.2790219 -0.05503528\nCzechia    0.7385150  1.2426058  0.5534738  0.7415003 -0.9713149  1.07144665\n            SDG3.1_2   SDG3.b_5\nTogo      -1.3841046  0.7500056\nNicaragua  0.5008573  0.1814304\nCzechia    0.9941603 -0.5157917\n\n\nWe can do a coloured pairwise scatterplot to check data details. I limit it here to the pairs of the first 7 columns because of the large number of possible combinations:\n\npairs(SDGs[, 3:10],\n      col = c(\"#FC4E07\", \"violetred3\", \"deepskyblue3\")[SDGs_pam$clustering])"
  },
  {
    "objectID": "BCB743/13-cluster_analysis.html#references",
    "href": "BCB743/13-cluster_analysis.html#references",
    "title": "13. Cluster Analysis",
    "section": "\n4 References",
    "text": "4 References"
  },
  {
    "objectID": "BCB743/06-deep_dive.html",
    "href": "BCB743/06-deep_dive.html",
    "title": "6. Deep Dive into Gradients",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nReading\nSmit et al. (2017)\n💾 Smit_et_al_2017.pdf\n\n\n\nSmit et al. (2013)\n💾 Smit_et_al_2013.pdf\n\n\n\nSupp. to Smit et al. (2017)\n💾 Smit_the_seaweed_data.pdf\n\n\nData\nThe seaweed environmental data\n💾 SeaweedEnv.RData\n\n\n\nThe seaweed species data\n💾 dists_mat.RData\n\n\n\nThe bioregions\n💾 bioregions.csv\nIn the previous Chapter we looked at calculations involving biodiversity (specifically the dissimilarity matrices made from a species table) and environmental variables (distances). What can we do with the two forms of contemporary β-diversity? What do they mean? Can we look to environmental distances for more insight?\nLet’s do a deeper analysis and create a figure to demonstrate these findings. I regress \\(\\beta_{\\text{sør}}\\) on the spatial distance between section pairs (see below) and on the environmental distance \\(\\beta_{\\text{E}}\\) in each bioregion and used the magnitude of the slope (per 100 km) of this relationship as a metric of β-diversity or ‘distance decay’ of dissimilarity.\nWhat these lines of code do is recreate Figure 5 in Smit et al. (2017). Please read the paper for an interpretation of this figure as this is critical for an understanding of the role that gradients play in structuring patterns of biodiversity.\n(To be updated…)\n## Setting up the analysis environment\nlibrary(tidyverse)\nlibrary(plyr)\nlibrary(vegan)\nlibrary(betapart) # for partitioning beta-diversity\nlibrary(BiodiversityR) # for calcs of species richness etc."
  },
  {
    "objectID": "BCB743/06-deep_dive.html#load-all-of-the-data",
    "href": "BCB743/06-deep_dive.html#load-all-of-the-data",
    "title": "6. Deep Dive into Gradients",
    "section": "\n1 Load all of the data",
    "text": "1 Load all of the data\n\n# load the environmental data...\nload(\"../data/seaweed/SeaweedEnv.RData\")\nenv &lt;- as.data.frame(env)\n# keep only some...\nenv &lt;- env[, c(\"annMean\", \"annRange\", \"annSD\", \"febMean\", \"febRange\",\n               \"febSD\", \"augMean\", \"augRange\", \"augSD\")]\n\nSince the connectivity between sections is constrained by their location along a shoreline, we calculated the distances between sections not as ‘as the crow flies’ distances (e.g. Section 1 is not connected in a straight line to Section 58 because of the intervening land in-between), but as the great circle geodesic distances between each pair of sections along a ‘route’. Travelling from 1 to 58 therefore requires visiting 2, then 3, and eventually all the way up to 58. The total distance between a pair of arbitrary sections is thus the cumulative sum of the great circle distances between each consecutive pair of intervening sections along the route. These data are contained in dists_mat.RData (I prepared it earlier):\n\n# load the distances matrix...\nload(\"../data/seaweed/dists_mat.RData\")\n# loaded as dists.mat\ndists.mat[1:10, 1:8]\n\n         1       2       3       4       5       6       7       8\n1    0.000  51.138 104.443 153.042 207.386 253.246 305.606 359.799\n2   51.138   0.000  53.305 101.904 156.248 202.108 254.468 308.661\n3  104.443  53.305   0.000  48.599 102.943 148.803 201.163 255.356\n4  153.042 101.904  48.599   0.000  54.344 100.204 152.564 206.757\n5  207.386 156.248 102.943  54.344   0.000  45.860  98.220 152.413\n6  253.246 202.108 148.803 100.204  45.860   0.000  52.360 106.553\n7  305.606 254.468 201.163 152.564  98.220  52.360   0.000  54.193\n8  359.799 308.661 255.356 206.757 152.413 106.553  54.193   0.000\n9  409.263 358.125 304.820 256.221 201.877 156.017 103.657  49.464\n10 457.857 406.719 353.414 304.815 250.471 204.611 152.251  98.058\n\n\nVarious bioregions have been defined for South African marine biota. I prefer to use the one made by Bolton and Stegenga (2002):\n\n# load the bioregions data...\nbioreg &lt;- read.csv(\"../data/seaweed/bioregions.csv\",\n                   header = TRUE)\nrbind(head(bioreg, 3), tail(bioreg, 3))\n\n   spal.prov spal.ecoreg lombard bolton\n1        BMP          NE   NamBR    BMP\n2        BMP          NE   NamBR    BMP\n3        BMP          NE   NamBR    BMP\n56       AMP          NE     NBR   ECTZ\n57       AMP          NE     NBR   ECTZ\n58       AMP          NE     NBR   ECTZ\n\n\nMake a copy of the original matrix of distances between pairs of sites to create a full matrix which constrains pairwise comparisons to pairs within bioregions:\n\nbioreg.mat &lt;- dists.mat\nbioreg.mat[1:58, 1:58] &lt;- \"out\"\nbioreg.mat[1:16, 1:16] &lt;- \"BMP\"\nbioreg.mat[17:21, 17:21] &lt;- \"B-ATZ\"\nbioreg.mat[22:41, 22:41] &lt;- \"AMP\"\nbioreg.mat[42:58, 42:58] &lt;- \"ECTZ\"\ndim(bioreg.mat)\n\n[1] 58 58\n\n# see what is inside the matrix...\nbioreg.mat[1:3, 1:10] \n\n  1     2     3     4     5     6     7     8     9     10   \n1 \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\"\n2 \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\"\n3 \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\" \"BMP\"\n\nbioreg.mat[56:58, 53:58]\n\n   53     54     55     56     57     58    \n56 \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\"\n57 \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\"\n58 \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\" \"ECTZ\"\n\n# convert to show only the lower left triangle\n# requires the gdata package...\nbioreg.tri &lt;- gdata::lowerTriangle(bioreg.mat, diag = FALSE) \n\nIn bioreg.mat, pairs of sites that do not fall within any of the bioregions is called ‘out’:\n\n# print output below...\nbioreg.mat[1:3, 53:58]\n\n  53    54    55    56    57    58   \n1 \"out\" \"out\" \"out\" \"out\" \"out\" \"out\"\n2 \"out\" \"out\" \"out\" \"out\" \"out\" \"out\"\n3 \"out\" \"out\" \"out\" \"out\" \"out\" \"out\"\n\n\nWe extract the slices (groups of rows) of the original species table into separate dataframes, one for each of the four bioregions:\n\nenv.BMP &lt;- env[1:16, ]\nenv.BATZ &lt;- env[17:21, ]\nenv.AMP &lt;- env[22:41, ]\nenv.ECTZ &lt;- env[42:58, ]\n\nNow we make an environmental dataframe for use with plots of pairwise correlations etc.:\n\nenv.df &lt;- data.frame(bio = bioreg$bolton, round(env, 3))\nrbind(head(env.df, 3), tail(env.df, 3))\n\n    bio annMean annRange annSD febMean febRange febSD augMean augRange augSD\n1   BMP  12.335    1.249 1.255  13.001    6.070 1.626  11.752    2.502 0.767\n2   BMP  12.388    1.802 1.402  13.379    5.889 1.754  11.577    2.973 0.897\n3   BMP  12.243    2.068 1.475  13.362    5.431 1.704  11.294    3.084 0.941\n56 ECTZ  23.729    4.609 1.942  26.227    3.474 1.191  21.618    2.163 0.663\n57 ECTZ  24.710    4.969 1.976  27.328    3.372 1.143  22.359    1.584 0.499\n58 ECTZ  25.571    5.574 2.023  28.457    3.267 1.000  22.883    1.098 0.349\n\n\n\n# load the seaweed data...\nspp &lt;- read.csv('../data/seaweed/SeaweedSpp.csv')\nspp &lt;- dplyr::select(spp, -1)"
  },
  {
    "objectID": "BCB743/06-deep_dive.html#start-calculating",
    "href": "BCB743/06-deep_dive.html#start-calculating",
    "title": "6. Deep Dive into Gradients",
    "section": "\n2 Start calculating",
    "text": "2 Start calculating\nCalculate β-diversity using the Sørensen index of dissimilarity. This is used throughout; binary Bray-Curtis is equivalent to Sørensen in vegan.\n\n# ---- Sorensen-index ----\n# this is used throughout...\nY &lt;- vegdist(spp, binary = TRUE)\nY.mat &lt;- as.matrix(Y)\n# extract the subdiagonal...\nY.diag &lt;- diag(Y.mat[-1, -nrow(Y.mat)]) \n# add a zero in front...\nY.diag &lt;- append(0, Y.diag, after = 1) \n\nDecompose into turnover and nestedness-resultant beta-diversity:\n\n# ---- do-betapart ----\n## Calculations with betapart...\nY.core &lt;- betapart.core(spp)\n\n# Using the Sørensen index, compute three distance matrices accounting for\n# the (i) turnover (replacement), (ii) nestedness-resultant component, and\n# (iii) total dissimilarity (i.e. the sum of both components)\n# use for pairwise plotting...\nY.pair &lt;- beta.pair(Y.core, index.family = \"sor\") \n\nExtract the subdiagonal for plotting later on:\n\nY1.mat &lt;- as.matrix(Y.pair$beta.sim)\n# extract the subdiagonal...\nY1.diag &lt;- diag(Y1.mat  [-1, -nrow(Y1.mat)]) \n# add a zero in front...\nY1.diag &lt;- append(0, Y1.diag, after = 1) \n\nY2.mat &lt;- as.matrix(Y.pair$beta.sne)\nY2.diag &lt;- diag(Y2.mat[-1, -nrow(Y2.mat)])\nY2.diag &lt;- append(0, Y2.diag, after = 1)\n\nCreate separate matrices for each bioregion:\n\n# ---- spp-bioregion ----\nspp.BMP &lt;- spp[1:16, ]\nY.BMP &lt;- vegdist(spp.BMP, binary = TRUE)\nspp.core.BMP &lt;- betapart.core(spp.BMP)\n# use below for pairwise plotting...\nY.pair.BMP &lt;- beta.pair(spp.core.BMP, index.family = \"sor\")\n\nspp.BATZ &lt;- spp[17:21, ]\nY.BATZ &lt;- vegdist(spp.BATZ, binary = TRUE)\nspp.core.BATZ &lt;- betapart.core(spp.BATZ)\n# use below for pairwise plotting...\nY.pair.BATZ &lt;- beta.pair(spp.core.BATZ, index.family = \"sor\")\n\nspp.AMP &lt;- spp[22:41, ]\nY.AMP &lt;- vegdist(spp.AMP, binary = TRUE)\nspp.core.AMP &lt;- betapart.core(spp.AMP)\n# use below for pairwise plotting...\nY.pair.AMP &lt;- beta.pair(spp.core.AMP, index.family = \"sor\")\n\nspp.ECTZ &lt;- spp[42:58, ]\nY.ECTZ &lt;- vegdist(spp.ECTZ, binary = TRUE)\nspp.core.ECTZ &lt;- betapart.core(spp.ECTZ)\n# use below for pairwise plotting...\nY.pair.ECTZ &lt;- beta.pair(spp.core.ECTZ, index.family = \"sor\")\n\nCalculate species richness (alpha-diversity):\n\n# ---- do-species-richness ----\nspp.richness.site &lt;- diversityresult(spp, index = 'richness',\n                                     method = 'each site')\n\nCalculate the environmental distances:\n\n# ---- environmental-distance ----\n# Euclidian distances on temperatures\n# first make a copy so we can use untransformed data later on...\nenv.raw &lt;- env \n# calculate z-scores...\nenv &lt;- decostand(env, method = \"standardize\")\n\nUsing individual thermal variables, calculate Euclidian distances, make a matrix and extract the subdiagonal. The data have already been standardised in env:\n\n# augMean\n# to be used in env.rda2...\nenv4 &lt;- dplyr::select(env, augMean) \nenv4 &lt;- vegdist(env4, method = 'euclidian')\nenv4.mat &lt;- as.matrix(env4)\nenv4.diag &lt;- diag(env4.mat[-1, -nrow(env4.mat)])\nenv4.diag &lt;- append(0, env4.diag, after = 1)\n\n\n# febRange\n# to be used in env.rda2...\nenv5 &lt;- dplyr::select(env, febRange) \nenv5 &lt;- vegdist(env5, method = 'euclidian')\nenv5.mat &lt;- as.matrix(env5)\nenv5.diag &lt;- diag(env5.mat[-1, -nrow(env5.mat)])\nenv5.diag &lt;- append(0, env5.diag, after = 1)\n\n\n# febSD\n# to be used in env.rda2...\nenv6 &lt;- dplyr::select(env, febSD) \nenv6 &lt;- vegdist(env6, method = 'euclidian')\nenv6.mat &lt;- as.matrix(env6)\nenv6.diag &lt;- diag(env6.mat[-1, -nrow(env6.mat)])\nenv6.diag &lt;- append(0, env6.diag, after = 1)\n\n\n# augSD\n# to be used in env.rda2...\nenv7 &lt;- dplyr::select(env, augSD) \nenv7 &lt;- vegdist(env7, method = 'euclidian')\nenv7.mat &lt;- as.matrix(env7)\nenv7.diag &lt;- diag(env7.mat[-1, -nrow(env7.mat)])\nenv7.diag &lt;- append(0, env7.diag, after = 1)\n\n\n# annMean\n# to be used in env.rda2...\nenv8 &lt;- dplyr::select(env, annMean) \nenv8 &lt;- vegdist(env8, method = 'euclidian')\nenv8.mat &lt;- as.matrix(env8)\nenv8.diag &lt;- diag(env8.mat[-1, -nrow(env8.mat)])\nenv8.diag &lt;- append(0, env8.diag, after = 1)\n\n\n# combined variables selected with the db-RDA\n# these have a far poorer fit...\nenv.comb &lt;- dplyr::select(env, augMean, febRange, febSD, augSD)\nenv.comb &lt;- vegdist(env.comb, method = 'euclidian')\nenv.comb.mat &lt;- as.matrix(env.comb)\nenv.comb.diag &lt;- diag(env.comb.mat[-1, -nrow(env.comb.mat)])\nenv.comb.diag &lt;- append(0, env.comb.diag, after = 1)\n\n\n# ---- do-figure-5 ----\n# Assemble data frame for plotting...\nspp.df &lt;- data.frame(dist = as.vector(dists.mat),\n                     bio = as.vector(bioreg.mat),\n                     augMean = as.vector(env4.mat),\n                     febRange = as.vector(env5.mat),\n                     febSD = as.vector(env6.mat),\n                     augSD = as.vector(env7.mat),\n                     annMean = as.vector(env8.mat),\n                     Y = as.vector(Y.mat),\n                     Y1 = as.vector(Y1.mat),\n                     Y2 = as.vector(Y2.mat))\n\nDo the various linear regressions of Sørensen dissimilarities (βsor), turnover (βsim) and nestedness-related β-diversity (βsne) as a function of the various thermal distances. I only display the results of the linear regression for \\(Y1\\) regressed on geographical distance, dist, but do all the calculations:\n\nspp.df2 &lt;- droplevels(subset(spp.df, bio !=  \"out\"))\nhead(spp.df2)\n\n     dist bio     augMean    febRange       febSD      augSD     annMean\n1   0.000 BMP 0.000000000 0.000000000 0.000000000 0.00000000 0.000000000\n2  51.138 BMP 0.057413686 0.098844037 0.162952710 0.31327996 0.015018458\n3 104.443 BMP 0.150439037 0.348877544 0.099341627 0.41882390 0.026022466\n4 153.042 BMP 0.203224406 0.557654418 0.040720995 0.41525879 0.051341700\n5 207.386 BMP 0.169908896 0.596480756 0.138250544 0.45110735 0.110354710\n6 253.246 BMP 0.082131062 0.506493762 0.158074740 0.50536019 0.141115516\n             Y           Y1           Y2\n1 0.0000000000 0.0000000000 0.0000000000\n2 0.0036101083 0.0000000000 0.0036101083\n3 0.0036101083 0.0000000000 0.0036101083\n4 0.0071942446 0.0000000000 0.0071942446\n5 0.0249110320 0.0072463768 0.0176646552\n6 0.0391459075 0.0217391304 0.0174067770\n\n# turnover...\nY1.lm1 &lt;- dlply(spp.df2, .(bio), function(x) lm(Y1 ~ dist, data = x))\nlapply(Y1.lm1, summary)\n\n$AMP\n\nCall:\nlm(formula = Y1 ~ dist, data = x)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0595754 -0.0195099 -0.0045457  0.0150607  0.0676548 \n\nCoefficients:\n               Estimate  Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.1753e-03  2.4057e-03 -2.1513  0.03205 *  \ndist         2.9388e-04  6.5671e-06 44.7508  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.027858 on 398 degrees of freedom\nMultiple R-squared:  0.83421,   Adjusted R-squared:  0.83379 \nF-statistic: 2002.6 on 1 and 398 DF,  p-value: &lt; 2.22e-16\n\n\n$`B-ATZ`\n\nCall:\nlm(formula = Y1 ~ dist, data = x)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.070629 -0.024865  0.008058  0.022698  0.059443 \n\nCoefficients:\n               Estimate  Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -0.00805800  0.01364463 -0.5906    0.5606    \ndist         0.00109279  0.00015899  6.8732 5.229e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.041102 on 23 degrees of freedom\nMultiple R-squared:  0.67256,   Adjusted R-squared:  0.65832 \nF-statistic: 47.241 on 1 and 23 DF,  p-value: 5.2292e-07\n\n\n$BMP\n\nCall:\nlm(formula = Y1 ~ dist, data = x)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0377507 -0.0274619 -0.0238939  0.0015289  0.2693766 \n\nCoefficients:\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 2.3923e-02 5.4997e-03  4.3498 1.975e-05 ***\ndist        7.0949e-05 1.8256e-05  3.8864 0.0001299 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.051294 on 254 degrees of freedom\nMultiple R-squared:  0.056126,  Adjusted R-squared:  0.05241 \nF-statistic: 15.104 on 1 and 254 DF,  p-value: 0.00012991\n\n\n$ECTZ\n\nCall:\nlm(formula = Y1 ~ dist, data = x)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.1188184 -0.0268483  0.0053999  0.0244034  0.1196065 \n\nCoefficients:\n               Estimate  Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.3999e-03  4.2574e-03 -1.2683   0.2057    \ndist         7.8600e-04  1.2086e-05 65.0326   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.041937 on 287 degrees of freedom\nMultiple R-squared:  0.93645,   Adjusted R-squared:  0.93623 \nF-statistic: 4229.2 on 1 and 287 DF,  p-value: &lt; 2.22e-16\n\nY1.lm2 &lt;- dlply(spp.df2, .(bio), function(x) lm(Y1 ~ augMean , data = x))\n# lapply(Y1.lm2, summary)\nY1.lm3 &lt;- dlply(spp.df2, .(bio), function(x) lm(Y1 ~ augSD , data = x))\n# lapply(Y1.lm3, summary)\nY1.lm4 &lt;- dlply(spp.df2, .(bio), function(x) lm(Y1 ~ febRange , data = x))\n# lapply(Y1.lm4, summary)\nY1.lm5 &lt;- dlply(spp.df2, .(bio), function(x) lm(Y1 ~ febSD , data = x))\n# lapply(Y1.lm5, summary)\n\n# nestedness-resultant...\nY2.lm1 &lt;- dlply(spp.df2, .(bio), function(x) lm(Y2 ~ dist, data = x))\n# lapply(Y2.lm1, summary)\nY2.lm2 &lt;- dlply(spp.df2, .(bio), function(x) lm(Y2 ~ annMean , data = x))\n# lapply(Y2.lm2, summary)"
  },
  {
    "objectID": "BCB743/06-deep_dive.html#make-the-plots",
    "href": "BCB743/06-deep_dive.html#make-the-plots",
    "title": "6. Deep Dive into Gradients",
    "section": "\n3 Make the plots",
    "text": "3 Make the plots\nNow assemble Figure 5. in Smit et al. (2017). It is a plot of pairwise (a) Sørensen dissimilarities (βsor), (b) turnover (βsim) and (c) nestedness-related β-diversity (βsne) (sensu Baselga 2010) as a function of distance between sections. Section pairs falling within individual bioregions are colour-coded; where the pairs include sections across different bioregions the symbols are coloured grey and labeled ‘out’.\nCombine the data in a way that makes for easy plotting:\n\n# Plots...\nspp.long &lt;- spp.df %&gt;%\n  gather(beta, dissim, Y:Y2) %&gt;%\n  gather(metric, distance, c(dist, augMean:annMean))\nspp.long$metric = factor(spp.long$metric,\n                         levels = c('dist', 'augMean', 'febRange',\n                                    'febSD', 'augSD', 'annMean'))\n\nThe repetive portions of code needed to create each of the panels. I was too lazy to write neater and more concise code:\n\n# sim as a function of geographic distance...\nplt5a &lt;- spp.long %&gt;%\n  dplyr::filter(beta %in% \"Y1\" & metric %in% \"dist\") %&gt;%\n  ggplot(aes(x = distance, y = dissim, group = bio)) +\n  geom_point(aes(colour = bio, shape = bio), size = 1.2, alpha = 0.8) +\n  geom_point(aes(colour = bio, size = bio, alpha = bio, shape = bio)) +\n  geom_line(stat = \"smooth\", method = \"lm\", formula = y ~ x, alpha = 1.0,\n            size = 0.6, colour = \"black\", aes(linetype = bio)) +\n  scale_linetype_manual(name = \"Bioregion\",\n                        values = c(\"dashed\", \"solid\", \"dotted\",\n                                   \"longdash\", \"blank\")) +\n  scale_colour_brewer(name = \"Bioregion\",\n                      palette = \"Set1\") +\n  scale_shape_manual(name = \"Bioregion\",\n                     values = c(0, 19, 2, 5, 46)) +\n  scale_size_manual(name = \"Bioregion\",\n                    values = c(1.0, 1.2, 1.0, 1.0, 0.6)) +\n  scale_alpha_manual(name = \"Bioregion\",\n                     values = c(0.85, 1.0, 0.85, 0.85, 0.1)) +\n  xlab(expression(paste(\"Distance (km)\"))) +\n  ylab(expression(paste(beta[sim]))) +\n  scale_y_continuous(limits = c(0, 0.75)) +\n  scale_x_continuous(limits = c(0, 1000)) +\n  theme_grey() +\n  theme(panel.grid.minor = element_line(colour = NA),\n        plot.title = element_text(hjust = 0, size = 10),\n        # legend.position = c(0.2, 0.7),\n        # legend.direction = \"vertical\",\n        aspect.ratio = 0.6) +\n  ggtitle(expression(paste(beta[sim], \" as a function of distance\")))\n\n\n# sim as a function of augMean...\nplt5b &lt;- spp.long %&gt;%\n  dplyr::filter(beta %in% \"Y1\" & metric %in% \"augMean\") %&gt;%\n  ggplot(aes(x = distance, y = dissim, group = bio)) +\n  geom_point(aes(colour = bio, shape = bio), size = 1.2, alpha = 0.8) +\n  # geom_point(aes(colour = bio, size = bio, alpha = bio, shape = bio)) +\n  geom_line(stat = \"smooth\", method = \"lm\", formula = y ~ x,\n            alpha = 1.0, size = 0.6, colour = \"black\", aes(linetype = bio)) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\", \"dotted\",\n                                   \"longdash\", \"blank\")) +\n  scale_colour_brewer(palette = \"Set1\") +\n  scale_shape_manual(values = c(0, 19, 2, 5, 46)) +\n  scale_size_manual(values = c(1.0, 1.2, 1.0, 1.0, 0.6)) +\n  scale_alpha_manual(values = c(0.85, 1.0, 0.85, 0.85, 0.1)) +\n  xlab(expression(paste(d[E]))) +\n  ylab(expression(paste(beta[sim]))) +\n  scale_y_continuous(limits = c(0, 0.75)) +\n  scale_x_continuous(limits = c(0, 2)) +\n  theme_grey() +\n  theme(panel.grid.minor = element_line(colour = NA),\n        plot.title = element_text(hjust = 0, size = 10),\n        legend.position = \"none\",\n        # legend.title = element_blank(),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 8),\n        legend.key = element_blank(),\n        legend.key.height = unit(.22, \"cm\"),\n        legend.background = element_blank(),\n        aspect.ratio = 0.6) +\n  ggtitle(expression(paste(beta[sim], \" as a function of augMean\")))\n\n\n# sim as a function of febRange...\nplt5c &lt;- spp.long %&gt;%\n  dplyr::filter(beta %in% \"Y1\" & metric %in% \"febRange\") %&gt;%\n  ggplot(aes(x = distance, y = dissim, group = bio)) +\n  geom_point(aes(colour = bio, shape = bio), size = 1.2, alpha = 0.8) +\n  # geom_point(aes(colour = bio, size = bio, alpha = bio, shape = bio)) +\n  geom_line(stat = \"smooth\", method = \"lm\", formula = y ~ x,\n            alpha = 1.0, size = 0.6, colour = \"black\", aes(linetype = bio)) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\", \"dotted\",\n                                   \"longdash\", \"blank\")) +\n  scale_colour_brewer(palette = \"Set1\") +\n  scale_shape_manual(values = c(0, 19, 2, 5, 46)) +\n  scale_size_manual(values = c(1.0, 1.2, 1.0, 1.0, 0.6)) +\n  scale_alpha_manual(values = c(0.85, 1.0, 0.85, 0.85, 0.1)) +\n  xlab(expression(paste(d[E]))) +\n  ylab(expression(paste(beta[sim]))) +\n  scale_y_continuous(limits = c(0, 0.75)) +\n  scale_x_continuous(limits = c(0, 4)) +\n  theme_grey() +\n  theme(panel.grid.minor = element_line(colour = NA),\n        plot.title = element_text(hjust = 0, size = 10),\n        legend.position = \"none\",\n        aspect.ratio = 0.6) +\n  ggtitle(expression(paste(beta[sim], \" as a function of febRange\")))\n\n\n# sim as a function of febSD...\nplt5d &lt;- spp.long %&gt;%\n  dplyr::filter(beta %in% \"Y1\" & metric %in% \"febSD\") %&gt;%\n  ggplot(aes(x = distance, y = dissim, group = bio)) +\n  geom_point(aes(colour = bio, shape = bio), size = 1.2, alpha = 0.8) +\n  # geom_point(aes(colour = bio, size = bio, alpha = bio, shape = bio)) +\n  geom_line(stat = \"smooth\", method = \"lm\", formula = y ~ x,\n            alpha = 1.0, size = 0.6, colour = \"black\", aes(linetype = bio)) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\", \"dotted\",\n                                   \"longdash\", \"blank\")) +\n  scale_colour_brewer(palette = \"Set1\") +\n  scale_shape_manual(values = c(0, 19, 2, 5, 46)) +\n  scale_size_manual(values = c(1.0, 1.2, 1.0, 1.0, 0.6)) +\n  scale_alpha_manual(values = c(0.85, 1.0, 0.85, 0.85, 0.1)) +\n  xlab(expression(paste(d[E]))) +\n  ylab(expression(paste(beta[sim]))) +\n  scale_y_continuous(limits = c(0, 0.75)) +\n  scale_x_continuous(limits = c(0, 3)) +\n  theme_grey() +\n  theme(panel.grid.minor = element_line(colour = NA),\n        plot.title = element_text(hjust = 0, size = 10),\n        legend.position = \"none\",\n        aspect.ratio = 0.6) +\n  ggtitle(expression(paste(beta[sim], \" as a function of febSD\")))\n\n\n# sim as a function of augSD...\nplt5e &lt;- spp.long %&gt;%\n  dplyr::filter(beta %in% \"Y1\" & metric %in% \"augSD\") %&gt;%\n  ggplot(aes(x = distance, y = dissim, group = bio)) +\n  geom_point(aes(colour = bio, shape = bio), size = 1.2, alpha = 0.8) +\n  # geom_point(aes(colour = bio, size = bio, alpha = bio, shape = bio)) +\n  geom_line(stat = \"smooth\", method = \"lm\", formula = y ~ x,\n            alpha = 1.0, size = 0.6, colour = \"black\", aes(linetype = bio)) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\", \"dotted\",\n                                   \"longdash\", \"blank\")) +\n  scale_colour_brewer(palette = \"Set1\") +\n  scale_shape_manual(values = c(0, 19, 2, 5, 46)) +\n  scale_size_manual(values = c(1.0, 1.2, 1.0, 1.0, 0.6)) +\n  scale_alpha_manual(values = c(0.85, 1.0, 0.85, 0.85, 0.1)) +\n  xlab(expression(paste(d[E]))) +\n  ylab(expression(paste(beta[sim]))) +\n  scale_y_continuous(limits = c(0, 0.75)) +\n  scale_x_continuous(limits = c(0, 3)) +\n  theme_grey() +\n  theme(panel.grid.minor = element_line(colour = NA),\n        plot.title = element_text(hjust = 0, size = 10),\n        legend.position = \"none\",\n        aspect.ratio = 0.6) +\n  ggtitle(expression(paste(beta[sim], \" as a function of augSD\")))\n\n\n# sne as a function of distance...\nplt5f &lt;- spp.long %&gt;%\n  dplyr::filter(beta %in% \"Y2\" & metric %in% \"dist\") %&gt;%\n  ggplot(aes(x = distance, y = dissim, group = bio)) +\n  geom_point(aes(colour = bio, shape = bio), size = 1.2, alpha = 0.8) +\n  # geom_point(aes(colour = bio, size = bio, alpha = bio, shape = bio)) +\n  geom_line(stat = \"smooth\", method = \"lm\", formula = y ~ x,\n            alpha = 1.0, size = 0.6, colour = \"black\", aes(linetype = bio)) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\", \"dotted\",\n                                   \"longdash\", \"blank\")) +\n  scale_colour_brewer(palette = \"Set1\") +\n  scale_shape_manual(values = c(0, 19, 2, 5, 46)) +\n  scale_size_manual(values = c(1.0, 1.2, 1.0, 1.0, 0.6)) +\n  scale_alpha_manual(values = c(0.85, 1.0, 0.85, 0.85, 0.1)) +\n  xlab(expression(paste(\"Distance (km)\"))) +\n  ylab(expression(paste(beta[sne]))) +\n  scale_y_continuous(limits = c(0, 0.22)) +\n  scale_x_continuous(limits = c(0, 1000)) +\n  theme_grey() +\n  theme(panel.grid.minor = element_line(colour = NA),\n        plot.title = element_text(hjust = 0, size = 10),\n        legend.position = \"none\",\n        aspect.ratio = 0.6) +\n  ggtitle(expression(paste(beta[sne], \" as a function of distance\")))\n\n\n# sne as a function of annMean...\nplt5g &lt;- spp.long %&gt;%\n  dplyr::filter(beta %in% \"Y2\" & metric %in% \"annMean\") %&gt;%\n  ggplot(aes(x = distance, y = dissim, group = bio)) +\n  geom_point(aes(colour = bio, shape = bio), size = 1.2, alpha = 0.8) +\n  # geom_point(aes(colour = bio, size = bio, alpha = bio, shape = bio)) +\n  geom_line(stat = \"smooth\", method = \"lm\", formula = y~x, alpha = 1.0, size = 0.6,\n            colour = \"black\",\n            aes(linetype = bio)) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\", \"dotted\", \"longdash\", \"blank\")) +\n  scale_colour_brewer(palette = \"Set1\") +\n  scale_shape_manual(values = c(0, 19, 2, 5, 46)) +\n  scale_size_manual(values = c(1.0, 1.2, 1.0, 1.0, 0.6)) +\n  scale_alpha_manual(values = c(0.85, 1.0, 0.85, 0.85, 0.1)) +\n  xlab(expression(paste(d[E]))) +\n  ylab(expression(paste(beta[sne]))) +\n  scale_y_continuous(limits = c(0, 0.22)) +\n  scale_x_continuous(limits = c(0, 2)) +\n  theme_grey() +\n  theme(panel.grid.minor = element_line(colour = NA),\n        plot.title = element_text(hjust = 0, size = 10),\n        legend.position = \"none\",\n        aspect.ratio = 0.6) +\n  ggtitle(expression(paste(beta[sne], \" as a function of annMean\")))\n\n\nplt5h &lt;- ggplot(spp.long, aes(x = distance, y = dissim)) +\n  geom_blank() +\n  theme(plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank())\n\nAssemble using the cowplot package:\n\nlibrary(cowplot)\n\n# turn off warnings...\noldw &lt;- getOption(\"warn\") \noptions(warn = -1)\n\nl &lt;- get_legend(plt5a)\n# pdf(\"Fig5.pdf\", width = 9, height = 6.5)\nggdraw() +\n  draw_plot(plot_grid(plt5a + theme(legend.position = 'none'), plt5b, plt5c,\n                      plt5d, plt5e, l,\n                      plt5f, plt5g, plt5h,\n                      ncol = 3, align = 'hv'),\n            width = 1.0)"
  },
  {
    "objectID": "BCB743/06-deep_dive.html#references",
    "href": "BCB743/06-deep_dive.html#references",
    "title": "6. Deep Dive into Gradients",
    "section": "\n4 References",
    "text": "4 References\n\n\nBaselga A (2010) Partitioning the turnover and nestedness components of beta diversity. Global Ecology and Biogeography 19:134–143.\n\n\nBolton J, Stegenga H (2002) Seaweed species diversity in South Africa. South African Journal of Marine Science 24:9–18.\n\n\nSmit AJ, Bolton JJ, Anderson RJ (2017) Seaweeds in two oceans: Beta-diversity. Frontiers in Marine Science 4:404."
  },
  {
    "objectID": "BCB743/05-correlations.html",
    "href": "BCB743/05-correlations.html",
    "title": "5. Correlations and Associations",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nSlides\nCorrelation lecture slides\n💾 BCB743_06_correlations.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData"
  },
  {
    "objectID": "BCB743/05-correlations.html#set-up-the-analysis-environment",
    "href": "BCB743/05-correlations.html#set-up-the-analysis-environment",
    "title": "5. Correlations and Associations",
    "section": "\n1 Set-up the analysis environment",
    "text": "1 Set-up the analysis environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(Hmisc) # for rcorr()"
  },
  {
    "objectID": "BCB743/05-correlations.html#the-doubs-river-data",
    "href": "BCB743/05-correlations.html#the-doubs-river-data",
    "title": "5. Correlations and Associations",
    "section": "\n2 The Doubs River data",
    "text": "2 The Doubs River data\nThe background to the data is described by David Zelený on his excellent website and in the book Numerical Ecology with R by Borcard et al. (2011). These data are a beautiful example of how gradients structure biodiversity. It will be in your own interest to fully understand how the various environmental factors used as explanatory variables vary along a riverine gradient from the source to the terminus of the river.\n\n2.1 Correlations between environmental variables\nCorrelation refers to the statistical (non-causal) relationship between two continuous variables. It measures the extent to which changes in one variable correspond to changes in another variable. Correlations are quantified into values ranging from -1 and +1, with -1 indicating a perfect negative correlation, +1 indicating a perfect positive correlation, and 0 indicating no correlation. A positive correlation implies that as one variable increases, the other variable also increases. Conversely, a negative correlation implies that as one variable increases, the other decreases. Correlation can be calculated using several methods, the most common one being the Pearson correlation coefficient. Non-parametric correlations can be applied to ordinal or non-normal data.\n\nload(\"../data/NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\")\n\nhead(env, 5)\n\n   dfs ele  slo  dis  pH har  pho  nit  amm  oxy bod\n1  0.3 934 48.0 0.84 7.9  45 0.01 0.20 0.00 12.2 2.7\n2  2.2 932  3.0 1.00 8.0  40 0.02 0.20 0.10 10.3 1.9\n3 10.2 914  3.7 1.80 8.3  52 0.05 0.22 0.05 10.5 3.5\n4 18.5 854  3.2 2.53 8.0  72 0.10 0.21 0.00 11.0 1.3\n5 21.5 849  2.3 2.64 8.1  84 0.38 0.52 0.20  8.0 6.2\n\n\nWe use correlations to establish how the environmental variables relate to one another across the sample sites. We do not need to standardise as one would do for the calculation of Euclidian distances, but in some instances data transformations might be necessary:\n\nenv_cor &lt;- round(cor(env), 2)\nenv_cor\n\n      dfs   ele   slo   dis    pH   har   pho   nit   amm   oxy   bod\ndfs  1.00 -0.94 -0.38  0.95  0.01  0.70  0.48  0.75  0.41 -0.51  0.39\nele -0.94  1.00  0.44 -0.87 -0.04 -0.74 -0.44 -0.76 -0.38  0.36 -0.34\nslo -0.38  0.44  1.00 -0.34 -0.22 -0.53 -0.19 -0.31 -0.17  0.31 -0.18\ndis  0.95 -0.87 -0.34  1.00  0.02  0.70  0.39  0.61  0.29 -0.36  0.25\npH   0.01 -0.04 -0.22  0.02  1.00  0.09 -0.08 -0.05 -0.12  0.18 -0.15\nhar  0.70 -0.74 -0.53  0.70  0.09  1.00  0.36  0.51  0.29 -0.38  0.34\npho  0.48 -0.44 -0.19  0.39 -0.08  0.36  1.00  0.80  0.97 -0.72  0.89\nnit  0.75 -0.76 -0.31  0.61 -0.05  0.51  0.80  1.00  0.80 -0.63  0.64\namm  0.41 -0.38 -0.17  0.29 -0.12  0.29  0.97  0.80  1.00 -0.72  0.89\noxy -0.51  0.36  0.31 -0.36  0.18 -0.38 -0.72 -0.63 -0.72  1.00 -0.84\nbod  0.39 -0.34 -0.18  0.25 -0.15  0.34  0.89  0.64  0.89 -0.84  1.00\n\n\nOr if we want to see the associated p-values to establish a statistical significance:\n\nrcorr(as.matrix(env))\n\n      dfs   ele   slo   dis    pH   har   pho   nit   amm   oxy   bod\ndfs  1.00 -0.94 -0.38  0.95  0.01  0.70  0.48  0.75  0.41 -0.51  0.39\nele -0.94  1.00  0.44 -0.87 -0.04 -0.74 -0.44 -0.76 -0.38  0.36 -0.34\nslo -0.38  0.44  1.00 -0.34 -0.22 -0.53 -0.19 -0.31 -0.17  0.31 -0.18\ndis  0.95 -0.87 -0.34  1.00  0.02  0.70  0.39  0.61  0.29 -0.36  0.25\npH   0.01 -0.04 -0.22  0.02  1.00  0.09 -0.08 -0.05 -0.12  0.18 -0.15\nhar  0.70 -0.74 -0.53  0.70  0.09  1.00  0.36  0.51  0.29 -0.38  0.34\npho  0.48 -0.44 -0.19  0.39 -0.08  0.36  1.00  0.80  0.97 -0.72  0.89\nnit  0.75 -0.76 -0.31  0.61 -0.05  0.51  0.80  1.00  0.80 -0.63  0.64\namm  0.41 -0.38 -0.17  0.29 -0.12  0.29  0.97  0.80  1.00 -0.72  0.89\noxy -0.51  0.36  0.31 -0.36  0.18 -0.38 -0.72 -0.63 -0.72  1.00 -0.84\nbod  0.39 -0.34 -0.18  0.25 -0.15  0.34  0.89  0.64  0.89 -0.84  1.00\n\nn= 30 \n\n\nP\n    dfs    ele    slo    dis    pH     har    pho    nit    amm    oxy   \ndfs        0.0000 0.0365 0.0000 0.9771 0.0000 0.0076 0.0000 0.0251 0.0040\nele 0.0000        0.0146 0.0000 0.8447 0.0000 0.0144 0.0000 0.0376 0.0493\nslo 0.0365 0.0146        0.0625 0.2362 0.0028 0.3067 0.0997 0.3593 0.1006\ndis 0.0000 0.0000 0.0625        0.9147 0.0000 0.0355 0.0004 0.1136 0.0522\npH  0.9771 0.8447 0.2362 0.9147        0.6405 0.6619 0.7976 0.5134 0.3494\nhar 0.0000 0.0000 0.0028 0.0000 0.6405        0.0481 0.0039 0.1191 0.0370\npho 0.0076 0.0144 0.3067 0.0355 0.6619 0.0481        0.0000 0.0000 0.0000\nnit 0.0000 0.0000 0.0997 0.0004 0.7976 0.0039 0.0000        0.0000 0.0002\namm 0.0251 0.0376 0.3593 0.1136 0.5134 0.1191 0.0000 0.0000        0.0000\noxy 0.0040 0.0493 0.1006 0.0522 0.3494 0.0370 0.0000 0.0002 0.0000       \nbod 0.0309 0.0677 0.3546 0.1770 0.4232 0.0619 0.0000 0.0001 0.0000 0.0000\n    bod   \ndfs 0.0309\nele 0.0677\nslo 0.3546\ndis 0.1770\npH  0.4232\nhar 0.0619\npho 0.0000\nnit 0.0001\namm 0.0000\noxy 0.0000\nbod       \n\n\nWe can also do a visual exploration (see Question 1, below).\n\n\n\n\n\n\n\n2.2 Association between species\nSpecies associations refer to the relationships or interactions between different species within an ecosystem or community. The term can be used to describe the outcome of a wide range of relationships, including competition, predation, symbiosis (mutualism, commensalism, parasitism), or simply the tendency for different species to occur in the same habitats or microhabitats.\nWhen two or more species are frequently found in the same area or under the same conditions, they are positively associated. This could be due to similar environmental preferences, mutualistic relationships, or one species depending on the presence of another. For example, bees and flowering plants have a mutualistic relationship where the bees gather nectar for food, and in the process, they pollinate the flowers. In this sense, bees would be positively associated with some flowering plants.\nConversely, if two species are rarely found in the same area or under the same conditions, they are negatively associated. This can be due to competition for resources, predation, or differing environmental preferences.\nAnalyses of species associations can help us understand the complex dynamics of ecological communities, including how species interact with each other and their environment, the roles they play in their ecosystems, and the effects of environmental changes on species distributions and community composition. A first glance insight into the existence of some of these types of interactions can be found by examining tables of association among spocies.\nThe Doubs River fish species dataset is an example of abundance data and it will serve well to examine the properties of an association matrix:\n\nhead(spe)\n\n  Cogo Satr Phph Babl Thth Teso Chna Pato Lele Sqce Baba Albi Gogo Eslu Pefl\n1    0    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0    0    1    0\n4    0    4    5    5    0    0    0    0    0    1    0    0    1    2    2\n5    0    2    3    2    0    0    0    0    5    2    0    0    2    4    4\n6    0    3    4    5    0    0    0    0    1    2    0    0    1    1    1\n  Rham Legi Scer Cyca Titi Abbr Icme Gyce Ruru Blbj Alal Anan\n1    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    0    0    0    0    0    0    0    0    0    0    0\n3    0    0    0    0    0    0    0    0    0    0    0    0\n4    0    0    0    0    1    0    0    0    0    0    0    0\n5    0    0    2    0    3    0    0    0    5    0    0    0\n6    0    0    0    0    2    0    0    0    1    0    0    0\n\n\nIn order to calculate an association matrix for the fish species we first need to transpose the data:\n\nspe_t &lt;- t(spe)\n\nNow we can calculate the association matrix:\n\nspe_assoc1 &lt;- vegdist(spe_t, method = \"jaccard\")\n # display only a portion of the data...\nas.matrix((spe_assoc1))[1:10, 1:10]\n\n          Cogo      Satr      Phph      Babl      Thth      Teso      Chna\nCogo 0.0000000 0.7368421 0.7794118 0.7945205 0.3333333 0.4545455 0.9354839\nSatr 0.7368421 0.0000000 0.3108108 0.4705882 0.7368421 0.7333333 0.9583333\nPhph 0.7794118 0.3108108 0.0000000 0.2804878 0.7794118 0.7571429 0.9113924\nBabl 0.7945205 0.4705882 0.2804878 0.0000000 0.8108108 0.7397260 0.8481013\nThth 0.3333333 0.7368421 0.7794118 0.8108108 0.0000000 0.5833333 0.9000000\nTeso 0.4545455 0.7333333 0.7571429 0.7397260 0.5833333 0.0000000 0.8787879\nChna 0.9354839 0.9583333 0.9113924 0.8481013 0.9000000 0.8787879 0.0000000\nPato 0.8918919 0.9078947 0.7948718 0.7307692 0.9210526 0.7500000 0.4827586\nLele 0.8627451 0.8235294 0.7386364 0.6666667 0.9056604 0.7346939 0.6136364\nSqce 0.8360656 0.7978723 0.7346939 0.6562500 0.8730159 0.8281250 0.7017544\n          Pato      Lele      Sqce\nCogo 0.8918919 0.8627451 0.8360656\nSatr 0.9078947 0.8235294 0.7978723\nPhph 0.7948718 0.7386364 0.7346939\nBabl 0.7307692 0.6666667 0.6562500\nThth 0.9210526 0.9056604 0.8730159\nTeso 0.7500000 0.7346939 0.8281250\nChna 0.4827586 0.6136364 0.7017544\nPato 0.0000000 0.5000000 0.6774194\nLele 0.5000000 0.0000000 0.4531250\nSqce 0.6774194 0.4531250 0.0000000\n\n\n\nspe_assoc2 &lt;- vegdist(spe_t, method = \"jaccard\", binary = TRUE)\nas.matrix((spe_assoc2))[1:10, 1:10]\n\n          Cogo      Satr      Phph      Babl      Thth      Teso      Chna\nCogo 0.0000000 0.5294118 0.6000000 0.6666667 0.2222222 0.4000000 0.8888889\nSatr 0.5294118 0.0000000 0.2380952 0.3600000 0.5294118 0.6111111 0.8846154\nPhph 0.6000000 0.2380952 0.0000000 0.1666667 0.6000000 0.6000000 0.7692308\nBabl 0.6666667 0.3600000 0.1666667 0.0000000 0.6666667 0.6666667 0.6153846\nThth 0.2222222 0.5294118 0.6000000 0.6666667 0.0000000 0.4000000 0.8235294\nTeso 0.4000000 0.6111111 0.6000000 0.6666667 0.4000000 0.0000000 0.7500000\nChna 0.8888889 0.8846154 0.7692308 0.6153846 0.8235294 0.7500000 0.0000000\nPato 0.8125000 0.8333333 0.7083333 0.6000000 0.8125000 0.6428571 0.2307692\nLele 0.8181818 0.6538462 0.5384615 0.3846154 0.8181818 0.7000000 0.4210526\nSqce 0.7307692 0.5517241 0.3928571 0.2500000 0.7307692 0.7307692 0.5200000\n          Pato      Lele      Sqce\nCogo 0.8125000 0.8181818 0.7307692\nSatr 0.8333333 0.6538462 0.5517241\nPhph 0.7083333 0.5384615 0.3928571\nBabl 0.6000000 0.3846154 0.2500000\nThth 0.8125000 0.8181818 0.7307692\nTeso 0.6428571 0.7000000 0.7307692\nChna 0.2307692 0.4210526 0.5200000\nPato 0.0000000 0.3888889 0.5600000\nLele 0.3888889 0.0000000 0.2800000\nSqce 0.5600000 0.2800000 0.0000000"
  },
  {
    "objectID": "BCB743/05-correlations.html#references",
    "href": "BCB743/05-correlations.html#references",
    "title": "5. Correlations and Associations",
    "section": "\n3 References",
    "text": "3 References\n\n\nBorcard D, Gillet F, Legendre P, others (2011) Numerical ecology with R. Springer"
  },
  {
    "objectID": "BCB743/00-review.html",
    "href": "BCB743/00-review.html",
    "title": "1-4. Review Biogeography and Global Ecology",
    "section": "",
    "text": "This module builds on the solid foundation of the biodiversity concepts we have developed in BDC334. Please refer back to that module for a refresher (links below).\n\n\n\n\n\n\nBDC334 material for review in Week 1\n\n\n\n\n1. Ecological Data\n2. Environmental Distance\n3. Quantifying Biodiversity\n4. Describing Biodiversity Patterns\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {1-4. {Review} {Biogeography} and {Global} {Ecology}},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB743/00-review.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 1-4. Review Biogeography and Global Ecology. https://tangledbank.netlify.app/BCB743/00-review.html."
  },
  {
    "objectID": "BCB743/07-ordination.html",
    "href": "BCB743/07-ordination.html",
    "title": "7. Ordination",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\n\nType\nName\nLink\n\n\n\n\nSlides\nOrdination lecture slides\n💾 BCB743_07_ordination.pdf\n\n\nReading\nVegan–An Introduction to Ordination\n💾 Oksanen_intro-vegan.pdf\nRefer to the lecture slides.\nflowchart TB\n  A[Continuous multivariate data] --&gt; B{Ordination}\n  B --&gt; C[Unconstrained ordination]\n  B --&gt; D[Constrained ordination]\n  C --&gt; E(*PCA)\n  C --&gt; F(*CA)\n  C --&gt; G(DCA)\n  C --&gt; H(*PCoA)\n  C --&gt; I(*nMDS)\n  D --&gt; J(*RDA)\n  D --&gt; K(*CCA)\n  D --&gt; L(*db-RDA)\n\n\nFigure 1: Typically, ordination is classified into unconstrained ordination (also indirect gradient analysis) and constrained ordination (a.k.a. direct gradient analysis). These will be covered in the next few chapters. The ’*’ indicates material covered in the lectures."
  },
  {
    "objectID": "BCB743/07-ordination.html#unconstrained-ordination",
    "href": "BCB743/07-ordination.html#unconstrained-ordination",
    "title": "7. Ordination",
    "section": "1 Unconstrained ordination",
    "text": "1 Unconstrained ordination\nWe apply indirect gradient analysis when the gradients are unknown a priori and we do not have environmental data related to the species. Gradients or other influences that structure species in space are therefore inferred from the species composition data only. The communities thus tell us that gradients are present (or not), but it might not offer insight into the identity of the structuring gradients.\n\nPlease refer to:\n\nPrincipal Component Analysis (PCA)\nCorrespondence Analysis (CA)\nPrincipal Coordinate Analysis (PCoA)\nnon-Metric Multidimensional Scaling (nMDS)"
  },
  {
    "objectID": "BCB743/07-ordination.html#constrained-ordination",
    "href": "BCB743/07-ordination.html#constrained-ordination",
    "title": "7. Ordination",
    "section": "2 Constrained ordination",
    "text": "2 Constrained ordination\nAny ordination approach in which important gradients are hypothesised. Likely evidence for the existence of gradients is measured and captured in a complementary environmental dataset that has the same spatial structure (rows) as the species dataset. Direct gradient analysis is performed using linear or non-linear regression approaches that relate the ordinations performed on the species and their matching environmental variables.\n\n\nRedundancy Analysis\nConstrained Correspondence Analysis (CCA)\nDistance-based Redundancy Analysis (db-RDA)"
  },
  {
    "objectID": "BCB743/07-ordination.html#references",
    "href": "BCB743/07-ordination.html#references",
    "title": "7. Ordination",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html",
    "href": "BCB743/11-nMDS_diatoms.html",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nReading\nSerge Mayobo’s diatom paper\n💾 Mayombo_et_al_2019.pdf\n\n\nData\nAbbreviated diatom data matrix\n💾 PB_data_matrix_abrev.csv\n\n\n\nDiatoms data matrix\n💾 PB_data_matrix.csv\n\n\n\nDiatom environmental data\n💾 PB_diat_env.csv\nKelp forests are known to host a large biomass of epiphytic fauna and flora, including diatoms, which constitute the base of aquatic food webs and play an important role in the transfer of energy to higher trophic levels. Epiphytic diatom assemblages associated with two common species of South African kelps, Ecklonia maxima and Laminaria pallida, were investigated in this study. Primary blades of adult and juvenile thalli of both kelp species were sampled at False Bay in July 2017 and analysed using scanning electron microscopy. The diatom community data are here subjected to a suit of multivariate methods in order to show the structure of the diatom flora as a function of i) kelp species, and ii) kelp size. Read Mayombo et al. (2019) for more details and the findings of the research.\nSome feedback was received by anonymous reviewers as part of the peer review process, and it together with my response is repeated below.\nReviewer 1\nThe design of the observational study includes 2 treatments - age (young versus old) and host species (Laminaria versus Ecklonia), 4 replicates (4 primary blades from each combination of host algae and age), and 3 subsamples from each blade (pseudoreplicates, if treated incorrectly as replicates). The experimental design is analogous to a 2-way ANOVA, but with community data instead of a single individual response variable. This design can evaluate interactive effects between the two treatments (age and species). The authors’ experimental design is most suited to analyses using PERMANOVA, which is the community statistics version of the ANOVA.\nPlease indicate for the readers why the data were transformed and standardised using the stated procedures. Definitely a good idea to transform data, but the readers need to understand why particular procedures were employed. Please describe the Wisconsin double standardisation (row/column standardised by row/column total – to produce relative abundance to total and column/row standardised by column/row max – to produce abundance relative to species max abundance). Why a double standardisation + square-root transformation, as opposed to a single row/column standardisation by row/column total + square-root transformation?\nPlease indicate for the readers why the data were transformed and standardised using the stated procedures. Definitely a good idea to transform data, but the readers need to understand why particular procedures were employed. Please describe the Wisconsin double standardisation:\nAJS: About ANOSIM and PERMANOVA\n“Overall, Analysis of Similarities (ANOSIM) and the Mantel test were very sensitive to heterogeneity in dispersions, with ANOSIM generally being more sensitive than the Mantel test. In contrast, PERMANOVA and Pillai’s trace were largely unaffected by heterogeneity for balanced designs. […]. PERMANOVA was also unaffected by differences in correlation structure. […] PERMANOVA was generally, but not always, more powerful than the others to detect changes in community structure.”\nAJS: About data transformation\nUseful when the range of data values is very large. Data are square root transformed, and then submitted to Wisconsin double standardisation, or species divided by their maxima, and stands standardised to equal totals. These two standardisations often improve the quality of ordinations."
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html#set-up-the-analysis-environment",
    "href": "BCB743/11-nMDS_diatoms.html#set-up-the-analysis-environment",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "\n1 Set-up the analysis environment",
    "text": "1 Set-up the analysis environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(plyr)\n# library(BiodiversityR)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/diatoms/\""
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html#load-and-prepare-the-data",
    "href": "BCB743/11-nMDS_diatoms.html#load-and-prepare-the-data",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "\n2 Load and prepare the data",
    "text": "2 Load and prepare the data\n\n2.1 The species data\nThe diatom species data include the following:\n\ncolumns: diatom genera\nrows: samples (samples taken from two species of kelp; equivalent to sites in other species x sites tables)\nrow names correspond to combinations of the factors in the columns inside PB_diat_env.csv\n\n\nwhere host_size is A for adult kelp plant (host), J for juvenile kelp plant (host), host_spp is Lp for kelp species Laminaria pallida (host), Em for kelp plant Ecklonia maxima (host), plant is the unique number identifying a specific kelp plant, and rep is the replicate tissue sample from each kelp host plant from which the diatoms were extracted.\n\n# with shortened name to fix nMDS overplotting\nspp &lt;- read.csv(paste0(root, \"PB_data_matrix_abrev.csv\"),\n                row.names = \"Replicate\", sep = \",\", header = TRUE)\nspp[1:6, 1:6]\n\n        Amphora.spp Asteromphalus.spp Cocconeis.spp Craspedostauros.spp\nAPB1LP1           0                 0             0                   0\nAPB1LP2           0                 0             0                   0\nAPB1LP3           0                 0             0                   0\nAPB2LP1           0                 0             0                   0\nAPB2LP2           0                 0             0                   0\nAPB2LP3           0                 0             0                   0\n        Cylindrotheca.spp Diploneis.spp\nAPB1LP1                 0             0\nAPB1LP2                 0             0\nAPB1LP3                 0             0\nAPB2LP1                 0             0\nAPB2LP2                 0             0\nAPB2LP3                 0             0\n\n# with full names\nspp2 &lt;- read.csv(paste0(root, \"PB_data_matrix.csv\"),\n                 row.names = \"Replicate\", sep = \",\", header = TRUE)\nspp2[1:6, 1:6]\n\n        Amphora.spp Asteromphalus.spp Cocconeis.spp Craspedostauros.spp\nAPB1LP1           0                 0             0                   0\nAPB1LP2           0                 0             0                   0\nAPB1LP3           0                 0             0                   0\nAPB2LP1           0                 0             0                   0\nAPB2LP2           0                 0             0                   0\nAPB2LP3           0                 0             0                   0\n        Cylindrotheca.spp Diploneis.spp\nAPB1LP1                 0             0\nAPB1LP2                 0             0\nAPB1LP3                 0             0\nAPB2LP1                 0             0\nAPB2LP2                 0             0\nAPB2LP3                 0             0\n\n# remove \".spp\" from column header name\ncolnames(spp) &lt;- str_replace(colnames(spp), \"\\\\.spp\", \"\")\ncolnames(spp2) &lt;- str_replace(colnames(spp2), \"\\\\.spp\", \"\")\n\nLogarithmic transformation as suggested by Anderson (2006): \\(log_{b}(x) + 1\\) for \\(x &gt; 0\\), where \\(b\\) is the base of the logarithm; zeros are left as zeros. Higher bases give less weight to quantities and more to presences.\n\nspp.log &lt;- decostand(spp, method = \"log\")\nspp.log.dis &lt;- vegdist(spp.log, method = \"bray\")\n\n\n2.2 The ‘environmental’ data\nThe content is described above; these variables are categorical vars – they are not actually ‘environmental’ data, but their purpose in the analysis is analogous to true environmental data; it’s simply data that describe where the samples were taken from.\n\nenv &lt;- tibble(read.csv(paste0(root, \"PB_diat_env.csv\")),\n              sep = \",\", header = TRUE)\nenv$plant &lt;- as.factor(env$plant)\nenv$rep &lt;- as.factor(env$rep)\nhead(env)\n\n# A tibble: 6 × 7\n  replicate host_size host_spp plant rep   sep   header\n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;lgl&gt; \n1 APB1LP1   A         Lp       1     1     ,     TRUE  \n2 APB1LP2   A         Lp       1     2     ,     TRUE  \n3 APB1LP3   A         Lp       1     3     ,     TRUE  \n4 APB2LP1   A         Lp       2     1     ,     TRUE  \n5 APB2LP2   A         Lp       2     2     ,     TRUE  \n6 APB2LP3   A         Lp       2     3     ,     TRUE  \n\n\nWith the environmental data (factors), the following analyses can be done:\n\n✘ Discriminant Analysis (DA)\n✘ Analysis of Similarities (ANOSIM)\n✔︎ Permutational Analysis of Variance (PERMANOVA)\n✘ Mantel test\n\nWe will do an nMDS and PERMANOVA."
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html#multivariate-homogeneity-of-group-dispersions-variances",
    "href": "BCB743/11-nMDS_diatoms.html#multivariate-homogeneity-of-group-dispersions-variances",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "\n3 Multivariate homogeneity of group dispersions (variances)",
    "text": "3 Multivariate homogeneity of group dispersions (variances)\nBefore doing the PERMANOVA (testing differences between means), first check to see if the dispersion is the same. See ?adonis2 for more on this.\nHomogeneity of groups betadisper() evaluates the differences in group homogeneities. We can view it as being analogous to Levene’s test of the equality of variances. The null hypothesis evaluated is that the population variances are equal. Unfortunately we can only use one factor as an independent variable so it is not yet possible to look for interactions (species × size).\nSo, we test the \\(H_{0}\\) that the dispersion (variance) in diatom community structure does not differ between the two host species:\n\n(mod.spp &lt;- with(env, betadisper(spp.log.dis, host_spp)))\n\n\n    Homogeneity of multivariate dispersions\n\nCall: betadisper(d = spp.log.dis, group = host_spp)\n\nNo. of Positive Eigenvalues: 20\nNo. of Negative Eigenvalues: 21\n\nAverage distance to median:\n    Em     Lp \n0.3640 0.4391 \n\nEigenvalues for PCoA axes:\n(Showing 8 of 41 eigenvalues)\n PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n1.9619 1.7968 1.3888 1.0040 0.8491 0.6366 0.3132 0.3008 \n\nanova(mod.spp)\n\nAnalysis of Variance Table\n\nResponse: Distances\n          Df  Sum Sq  Mean Sq F value Pr(&gt;F)\nGroups     1 0.05876 0.058761  2.6087 0.1141\nResiduals 40 0.90101 0.022525               \n\n\nThere is no difference in dispersion between the diatom communities on the two host species. Apply the same procedure to see if host size has an effect:\n\n(mod.size &lt;- with(env, betadisper(spp.log.dis, host_size)))\n\n\n    Homogeneity of multivariate dispersions\n\nCall: betadisper(d = spp.log.dis, group = host_size)\n\nNo. of Positive Eigenvalues: 20\nNo. of Negative Eigenvalues: 21\n\nAverage distance to median:\n     A      J \n0.4005 0.3889 \n\nEigenvalues for PCoA axes:\n(Showing 8 of 41 eigenvalues)\n PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n1.9619 1.7968 1.3888 1.0040 0.8491 0.6366 0.3132 0.3008 \n\nanova(mod.size)\n\nAnalysis of Variance Table\n\nResponse: Distances\n          Df  Sum Sq   Mean Sq F value Pr(&gt;F)\nGroups     1 0.00141 0.0014134  0.0604 0.8071\nResiduals 40 0.93615 0.0234038               \n\n\nNo, it does not have an effect either. Make some plots to visualise the patterns:\n\npar(mfrow = c(2, 2))\nplot(mod.spp, sub = NULL)\nboxplot(mod.spp)\n\nplot(mod.size)\nboxplot(mod.size)\n\n\n\n\nOptionally, we can confirm the above analysis with the permutest() function. permutest() is a permutational ANOVA-like test that tests the \\(H_{0}\\) that there is no difference in the multivariate dispersion of diatom community structure between Ecklonia maxima and Laminaria pallida, and between adult and juvenile plants:\n\npermutest(mod.spp) # there is in fact no difference\n\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 999\n\nResponse: Distances\n          Df  Sum Sq  Mean Sq      F N.Perm Pr(&gt;F)\nGroups     1 0.05876 0.058761 2.6087    999  0.116\nResiduals 40 0.90101 0.022525                     \n\npermutest(mod.size) # nope...\n\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 999\n\nResponse: Distances\n          Df  Sum Sq   Mean Sq      F N.Perm Pr(&gt;F)\nGroups     1 0.00141 0.0014134 0.0604    999  0.804\nResiduals 40 0.93615 0.0234038                     \n\n\nIt should be sufficient to do the anova(), above, though. You can safely ignore the permutest()."
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html#permanova",
    "href": "BCB743/11-nMDS_diatoms.html#permanova",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "\n4 PERMANOVA",
    "text": "4 PERMANOVA\nPermutational multivariate Analysis of Variance (PERMANOVA; Anderson and Walsh (2013)) uses distance matrices (Bray-Curtis similarities by default), whereas ANOSIM uses only ranks of Bray-Curtis. The former therefore preserves more information and it is the recommended approach to test for differences between multivariate means. PERMANOVA also allows for variation partitioning and permits for more complex designs (multiple factors, nested factors, interactions, covariates, etc.). To this end, we use adonis2() to evaluate the differences in the group means, which makes it analogous to multivariate analysis of variance.\nNote that nestedness should be stated in the blocks (plants): “If you have a nested error structure, so that you do not want your data be shuffled over classes (blocks), you should define blocks in your permutation” – Jari Oksannen\n\n# the permutational structure captures the nesting of replicates within plant\nperm &lt;- how(nperm = 1000)\nsetBlocks(perm) &lt;- with(env, plant)\n\n(perm.1 &lt;- adonis2(spp.log.dis ~ host_spp * host_size,\n                   method = p, data = env,\n                   permutations = perm))\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nBlocks:  with(env, plant) \nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = spp.log.dis ~ host_spp * host_size, data = env, permutations = perm, method = p)\n                   Df SumOfSqs      R2      F Pr(&gt;F)\nhost_spp            1   0.2991 0.03815 1.7234      1\nhost_size           1   0.3726 0.04754 2.1475      1\nhost_spp:host_size  1   0.5727 0.07306 3.3003      1\nResidual           38   6.5938 0.84124              \nTotal              41   7.8381 1.00000              \n\n\nThere is no effect resulting from host species, host size, or interactions between the two."
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html#nmds",
    "href": "BCB743/11-nMDS_diatoms.html#nmds",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "\n5 nMDS",
    "text": "5 nMDS\nDo the nMDS and assemble the figures:\n\nspp.nmds &lt;- metaMDS(spp.log, k = 2,trymax = 100, trace = 0,\n                    distance = \"bray\", wascores = TRUE)\n\n# not printed as it is too long...\n# scores(spp.nmds, display = \"species\")\n# scores(spp.nmds, display = \"sites\")\n\n\ncol &lt;- c(\"indianred3\", \"steelblue4\")\npch &lt;- c(17, 19)\nopar &lt;- par()\nplt1 &lt;- layout(rbind(c(1, 1, 2, 2, 3, 3),\n                     c(4, 4, 4, 5, 5, 5)),\n               heights = c(2, 3),\n               respect = TRUE)\n\n# layout.show(plt1)\n\npar(mar = c(3,3,1,1))\n\n# plot 1\nplot(mod.spp, main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0), col = col, pch = pch,\n     sub = NULL)\n\n# plot 2\nplot(mod.size, main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0), col = col, pch = pch,\n     sub = NULL)\n\n# plot 3\nstressplot(spp.nmds, p.col = \"steelblue4\", l.col = \"indianred3\",\n           tck = .05, mgp = c(1.8, 0.5, 0))\n\n# plot 4\npar(mar = c(3,3,2,1))\nplot(spp.nmds, display = \"sites\", type = \"n\",\n     main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0),\n     xlim = c(-2, 2), ylim = c(-1, 2))\nwith(env,\n     points(spp.nmds, display = \"sites\", col = col[host_spp],\n            pch = pch[host_spp]))\nwith(env,\n     ordispider(spp.nmds, groups = host_spp,\n                label = TRUE,\n                col = col))\nwith(env, ordiellipse(spp.nmds, groups = host_spp,\n                      col = col, label = FALSE))\npoints(spp.nmds, display = \"species\", pch = 1, col = \"seagreen\")\norditorp(spp.nmds, display = \"species\", cex = 0.8,\n         col = \"black\", air = 0.01)\n\n# plot 5\npar(mar = c(3, 3, 2, 1))\nplot(spp.nmds, display = \"sites\", type = \"n\",\n     main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0),\n     xlim = c(-2, 2), ylim = c(-1, 2))\nwith(env,\n     points(spp.nmds, display = \"sites\", col = col[host_size],\n            pch = pch[host_size]))\nwith(env,\n     ordispider(spp.nmds, groups = host_size,\n                label = TRUE,\n                col = col))\nwith(env, ordiellipse(spp.nmds, groups = host_size,\n                      col = col, label = FALSE))\npoints(spp.nmds, display = \"species\", pch = 1, col = \"seagreen\")\norditorp(spp.nmds, display = \"species\", cex = 0.8,\n         col = \"black\", air = 0.01)\n\n\n\n# dev.off()\npar(opar)"
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html#multivariate-abundance-using-generalised-linear-models",
    "href": "BCB743/11-nMDS_diatoms.html#multivariate-abundance-using-generalised-linear-models",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "\n6 Multivariate abundance using Generalised Linear Models",
    "text": "6 Multivariate abundance using Generalised Linear Models\nWhat follows is an example of ‘Model-based Multivariate Analyses.’ I’ll not discuss this method here, but merely repeat the code as used in the Mayombo et al. (2019) paper. For background to the Multivariate abundance using Generalised Linear Models approach, refer to Wang et al. (2012) and Wang et al. (2017).\n\nlibrary(mvabund)\ndiat_spp &lt;- mvabund(spp2)\n\nLook at the spread of the data using the boxplot function. The figure is not used in paper:\n\npar(mar = c(2, 10, 2, 2)) # adjusts the margins\nboxplot(spp, horizontal = TRUE, las = 2, main = \"Abundance\", col = \"indianred\")\n\n\n\n\nCheck the mean-variance relationship:\n\nmeanvar.plot(diat_spp)\n\n\n\n\nThe above plot shows that spp with a high mean also have a high variance.\n\nAre there differences in the species composition of the diatom spp. sampled? This has already been addressed above, but we can apply an lternative approach below.\nDo some of them specialise on particular spp of kelp, while others are more generalised? Addressed below.\nDo some occur more on juveniles, while some are on adults, and which ones indiscriminately live across age classes? Addressed below.\nWhich species? Addressed below.\n\nScale manually for ggplot2() custom plot. Create a scale function:\n\nlog_fun &lt;- function(x) {\n  min_x &lt;- min(x[x != 0], na.rm = TRUE)\n  a &lt;- log(x) / min_x\n  a[which(!is.finite(a))] &lt;- 0\n  return(a)\n}\n\nMake a plot that shows which diatoms species are responsible for differences between adult and juvenile kelps:\n\nspp2 %&gt;%\n  mutate(host_size = env$host_size) %&gt;%\n  gather(key = species, value = abund, -host_size) %&gt;%\n  as_tibble() %&gt;%\n  group_by(species) %&gt;%\n  mutate(log.abund = log_fun(abund)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = fct_reorder(species, abund, .fun = mean), y = log.abund)) +\n  geom_boxplot(aes(colour = host_size), size = 0.4, outlier.size = 0,\n               fill = \"grey90\") +\n  geom_point(aes(colour = host_size, shape = host_size),\n             position = position_dodge2(width = 0.8),\n             alpha = 0.6, size = 2.5) +\n  scale_colour_manual(name = \"Age\", values = c(\"indianred3\", \"steelblue4\")) +\n  scale_shape_manual(name = \"Age\", values = c(17, 19)) +\n  annotate(\"text\", x = 15, y = 3, size = 4.5,\n           label = expression(paste(italic(\"p\"), \"=0.017\"))) +\n  annotate(\"text\", x = 14, y = 3, size = 4.5,\n           label = expression(paste(italic(\"p\"), \"=0.004\"))) +\n  scale_y_continuous(name = \"Log abundance\") +\n  coord_flip() + theme_bw() +\n  theme(panel.grid.major = element_line(linetype = \"dashed\",\n                                        colour = \"seagreen3\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_text(size = 13, color = \"black\",\n                                   margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")),\n        axis.text.y = element_text(size = 13, color = \"black\", face = \"italic\",\n                                   margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")),\n        axis.title.x = element_text(size = 14, vjust = 5.75, color = \"black\"),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(-0.25, \"cm\"),\n        axis.ticks = element_line(color = \"black\", size = 0.5))\n\n\n\n\nI settle on a negative binomial distribution for the species data. This will be provided to the manyglm() function:\n\nsize_mod2 &lt;- manyglm(diat_spp ~ (env$host_spp * env$host_size) / env$plant,\n                     family = \"negative binomial\")\nplot(size_mod2) # better residuals...\n\n\n\n\n\n# anova(size_mod2, test = \"wald\")\nout &lt;- anova(size_mod2, p.uni = \"adjusted\", test = \"wald\")\n\nTime elapsed: 0 hr 0 min 15 sec\n\nout$table\n\n                                     Res.Df Df.diff     wald Pr(&gt;wald)\n(Intercept)                              41      NA       NA        NA\nenv$host_spp                             40       1 5.227772     0.309\nenv$host_size                            39       1 7.799205     0.004\nenv$host_spp:env$host_size               38       1 5.434128     0.011\nenv$host_spp:env$host_size:env$plant     26      16      NaN     0.001\n\n\nWhat is the proportional contribution of some important species to juvenile and adult plants?\n\nprop.contrib &lt;- data.frame(spp = colnames(out$uni.test),\n                           prop = out$uni.test[3, ],\n                           row.names = NULL)\nprop.contrib %&gt;%\n  mutate(perc = round((prop / sum(prop)) * 100, 1)) %&gt;%\n  arrange(desc(perc)) %&gt;%\n  mutate(cum = cumsum(perc))\n\n               spp       prop perc   cum\n1    Rhoicosphenia 4.05979831 16.7  16.7\n2         Navicula 3.65789127 15.1  31.8\n3        Nitzschia 2.65842186 10.9  42.7\n4          Amphora 2.39874550  9.9  52.6\n5        Cocconeis 2.10936669  8.7  61.3\n6         Nagumoea 1.92435482  7.9  69.2\n7   Gomphoseptatum 1.86301773  7.7  76.9\n8    Cylindrotheca 1.79282759  7.4  84.3\n9      Parlibellus 1.40898477  5.8  90.1\n10      Licmophora 0.83203299  3.4  93.5\n11       Tabularia 0.64240853  2.6  96.1\n12 Craspedostauros 0.40412954  1.7  97.8\n13   Grammatophora 0.14035115  0.6  98.4\n14   Thalassionema 0.09310783  0.4  98.8\n15   Asteromphalus 0.08434341  0.3  99.1\n16       Diploneis 0.07915274  0.3  99.4\n17          Haslea 0.07915274  0.3  99.7\n18      Trachyneis 0.07149347  0.3 100.0"
  },
  {
    "objectID": "BCB743/11-nMDS_diatoms.html#references",
    "href": "BCB743/11-nMDS_diatoms.html#references",
    "title": "11b. nMDS of Mayombo’s Diatom Data",
    "section": "\n7 References",
    "text": "7 References\n\n\nAnderson MJ (2006) Distance-based tests for homogeneity of multivariate dispersions. Biometrics 62:245–253.\n\n\nAnderson MJ, Walsh DC (2013) PERMANOVA, ANOSIM, and the mantel test in the face of heterogeneous dispersions: What null hypothesis are you testing? Ecological monographs 83:557–574.\n\n\nMayombo N, Majewska R, Smit A (2019) Diatoms associated with two south african kelp species: Ecklonia maxima and laminaria pallida. African Journal of Marine Science 41:221–229.\n\n\nWang Y, Naumann U, Wright ST, Warton DI (2012) Mvabund–an r package for model-based analysis of multivariate abundance data. Methods in Ecology and Evolution 3:471–474.\n\n\nWang Y, Naumann U, Eddelbuettel D, Wilshire J, Warton D, Byrnes J, Santos Silva R dos, Niku J, Renner I, Wright S (2017) Mvabund: Statistical methods for analysing multivariate abundance data."
  },
  {
    "objectID": "BCB743/09-CA.html",
    "href": "BCB743/09-CA.html",
    "title": "9. Correspondence Analysis (CA)",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nSlides\nCA lecture slides\n💾 BCB743_09_CA.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\nCA is an eigenvalue method that can be applied to calculate the degree of ‘correspondence’ between the rows and columns of a presence-absence or abundance species table. We say that CA maximises the correspondence between species scores and sample scores, whereas a PCA maximises the variance explained. The technique is best applied to a species dataset where the species (in the columns) show unimodal distributions across sites (down the rows). CA represents the distances among sites and among species by the \\(\\chi^{2}\\) distance metric (rather than Euclidean distances in PCA), which does not suffer from the double-zero problem. Some ecologists argue that CA might be too much influenced by rare species. In ordination diagrams, species and sites are also represented by points as per usual convention. The relative positions of these points (species vs. sites, species vs. other species, or sites vs. other sites) indicate how strongly they ‘correspond’ to one another.\nCA therefore finds the ordination space that answers questions such as, “Which sites do my species prefer?” or “Which sites to my species correspond to?” (from GUSTA ME). Also see David Zelený’s Analysis of community ecology data in R."
  },
  {
    "objectID": "BCB743/09-CA.html#set-up-the-analysis-environment",
    "href": "BCB743/09-CA.html#set-up-the-analysis-environment",
    "title": "9. Correspondence Analysis (CA)",
    "section": "\n1 Set-up the analysis environment",
    "text": "1 Set-up the analysis environment\n\nlibrary(tidyverse)\nlibrary(vegan)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/\""
  },
  {
    "objectID": "BCB743/09-CA.html#the-doubs-river-data",
    "href": "BCB743/09-CA.html#the-doubs-river-data",
    "title": "9. Correspondence Analysis (CA)",
    "section": "\n2 The Doubs River data",
    "text": "2 The Doubs River data\nThis time we work with the species data:\n\nload(paste0(root, \"NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\"))\nhead(spe, 8)\n\n  Cogo Satr Phph Babl Thth Teso Chna Pato Lele Sqce Baba Albi Gogo Eslu Pefl\n1    0    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0    0    1    0\n4    0    4    5    5    0    0    0    0    0    1    0    0    1    2    2\n5    0    2    3    2    0    0    0    0    5    2    0    0    2    4    4\n6    0    3    4    5    0    0    0    0    1    2    0    0    1    1    1\n7    0    5    4    5    0    0    0    0    1    1    0    0    0    0    0\n8    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n  Rham Legi Scer Cyca Titi Abbr Icme Gyce Ruru Blbj Alal Anan\n1    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    0    0    0    0    0    0    0    0    0    0    0\n3    0    0    0    0    0    0    0    0    0    0    0    0\n4    0    0    0    0    1    0    0    0    0    0    0    0\n5    0    0    2    0    3    0    0    0    5    0    0    0\n6    0    0    0    0    2    0    0    0    1    0    0    0\n7    0    0    0    0    0    0    0    0    0    0    0    0\n8    0    0    0    0    0    0    0    0    0    0    0    0"
  },
  {
    "objectID": "BCB743/09-CA.html#do-the-ca",
    "href": "BCB743/09-CA.html#do-the-ca",
    "title": "9. Correspondence Analysis (CA)",
    "section": "\n3 Do the CA",
    "text": "3 Do the CA\nThe vegan function cca() can be used for CA and Constrained Correspondence Analysis (CCA). When we do not specify constraints, as we do here, we will do a simple CA:\n\nspe_ca &lt;- cca(spe)\n\nError in cca.default(spe): all row sums must be &gt;0 in the community data matrix\n\n\nOkay, so there’s a problem. The error message says that at least one of the rows sums to 0. Which one?\n\napply(spe, 1, sum)\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 3 12 16 21 34 21 16  0 14 14 11 18 19 28 33 40 44 42 46 56 62 72  4 15 11 43 \n27 28 29 30 \n63 70 87 89 \n\n\nWe see that the offending row is row 8, so we can omit it. This function will omit any row that sums to zero (or less):\n\nspe &lt;- spe[rowSums(spe) &gt; 0, ]\nhead(spe, 8)\n\n  Cogo Satr Phph Babl Thth Teso Chna Pato Lele Sqce Baba Albi Gogo Eslu Pefl\n1    0    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0    0    1    0\n4    0    4    5    5    0    0    0    0    0    1    0    0    1    2    2\n5    0    2    3    2    0    0    0    0    5    2    0    0    2    4    4\n6    0    3    4    5    0    0    0    0    1    2    0    0    1    1    1\n7    0    5    4    5    0    0    0    0    1    1    0    0    0    0    0\n9    0    0    1    3    0    0    0    0    0    5    0    0    0    0    0\n  Rham Legi Scer Cyca Titi Abbr Icme Gyce Ruru Blbj Alal Anan\n1    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    0    0    0    0    0    0    0    0    0    0    0\n3    0    0    0    0    0    0    0    0    0    0    0    0\n4    0    0    0    0    1    0    0    0    0    0    0    0\n5    0    0    2    0    3    0    0    0    5    0    0    0\n6    0    0    0    0    2    0    0    0    1    0    0    0\n7    0    0    0    0    0    0    0    0    0    0    0    0\n9    0    0    0    0    1    0    0    0    4    0    0    0\n\n\nNow we are ready for the CA:\n\nspe_ca &lt;- cca(spe)\nspe_ca\n\nCall: cca(X = spe)\n\n              Inertia Rank\nTotal           1.167     \nUnconstrained   1.167   26\nInertia is scaled Chi-square \n\nEigenvalues for unconstrained axes:\n   CA1    CA2    CA3    CA4    CA5    CA6    CA7    CA8 \n0.6010 0.1444 0.1073 0.0834 0.0516 0.0418 0.0339 0.0288 \n(Showing 8 of 26 unconstrained eigenvalues)\n\n\nThe more verbose summary() output:\n\nsummary(spe_ca)\n\n\nCall:\ncca(X = spe) \n\nPartitioning of scaled Chi-square:\n              Inertia Proportion\nTotal           1.167          1\nUnconstrained   1.167          1\n\nEigenvalues, and their contribution to the scaled Chi-square \n\nImportance of components:\n                        CA1    CA2     CA3     CA4     CA5     CA6     CA7\nEigenvalue            0.601 0.1444 0.10729 0.08337 0.05158 0.04185 0.03389\nProportion Explained  0.515 0.1237 0.09195 0.07145 0.04420 0.03586 0.02904\nCumulative Proportion 0.515 0.6387 0.73069 0.80214 0.84634 0.88220 0.91124\n                          CA8     CA9     CA10     CA11     CA12     CA13\nEigenvalue            0.02883 0.01684 0.010826 0.010142 0.007886 0.006123\nProportion Explained  0.02470 0.01443 0.009278 0.008691 0.006758 0.005247\nCumulative Proportion 0.93594 0.95038 0.959655 0.968346 0.975104 0.980351\n                          CA14     CA15     CA16     CA17     CA18     CA19\nEigenvalue            0.004867 0.004606 0.003844 0.003067 0.001823 0.001642\nProportion Explained  0.004171 0.003948 0.003294 0.002629 0.001562 0.001407\nCumulative Proportion 0.984522 0.988470 0.991764 0.994393 0.995955 0.997362\n                          CA20      CA21      CA22      CA23      CA24\nEigenvalue            0.001295 0.0008775 0.0004217 0.0002149 0.0001528\nProportion Explained  0.001110 0.0007520 0.0003614 0.0001841 0.0001309\nCumulative Proportion 0.998472 0.9992238 0.9995852 0.9997693 0.9999002\n                           CA25      CA26\nEigenvalue            8.949e-05 2.695e-05\nProportion Explained  7.669e-05 2.310e-05\nCumulative Proportion 1.000e+00 1.000e+00\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n\n\nSpecies scores\n\n          CA1       CA2      CA3       CA4       CA5       CA6\nCogo  1.50075 -1.410293  0.26049 -0.307203  0.271777 -0.003465\nSatr  1.66167  0.444129  0.57571  0.166073 -0.261870 -0.326590\nPhph  1.28545  0.285328 -0.04768  0.018126  0.043847  0.200732\nBabl  0.98662  0.360900 -0.35265 -0.009021 -0.012231  0.253429\nThth  1.55554 -1.389752  0.80505 -0.468471  0.471301  0.225409\nTeso  0.99709 -1.479902 -0.48035  0.079397 -0.105715 -0.332445\nChna -0.54916 -0.051534  0.01123 -0.096004 -0.382763  0.134807\nPato -0.18478 -0.437710 -0.57438  0.424267 -0.587150  0.091866\nLele  0.01337 -0.095342 -0.57672  0.212017  0.126668 -0.389103\nSqce  0.01078  0.140577 -0.34811 -0.538268  0.185286  0.167087\nBaba -0.33363 -0.300682 -0.04929  0.170961 -0.157203  0.103068\nAlbi -0.38357 -0.255310 -0.20136  0.374057 -0.385866  0.239001\nGogo -0.32152 -0.034382 -0.07423 -0.031236  0.014417 -0.156351\nEslu -0.26165  0.187282  0.00617  0.183771  0.295142 -0.262808\nPefl -0.28913  0.121044 -0.18919  0.367615  0.218087 -0.163675\nRham -0.60298 -0.057369  0.20341  0.214299 -0.050977  0.211926\nLegi -0.58669 -0.082467  0.21198  0.050175 -0.120456  0.108724\nScer -0.61815  0.124733  0.13339  0.147190  0.317736 -0.340380\nCyca -0.57951 -0.110732  0.20173  0.308547  0.006854  0.153224\nTiti -0.37880  0.138023 -0.07825  0.095793  0.256285 -0.029245\nAbbr -0.70235  0.011155  0.40242  0.211582  0.138186  0.132297\nIcme -0.73238 -0.009098  0.55678  0.321852  0.281812  0.172271\nGyce -0.69300  0.038971  0.37688 -0.183965 -0.051945 -0.011126\nRuru -0.44181  0.176915 -0.23691 -0.345104  0.129676 -0.043802\nBlbj -0.70928  0.032317  0.40924  0.030224  0.049050  0.114560\nAlal -0.63114  0.053594  0.15204 -0.661381 -0.414796 -0.206611\nAnan -0.63578 -0.041894  0.30093  0.224044  0.030444  0.203160\n\n\nSite scores (weighted averages of species scores)\n\n        CA1       CA2        CA3      CA4      CA5      CA6\n1   2.76488  3.076306  5.3657489  1.99192 -5.07714 -7.80447\n2   2.27540  2.565531  1.2659130  0.87538 -1.89139 -0.13887\n3   2.01823  2.441224  0.5144079  0.79436 -1.03741  0.56015\n4   1.28485  1.935664 -0.2509482  0.76470  0.54752  0.10579\n5   0.08875  1.015182 -1.4555434  0.47672  2.69249 -2.92498\n6   1.03188  1.712163 -0.9544059  0.01584  0.91932  0.39856\n7   1.91427  2.256208 -0.0001407  0.39844 -1.07017  0.32127\n9   0.25591  1.443008 -2.5777721 -3.41400  2.36613  2.71741\n10  1.24517  1.526391 -1.9635663 -0.41230  0.69647  1.51859\n11  2.14501  0.110278  1.6108693 -0.82023  0.53918  1.01153\n12  2.17418 -0.251649  1.5845397 -0.81483  0.52623  1.05501\n13  2.30944 -2.034439  1.9181448 -0.60481  0.64435 -0.14844\n14  1.87141 -2.262503  1.1066796 -0.80840  1.09542  0.11038\n15  1.34659 -1.805967 -0.6441505 -0.52803  0.76871 -0.67165\n16  0.70214 -1.501167 -1.9735888  0.98502 -0.93585 -1.27168\n17  0.28775 -0.836803 -1.2259108  0.73302 -1.57036  0.57315\n18  0.05299 -0.647950 -0.9234228  0.35770 -0.95401  0.77738\n19 -0.20584 -0.007252 -1.0154343  0.07041 -1.03450  0.51442\n20 -0.57879  0.042849 -0.3660551 -0.15019 -0.61357  0.10115\n21 -0.67320  0.038875  0.1194956  0.17256 -0.14686 -0.12018\n22 -0.71933  0.014694  0.2204186  0.13598  0.09459 -0.02068\n23 -0.70438  0.735398 -0.6546250 -6.61523 -2.49441 -1.73215\n24 -0.83976  0.390120  0.5605295 -4.38864 -2.56916 -0.96702\n25 -0.68476  0.418842 -0.2860819 -2.80336 -0.37540 -3.93791\n26 -0.75808  0.210204  0.5894091 -0.70004 -0.01880 -0.10779\n27 -0.75046  0.100869  0.5531191 -0.12946  0.29164  0.11280\n28 -0.77878  0.088976  0.7379012  0.05204  0.40940  0.43236\n29 -0.60815 -0.203235  0.5522726  0.43621  0.15010  0.25618\n30 -0.80860 -0.019592  0.6686542  0.88136  0.52744  0.16456\n\n\nThe output looks similar to that of a PCA. The important things to note are the inertia (unconstrained and total inertia are the same), the Eigenvalues for the unconstrained axes, the Species scores, and the Site scores. Their interpretation is the same as before, but we can reiterate. Let us calculate the total inertia:\n\nround(sum(spe_ca$CA$eig), 5)\n\n[1] 1.16691\n\n\nThe inertia for the first axis (CA1) is:\n\nround(spe_ca$CA$eig[1], 5)\n\n    CA1 \n0.60099 \n\n\nThe inertia of CA1 and CA2 is:\n\nround(sum(spe_ca$CA$eig[1:2]), 5)\n\n[1] 0.74536\n\n\nThe fraction of the variance explained by CA1 and CA2 is:\n\nround(sum(spe_ca$CA$eig[1:2]) / sum(spe_ca$CA$eig) * 100, 2) # result in %\n\n[1] 63.87\n\n\nAbove, the value is the same one as in Cumulative Proportion in the summary(spe_ca) output under the CA2 column.\nSpecies scores are actual species scores as they now relate to species data. The most positive and most negative eigenvectors (or loadings) indicate those species that dominate in their influence along particular CA axes. For example, CA1 will be most heavily loaded by the species Cogo and Satr (eigenvectors of 1.50075 and 1.66167, respectively). If there is an environmental gradient, it will be these species that will be most affected. At the very least, we can say that the contributions of these species are having an overriding influence on the community differences seen between sites.\nSite scores are also as seen earlier in PCA. The highest positive or negative loadings indicate sites that are dispersed far apart on the biplot (in ordination space). They will have large differences in fish community composition.\nPlease see Numerical Ecology in R (pp. 133 to 140). There you will find explanations for how to interpret the ordinations and the ordination diagrams shown below."
  },
  {
    "objectID": "BCB743/09-CA.html#ordination-diagrams",
    "href": "BCB743/09-CA.html#ordination-diagrams",
    "title": "9. Correspondence Analysis (CA)",
    "section": "\n4 Ordination diagrams",
    "text": "4 Ordination diagrams\nThe biplots for the above ordination are given here:\n\nplot(spe_ca, scaling = 1, main = \"CA fish abundances - biplot scaling 1\")\n\n\n\nplot(spe_ca, scaling = 2, main = \"CA fish abundances - biplot scaling 2\")\n\n\n\ndev.off()\n\nnull device \n          1 \n\ndev.new()\n\nScaling 1: This scaling emphasises relationships between rows accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are approximations of their \\(\\chi^{2}\\) distances in multidimensional space. Objects found near a point representing a species are likely to contain a high contribution of that species.\nScaling 2: This scaling emphasises relationships between columns accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are not approximations of their \\(\\chi^{2}\\) distances in multidimensional space, but the distances among species are. Species positioned close to the point representing an object (a sample or site) are more likely to be found in that object or to have higher frequency there.\nBelow are biplots with site and species scores for four selected species. The plots are augmented with response surfaces made with penalised splines for each species showing i) where it are most abundant and ii) the direction of the response (here non-linear). On the last panel (bottom right) I also project vectors for the environmental drivers and mark the ones with the greatest influence in red. Lastly, the point size scales with species scores:\n\npar(mar = c(4, 4, 0.9, 0.5) + .1, mfrow = c(2, 2))\nwith(spe, ordisurf(spe_ca ~ Satr, bubble = 3,\n                   family = quasipoisson, knots = 2, col = \"turquoise\",\n                   display = \"sites\", main = \"Satr\", plot = TRUE))\n\n\nFamily: quasipoisson \nLink function: log \n\nFormula:\ny ~ poly(x1, 2) + poly(x2, 2) + poly(x1, 1):poly(x2, 1)\nTotal model degrees of freedom 6 \n\nREML score: 3.375507     \n\nabline(h = 0, v = 0, lty = 3)\nwith(spe, ordisurf(spe_ca ~ Scer, bubble = 3,\n                   family = quasipoisson, knots = 2, col = \"turquoise\",\n                   display = \"sites\", main = \"Scer\"))\n\n\nFamily: quasipoisson \nLink function: log \n\nFormula:\ny ~ poly(x1, 2) + poly(x2, 2) + poly(x1, 1):poly(x2, 1)\nTotal model degrees of freedom 6 \n\nREML score: -12.42414     \n\nabline(h = 0, v = 0, lty = 3)\nwith(spe, ordisurf(spe_ca ~ Teso, bubble = 3,\n                   family = quasipoisson, knots = 2, col = \"turquoise\",\n                   display = \"sites\", main = \"Teso\"))\n\n\nFamily: quasipoisson \nLink function: log \n\nFormula:\ny ~ poly(x1, 2) + poly(x2, 2) + poly(x1, 1):poly(x2, 1)\nTotal model degrees of freedom 6 \n\nREML score: -10.15301     \n\nabline(h = 0, v = 0, lty = 3)\nwith(spe, ordisurf(spe_ca ~ Cogo, bubble = 3,\n                   family = quasipoisson, knots = 2, col = \"turquoise\",\n                   display = \"sites\", main = \"Cogo\"))\n\n\nFamily: quasipoisson \nLink function: log \n\nFormula:\ny ~ poly(x1, 2) + poly(x2, 2) + poly(x1, 1):poly(x2, 1)\nTotal model degrees of freedom 6 \n\nREML score: -27.82059     \n\nabline(h = 0, v = 0, lty = 3)\n\n# a posteriori projection of environmental variables in a CA\nenv &lt;- dplyr::select(env, -1)\n\n# we removed the 8th row in spe, so do it here too\nenv &lt;- dplyr::slice(env, -8)\n\n# the last plot produced (CA scaling 2) must be active\n# scaling 2 is default\n(spe_ca_env &lt;- envfit(spe_ca, env, scaling = 2))\n\n\n***VECTORS\n\n         CA1      CA2     r2 Pr(&gt;r)    \nele  0.81159  0.58423 0.8078  0.001 ***\nslo  0.73753  0.67531 0.2976  0.002 ** \ndis -0.92837 -0.37166 0.4440  0.002 ** \npH   0.50723 -0.86181 0.0908  0.219    \nhar -0.71728 -0.69678 0.4722  0.002 ** \npho -0.99897  0.04533 0.1757  0.059 .  \nnit -0.94906 -0.31511 0.4510  0.001 ***\namm -0.97495  0.22241 0.1762  0.071 .  \noxy  0.93352 -0.35854 0.6263  0.001 ***\nbod -0.94094  0.33857 0.2237  0.025 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\nplot(spe_ca_env, col = \"grey40\")\nplot(spe_ca_env, p.max = 0.05, col = \"red\") # plot significant variables with a different colour"
  },
  {
    "objectID": "BCB743/09-CA.html#references",
    "href": "BCB743/09-CA.html#references",
    "title": "9. Correspondence Analysis (CA)",
    "section": "\n5 References",
    "text": "5 References"
  },
  {
    "objectID": "BCB743/11-nMDS.html",
    "href": "BCB743/11-nMDS.html",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nSlides\nnMDS lecture slides\n💾 BCB743_11_nMDS.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\nnMDS is a rank-based indirect gradient analysis (i.e. not an eigen-analysis) that uses a distance or dissimilarity matrix as input. Whereas the ordination methods discussed thus far try to maximise the variance or correspondence between sites, nMDS strives to represent pairwise dissimilarities between sites in ordination space. It does not use the distances or dissimilarities directly (hence indirect), but these are substituted with their ranks (e.g. dissimilarities between pairs of sites are ordered by rank) (and hence non-metric in the name). This results in a loss of insight into the magnitude of difference between site pairs, but we benefit from the technique being more robust and less influenced by deviations from idealised data distributions.\nnMDS is the non-metric equivalent to PCoA, the latter sometimes being called metric multi-dimensional scaling."
  },
  {
    "objectID": "BCB743/11-nMDS.html#set-up-the-analysis-environment",
    "href": "BCB743/11-nMDS.html#set-up-the-analysis-environment",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "\n1 Set-up the analysis environment",
    "text": "1 Set-up the analysis environment\n\nlibrary(tidyverse)\nlibrary(vegan)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/\""
  },
  {
    "objectID": "BCB743/11-nMDS.html#the-doubs-river-data",
    "href": "BCB743/11-nMDS.html#the-doubs-river-data",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "\n2 The Doubs River data",
    "text": "2 The Doubs River data\nWe continue to use the species data:\n\nload(paste0(root, \"NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\"))\nspe &lt;- dplyr::slice(spe, -8)"
  },
  {
    "objectID": "BCB743/11-nMDS.html#do-the-nmds",
    "href": "BCB743/11-nMDS.html#do-the-nmds",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "\n3 Do the nMDS",
    "text": "3 Do the nMDS\n\nspe_nmds &lt;- metaMDS(spe, distance = \"bray\")\n\nRun 0 stress 0.07477835 \nRun 1 stress 0.07376217 \n... New best solution\n... Procrustes: rmse 0.01939832  max resid 0.09462732 \nRun 2 stress 0.1127684 \nRun 3 stress 0.1111013 \nRun 4 stress 0.08841667 \nRun 5 stress 0.074778 \nRun 6 stress 0.07376231 \n... Procrustes: rmse 0.000138231  max resid 0.0006633523 \n... Similar to previous best\nRun 7 stress 0.1116204 \nRun 8 stress 0.112439 \nRun 9 stress 0.08797356 \nRun 10 stress 0.1152378 \nRun 11 stress 0.1237587 \nRun 12 stress 0.1123501 \nRun 13 stress 0.07376248 \n... Procrustes: rmse 0.000300032  max resid 0.00144343 \n... Similar to previous best\nRun 14 stress 0.1111025 \nRun 15 stress 0.07477823 \nRun 16 stress 0.1209554 \nRun 17 stress 0.07383679 \n... Procrustes: rmse 0.003803445  max resid 0.01477002 \nRun 18 stress 0.120581 \nRun 19 stress 0.08801542 \nRun 20 stress 0.07477826 \n*** Best solution repeated 2 times\n\nspe_nmds\n\n\nCall:\nmetaMDS(comm = spe, distance = \"bray\") \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe \nDistance: bray \n\nDimensions: 2 \nStress:     0.07376217 \nStress type 1, weak ties\nBest solution was repeated 2 times in 20 tries\nThe best solution was from try 1 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe' \n\n\nAs always, reading the help file for (accessible as ?metaMDS) is invaluable (as are the help files for all other ordination techniques).\nThere’s a summary method available, but it is not particularly useful and I don’t display the output here:\n\nsummary(spe_nmds)\n\nAlthough summary(spe_nmds) does not return anything interesting, the species and site scores are nevertheless available directly through the scores() command, and they can be plotted as layer in ggplot2 if need be:\n\nscores(spe_nmds)\n\n$sites\n         NMDS1       NMDS2\n1  -1.78980195  0.81469780\n2  -1.14370577 -0.15607428\n3  -1.00359605 -0.07439971\n4  -0.62133504 -0.07827003\n5   0.07202297  0.45937288\n6  -0.42771160 -0.15267560\n7  -0.87065136 -0.22997295\n8  -0.01429597 -0.86248886\n9  -0.52654428 -0.40370942\n10 -1.00859725 -0.37651774\n11 -0.97239881 -0.16340206\n12 -1.16259225  0.09756002\n13 -0.80290814  0.12421387\n14 -0.49314773  0.17979525\n15 -0.18896807  0.27606344\n16  0.08272191  0.12262114\n17  0.29906116  0.11268964\n18  0.44600645  0.14941142\n19  0.76735302  0.28448871\n20  0.86247262  0.37757654\n21  0.95535718  0.44478248\n22  0.75369040 -1.44466700\n23  1.13003147 -0.63138910\n24  0.85947646 -0.87234155\n25  0.93346893  0.12338650\n26  0.97530994  0.36343456\n27  1.02381414  0.37369193\n28  0.79624579  0.55874059\n29  1.06922184  0.58338154\n\n$species\n          NMDS1       NMDS2\nCogo -0.9122315 -0.10424649\nSatr -1.1125705 -0.22550486\nPhph -0.7890843 -0.32082896\nBabl -0.5341364 -0.28950040\nThth -0.9380699 -0.09681930\nTeso -0.5248950  0.16417001\nChna  0.9017232  0.36615033\nPato  0.5172456  0.38526789\nLele  0.3302663  0.28571322\nSqce  0.3585971 -0.15454886\nBaba  0.7038600  0.47497252\nAlbi  0.7281765  0.47030587\nGogo  0.6852815  0.30429897\nEslu  0.6095539  0.42162535\nPefl  0.6172279  0.50363469\nRham  0.9667205  0.58280328\nLegi  0.9562490  0.50928552\nScer  0.9659150  0.55194015\nCyca  0.9586519  0.62827127\nTiti  0.7242180  0.41802279\nAbbr  1.0825254  0.67976340\nIcme  1.1281813  0.78237089\nGyce  1.0743958  0.40737275\nRuru  0.7594075  0.15155048\nBlbj  1.0951696  0.58044030\nAlal  0.9935839  0.02753029\nAnan  1.0091570  0.61225379\n\n\nSee Numerical Ecology in R (pp. 145 to 149) for information about the interpretation of a nMDSand the ordination diagrams shown below."
  },
  {
    "objectID": "BCB743/11-nMDS.html#ordination-diagrams",
    "href": "BCB743/11-nMDS.html#ordination-diagrams",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "\n4 Ordination diagrams",
    "text": "4 Ordination diagrams\nWe create the ordination diagrammes as before, but new concepts introduced here are stress, Shepard plots, and goodness of fit. The stress indicates the scatter of observed dissimilarities against an expected monotone regression, while a Shepard diagram plots ordination distances against original dissimilarities, and adds a monotone or linear fit line to highlight this relationship. The stressplot() function also produces two fit statistics. The goodness-of-fit of the ordination is measured as the \\(R^{2}\\) of either a linear or a non-linear regression of the nMDS distances on the original ones.\n\npar(mfrow = c(2, 2))\nstressplot(spe_nmds, main = \"Shepard plot\")\nordiplot(spe_nmds, type = \"t\", cex = 0.6,\n         main = paste0(\"nMDS stress = \", round(spe_nmds$stress, 2)))\ngof = goodness(spe_nmds)\nplot(spe_nmds, type = \"t\", main = \"Goodness of fit\")\npoints(spe_nmds, display = \"sites\", cex = gof * 200)\n# ...bigger bubbles indicate a worse fit\n\n\n\n\nA good rule of thumb: stress &lt;0.05 provides an excellent representation in reduced dimensions, &lt;0.1 is great, &lt;0.2 is so-so, and stress &lt;0.3 provides a poor representation.\nWe can also build ordination plots from scratch to suit specific needs:\n\npl &lt;- ordiplot(spe_nmds, type = \"none\", main = \"nMDS fish abundances \")\npoints(pl, \"sites\", pch = 21, cex = 1.75, col = \"grey80\", bg = \"grey80\")\npoints(pl, \"species\", pch = 21, col = \"turquoise\", arrows = TRUE)\ntext(pl, \"species\", col = \"blue4\", cex = 0.9)\ntext(pl, \"sites\", col = \"red4\", cex = 0.9)\n\n\n\n\nOr we can fit response surfaces using ordisurf() and project environmental drivers:\n\nrequire('viridis')\npalette(viridis(8))\npar(mar = c(4, 4, 0.9, 0.5) + .1, mfrow = c(2, 2))\nwith(spe, tmp &lt;- ordisurf(spe_nmds ~ Satr, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Satr\"))\nabline(h = 0, v = 0, lty = 3)\nwith(spe, tmp &lt;- ordisurf(spe_nmds ~ Scer, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Scer\"))\nabline(h = 0, v = 0, lty = 3)\nwith(spe, tmp &lt;- ordisurf(spe_nmds ~ Teso, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Teso\"))\nabline(h = 0, v = 0, lty = 3)\nwith(spe, tmp &lt;- ordisurf(spe_nmds ~ Cogo, bubble = 3,\n                          family = quasipoisson, knots = 2, col = 6,\n                          display = \"sites\", main = \"Cogo\"))\nabline(h = 0, v = 0, lty = 3)\n\nenv &lt;- dplyr::slice(env, -8)\n\n(spe_nmds_env &lt;- envfit(spe_nmds, env)) \n\n\n***VECTORS\n\n       NMDS1    NMDS2     r2 Pr(&gt;r)    \ndfs  0.97103  0.23896 0.7488  0.001 ***\nele -0.98917 -0.14679 0.6114  0.001 ***\nslo -0.74768  0.66406 0.3268  0.017 *  \ndis  0.88592  0.46383 0.5684  0.001 ***\npH  -0.26393  0.96454 0.0179  0.806    \nhar  0.96326 -0.26855 0.3773  0.004 ** \npho  0.45988 -0.88798 0.5439  0.001 ***\nnit  0.87817 -0.47834 0.5819  0.001 ***\namm  0.42988 -0.90289 0.5546  0.001 ***\noxy -0.74294  0.66935 0.7395  0.001 ***\nbod  0.44253 -0.89675 0.6572  0.001 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\nplot(spe_nmds_env, col = \"grey40\")\nplot(spe_nmds_env, p.max = 0.05, col = \"red\")"
  },
  {
    "objectID": "BCB743/11-nMDS.html#references",
    "href": "BCB743/11-nMDS.html#references",
    "title": "11a. non-Metric Multidimensional Scaling (nMDS)",
    "section": "\n5 References",
    "text": "5 References"
  },
  {
    "objectID": "BCB743/08-PCA_examples.html",
    "href": "BCB743/08-PCA_examples.html",
    "title": "8b. PCA Additional Examples",
    "section": "",
    "text": "library(tidyverse)\nlibrary(vegan)\nlibrary(ggcorrplot) # for the correlations\nlibrary(ggpubr)"
  },
  {
    "objectID": "BCB743/08-PCA_examples.html#set-up-the-analysis-environment",
    "href": "BCB743/08-PCA_examples.html#set-up-the-analysis-environment",
    "title": "8b. PCA Additional Examples",
    "section": "",
    "text": "library(tidyverse)\nlibrary(vegan)\nlibrary(ggcorrplot) # for the correlations\nlibrary(ggpubr)"
  },
  {
    "objectID": "BCB743/08-PCA_examples.html#the-iris-data",
    "href": "BCB743/08-PCA_examples.html#the-iris-data",
    "title": "8b. PCA Additional Examples",
    "section": "\n2 The Iris data",
    "text": "2 The Iris data\nThe Iris dataset is a well-known collection of data that represent the morphological characteristics of three species of Iris, viz. I. setosa, I. versicolor, and I. virginica. The morphological characteristics measured include sepal length and width and petal length and width.\nThe question we can address using a PCA is, “which of these variables (sepal length and width, petal length and width) is most responsible for causing visual morphological differences between the three species?”\n\ndata(\"iris\")\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n2.1 Visualise the raw data\nThe first thing to do after having loaded the data is to see how the variables are correlated with one-another, and we can do so with a simple pairwise correlation. I’ll demonstrate five ways of doing so.\n\n2.1.1 Method 1:\n\ncorr &lt;- cor(iris[, 1:4])\n\nggcorrplot(corr, type = 'upper', outline.col = \"white\",\n           colors = c(\"#00AFBB\", \"white\", \"#FC4E07\"),\n           lab = TRUE)\n\n\n\n\n\n2.1.2 Method 2:\n\ncols &lt;- c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\npairs(iris[, 1:4], pch = 19,  cex = 0.5,\n      col = cols[iris$Species],\n      lower.panel = NULL)\n\n\n\n\n\n2.1.3 Method 3:\n\nlibrary(GGally)\nggpairs(iris, aes(colour = Species, alpha = 0.4)) +\n  scale_color_discrete(type = cols) +\n  scale_fill_discrete(type = cols)\n\n\n\n\n\n2.1.4 Method 4:\n\nlibrary(scatterPlotMatrix)\nscatterPlotMatrix(iris, zAxisDim = \"Species\")\n\n\n\n\n\n\n2.1.5 Method 5:\n\niris |&gt; \n  pivot_longer(cols = Sepal.Length:Petal.Width,\n               values_to = \"mm\",\n               names_to = \"structure\") |&gt; \n  ggplot(aes(x = structure, y = mm)) +\n  geom_jitter(aes(colour = Species), shape = 9, width = 0.3, alpha = 0.6) +\n  scale_color_discrete(type = cols) +\n  coord_flip() +\n  theme_bw() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"grey60\", linetype = \"dashed\")\n  )\n\n\n\n\nBy examining all the plots, above (but particularly the simplest one in Method 5), what can we conclude about which morphological variable is most responsible for the visual differences among species? The petal dimensions seem to be the most telling by virtue of their being less overlap of point representing the three species, particularly that of its length. The dimensions of the sepals seem to be less important as offering a way to distinguish the species.\nA PCA should be able to reduce the complexity of measurements and tell us which of the four variables is most able to tell the species apart. It should reduce the four dimensions (sepal width and length, and petal width and length) into the most influential one or two rotated and scaled orthogonal dimensions (axes).\n\n2.2 Do the PCA\n\niris_pca &lt;- rda(iris[, 1:4], scale = FALSE)\niris_pca\n\nCall: rda(X = iris[, 1:4], scale = FALSE)\n\n              Inertia Rank\nTotal           4.573     \nUnconstrained   4.573    4\nInertia is variance \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4 \n4.228 0.243 0.078 0.024 \n\n\n\nsummary(iris_pca, display = \"sp\") # omit display of site scores\n\n\nCall:\nrda(X = iris[, 1:4], scale = FALSE) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal           4.573          1\nUnconstrained   4.573          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                         PC1     PC2     PC3      PC4\nEigenvalue            4.2282 0.24267 0.07821 0.023835\nProportion Explained  0.9246 0.05307 0.01710 0.005212\nCumulative Proportion 0.9246 0.97769 0.99479 1.000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\n\nSpecies scores\n\n                 PC1      PC2      PC3     PC4\nSepal.Length  1.7754 -0.77277  0.38889  0.1164\nSepal.Width  -0.4152 -0.85936 -0.39950 -0.1179\nPetal.Length  4.2086  0.20405 -0.05094 -0.1770\nPetal.Width   1.7602  0.08884 -0.36470  0.2780\nattr(,\"const\")\n[1] 5.109\n\n\n\n2.3 Plot the PC scores as a normal panel of points\n\nPC1_scores &lt;- as.data.frame(scores(iris_pca, choices = c(1, 2, 3, 4), display = \"sites\"))\nPC1_scores$Species &lt;- iris$Species\n\nPC1_scores |&gt; \n  pivot_longer(cols = PC1:PC4,\n               values_to = \"score\",\n               names_to = \"PC\") |&gt; \n  ggplot(aes(x = PC, y = score)) +\n  geom_jitter(aes(colour = Species), shape = 9, width = 0.3, alpha = 0.6) +\n  scale_color_discrete(type = cols) +\n  coord_flip() +\n  theme_bw() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"pink\", linetype = \"dashed\")\n  )\n\n\n\n\n\n2.4 Make biplots\n\n2.4.1 A default biplot\n\nbiplot(iris_pca, type = c(\"text\", \"points\"))\n\n\n\n\n\n2.4.2 A ggplot() biplot\nAssemble a biplot from scratch in ggplot2. This requires that we extract from the iris_pca object all the necessary components and layer them one-by-one using ggplot():\n\nlibrary(ggforce) # for geom_circle\n\n# species scores (actually morph properties here) for biplot arrows:\niris_spp_scores &lt;- data.frame(scores(iris_pca, display = \"species\"))\n\n# add center point for arrows to start at:\niris_spp_scores$xy_start &lt;- rep(0, 4)\n\n# add the rownames as a column for plotting at the arrow heads:\niris_spp_scores$morph &lt;- rownames(iris_spp_scores)\nrownames(iris_spp_scores) &lt;- NULL\n\n# var explained along PC1 used for labeling the x-axis:\nPC1_var &lt;- round(iris_pca$CA$eig[1] / sum(iris_pca$CA$eig) * 100, 1)\n\n# var explained along PC2 used for labeling the y-axis:\nPC2_var &lt;- round(iris_pca$CA$eig[2] / sum(iris_pca$CA$eig) * 100, 1)\n\n# calculate the radius of the circle of equilibrium contribution\n# (Num Ecol with R, p. 125):\nr &lt;- sqrt(2/4)\n\n# species scores (actually indiv measurements here) for biplot points:\niris_site_scores &lt;- data.frame(scores(iris_pca, display = \"sites\"))\niris_site_scores$Species &lt;- iris$Species\n\nggplot(iris_site_scores, aes(x = PC1, y = PC2)) +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 0), linetype = \"dashed\") +\n  geom_point(aes(colour = Species), shape = 9) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = r), # not yet correctly scaled!!\n              linetype = 'dashed',\n              lwd = 0.6, inherit.aes = FALSE) +\n  geom_segment(data = iris_spp_scores, aes(x = xy_start, y = xy_start,\n                                           xend = PC1, yend = PC2),\n               lineend = \"butt\",\n               arrow = arrow(length = unit(3, \"mm\"),\n                             type = \"closed\",\n                             angle = 20),\n               alpha = 0.7, colour = \"dodgerblue\") +\n  geom_label(data = iris_spp_scores, aes(x = PC1, y = PC2, label = morph),\n             nudge_y = -0.12,\n             colour = \"dodgerblue\") +\n  scale_color_discrete(type = cols) +\n  coord_equal() +\n  scale_x_continuous(limits = c(-1, 4.6)) +\n  labs(x = paste0(\"PC1 (\", PC1_var, \"% variance explained)\"),\n       y = paste0(\"PC2 (\", PC2_var, \"% variance explained)\")) +\n  theme_bw() +\n  theme(\n    panel.grid.major.x = element_line(colour = \"pink\", linetype = \"dashed\"),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"pink\", linetype = \"dashed\"),\n    panel.grid.minor.y = element_blank(),\n    legend.position = c(0.9, 0.2),\n    legend.box.background = element_rect(colour = \"black\")\n  )\n\n\n\n\nWhat do we see in the biplot? We see that most of the variation in morphology between the three Iris species is explained by PC1 (obviously), which accounts for 92.5% of the total inertia. Very little is added along PC2 (only an additional 5.3% variance explained), so we may safely ignore it. Looking at the ‘Species scores’ associated with PC1 (see summary(iris_pca)), we see that the heaviest loading is with petal length, which causes the long arrow in the positive PC1 direction; it has virtually no loading along PC2, and this is confirmed by the fact that the arrow is positioned almost parallel along PC1 and does not deviate up or down in the PC2 direction. We can also see that the biplot arrow for petal width sits completely on top of the petal length arrow. This means that petal length and width are almost perfectly correlated (we can also see this in the pairwise correlations where the r-value is 0.96)."
  },
  {
    "objectID": "BCB743/08-PCA_examples.html#references",
    "href": "BCB743/08-PCA_examples.html#references",
    "title": "8b. PCA Additional Examples",
    "section": "\n3 References",
    "text": "3 References"
  },
  {
    "objectID": "BCB743/08-PCA.html",
    "href": "BCB743/08-PCA.html",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nSlides\nPCA lecture slides\n💾 BCB743_08_PCA.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\n\n\nR function\nA function for ordination plots\n💾 cleanplot.pca.R\nPrincipal Component Analysis (PCA) is one of many ordination techniques. Depending on the application of technical/scientific discipline it is applied in, you might see it mentioned under dimensionality reduction techniques; other times it is named as a form of unsupervised learning. Regardless of what it is called, ordination refers to a suite of multivariate techniques that reduces a multivariate (multi-dimensional) dataset in such a way that when it is projected onto a lower dimensional space, typically 2D or 3D space, any intrinsic structure in the data forms visually-discernible patterns (Pielou, 1984). Ordination summarises community data (e.g. samples of species presence-absence or abundance across multiple sites) by producing a low-dimensional ordination space where similar samples (typically species) plot close together, and dissimilar samples far apart. Dimensions of this low dimensional space represent important and interpretable environmental gradients.\nIn ecology, ordination techniques are used to describe relationships between community structure patterns and underlying environmental gradients. So, we can ask questions such as, “Which environmental variables cause a community to vary across a landscape?” Ordinations allow us to determine the relative importance of different gradients, and graphical presentations of the results can lead to intuitive interpretations of species-environment relationships."
  },
  {
    "objectID": "BCB743/08-PCA.html#set-up-the-analysis-environment",
    "href": "BCB743/08-PCA.html#set-up-the-analysis-environment",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "\n1 Set-up the analysis environment",
    "text": "1 Set-up the analysis environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(ggcorrplot) # for the correlations\nlibrary(ggpubr)"
  },
  {
    "objectID": "BCB743/08-PCA.html#the-doubs-river-data",
    "href": "BCB743/08-PCA.html#the-doubs-river-data",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "\n2 The Doubs River data",
    "text": "2 The Doubs River data\n\nload(\"../data/NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\")\nhead(env)\n\n   dfs ele  slo  dis  pH har  pho  nit  amm  oxy bod\n1  0.3 934 48.0 0.84 7.9  45 0.01 0.20 0.00 12.2 2.7\n2  2.2 932  3.0 1.00 8.0  40 0.02 0.20 0.10 10.3 1.9\n3 10.2 914  3.7 1.80 8.3  52 0.05 0.22 0.05 10.5 3.5\n4 18.5 854  3.2 2.53 8.0  72 0.10 0.21 0.00 11.0 1.3\n5 21.5 849  2.3 2.64 8.1  84 0.38 0.52 0.20  8.0 6.2\n6 32.4 846  3.2 2.86 7.9  60 0.20 0.15 0.00 10.2 5.3"
  },
  {
    "objectID": "BCB743/08-PCA.html#first-do-a-correlation",
    "href": "BCB743/08-PCA.html#first-do-a-correlation",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "\n3 First do a correlation",
    "text": "3 First do a correlation\n\n# computing a correlation matrix\ncorr &lt;- round(cor(env), 1)\n\n# visualization of the correlation matrix\nggcorrplot(corr, type = 'upper', outline.col = \"white\",\n           colors = c(\"#1679a1\", \"white\", \"#f8766d\"),\n           lab = TRUE)\n\n\n\n\nSome variables are very correlated, and they might be omitted from the subsequent analyses. We say that these variables are ‘collinear.’ Collinear variables cannot be teased apart in terms of finding out which one is most influential in structuring the community. There are more advanced ways to search for collinear variables (e.g. Variance Inflation Factors, VIF) and in this way we can systematically exclude them from the PCA. See Graham (2003) for a discussion on collinearity. Here we will proceed with all the variables."
  },
  {
    "objectID": "BCB743/08-PCA.html#see-the-spatial-context",
    "href": "BCB743/08-PCA.html#see-the-spatial-context",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "\n4 See the spatial context",
    "text": "4 See the spatial context\nThe patterns in the data and the correlations between them will make more sense if we can visualise a spatial context. Thankfully spatial data are available:\n\nhead(spa)\n\n       X      Y\n1 85.678 20.000\n2 84.955 20.100\n3 92.301 23.796\n4 91.280 26.431\n5 92.005 29.163\n6 95.954 36.315\n\nggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1) +\n  geom_label(vjust = 0, nudge_y = 0.5, check_overlap = TRUE)\n\n\n\n\nThese site numbers correspond approximately to the ones in Verneaux (1973) but some of the numbers may have been shifted slightly in the example Doubs dataset used here compared to how they were originally numbered in Verneaux’s thesis and subsequent publication. This should not affect the interpretation. We can also scale the symbol size by the magnitude of the environmental variables. Lets look at two pairs of variables that are strongly correlated with one-another:\n\n# We scale the data first so as to better represent the full\n# magnitude of all variables with a common symbol size\nenv_std &lt;- decostand(env, method = \"standardize\")\n\n# positive correlations\nplt1 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"red\", aes(size = env_std$amm, shape = 3)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(size = \"Magnitude\", title = \"Ammonium concentration\")\n\nplt2 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"red\", aes(size = env_std$bod)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(title = \"Biological oxygen demand\")\n\n# inverse correlations\nplt3 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"blue\", aes(size = env_std$alt)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(title = \"Altitude\")\n\nplt4 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"blue\", aes(size = env_std$flo)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(title = \"Flow rate\")\n\nggarrange(plt1, plt2, plt3, plt4, nrow = 2, ncol = 2,\n          common.legend = TRUE, labels = \"AUTO\")"
  },
  {
    "objectID": "BCB743/08-PCA.html#do-the-pca",
    "href": "BCB743/08-PCA.html#do-the-pca",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "\n5 Do the PCA",
    "text": "5 Do the PCA\nWe use the function rda() to do the PCA, but it can also be performed in base R with the functions prcomp() and princomp(). rda() is the same function that we will use later for a Redundancy Analysis, but when used without specifying constraints (as we do here) it amounts to simply doing a PCA. Typically we standardise environmental data to unit variance, but the PCA done by the rda() function accomplishes this step automagically when scale = TRUE. When applied to environmental data (as we typically do with a PCA) it works with correlations amongst the scaled variables. PCA preserves Euclidean distance and the relationships detected are linear, and for this reason it is not typically applied to species data without suitable transformations. In fact, in this module we will seldom apply a PCA to species data at all.\n\nenv_pca &lt;- rda(env, scale = TRUE)\nenv_pca\n\nCall: rda(X = env, scale = TRUE)\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\nInertia is correlations \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n5.969 2.164 1.065 0.739 0.400 0.336 0.173 0.108 0.024 0.017 0.006 \n\n# same ...\n# env_std &lt;- scale(env)\n# env_pca &lt;- rda(env_std, scale = FALSE)\n# env_pca\n\nEcologists use the term inertia as a synonym for ‘variation’, but some PCA software (such as R’s prcomp() and princomp()) simply uses the term sdev for standard deviations. In PCA, when we use a correlation matrix (as we do here), the inertia is the sum of the diagonal values of the correlation matrix, which is simply the number of variables (11 in this example). When a PCA uses a covariance matrix the inertia is the sum of the variances of the variables.\nYou will also see in the output the mention of the term ‘unconstrained’. In a PCA the analysis is always unconstrained (i.e. not influenced by some a priori defined variables we hypothesise to explain the between site patterns in the multivariate data).\nThe section headed Eigenvalues for unconstrained axes shows the relative importance of the resultant reduced axes, and they can be used to determine the proportion of the total inertia (sum of the eigenvalues) captured by any one of the axes. They can be accessed with the function eigenvals() (the preferred function; see ?rda for help), but an alternative method is given below. The first eigenvalue (the one associated with PC1) always explains the most variation (the largest fraction), and each subsequent one explains the largest proportion of the remaining variance. We say the axes are orthogonal and ranked in decreasing order of importance. The sum of all eigenvalues is the total inertia, so collectively they theoretically can explain all of the variation in the dataset (but clearly they should not be used to explain all the variance). To extract the first eigenvalue we can do:\n\nround(eigenvals(env_pca)[1], 3)\n\n  PC1 \n5.969 \n\n# or\n\nround(env_pca$CA$eig[1], 3)\n\n  PC1 \n5.969 \n\n\nThe total inertia is:\n\nsum(eigenvals(env_pca))\n\n[1] 11\n\n# or\n\nsum(env_pca$CA$eig)\n\n[1] 11\n\n\nSo the proportion of variation explained by the first PC is:\n\nround(env_pca$CA$eig[1] / sum(env_pca$CA$eig) * 100, 1) # result in %\n\n PC1 \n54.3 \n\n\nWe can show the same information as part of a more verbose summary. Here we see the pre-calculated Proportion Explained and Cumulative Proportion (it should be obvious what this is). There is also an assortment of other information, viz. Scaling 2 for species and site scores, Species scores, and Site scores.\n\nsummary(env_pca)\n\n\nCall:\nrda(X = env, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal              11          1\nUnconstrained      11          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                         PC1    PC2     PC3     PC4     PC5     PC6     PC7\nEigenvalue            5.9687 2.1639 1.06517 0.73875 0.40019 0.33563 0.17263\nProportion Explained  0.5426 0.1967 0.09683 0.06716 0.03638 0.03051 0.01569\nCumulative Proportion 0.5426 0.7393 0.83616 0.90332 0.93970 0.97022 0.98591\n                           PC8      PC9     PC10     PC11\nEigenvalue            0.108228 0.023701 0.017083 0.005983\nProportion Explained  0.009839 0.002155 0.001553 0.000544\nCumulative Proportion 0.995748 0.997903 0.999456 1.000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  4.226177 \n\n\nSpecies scores\n\n         PC1     PC2      PC3      PC4      PC5      PC6\ndfs  1.08657  0.5342 -0.27333 -0.13477  0.07336  0.22566\nele -1.04396 -0.6148  0.20712  0.12854  0.14610  0.02111\nslo -0.57703 -0.4893 -0.63490 -0.71684  0.33349 -0.11782\ndis  0.95843  0.6608 -0.32456 -0.16183  0.11542  0.13935\npH  -0.06364  0.4629  1.01317 -0.58606  0.17094  0.07360\nhar  0.90118  0.5850  0.06449  0.25696  0.30995 -0.53390\npho  1.05821 -0.6014  0.13866 -0.17883 -0.11125 -0.13751\nnit  1.15013 -0.1005 -0.05167 -0.24537 -0.35105  0.02145\namm  1.00679 -0.6969  0.14077 -0.14684 -0.19200 -0.11904\noxy -0.97459  0.4991 -0.09017 -0.31040 -0.38066 -0.36500\nbod  0.97315 -0.7148  0.15145  0.07193  0.23633 -0.05540\n\n\nSite scores (weighted sums of species scores)\n\n        PC1      PC2      PC3      PC4       PC5       PC6\n1  -1.41274 -1.40098 -2.03484 -2.67759  1.117150 -0.184951\n2  -1.03725 -0.77955  0.24400  0.25635 -1.192043  1.849810\n3  -0.94507 -0.46765  1.25042 -0.49330 -0.234194  1.319198\n4  -0.87371 -0.26988  0.19304  0.51979 -0.494639 -0.116092\n5  -0.42088 -0.66944  0.83191  0.71729  0.867751 -0.112219\n6  -0.77224 -0.72067 -0.07357  0.77902 -0.386130  0.654273\n7  -0.77466 -0.08103  0.39630  0.19224  0.416470 -1.026304\n8  -0.28840 -0.60589  0.83822  1.01440  1.707316 -0.295861\n9  -0.28305 -0.47710  0.39908  1.13075  0.882098 -0.002961\n10 -0.48714 -0.41860 -1.27555  0.90267  0.013704 -0.542270\n11 -0.26940  0.45384  0.09119 -0.15127 -0.233814 -1.157483\n12 -0.43834  0.36049 -0.52352  0.57279 -0.650095 -0.817673\n13 -0.37794  0.70379  0.10339  0.06127 -0.101571 -1.376623\n14 -0.23878  0.75522  0.83648 -0.55822 -0.011527 -1.221217\n15 -0.30425  0.95026  1.80274 -1.48211  0.135021 -0.031795\n16 -0.13354  0.33951 -0.23252  0.19177 -0.667112 -0.227348\n17  0.10111  0.32379 -0.20380  0.18495 -0.676546 -0.364915\n18  0.06913  0.37913 -0.25881  0.06998 -0.851379 -0.289054\n19  0.05746  0.43915  0.04566 -0.32171 -0.899449  0.090759\n20  0.17478  0.39927 -0.36244 -0.15647 -1.300718  0.093396\n21  0.16944  0.35608 -0.73929  0.42751 -0.509249  0.653892\n22  0.14898  0.55339 -0.08008 -0.04972  0.196636  0.621753\n23  1.39778 -1.19102  0.66424 -0.46178  0.252908 -0.573369\n24  0.99357 -0.52036  0.07186  0.48088  1.068785  0.373991\n25  2.22002 -2.03168  0.17940 -0.52606 -1.148014 -0.786506\n26  0.89388 -0.10410 -0.61440  0.42034  0.343649  0.800522\n27  0.64866  0.41296 -0.17444 -0.26105  0.274443  1.259099\n28  0.77100  0.82592  0.43387 -1.00092 -0.001674  0.703378\n29  0.66413  1.11562 -1.58043  0.65099  0.650327  0.020001\n30  0.74743  1.36955 -0.22810 -0.43281  1.431895  0.686570\n\n\nSpecies scores are the loadings (a.k.a. rotated and scaled eigenvectors) that indicate the strength of contribution of the original environmental variables to the new variables, the Principal Components (PC1, PC2, etc.). These loadings effectively indicate the degree of correlation between the original variables and the new principal components, and the sign of the eigenvectors indicate the polarity (inverse or positive) of the correlation between the original variable and the new variable. Even though we work with environmental data here, these scores are still called species scores by the software—don’t let the name confuse you! (Why do you think this is?) They indicate how much each of the original environmental variables contribute to PC1, PC2, etc. The larger (more positive) and smaller (more negative) values indicate a greater contribution, albeit in opposite directions. In the example, PC1 is made up of uneven contributions from most of the original variables, with the largest value being nitrate (1.15013) and smallest oxygen (-0.97459). Nitrate and oxygen therefore contribute most towards the differences between sites so that places with more nitrate are also the places with the lowest dissolved oxygen concentration. This makes ecological sense too. pH and slope are the least important variables, i.e. they are least able to explain the differences between sites along PC1. Given the strength of PC1 (it explains 54.3% of the inertia), one might hypothesise that its constituent variables, particularly nitrate and oxygen, influence many aspects of the community. The species scores are presented as arrows on the ordination diagrams (see below). Longer vectors have a greater influence (are stronger drivers) on environmental (and possibly species) differences between sites, and their direction indicates along which PC axes their influence is greatest.\nSite scores are the scaled and rotated coordinates of the objects (sites or samples, one for each row of the raw data table). They are used to plot the position of the sites in 2D or 3D ordination space. Sites spread further apart from others in this space differ much in terms of the environmental conditions. How far they spread apart depends on the major environmental gradients indicated by the species scores—i.e. along PC1, sites that are spread far apart in this dimension experience very different concentrations of nitrate and oxygen (see the species scores for the identity of the influential variables).\nIn ecology, it is customary to plot the Site and Species Scores as ordination diagrams called biplots for a few of the reduced axes. We will get to this below.\nScaling 1 and Scaling 2, depending on what was specified in the rda() function call, are useful for whether one wants to interpret species (scaling 1) or variables (scaling 2). When calling Scaling 1, the distances between points plotted on the ordination diagram will retain their Euclidian distances, which allows for better interpretation of how sites relate to one-another. Calling Scaling 2 preserves more accurately the angles between variables with the consequence that in the biplot smaller angles between variable vectors will reflect stronger correlations. More on scaling below."
  },
  {
    "objectID": "BCB743/08-PCA.html#graphical-represenations-of-ordinations",
    "href": "BCB743/08-PCA.html#graphical-represenations-of-ordinations",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "\n6 Graphical represenations of ordinations",
    "text": "6 Graphical represenations of ordinations\n\n6.1 Plots along one-dimension\nNow I will construct some primitive graphs of the Site and Species Scores to demonstrate how to interpret the eigenvectors associated with each eigenvalue. We will typically not do this kind of graphical display—for plots suitable for publication see the Biplots section, below.\nThe first thing we need to do is extract the Species and Site Scores in a manner that makes them convenient for plotting. To do this, we can apply the scores() function to the PCA object, env_pca, and assign the output to tidied dataframes. The scores() function can tidy the data to some extent, but I make it even tidier in subsequent steps by creating long format data (rather than wide) using the pivot_longer() function. Various other bits of code lines accomplish additional restructuring of the data to make datasets that are fully compliant for creating the kind of figure I have in mind:\n\n# species scores first by setting 'display' to species\n# we are interested in all axes (PC1 to PC11) so set the 'choices' argument\n# appropriately (see `?scores`):\nspp_sc &lt;- scores(env_pca, display = \"species\", choices = seq(1:11), tidy = TRUE)\n\n# now pivot longer to make the data even tidier:\nspp_sc &lt;- spp_sc |&gt; \n  select(-score) |&gt; # remove column\n  pivot_longer(cols = PC1:PC11, # pivot\n               names_to = \"PC_axis\",\n               values_to = \"score\") |&gt; \n  group_by(PC_axis) |&gt; \n  mutate(rank = rank(abs(score)), # rank absolute scores\n         origin = 0, # create a column for start of arrows\n         PC_axis = factor(PC_axis, levels = paste0(rep(\"PC\", 11), seq(1:11)))) |&gt; \n  # above, reorder the factor levels so PC axis plot in right order\n  filter(rank &gt;= 10) |&gt; # keep only 2 higheest ranked scores\n  ungroup()\n\nhead(spp_sc)\n\n# A tibble: 6 × 5\n  label PC_axis   score  rank origin\n  &lt;chr&gt; &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 dfs   PC1      1.09      37      0\n2 dfs   PC2      0.534     21      0\n3 dfs   PC3     -0.273     22      0\n4 dfs   PC6      0.226     17      0\n5 dfs   PC11    -0.0774    13      0\n6 ele   PC1     -1.04      35      0\n\n# now the site scores:\nsite_sc &lt;- scores(env_pca, display = \"sites\", choices = seq(1:11), tidy = TRUE)\n\nsite_sc &lt;- site_sc |&gt; \n  select(-score) |&gt; \n  pivot_longer(cols = PC1:PC11,\n               names_to = \"PC_axis\",\n               values_to = \"score\") |&gt; \n  mutate(label = as.numeric(label),\n         PC_axis = factor(PC_axis, levels = paste0(rep(\"PC\", 11), seq(1:11))))\n\nhead(site_sc)\n\n# A tibble: 6 × 3\n  label PC_axis   score\n  &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;\n1    NA PC1      1.09  \n2    NA PC2      0.534 \n3    NA PC3     -0.273 \n4    NA PC4     -0.135 \n5    NA PC5      0.0734\n6    NA PC6      0.226 \n\n\n\n# var explained along PC1, PC2, and PC3 for adding to plot:\nPC1_var &lt;- round(env_pca$CA$eig[1] / sum(env_pca$CA$eig) * 100, 1)\nPC2_var &lt;- round(env_pca$CA$eig[2] / sum(env_pca$CA$eig) * 100, 1)\nPC3_var &lt;- round(env_pca$CA$eig[3] / sum(env_pca$CA$eig) * 100, 1)\n\nHow do we know how many reduced axes are influential and should be kept? Commonly recommended is the broken stick method—keep the principal components whose eigenvalues are higher than corresponding random broken stick components:\n\n# make a scree plot using the vegan function:\nscreeplot(env_pca, bstick = TRUE, type = \"lines\")\n\n\n\n# or assemble from scratch in ggplot2:\n# extract eigenvalues and calc the broken stick values...\nscree_dat &lt;- data.frame(eigenvalue = as.vector(eigenvals(env_pca)),\n                        bstick = bstick(env_pca))\nscree_dat$axis &lt;- rownames(scree_dat)\nrownames(scree_dat) &lt;- NULL\nscree_dat &lt;- scree_dat |&gt; \n  mutate(axis = factor(axis, levels = paste0(rep(\"PC\", 11), seq(1:11))))\n\nggplot(data = scree_dat, aes(x = axis, y = eigenvalue)) +\n  geom_point() +\n  geom_line(aes(group = 1)) +\n  geom_point(aes(y = bstick), colour = \"red\") +\n  geom_line(aes(y = bstick, group = 1), colour = \"red\") +\n  labs(x = \"Principal component\", y = \"Inertia\")\n\n\n\n\nIn the plot, above, the red line is the broken stick components and the black line the eigenvalues for the different PCs. See Numerical Ecology with R pp. 121-122 for more information about how to decide how many PCs to retain.\nNow we can assemble a plot, and in it focus on the first two PCs. It seems somewhat complex, but the code can easily be deciphered if you read through it ‘layer-by-layer’:\n\nlibrary(ggrepel)\n\nggplot(data = site_sc, aes(x = PC_axis, y = score)) +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n  geom_jitter(shape = 19, width = 0.09, aes(colour = label)) +\n  scale_colour_viridis_c(name = \"Site no.\") +\n  geom_segment(data = spp_sc, aes(x = PC_axis, y = origin,\n                                  xend = PC_axis, yend = score),\n               lineend = \"butt\",\n               arrow = arrow(length = unit(3, \"mm\"),\n                             type = \"open\",\n                             angle = 30),\n               alpha = 0.8, size = 0.7, colour = \"red\") +\n  geom_text_repel(data = spp_sc, aes(label = label), size = 3.0,\n                  direction = \"y\", colour = \"red\") +\n  annotate(geom = \"text\", x = 1, y = -2.5, size = 3.0, colour = \"red\",\n           label = paste0(PC1_var, \"% var. expl.\")) +\n  annotate(geom = \"text\", x = 2, y = -2.5, size = 3.0, colour = \"red\",\n           label = paste0(PC2_var, \"% var. expl.\")) +\n  annotate(geom = \"text\", x = 3, y = -2.5, size = 3.0, colour = \"red\",\n           label = paste0(PC3_var, \"% var. expl.\")) +\n  coord_flip() +\n  labs(x = NULL, y = \"Score\") +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"pink\", linetype = \"dashed\"),\n    legend.position = c(0.075, 0.75),\n    legend.box.background = element_rect(colour = \"black\")\n  )\n\n\n\n\nAlthough you will never see a graph like this one, examining it is nevertheless informative. What I did was:\n\n\nplot all new PC axes on the vertical axis; they represent the reduced, simplified ecological space; these PC axes are ranked from most important (PC1) to least important (PC11)—and each one’s ability to explain some property of the environment is ranked by the magnitude of their eigenvalues\nPC1 explains 54.3% of the total variation in the environmental dataset\nPC2 explains an additional 19.7% of the remaining variance left over after accounting for the influence of PC1\nthe cumulative % variance explained by PC1 and PC2 is 74%\nfor each PC axis I plot\nthe Site scores as coloured points\n\nthe colours indicate the sampled sites’ numbers\nthe points indicate the spread of the sites across linear Euclidian space with respect to the main environmental gradients represented by each PC axis\nthe Species scores as arrows\n\nI only plot the top two most heavily loaded absolute eigenvectors\nthe main environmental gradients are represented by the arrows\nthe gradient represented is annotated by text giving the name of the environmental variables\nthe location of the arrow heads is located in Euclidian space at the coordinates provided by the Species scores\nthe longer the arrow, the more influence it has on causing the sites to spread out in Euclidian space\nthe arrows point in the direction where the magnitude of the environmental variable is greater, and in the opposite direction the magnitude of the variable is less; for example, sites are spread out along PC1 primarily due to the influence of the variables nitrate and distance from source such that the sites further down the river (more yellow) tend to have a higher nitrate concentration and have a larger distance from source, and sites closer to the source (more blue) have a smaller distance from source and lower nitrate concentration.\n\n6.2 Biplots\nIt will be more informative if we represent the coordinates given by the eigenvectors (Species and Site scores) as points on a 2D plane where the axes are made from PC1 and PC2 (or PC1 and PC3…). Sites now will be spread out not along a 1D line but over 2D space along x and y directions, and the arrows will point at angles across this 2D Euclidian space. This is called a biplot because it plots two things, viz. sites as points and envionmental variables as vectors. In this way, we can more clearly see how combinations of variables influence the spatial arrangement of sites—arrows point in the direction of the gradient and sites spread out along the the arrow in both positive (indicated by arrow head) and negative directions (extend an imaginary line in the opposite direction from the arrow head). Do not attach too much meaning to the loadings plotted along the x and y axes as their sole purpose is to define the Euclidian ‘landscape’ across which sites are scattered. In this Euclidian representation of a reduced space, the arrangement of sites will represent the actual relative arrangement of sites in geographical space where the environmental variables actually operate. As indicated before, sites that plot far apart along a particular gradient (arrow) differ greatly in terms of the particular environmental property (inidcated by the arrow) that the sites exhibit.\nGraphical representations of ordination results are called ordination diagrams, and biplots are key examples of such diagrams. See David Zelený’s excellent writing on the topic.\nAlthough many of the examples provided here use the default plot options for the ordination—that rely on base graphics—the plots can also be set up in ggplot2. This requires some deeper knowledge of what goes on in the ordination objects. I provide some examples scattered throughout the course content (e.g. here), but you may also refer to the step-by-step walk throughs provided by Roeland Kindt.\nIn a PCA ordination diagram, following the tradition of scatter diagrams in Cartesian coordinate systems, objects are represented as points and variables are displayed as arrows. We first use the standard vegan biplot() function:\n\nbiplot(env_pca, scaling = 1, main = \"PCA scaling 1\", choices = c(1, 2))\n\n\n\nbiplot(env_pca, scaling = 2, main = \"PCA scaling 2\", choices = c(1, 2))\n\n\n\n\nScaling 1: This scaling emphasises relationships between rows accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are approximations of their Euclidian distances in multidimensional space. Objects positioned further apart show a greater degree of environmental dissimilarity. The angles among descriptor vectors should not be interpreted as indicating the degree of correlation between the variables.\nScaling 2: This scaling emphasises relationships between columns accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are not approximations of their Euclidian distances in multidimensional space. The angles among descriptor vectors can be interpreted as indicating the degree of correlation between the variables.\nNow we create biplots using the cleanplot.pca() function that comes with the Numerical Ecology in R book. The figures are more or less the same, except the plot showing the Site scores with Scaling 1 adds a ‘circle of equilibrium contribution’ (see Numerical Ecolology with R, p. 125). We only assign importance to the arrows that extend beyond the radius of the circle:\n\n# we need to load the function first from its R file:\nsource(\"../data/NEwR-2ed_code_data/NEwR2-Functions/cleanplot.pca.R\")\ncleanplot.pca(env_pca, scaling = 1)\n\n\n\ncleanplot.pca(env_pca, scaling = 2)\n\n\n\n\nAt this point it is essential that you refer to Numerical Ecology in R (pp. 118 to 126) for help with interpreting the ordination diagrams.\nWe can plot the underlying environmental gradients using the ordisurf() function in vegan. We plot the response surfaces for elevation and biological oxygen demand:\n\nbiplot(env_pca, type = c(\"text\", \"points\"), col = c(\"black\", \"black\"))\nordisurf(env_pca ~ bod, env, add = TRUE, col = \"turquoise\", knots = 1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ poly(x1, 1) + poly(x2, 1)\nTotal model degrees of freedom 3 \n\nREML score: 46.65864     \n\nordisurf(env_pca ~ ele, env, add = TRUE, col = \"salmon\", knots = 1)\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ poly(x1, 1) + poly(x2, 1)\nTotal model degrees of freedom 3 \n\nREML score: 160.6339     \n\n\nWe see that the contours form a linear trend surface, i.e. they are perpendicular to their vectors. This is the main weakness of PCA, as community data are non-linear (in fact, environmental gradients are also seldom very linear, but they can be more linear than species data). In general, therefore, PCA should not be used for community data."
  },
  {
    "objectID": "BCB743/08-PCA.html#references",
    "href": "BCB743/08-PCA.html#references",
    "title": "8a. Principal Component Analysis (PCA)",
    "section": "\n7 References",
    "text": "7 References\nGraham, M. H. (2003). Confronting multicollinearity in ecological multiple regression. Ecology 84, 2809–2815."
  },
  {
    "objectID": "BCB744/basic_stats/05-inference.html",
    "href": "BCB744/basic_stats/05-inference.html",
    "title": "5. Statistical inference and hypothesis testing",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nThe concept of inferential statistics\nHypothesis testing\nProbabilities\nAssumptions and parametric statistics\n\nNormality and the Shapiro-Wilk test\nHomoscedasticity\n\n\n\n\n\n\n\n\n\n\n\nTasks to complete in this Chapter\n\n\n\n\nTask D 1\n\n\n\n\n1 Introduction\nWe have seen in Chapter 2 and Chapter 3 how to summarise, describe, and visualise our data—these processes form part of descriptive statistics. The next step is the process of conducting inferential statistics.\nInferential statistics is a branch of statistics that focuses on drawing conclusions and making generalisations about a larger population based on the analysis of a smaller, representative sample. This is particularly valuable in research situations where it is impractical or impossible to collect data from every member of a population—i.e. all of biology and ecology. By employing probabilistic reasoning, inferential statistics enable us to estimate population parameters, make predictions, and test hypotheses with a certain level of confidence.\nOne of the key aspects of inferential statistics is the concept of sampling variability. Since samples are only a subset of the population, they imperfectly represent whole populations, leading to variations in the estimates of population parameters (repeatedly drawing samples at random from a population will result in slightly different values for key statistical parameters, such as the sample mean and variance). Inferential statistics accounts for this variability by providing measures of uncertainty, such as confidence intervals and margins of error, which convey the range within which the true population parameter is likely to fall.\n\n\n\n\n\n\nTask D\n\n\n\n\nDevise an experiment in which you demonstrate the following two principles: i) the effect of sample size on the estimate of the mean and variance (use SD), and ii) the effect of repeated sampling on the value of the mean and variance. In both cases, apply the correct t-test (later later in this Chapter) to test whether sample size and repeat sampling has a statistically significant effect. What conclusion do you reach?\n\n\n\nParametric statistics form the foundation of inferential statistics, and they are used to make inferences about population parameters based on sample data. These statistics assume that the data are generated from a specific probability distribution, such as the normal distribution. An alternative to parametric tests are non-parametric statistics, and we shall hear more about it in Chapter 6.\nThe most common parametric statistics used in inferential statistics include:\n\nt-tests (Chapter 7) used to determine if there is a significant difference between the means of two groups of continuous dependent (response) variables.\nANOVA (Chapter 8) used to determine if there is a significant difference between the means of three or more groups of continuous variables.\nRegression analysis (Chapter 9) used to model the relationship between one or more continuous predictor variables and a continuous response variable.\nPearson correlation (Chapter 10) used to measure the linear association or relationships between two continuous variables.\nChi-squared tests used to determine if there is a significant association between two categorical variables.\n\nThese tests typically involve the calculation of a test statistic and the comparison of this value with a critical value and then establishing a p-value to determine whether the results are statistically significant or likely due to chance. These methods are included within asubset of inferential statistics called probablilistic statistics.\n\n\n\n\n\n\nProbabilistic and Bayesian statistics\n\n\n\nProbabilistic and Bayesian statistics are two related but distinct branches of statistics that offer tools for modelling, analysing, and drawing inferences from complex data sets. At their core, both approaches rely on the use of probability theory to quantify uncertainty and variability in data, but they differ in their assumptions about the nature of this uncertainty and how it should be modelled.\nProbabilistic statistics is a classical approach that assumes that all sources of variability in a data set can be described by a fixed set of probability distributions, such as the normal distribution or the Poisson distribution. These distributions are characterised by a set of parameters, such as the mean and standard deviation, that can be estimated from the data. Probabilistic statistics is widely used in fields such as biology, physics, and economics, where the data are often assumed to be generated by a deterministic process with some random noise present. In contrast, Bayesian statistics takes a more flexible approach to modelling uncertainty, allowing for uncertainty in both the parameters of the model and the underlying distribution itself. Bayesian methods are useful when dealing with complex and high-dimensional data sets, with lots of unknowns and assumptions, and have become increasingly popular in fields such as ecology and machine learningin recent years.\n\n\n\n2 Hypothesis testing\nHypothesis testing is a fundamental aspect of the scientific method and is used to evaluate the validity of scientific hypotheses. A hypothesis is a proposed explanation for a phenomenon or observation that can be tested through experimentation or observation. To test a hypothesis, we design experiments or collect data, which we analyse using inferential statistical methods to determine whether the data support or refute the hypothesis.\nTwo competing hypotheses about the data are set up at the oonset of hypothesis testing: a null hypothesis (H0) and an alternative hypothesis (Ha). The null hypothesis typically represents the status quo or a default assumption (a statement of no difference), while the alternative hypothesis represents a new or alternative explanation for the data.\nThe goal is to make objective and evidence-based conclusions about the validity of the hypothesis, and to determine whether it can be accepted or rejected based on the available evidence. Hypothesis testing is a critical tool for advancing scientific knowledge and understanding, as it allows us to identify the most promising hypotheses and develop more accurate models of the natural world. Effectively, scientific progress can only be made if the null hypothesis is rejected and the alternative hypothesis accepted.\n\n\n\n\n\n\nHypotheses and theories\n\n\n\nHypotheses and theories are both important components of the scientific process, but they serve different functions and represent distinct levels of understanding.\nA hypothesis is a tentative explanation or proposition for a specific phenomenon, often based on observations and grounded in existing knowledge. It is a testable statement that can be either supported or refuted through further observation, experimentation, and hypothesis testing through the application of inferential statistics. Hypotheses are typically formulated at the beginning of a research study. They guide the design of experiments and the collection of data. Hypotheses help us make predictions and answer specific questions about the phenomena under investigation. If a hypothesis is repeatedly tested and confirmed through various experiments, it may gain credibility and contribute to the development of a theory.\nA theory is a well-substantiated explanation for a broad range of observed phenomena that has been consistently supported by a large body of evidence. Theories are more comprehensive and mature than hypotheses, as they integrate and generalise multiple related hypotheses and empirical findings to explain complex phenomena. They are built upon a solid foundation of tested hypotheses and provide a coherent framework that enables us to make accurate predictions, generate new hypotheses, and further advance our understanding of the natural world.\n\n\nAt the heart of many basic scientific inquiries, and hence hypotheses, is the simple question “Is A different from B?” The scientific notation for this question is:\n\n\nH0: Group A is not different from Group B\n\nHa: Group A is different from Group B\n\nMore formally, one would say:\n\n\n\\(H_{0}: \\bar{A} = \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} \\neq \\bar{B}\\)\n\n\n\\(H_{0}: \\bar{A} \\leq \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} &gt; \\bar{B}\\)\n\n\n\\(H_{0}: \\bar{A} \\geq \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} &lt; \\bar{B}\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nHypothesis 1 is a two-sided t-test and hypotheses 2 and 3 are one-sided tests. This will make sense once you have studied the material in Chapter 7 about t-tests.\n\n\n\n3 Probabilities\nThe p-value (the significance level, \\(\\alpha\\)) is the probability of finding the observed (or measured) outcome to be more extreme (i.e. very different) than that suggested by the null hypothesis (\\(H_{0}\\)). Typically, biologists set the p-value at \\(\\alpha \\leq 0.05\\)—in other words, the measured outcome of our experiment only has a 1 in 20 chance of being the same as that of the reference (or control) group. So, when the p-value is \\(\\leq\\) 0.05, for example, we say that there is a very good probability that our experimental treatment resulted in an outcome that is very different (we say statistically significantly different) from the measurement obtained from the group to which the treatment had not been applied—in this case we do not accept \\(H_{0}\\) and by necessity \\(H_{a}\\) becomes true.\nThe choice of p-value at which we reject \\(H_{0}\\) is arbitrary and exists by convention only. Traditionally, the 5% cut-off (i.e. less than 1 in 20 chance of being wrong or \\(p \\leq 0.05\\)) is used in biology, but sometimes the threshold is set at 1% or 0.1% (0.01 or 0.001, respectively), particularly in the medical sciences where avoiding false positives or negatives could be a public health concern. However, more and more biologists shy away from the p-value as they argue that it can give a false sense of security.\n\n\nStatistical tests indicate a statistically significant outcome (the \\(p \\leq 0.05\\)) and we accept the \\(H_{a}\\), or it does not (\\(p \\gt 0.05\\)) and we do not reject the \\(H_{0}\\). There’s no “almost significant”. It is, or it is not. \nWe generally refer to \\(p \\leq 0.05\\) as being statistically significant. Statistically highly significant is seen at as \\(p \\leq 0.001\\). In the first instance there is a less than 1 in 20 chance that our experimental sample is not different from the reference group, and in the second instance there is a less than 1 in a 1000 chance tat they are the same. This says something about the acceptable error rates: there is a better chance the \\(H_{0}\\) may in fact be falsely accepted or rejected when the p-value is set at 0.05 than at 0.001.\n\n\n\n\n\n\nType I and Type II errors\n\n\n\nA Type I error is the false rejection of the \\(H_{0}\\) hypothesis (i.e. in reality we should not be rejecting it, but the p-value suggests that we must). A Type II error, on the other hand, is the false acceptance of the \\(H_{0}\\) hypothesis (i.e. the p-value suggests we should not reject the \\(H_{0}\\), but in fact we must). When a statistical test results in a p-value of, say, \\(p \\leq 0.05\\) we would conclude that our experimental sample is statistically different from the reference group, but probabilistically there is a 1 in 20 change that this outcome is incorrect (i.e. the difference was arrived at by random chance only).\nThe choice of p-value threshold depends on several factors, including the nature of the data, the research question, and the desired level of statistical significance. In medical sciences, where the consequences of false positive or false negative results can have significant implications for patient health, a more stringent threshold is often used. A p-value of 0.001 is commonly used in medical research to minimise the risk of Type I errors (rejecting the null hypothesis when it is actually true) and to ensure a high level of statistical confidence in the results.\nIn biological sciences, the consequences of false positive or false negative results may be less severe, and a p-value of 0.05 is often considered an appropriate threshold for statistical significance. However, it is important to note that the choice of p-value threshold is ultimately subjective and should be based on a careful consideration of the research question, the nature of the data, and the potential consequences of false positive or false negative results.\n\n\nTo conclude, when \\(p \\gt 0.05\\) there is a lack of compelling evidence to suggest that our experiment has had an influential effect of the hypothesised outcome—even if a graphs hints at differences between groups. When \\(p \\leq 0.05\\), however, there is a good probability that the experiment (etc.) has had an effect, and that the effect is likely not due to random chance. In this case we have a statistically significant finding.\n\n4 Assumptions\nIrrespective of the kind of statistical test we wish to perform, we have to make a couple of important assumptions that are not guaranteed to be true. In fact, these assumptions are often violated because real data, especially biological data, are messy.\nThe issue of assumption is an important one, and one that we need to understand well. This is will be the purpose of Chapter 6, where we will learn about how to test the assumptions, and discover what to do when it does.\n\n5 Conclusion\nWe use inferential statistics to draw conclusions about a population based on a sample of data. By using probability theory and statistical inference, we can make inferences about the characteristics of a larger population with a certain level of confidence. We must always keep the assumptions behind inferential statistics in mind so that we can apply the right statistical test and answer our research question within the limits of what our data can tell us.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {5. {Statistical} Inference and Hypothesis Testing},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/basic_stats/05-inference.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 5. Statistical inference and hypothesis testing. https://tangledbank.netlify.app/BCB744/basic_stats/05-inference.html."
  },
  {
    "objectID": "BCB744/basic_stats/10-correlations.html",
    "href": "BCB744/basic_stats/10-correlations.html",
    "title": "10. Correlations",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nCorrelations\nPearson’s product moment correlation\nPaired correlations\nSpearman rank correlation\nKendal rank correlation\n\n\n\n\n\n\n\n\n\nCheatsheet\n\n\n\nFind here a Cheatsheet on statistical methods.\n\n\n\n\n\n\n\n\nSlides with additional information about correlations\n\n\n\n\nSlide deck 1\nSlide deck 2\n\n\n\n\n\n\n\n\n\nTasks to complete in this Chapter\n\n\n\n\nTask H 1\n\n\n\n\n1 At a glance\nCorrelation analysis is used to quantify the strength and direction of the linear relationship between two continuous variables. The expectations about the data needed for a correlation analysis are:\n\nContinuous variables Both variables should be measured on a continuous scale (e.g., height, depth, income). Note that we do not have dependent and independent variables as no dependency of one variable upon the other is implied.\nBivariate relationship Correlation analysis is used to assess the relationship between two variables at a time. If you are interested in the relationship between multiple variables, you may need to consider pairwise correlations, or other multivariate techniques such as multiple regression or canonical correlation.\nLinear relationship The relationship between the two variables should be linear. This can be visually assessed using scatter plots. If the relationship is not linear, you may need to consider non-linear correlation measures, such as Spearman’s \\(\\rho\\) correlation or Kendall’s \\(\\tau\\).\nNo outliers Outliers can have a strong influence on the correlation coefficient, potentially leading to misleading conclusions. It’s important to visually inspect the data using scatter plots and address any outliers before performing correlation analysis.\nNormality While not strictly required for correlation analysis, the assumption of bivariate normality can be important when making inferences about the population correlation coefficient. If the variables are not normally distributed or have a non-linear relationship, consider using non-parametric correlation measures like Spearman’s \\(\\rho\\) correlation or Kendall’s \\(\\tau\\).\nIndependence of observations The observations should be independent of each other. In the case of time series data or clustered data, this assumption may be violated, requiring specific techniques to account for the dependence (e.g., autocorrelation, cross-correlation).\nRandom sampling The data should be obtained through random sampling, ensuring that each observation has an equal chance of being included in the sample.\n\nKeep in mind that correlation does not imply causation; it only describes the association between variables without establishing a cause-and-effect relationship. When the intention is to model causation you’ll need to apply a regression.\n\n2 Introduction to correlation\nA correlation is performed when we want to investigate the potential association between two continuous quantitative variables, or between some ordinal variables. We assume that the association is linear, like in a linear regression, and that one variable increases or decreases by a constant amount for a corresponding unit increase or decrease in the other variable. This does not suggest that one variable explains the other—that is the purpose of regression, as seen in Chapter 9. Like all statistical tests, correlation requires a series of assumptions:\n\npair-wise data\nabsence of outliers\nlinearity\nnormality of distribution\nhomoscedasticity\nlevel (type) of measurement\ncontinuous data (Pearson \\(r\\))\nnon-parametric correlations (Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\))\n\n3 Pearson correlation\n\n\nPearson’s \\(r\\):\n\\[r_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} \\tag{1}\\]\nwhere \\(r_{xy}\\) is the Pearson correlation coefficient, \\(x_i\\) and \\(y_i\\) are the observed values of the two variables for each observation \\(i\\), \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means of the two variables, and \\(n\\) is the sample size.\nPearson’s \\(r\\) is a measure of the linear relationship between two variables. It assumes that the relationship between the variables is linear, and is calculated as the ratio of the covariance between the variables to the product of their standard deviations (Equation 1).\nThe degree of association is measured by a correlation coefficient, denoted by \\(r\\) (note, in a regression we use the \\(r^{2}\\), or \\(R^{2}\\)). The \\(r\\) statistic is a measure of linear association. The value for \\(r\\) varies from -1 to 1, with 0 indicating that there is absolutely no association, 1 showing a perfect positive association, and -1 a perfect inverse correlation.\nIn order to investigate correlations in biological data lets load the ecklonia dataset.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(corrplot)\nlibrary(kableExtra)\n\n# Load data\necklonia &lt;- read.csv(\"../../data/ecklonia.csv\")\n\nWe will also create a subsetted version of our data by removing all of the categorical variables. If we have a dataframe where each column represents pair-wise continuous/ordinal measurements with all of the other columns we may very quickly and easily perform a much wider range of correlation analyses.\n\necklonia_sub &lt;- ecklonia %&gt;%\n  select(-species, - site, - ID)\n\n# order the columns alphabetically\necklonia_sub &lt;- ecklonia_sub[,order(colnames(ecklonia_sub))]\n\nWhen the values we are comparing are continuous, we may use a Pearson test. This is the default and so requires little work on our end. The resulting statistic from this test is known as the Pearson correlation coefficient:\n\n# Perform correlation analysis on two specific variables\n# Note that we do not need the final two arguments in this function to be stated\n# as they are the defaut settings.\n# They are only shown here to illustrate that they exist.\ncor.test(x = ecklonia$stipe_length, ecklonia$frond_length,\n         use = \"everything\", method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  ecklonia$stipe_length and ecklonia$frond_length\nt = 4.2182, df = 24, p-value = 0.0003032\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3548169 0.8300525\nsample estimates:\n      cor \n0.6524911 \n\n\nAbove we have tested the correlation between the length of Ecklonia maxima stipes and the length of their fronds. A perfect positive (negative) relationship would produce a value of 1 (-1), whereas no relationship would produce a value of 0. The result above, cor = 0.65 is relatively strong.\nAs is the case with everything else we have learned thus far, a good visualisation can go a long way to help us understand what the statistics are doing. Below we visualise the stipe length to frond length relationship.\n\n# Calculate Pearson r beforehand for plotting\nr_print &lt;- paste0(\"r = \",\n                  round(cor(x = ecklonia$stipe_length, ecklonia$frond_length),2))\n\n# Then create a single panel showing one correlation\nggplot(data = ecklonia, aes(x = stipe_length, y = frond_length)) +\n  geom_smooth(method = \"lm\", colour = \"blue3\", se = FALSE, size = 1.2) +\n  geom_point(size = 3, col = \"red3\", shape = 16) +\n  geom_label(x = 300, y = 240, label = r_print) +\n  labs(x = \"Stipe length (cm)\", y = \"Frond length (cm)\") +\n  theme_pubclean()\n\n\n\n\nFigure 1: Scatterplot showing relationship between Ecklonia maxima stipe length (cm) and frond length (cm). The correlation coefficient (Pearson r) is shown in the top left corner. Note that the best fit blue line was produced by a linear model and that it is not responsible for generating the correlation coefficient; rather it is included to help visually demonstrate the strength of the relationship.\n\n\nJust by eye-balling this scatterplot it should be clear that these data tend to increase at a roughly similar rate. Our Pearson r value is an indication of what that is.\nShould our dataset contain multiple variables, as ecklonia does, we may investigate all of the correlations simultaneously. Remember that in order to do so we want to ensure that we may perform the same test on each of our paired variables. In this case we will use ecklonia_sub as we know that it contains only continuous data and so are appropriate for use with a Pearson test. By default R will use all of the data we give it and perform a Pearson test so we do not need to specify any further arguments. Note however that this will only output the correlation coefficients, and does not produce a full test of each correlation. This will however be useful for us to have just now.\n\necklonia_pearson &lt;- round(cor(ecklonia_sub), 2)\necklonia_pearson |&gt; \n  kbl(caption = \"A pairwise matrix of the *Ecklonia* dataset.\") %&gt;%\n  kable_classic(full_width = FALSE)\n\n\nA pairwise matrix of the *Ecklonia* dataset.\n\n\ndigits\nepiphyte_length\nfrond_length\nfrond_mass\nprimary_blade_length\nprimary_blade_width\nstipe_diameter\nstipe_length\nstipe_mass\n\n\n\ndigits\n1.00\n0.05\n0.36\n0.28\n0.10\n0.14\n0.24\n0.24\n0.07\n\n\nepiphyte_length\n0.05\n1.00\n0.61\n0.44\n0.26\n0.41\n0.54\n0.61\n0.51\n\n\nfrond_length\n0.36\n0.61\n1.00\n0.57\n-0.02\n0.28\n0.39\n0.65\n0.39\n\n\nfrond_mass\n0.28\n0.44\n0.57\n1.00\n0.15\n0.36\n0.51\n0.51\n0.47\n\n\nprimary_blade_length\n0.10\n0.26\n-0.02\n0.15\n1.00\n0.34\n0.32\n0.13\n0.16\n\n\nprimary_blade_width\n0.14\n0.41\n0.28\n0.36\n0.34\n1.00\n0.83\n0.34\n0.83\n\n\nstipe_diameter\n0.24\n0.54\n0.39\n0.51\n0.32\n0.83\n1.00\n0.59\n0.82\n\n\nstipe_length\n0.24\n0.61\n0.65\n0.51\n0.13\n0.34\n0.59\n1.00\n0.58\n\n\nstipe_mass\n0.07\n0.51\n0.39\n0.47\n0.16\n0.83\n0.82\n0.58\n1.00\n\n\n\n\n\nHow would we visualise this matrix of correlations? It is relatively straightforward to quickly plot correlation results for all of our variables in one go. In order to show which variables associate most with which other variables all at once, without creating chaos, we will create what is known as a pairwise correlation plot. This visualisation uses a range of colours, usually blue to red, to demonstrate where more of something is. In this case, we use it to show where more correlation is occurring between morphometric properties of the kelp Ecklonia maxima.\n\n# extract the lower triangle and plot\necklonia_pearson[upper.tri(ecklonia_pearson)] &lt;- NA\ncorrplot(ecklonia_pearson, method = \"circle\", na.label.col = \"white\")\n\n\n\n\nFigure 2: Plot of pairwise correlations showing the strength of all correlations between all variables as a scale from red (negative) to blue (positive).\n\n\nLet’s do it is ggplot2 (Figure 3). Here I use the geom_tile() function. However, before I can use the data in ggplot2, I need to create a long dataframe from the correlation matrix, and I can do this with the pivot_longer() function. There are several other methods for plotting pairwise correlations available—please feel free to scratch around the internet for options you like. This graph is called a heatmap, which is not dissimilar to the heatmaps and Hovmöller diagrams created in Chapter 2.\nPairwise correlations are useful for identifying patterns and relationships between variables that may be hidden in the overall correlation structure of the dataset. This is particularly useful in a large dataset with many variables, where this type of analysis—especially when coupled with a suitable visualisation—can help identify subsets of variables that are strongly related to each other, which can then point the path to further analysis or modelling.\n\necklonia_pearson |&gt; \n  as.data.frame() |&gt; \n  mutate(x = rownames(ecklonia_pearson)) |&gt; \n  pivot_longer(cols = stipe_length:epiphyte_length,\n               names_to = \"y\",\n               values_to = \"r\") |&gt; \n  filter(x != \"digits\") |&gt; \n  ggplot(aes(x, y, fill = r)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1),\n                          na.value = \"grey95\",, space = \"Lab\",\n                         name = \"r\") +\n    xlab(NULL) + ylab(NULL) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1,\n                                     hjust = 1)) +\n    coord_fixed() \n\n\n\n\nFigure 3: Pairwise of the Ecklonia dataset correlations created in ggplot2.\n\n\n\n4 Spearman rank correlation\nSpearman correlation is used to measure the strength and direction of the relationship between two variables, based on their rank order. Unlike Pearson correlation, which assumes that the relationship between two variables is linear, Spearman correlation can be used to measure the strength of any monotonic relationship, whether it is linear or not. Additionally, this correlation is useful even when the data are not normally distributed, or contain outliers.\nTo calculate the Spearman correlation coefficient, \\(\\rho\\), the values of both variables are first ranked from lowest to highest and each value is assigned a numerical rank based on its position in the ordered list. Then, the difference between the ranks of the two variables is calculated for each observation, and the squared differences are summed across all observations. The Spearman correlation coefficient is then calculated as the ratio of the sum of the squared differences to the total number of observations, adjusted for ties (Equation 2). Like the Pearson correlation coefficient, \\(\\rho\\) can also range from -1 to +1.\n\n\nSpearman’s \\(\\rho\\): \\[\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2-1)} \\tag{2}\\]\nwhere \\(\\rho\\) is the Spearman correlation, \\(d_i\\) is the difference between the ranks of the two variables for the \\(i^{th}\\) observation, and \\(n\\) is the sample size. The factor of 6 in the equation is a normalisation constant that adjusts the range of possible values of the correlation coefficient to be between -1 and +1.\nIn the code below we will add a column of ordinal data to our ecklonia data to so that we may look at this test.\n\n# Create ordinal data\necklonia$length &lt;- as.numeric(cut((ecklonia$stipe_length + ecklonia$frond_length), breaks = 3))\n\n# What does this new column look like?\nhead(select(ecklonia, c(species, site, stipe_length, frond_length, length)), 10)\n\n   species           site stipe_length frond_length length\n1   maxima Boulders Beach          456          116      1\n2   maxima Boulders Beach          477          141      2\n3   maxima Boulders Beach          427          144      1\n4   maxima Boulders Beach          347          127      1\n5   maxima Boulders Beach          470          160      2\n6   maxima Boulders Beach          478          181      2\n7   maxima Boulders Beach          472          174      2\n8   maxima Boulders Beach          459           95      1\n9   maxima Boulders Beach          397           87      1\n10  maxima Boulders Beach          541          127      2\n\n\nNow let us correlate the new length variable with any one of the other variables:\n\ncor.test(ecklonia$length, ecklonia$digits, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  ecklonia$length and ecklonia$digits\nS = 1930, p-value = 0.08906\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.3401765 \n\n\n\n5 Kendall rank correlation\nKendall’s correlation, also known as Kendall’s \\(\\tau\\), is a non-parametric correlation method for assessing the strength and direction of the relationship between two variables. It is similar to Spearman’s rank correlation, but it is calculated differently.\nKendall’s \\(\\tau\\) is calculated based on the number of concordant and discordant pairs of observations between the two variables being correlated. A concordant pair is one in which the values of both variables have the same order, meaning that if the value of one variable is higher than the other for one observation, it is also higher for the other observation. A discordant pair is one in which the values of the two variables have different order, meaning that if one variable is higher than the other for one observation, it is lower for the other observation.\n\\(\\tau\\) is calculated as the difference between the number of concordant and discordant pairs of observations, divided by the total number of possible pairs (Equation 3). As in Pearson’s and Spearman’s correlations, the result also ranges from -1 and +1.\n\n\nKendal’s \\(\\tau\\): \\[\\tau = \\frac{n_c - n_d}{\\binom{n}{2}} \\tag{3}\\]\nwhere \\(\\tau\\) is Kendall’s \\(\\tau\\) correlation coefficient, \\(n\\) is the sample size, \\(n_c\\) is the number of concordant pairs of observations, \\(n_d\\) is the number of discordant pairs of observations, and \\(\\binom{n}{2}\\) is the number of possible pairs of observations in the sample.\nKendall’s \\(\\tau\\) is a useful correlation statistic for non-parametric data, such as ordinal or categorical data, and is robust to outliers and non-normal distributions.\nLet’s look at the normality of our ecklonia variables and pull out those that are not normal in order to see how the results of this test may differ from our Pearson tests.\n\necklonia_norm &lt;- ecklonia_sub %&gt;%\n  gather(key = \"variable\") %&gt;%\n  group_by(variable) %&gt;%\n  summarise(variable_norm = as.numeric(shapiro.test(value)[2]))\necklonia_norm\n\n# A tibble: 9 × 2\n  variable             variable_norm\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 digits                     0.0671 \n2 epiphyte_length            0.626  \n3 frond_length               0.202  \n4 frond_mass                 0.277  \n5 primary_blade_length       0.00393\n6 primary_blade_width        0.314  \n7 stipe_diameter             0.170  \n8 stipe_length               0.213  \n9 stipe_mass                 0.817  \n\n\nFrom this analysis we may see that the values for primary blade length are not normally distributed. In order to make up for this violation of our assumption of normality we may use the Kendall test.\n\ncor.test(ecklonia$primary_blade_length, ecklonia$primary_blade_width, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  ecklonia$primary_blade_length and ecklonia$primary_blade_width\nz = 2.3601, p-value = 0.01827\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.3426171 \n\n\nHere the correlation coefficient is called Kendall’s \\(\\tau\\) but it is interpreted as we would Pearson’s.\n\n\n\n\n\n\nTask H\n\n\n\n\nFind your own two datasets and do a full correlation analysis on it. Briefly describe the data and the reason for their existence. Start with setting the appropriate hypotheses. Follow with an EDA, make some exploratory figures, fit the correlation, make figures with the fitted correlation line, provide diagnostic plots to test assumptions, and present the output in a Results section suitable for publication.\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {10. {Correlations}},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/basic_stats/10-correlations.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 10. Correlations. https://tangledbank.netlify.app/BCB744/basic_stats/10-correlations.html."
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html",
    "title": "2. Exploring with summaries and descriptions",
    "section": "",
    "text": "“I think it is much more interesting to live with uncertainty than to live with answers that might be wrong.”\n—- Richard Feynman"
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-central-tendency",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-central-tendency",
    "title": "2. Exploring with summaries and descriptions",
    "section": "\n4.1 Measures of central tendency",
    "text": "4.1 Measures of central tendency\n\n\nStatistic\nFunction\nPackage\n\n\n\nMean\nmean()\nbase\n\n\nMedian\nmedian()\nbase\n\n\nMode\n\n\n\n\nSkewness\nskewness()\ne1071\n\n\nKurtosis\nkurtosis()\ne1071\n\n\n\nCentral tendency is a fundamental concept in statistics, referring to the central or typical value of a dataset that best represents its overall distribution. The measures of central tendency are also sometimes called ‘location’ statistics. As a key component of descriptive statistics, central tendency is essential for summarising and simplifying complex data, providing a single representative value that captures the data’s general behaviour and which might tell us something about the bigger population from which the random samples were drawn.\nA thorough assessment of the central tendency in EDA serves several purposes:\n\nSummary of data Measures of central tendency, such as the mean, median, and mode, provide a single value that represents the center or typical value of a dataset. They help summarise the data, enabling us to gain an early insight into the dataset’s general properties and behaviour.\nComparing groups or distributions Central tendency measures allow us to compare different datasets or groups within a dataset. They can help identify differences or similarities in the data, which can be useful for hypothesis testing and inferential statistics.\nData transformation decisions Understanding the central tendency of our data can inform decisions on whether to apply transformations to the data to better meet the assumptions of certain statistical tests or improve the interpretability of the results.\nIdentifying potential issues Examining the central tendency can help reveal issues with the data, such as outliers or data entry errors, that could influence the results of inferential statistics. Outliers, for example, can significantly impact the mean, making the median a more robust measure of central tendency in such cases.\n\nUnderstanding the central tendency informs the choice of inferential statistics in the following ways:\n\nAssumptions of statistical tests Many inferential statistical tests have assumptions about the distribution of the data, such as normality, linearity, or homoscedasticity. Analysing the central tendency helps assess whether these assumptions are met, thereby informing the choice of an appropriate test.\nChoice of statistical models The central tendency can influence the choice of statistical models or the selection of dependent and independent variables in regression analyses, as certain models or relationships may be more appropriate depending on the data’s distribution and central tendencies.\nChoice of estimators Central tendency measures can influence our choice of estimators for further inferential statistics, depending on the data’s distribution and presence of outliers (e.g., mean vs median).\n\nBefore I discuss each central tendency statistic, I’ll generate some random data to represent normal and skewed distributions. I’ll use these data in my discussions, below.\n\n# Generate random data from a normal distribution\nn &lt;- 5000 # Number of data points\nmean &lt;- 0\nsd &lt;- 1\nnormal_data &lt;- rnorm(n, mean, sd)\n\n# Generate random data from a slightly right-skewed beta distribution\nalpha &lt;- 2\nbeta &lt;- 5\nright_skewed_data &lt;- rbeta(n, alpha, beta)\n\n# Generate random data from a slightly left-skewed beta distribution\nalpha &lt;- 5\nbeta &lt;- 2\nleft_skewed_data &lt;- rbeta(n, alpha, beta)\n\n# Generate random data with a bimodal distribution\nmean1 &lt;- 0\nmean2 &lt;- 10\nsd1 &lt;- 3\nsd2 &lt;- 4\n\n# Generate data from two normal distributions\ndata1 &lt;- rnorm(n, mean1, sd1)\ndata2 &lt;- rnorm(n, mean2, sd2)\n\n# Combine the data from both distributions to create a bimodal distribution\nbimodal_data &lt;- c(data1, data2)\n\n\n# Set up a three-panel plot layout\npar(mfrow = c(2, 2))\n\n# Plot the histogram of the normal distribution\nhist(normal_data, main = \"Normal Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightblue\", border = \"black\")\n\n# Plot the histogram of the right-skewed distribution\nhist(right_skewed_data, main = \"Right-Skewed Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightgreen\", border = \"black\")\n\n# Plot the histogram of the left-skewed distribution\nhist(left_skewed_data, main = \"Left-Skewed Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightcoral\", border = \"black\")\n\n# Plot the histogram of the left-skewed distribution\nhist(bimodal_data, main = \"Bimodal Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"khaki2\", border = \"black\")\n\n\n\n# Reset the plot layout to default\npar(mfrow = c(1, 1))\n\nA three-panel series of plots with histograms for the previously generated normal, right-skewed, and left-skewed distributions is indicated in ?@fig-histos.\n\n4.1.1 The sample mean\nThe mean is the arithmetic average of the data, and it is calculated by summing all the data and dividing it by the sample size, n (Equation 1).\n\n\nThe mean, \\(\\bar{x}\\), is calculated thus: \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} \\tag{1}\\] where \\(x_{1} + x_{2} + \\cdots + x_{n}\\) are the observations and \\(n\\) is the number of observations in the sample.\nWe can calculate the mean of a sample using the … wait for it … mean() function:\n\nround(mean(normal_data), 3)\n\n[1] 0.021\n\n\n\n\n\n\n\n\nTask B\n\n\n\n\nHow would you manually calculate the mean value for the normal_data? Do it now!\n\n\n\nThe mean is quite sensitive to the presence of outliers or extreme values in the data, and it is advised that its use be reserved for normally distributed data from which the extremes/outliers have been removed. When extreme values are indeed part of our data and not simply ‘noise,’ then we have to resort to a different measure of central tendency: the median.\n\n4.1.2 The median\nThe median indicates the center value in our dataset. The simplest way to explain what is is is to describe how it is determined. It can be calculated by ‘hand’ (if you have a small enough amount of data) by arranging all the numbers in sequence from low to high, and then finding the middle value. If there are five numbers, say 5, 2, 6, 13, 1, then you would arrange them from low to high, i.e. 1, 2, 5, 6, 13. The middle number is 5. This is the median. But there is no middle if we have an even number of values. What now? Take this example sequence of six integers (they may also be floating point numbers), which has already been ordered for your pleasure: 1, 2, 5, 6, 9, 13. Find the middle two numbers (i.e. 5, 6) and take the mean. It is 5.5. That is the median.\nThe median is therefore the value that separates the lower half of the sample data from the upper half. In normally distributed continuous data the median is equal to the mean. Comparable concepts to the median are the 1st and 3rd quartiles, which, respectively, separate the first quarter of the data from the last quarter—see the later in the section on ‘Measures of variance and dispersal’ in this Chapter. The advantage of the median over the mean is that it is unaffected by extreme values or outliers. The median is also used to provide a robust description of non-parametric data (see Chapter 4 for a discussion on normal data and other data distributions).\nWhat is the median of the normal_data dataset? We use the median() function for this:\n\nround(median(normal_data), 3)\n\n[1] 0.033\n\n\nIt is easier to see what the median is by looking at a much smaller dataset. Let’s take 11 random data points:\n\nsmall_normal_data &lt;- round(rnorm(11, 13, 3), 1)\nsort(small_normal_data)\n\n [1]  8.7  9.5 10.8 13.1 13.4 13.8 14.0 14.6 14.9 15.9 16.1\n\nmedian(small_normal_data)\n\n[1] 13.8\n\n\nBy using both the mean and median, we can gain a more comprehensive understanding of the data’s central tendency and underlying distribution.\n\n\n\n\n\n\nWhat is the relationship between the median and quantiles?\n\n\n\nThe relation between the median and quantiles lies in their roles as measures that describe the relative position of data points within a dataset. Quantiles are values that partition a dataset into equal intervals, with each interval containing the same proportion of the data. The most common types of quantiles are quartiles, quintiles, deciles, and percentiles.\nThe median is a special case of a quantile, specifically the 50th percentile or the second quartile (Q2). It divides the dataset into two equal halves, with 50% of the data points falling below the median and 50% of the data points falling above the median. In this sense, the median is a central quantile that represents the middle value of the dataset.\nBoth the median and quantiles help describe the distribution and spread of a dataset, with the median providing information about the center and other quantiles (such as quartiles) offering insights into the overall shape, skewness, and dispersion of the data.\n\n\n\n4.1.3 The mode\nThe mode is a measure that represents the value or values that occur most frequently in a dataset. Unlike the mean and median, the mode can be used with both numerical and categorical data, making it quite versatile. For a dataset with a single value that appears most often, the distribution is considered unimodal. However, datasets can also be bimodal (having two modes) or multimodal (having multiple modes) when there are multiple values that occur with the same highest frequency.\nWhile the mode may not always be a good representative of the dataset’s center, especially in the presence of extreme values or skewed distributions, it can still offer valuable information about the data’s characteristics when used alongside the other measures of central tendency.\nThere is no built-in function to calculate the mode of a numeric vector, but you can make one if you need it. There are some examples on the internet that you will be able to adapt to your needs, but my cursory evaluation of them does not suggest they are particularly useful. The easiest way to see the data’s mode(s) is to examine a histogram of your data. All the data we have explored above are examples of unimodal distributions, but a bimodal distribution can also be seen in ?@fig-histos.\n\n4.1.4 Skewness\nSkewness is a measure of symmetry (or asymmetry) of the data distribution, and it is best understood by understanding the location of the median relative to the mean. A distribution with a skewness of zero is considered symmetric, with both tails extending equally on either side of the mean. Here, the mean will be the same as the median. A negative skewness indicates that the mean of the data is less than their median—the data distribution is left-skewed; that is, there is a longer or heavier tail to the left of the mean. A positive skewness results from data that have a mean that is larger than their median; these data have a right-skewed distribution; so there will be a longer or heavier tail to the right of the mean. Base R does not have a built-in skewness function, but we can use the one included with the e1071 package:\n\nlibrary(e1071)\n# Positive skewness\nskewness(right_skewed_data)\n\n[1] 0.5296222\n\n# Is the mean larger than the median?\nmean(right_skewed_data) &gt; median(right_skewed_data)\n\n[1] TRUE\n\n# Negative skewness\nskewness(left_skewed_data)\n\n[1] -0.6348656\n\n# Is the mean less than the median?\nmean(left_skewed_data) &lt; median(left_skewed_data)\n\n[1] TRUE\n\n\n\n4.1.5 Kurtosis\nKurtosis describes the tail shape of the data’s distribution. Kurtosis is effectively a measure of the ‘tailedness’ or the concentration of data in the tails of a distribution, relative to a normal distribution. A normal distribution has zero kurtosis (or close to) and thus the standard tail shape (mesokurtic). Negative kurtosis indicates data with a thin-tailed (platykurtic) distribution. Positive kurtosis indicates a fat-tailed distribution (leptokurtic).\nSimilarly as to skewness, we use the e1071 package for a kurtosis function. All the output shown below suggests a tendency towards thin-tailedness, but it is subtle.\n\nkurtosis(normal_data)\n\n[1] -0.06825458\n\nkurtosis(right_skewed_data)\n\n[1] -0.1542938\n\nkurtosis(left_skewed_data)\n\n[1] -0.0001723586\n\n\nI have seldom used the concepts of the skewness or kurtosis in any EDA, but it is worth being aware of them. The overall purpose of examining data using the range of central tendency statistics is to get an idea of whether our data are normally distributed—a normal distribution is a key requirement for all parametric inferential statistics. See Chapter 4 for a discourse of data distributions. These central tendency statistics will serve you well as a first glance, but formal tests for normality do exist and I encourage their use before embarking on the rest of the journey. We will explore these formal tests in Chapter 7.\n\n\n\n\n\n\nTask B\n\n\n\n\nFind the faithful dataset and describe both variables in terms of their measures of central tendency. Include graphs in support of your answers (use ggplot()), and conclude with a brief statement about the data distribution."
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-variance-or-dispersion-around-the-center",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-variance-or-dispersion-around-the-center",
    "title": "2. Exploring with summaries and descriptions",
    "section": "\n4.2 Measures of variance or dispersion around the center",
    "text": "4.2 Measures of variance or dispersion around the center\n\n\nStatistic\nFunction\n\n\n\nVariance\nvar()\n\n\nStandard deviation\nsd()\n\n\nMinimum\nmin()\n\n\nMaximum\nmax()\n\n\nRange\nrange()\n\n\nQuantile\nquantile()\n\n\nCovariance\ncov()\n\n\nCorrelation\ncor()\n\n\n\nA good understanding of variability, or variation around the central point, is crucial in EDA for several reasons:\n\nSignal vs noise Variability helps distinguish between the signal (true underlying pattern) and noise (random fluctuations that might arise from stochastic processes, measurement or experimental error, or other unaccounted for influences) in the data. High variability can make it difficult to detect meaningful patterns or relationships in the data, while low variability may indicate a strong underlying pattern.\nPrecision and reliability Variability is related to the precision and reliability of measurements. Smaller variability indicates more consistent and precise measurements, whereas larger variability suggests inconsistency and potential issues with the data collection process.\nComparing groups Understanding variability is essential when comparing different groups or datasets. Even if two groups have similar central tendencies, their variability may differ significantly, leading to different interpretations of the data.\nAssumptions of statistical tests Many inferential statistical tests have assumptions about the variability of the data, such as homoscedasticity (equal variances across groups) or independence of observations. Assessing variability helps determine whether these assumptions are met and informs the choice of appropriate tests.\nEffect sizes and statistical power Variability plays a role in determining the effect size (magnitude of the difference between groups or strength of relationships) and the statistical power (ability to detect a true effect) of a study. High variability can make it harder to detect significant effects, requiring larger sample sizes to achieve adequate statistical power.\n\nUnderstanding variability informs the choice of inferential statistics:\n\nParametric vs non-parametric tests If the data exhibits normality and homoscedasticity, parametric tests may be appropriate (see Chapter 7). However, if the data have high variability or violates the assumptions of parametric tests, non-parametric alternatives may be more suitable.\nChoice of estimators Variability can influence the choice of estimators (e.g., mean vs median) for central tendency, depending on the data’s distribution and presence of outliers.\nSample size calculations Variability informs sample size calculations for inferential statistics. Higher variability typically requires larger sample sizes to achieve sufficient statistical power.\nModel selection Variability can influence the choice of statistical models, as certain models may better accommodate the variability in the data than others (e.g., linear vs non-linear models, fixed vs random effects).\n\nLet us now look at the estimators of variance.\n\n4.2.1 Variance and standard deviation\nVariance and standard deviation (SD) are examples of interval estimates. The sample variance, \\(S^{2}\\), may be calculated according to the following formula (Equation 2). If we cannot be bothered to calculate the variance and SD by hand, we may use the built-in functions var() and sd():\n\n\nThe sample variance: \\[S^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\tag{2}\\]\nThis reads, “the sum of the squared differences from the mean, divided by the sample size minus 1.” To get the standard deviation, \\(S\\), we take the square root of the variance, i.e. \\(S = \\sqrt{S^{2}}\\).\n\nvar(normal_data)\n\n[1] 1.018911\n\nsd(normal_data)\n\n[1] 1.009411\n\n\n\n\n\n\n\n\nTask B\n\n\n\n\nHow would you manually calculate the variance and SD for the normal_data? Do it now! Make sure your answer is the same as those reported above.\n\n\n\nThe interpretation of the concepts of mean and median are fairly straight forward and intuitive. Not so for the measures of variance. What does \\(S\\) represent? Firstly, the unit of measurement of \\(S\\) is the same as that of \\(\\bar{x}\\) (but the variance doesn’t share this characteristic). If temperature is measured in °C, then \\(S\\) also takes a unit of °C. Since \\(S\\) measures the dispersion around the mean, we write it as \\(\\bar{x} \\pm S\\) (note that often the mean and standard deviation are written with the letters mu and sigma, respectively; i.e. \\(\\mu \\pm \\sigma\\)). The smaller \\(S\\) the closer the sample data are to \\(\\bar{x}\\), and the larger the value is the further away they will spread out from \\(\\bar{x}\\). So, it tells us about the proportion of observations above and below \\(\\bar{x}\\). But what proportion? We invoke the the 68-95-99.7 rule: ~68% of the population (as represented by a random sample of \\(n\\) observations taken from the population) falls within 1\\(S\\) of \\(\\bar{x}\\) (i.e. ~34% below \\(\\bar{x}\\) and ~34% above \\(\\bar{x}\\)); ~95% of the population falls within 2\\(S\\); and ~99.7% falls within 3\\(S\\) (Figure 2).\n\n\n\n\nFigure 2: The idealised Normal distribution showing the proportion of data within 1, 2, and 3SD from the mean.\n\nLike the mean, \\(S\\) is affected by extreme values and outliers, so before we attach \\(S\\) as a summary statistic to describe some data, we need to ensure that the data are in fact normally distributed. We will talk about how to do this in Chapter 7, where we will go over the numerous ways to check the assumption of normality. When the data are found to be non-normal, we need to find appropriate ways to express the spread of the data. Enter the quartiles.\n\n4.2.2 The minimum, maximum, and range\nA description of the extremes (edges of the distribution) of the data can also be provided by the functions min(), max() and range(). These concepts are straight forward and do not require elaboration. They apply to data of any distribution, and not only to normal data. These statistics are often the first places you want to start when looking at the data for the first time. Note that range() does something different from min() and max():\n\nmin(normal_data)\n\n[1] -3.448997\n\nmax(normal_data)\n\n[1] 4.177403\n\nrange(normal_data)\n\n[1] -3.448997  4.177403\n\n\nrange() actually gives us the minimum and maximum values, and not the difference between them. To find the range value properly we must be a bit more clever:\n\nrange(normal_data)[2] - range(normal_data)[1]\n\n[1] 7.6264\n\n\n\n4.2.3 Quartiles and the interquartile range\nA more forgiving approach (forgiving of the extremes, often called ‘robust’) is to divide the distribution of ordered data into quarters and finding the points below which 25% (0.25, the first quartile; Q1), 50% (0.50, the median; Q2) and 75% (0.75, the third quartile; Q3) of the data are distributed. These are called quartiles (for ‘quarter;’ not to be confused with quantile, which is a more general concept that divides the distribution into any arbitrary proportion from 0 to 1).\nThe interquartile range (IQR) is a measure of statistical dispersion that provides information about the spread of the middle 50% of a dataset. It is calculated by subtracting the first quartile (25th percentile) from the third quartile (75th percentile).\nThe quartiles and IQR have several important uses:\n\nIdentifying central tendency As I have shown earlier, the second quartile, or median, is a measure of central tendency that is less sensitive to outliers than the mean. It offers a more robust estimate of the typical value in skewed distributions or those with extreme values.\nMeasure of variability The IQR is a robust measure of variability that is less sensitive to outliers and extreme values compared to other measures like the range or standard deviation. It gives a better understanding of the data spread in the middle part of the distribution.\nIdentifying outliers The IQR can be used to identify potential outliers in the data. A common method is to define outliers as data points falling below the first quartile minus 1.5 times the IQR or above the third quartile plus 1.5 times the IQR.\nDescribing skewed data For skewed distributions, the quartiles and IQR provide a better description of the data spread than the standard deviation, as it is not influenced by the skewness of the data. It can help reveal the degree of asymmetry in the distribution and the concentration of values in the middle portion.\nComparing distributions The IQR can be used to compare the variability or spread of two or more distributions. It provides a more robust comparison than the standard deviation or range when the distributions have outliers or are not symmetric, and the median reveals departures from normality.\nBox plots The quartiles and IQR are key components of box plots, which are graphical representations of the distribution of a dataset. Box plots display the median, first quartile, third quartile, and potential outliers, providing a visual representation of the data’s central tendency, spread, and potential outliers.\n\nIn R we use the quantile() function to provide the quartiles. Here is a demonstration:\n\n# Look at the normal data\nquantile(normal_data, p = 0.25)\n\n       25% \n-0.6546966 \n\nquantile(normal_data, p = 0.75)\n\n      75% \n0.7044242 \n\n# Look at skewed data\nquantile(left_skewed_data, p = 0.25)\n\n      25% \n0.6137101 \n\nquantile(left_skewed_data, p = 0.75)\n\n     75% \n0.840831 \n\n\n\n\n\n\n\n\nTask B\n\n\n\n\nWrite a few lines of code to demonstrate that the above results indeed conform to the formal definition for what quantiles are? I.e., show manually how you can determine that 25% of the observations indeed falls below -0.65 for the normal_data. Explain the rationale to your approach.\n\n\n\n\nWe calculate the interquartile range using the IQR() function:\n\nIQR(normal_data)\n\n[1] 1.359121"
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#summary",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#summary",
    "title": "2. Exploring with summaries and descriptions",
    "section": "\n5.1 summary()\n",
    "text": "5.1 summary()\n\nThe first method is a generic function that can be applied to a range of R data structures, and whose output depends on the class of the structure. It is called summary(). This function can be applied to the dataset itself (here a tibble) and also to the output of some models fitted to the data (later we will see, for instance, how it is applied to t-tests, ANOVAs, correlations, and regressions). When applied to a dataframe or tibble, we will be presented with something quite useful. Let us return to the Palmer penguin dataset, and you’ll see many familiar descriptive statistics:\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\n\n\n\n\n\nTask B\n\n\n\n\nExplain the output of summary() when applied to the penguins dataset."
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#psychdescribe",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#psychdescribe",
    "title": "2. Exploring with summaries and descriptions",
    "section": "\n5.2 psych::describe()\n",
    "text": "5.2 psych::describe()\n\nThe psych package has the describe() function, which provides a somewhat more verbose output containing many of the descriptive statistics I introduced earlier in this Chapter:\n\npsych::describe(penguins)\n\n                  vars   n    mean     sd  median trimmed    mad    min    max\nspecies*             1 344    1.92   0.89    2.00    1.90   1.48    1.0    3.0\nisland*              2 344    1.66   0.73    2.00    1.58   1.48    1.0    3.0\nbill_length_mm       3 342   43.92   5.46   44.45   43.91   7.04   32.1   59.6\nbill_depth_mm        4 342   17.15   1.97   17.30   17.17   2.22   13.1   21.5\nflipper_length_mm    5 342  200.92  14.06  197.00  200.34  16.31  172.0  231.0\nbody_mass_g          6 342 4201.75 801.95 4050.00 4154.01 889.56 2700.0 6300.0\nsex*                 7 333    1.50   0.50    2.00    1.51   0.00    1.0    2.0\nyear                 8 344 2008.03   0.82 2008.00 2008.04   1.48 2007.0 2009.0\n                   range  skew kurtosis    se\nspecies*             2.0  0.16    -1.73  0.05\nisland*              2.0  0.61    -0.91  0.04\nbill_length_mm      27.5  0.05    -0.89  0.30\nbill_depth_mm        8.4 -0.14    -0.92  0.11\nflipper_length_mm   59.0  0.34    -1.00  0.76\nbody_mass_g       3600.0  0.47    -0.74 43.36\nsex*                 1.0 -0.02    -2.01  0.03\nyear                 2.0 -0.05    -1.51  0.04"
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#skimrskim",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#skimrskim",
    "title": "2. Exploring with summaries and descriptions",
    "section": "\n5.3 skimr::skim()\n",
    "text": "5.3 skimr::skim()\n\nThe skimr package offers something similar, but different. The skim() function returns:\n\nlibrary(skimr)\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇"
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#jmvdescriptives",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#jmvdescriptives",
    "title": "2. Exploring with summaries and descriptions",
    "section": "\n5.4 jmv::descriptives()\n",
    "text": "5.4 jmv::descriptives()\n\nHere’s yet another view into our data, this time courtesy of the jmv package:\n\nlibrary(jmv)\ndescriptives(penguins, freq = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                                                                           \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                         species    island    bill_length_mm    bill_depth_mm    flipper_length_mm    body_mass_g    sex    year        \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   N                         344       344               342              342                  342            342    333          344   \n   Missing                     0         0                 2                2                    2              2     11            0   \n   Mean                                             43.92193         17.15117             200.9152       4201.754            2008.029   \n   Median                                           44.45000         17.30000             197.0000       4050.000            2008.000   \n   Standard deviation                               5.459584         1.974793             14.06171       801.9545           0.8183559   \n   Minimum                                          32.10000         13.10000                  172           2700                2007   \n   Maximum                                          59.60000         21.50000                  231           6300                2009   \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n FREQUENCIES\n\n Frequencies of species                                \n ───────────────────────────────────────────────────── \n   Levels       Counts    % of Total    Cumulative %   \n ───────────────────────────────────────────────────── \n   Adelie          152      44.18605        44.18605   \n   Chinstrap        68      19.76744        63.95349   \n   Gentoo          124      36.04651       100.00000   \n ───────────────────────────────────────────────────── \n\n\n Frequencies of island                                 \n ───────────────────────────────────────────────────── \n   Levels       Counts    % of Total    Cumulative %   \n ───────────────────────────────────────────────────── \n   Biscoe          168      48.83721        48.83721   \n   Dream           124      36.04651        84.88372   \n   Torgersen        52      15.11628       100.00000   \n ───────────────────────────────────────────────────── \n\n\n Frequencies of sex                                 \n ────────────────────────────────────────────────── \n   Levels    Counts    % of Total    Cumulative %   \n ────────────────────────────────────────────────── \n   female       165      49.54955        49.54955   \n   male         168      50.45045       100.00000   \n ────────────────────────────────────────────────── \n\n\n Frequencies of year                                \n ────────────────────────────────────────────────── \n   Levels    Counts    % of Total    Cumulative %   \n ────────────────────────────────────────────────── \n   2007         110      31.97674        31.97674   \n   2008         114      33.13953        65.11628   \n   2009         120      34.88372       100.00000   \n ──────────────────────────────────────────────────"
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#summarytoolsdfsummary",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#summarytoolsdfsummary",
    "title": "2. Exploring with summaries and descriptions",
    "section": "\n5.5 summarytools::dfSummary()\n",
    "text": "5.5 summarytools::dfSummary()\n\nAnd lastly, there is the summarytools package and the dfSummary() function within:\n\nlibrary(summarytools)\nprint(dfSummary(penguins, \n                varnumbers   = FALSE, \n                valid.col    = FALSE, \n                graph.magnif = 0.76),\n      method = 'render')\n\n\nData Frame Summary\npenguins\nDimensions: 344 x 8\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nMissing\n\n\n\nspecies [factor]\n\n\n1. Adelie\n\n\n2. Chinstrap\n\n\n3. Gentoo\n\n\n\n\n152\n(\n44.2%\n)\n\n\n68\n(\n19.8%\n)\n\n\n124\n(\n36.0%\n)\n\n\n\n0 (0.0%)\n\n\nisland [factor]\n\n\n1. Biscoe\n\n\n2. Dream\n\n\n3. Torgersen\n\n\n\n\n168\n(\n48.8%\n)\n\n\n124\n(\n36.0%\n)\n\n\n52\n(\n15.1%\n)\n\n\n\n0 (0.0%)\n\n\nbill_length_mm [numeric]\n\n\nMean (sd) : 43.9 (5.5)\n\n\nmin ≤ med ≤ max:\n\n\n32.1 ≤ 44.5 ≤ 59.6\n\n\nIQR (CV) : 9.3 (0.1)\n\n\n164 distinct values\n\n2 (0.6%)\n\n\nbill_depth_mm [numeric]\n\n\nMean (sd) : 17.2 (2)\n\n\nmin ≤ med ≤ max:\n\n\n13.1 ≤ 17.3 ≤ 21.5\n\n\nIQR (CV) : 3.1 (0.1)\n\n\n80 distinct values\n\n2 (0.6%)\n\n\nflipper_length_mm [integer]\n\n\nMean (sd) : 200.9 (14.1)\n\n\nmin ≤ med ≤ max:\n\n\n172 ≤ 197 ≤ 231\n\n\nIQR (CV) : 23 (0.1)\n\n\n55 distinct values\n\n2 (0.6%)\n\n\nbody_mass_g [integer]\n\n\nMean (sd) : 4201.8 (802)\n\n\nmin ≤ med ≤ max:\n\n\n2700 ≤ 4050 ≤ 6300\n\n\nIQR (CV) : 1200 (0.2)\n\n\n94 distinct values\n\n2 (0.6%)\n\n\nsex [factor]\n\n\n1. female\n\n\n2. male\n\n\n\n\n165\n(\n49.5%\n)\n\n\n168\n(\n50.5%\n)\n\n\n\n11 (3.2%)\n\n\nyear [integer]\n\n\nMean (sd) : 2008 (0.8)\n\n\nmin ≤ med ≤ max:\n\n\n2007 ≤ 2008 ≤ 2009\n\n\nIQR (CV) : 2 (0)\n\n\n\n\n2007\n:\n110\n(\n32.0%\n)\n\n\n2008\n:\n114\n(\n33.1%\n)\n\n\n2009\n:\n120\n(\n34.9%\n)\n\n\n\n0 (0.0%)\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.3.0)2023-05-06\n\n\n\nAs you can see, there are many option and you may use the one you least dislike. I’ll not be prescriptive or openly opinionated about it."
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html",
    "href": "BCB744/basic_stats/08-anova.html",
    "title": "8. Analysis of Variance (ANOVA)",
    "section": "",
    "text": "“He uses statistics as a drunken man uses lamp posts—for support rather than for illumination.”\n— Marissa Mayer"
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#single-factor",
    "href": "BCB744/basic_stats/08-anova.html#single-factor",
    "title": "8. Analysis of Variance (ANOVA)",
    "section": "\n5.1 Single factor",
    "text": "5.1 Single factor\nWe continue with the chicken data. The t-test showed that Diets 1 and 2 resulted in the same chicken mass at Day 21. What about the other two diets? Our H0 is that, at Day 21, \\(\\mu_{1}=\\mu_{2}=\\mu_{3}=\\mu_{4}\\). Is there a statistical difference between chickens fed these four diets, or do we retain the H0? The R function for an ANOVA is aov(). To look for significant differences between all four diets on the last day of sampling we use this one line of code:\n\nchicks.aov1 &lt;- aov(weight ~ Diet, data = filter(chicks, Time == 21))\nsummary(chicks.aov1)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  57164   19055   4.655 0.00686 **\nResiduals   41 167839    4094                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nWhat does the outcome say about the chicken masses? Which ones are different from each other?\nDevise a graphical display of this outcome.\n\n\n\nIf this seems too easy to be true, it’s because we aren’t quite done yet. You could use your graphical display to eyeball where the significant differences are, or we can turn to a more ‘precise’ approach. The next step one could take is to run a Tukey HSD test on the results of the ANOVA by wrapping tukeyHSD() around aov():\n\nTukeyHSD(chicks.aov1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ Diet, data = filter(chicks, Time == 21))\n\n$Diet\n         diff        lwr       upr     p adj\n2-1  36.95000  -32.11064 106.01064 0.4868095\n3-1  92.55000   23.48936 161.61064 0.0046959\n4-1  60.80556  -10.57710 132.18821 0.1192661\n3-2  55.60000  -21.01591 132.21591 0.2263918\n4-2  23.85556  -54.85981 102.57092 0.8486781\n4-3 -31.74444 -110.45981  46.97092 0.7036249\n\n\nThe output of tukeyHSD() shows us that pairwise comparisons of all of the groups we are comparing.\nWe may also produce a graphical summary (Figure 2):\n\nset.seed(666)\n\n## parametric t-test and box plot\nggbetweenstats(\n  data = filter(chicks, Time == 21),\n  x = Diet,\n  y = weight,\n  xlab = \"Diet\",\n  ylab = \"Chick mass (g)\",\n  plot.type = \"box\",\n  type = \"parametric\",\n  results.subtitle = TRUE,\n  pairwise.comparisons = TRUE,\n  p.adjust.method = \"bonferroni\",\n  pairwise.display = \"s\",\n  conf.level = 0.95,\n  title = \"ANOVA\",\n  ggtheme = ggthemes::theme_fivethirtyeight(),\n  package = \"basetheme\",\n  palette = \"ink\"\n)\n\n\n\n\nFigure 2: Box-and-whisker plot showing the differences in means between chicks reared to 21 days old and fed four diets.\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nLook at the help file for this function to better understand what the output means.\nHow does one interpret the results? What does this tell us about the effect that that different diets has on the chicken weights at Day 21?\nFigure out a way to plot the Tukey HSD outcomes.\nWhy does the ANOVA return a significant result, but the Tukey test shows that not all of the groups are significantly different from one another?"
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#multiple-factors",
    "href": "BCB744/basic_stats/08-anova.html#multiple-factors",
    "title": "8. Analysis of Variance (ANOVA)",
    "section": "\n5.2 Multiple factors",
    "text": "5.2 Multiple factors\nWhat if we have multiple grouping variables, and not just one? We would encounter this kind of situation in factorial designs. In the case of the chicken data, there is also time that seems to be having an effect.\n\n\n\n\n\n\nTask F\n\n\n\n\nHow is time having an effect?\nWhat hypotheses can we construct around time?\n\n\n\nLet us look at some variations around questions concerning time. We might ask, at a particular time step, are there differences amongst the effect due to diet on chicken mass? Let’s see when diets are starting the have an effect by examining the outcomes at times 0, 2, 10, and 21:\n\n# effect at time = 0\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 0)))\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nDiet         3   4.32   1.440   1.132  0.346\nResiduals   46  58.50   1.272               \n\n# effect at time = 2\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 2)))\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  158.4   52.81   4.781 0.00555 **\nResiduals   46  508.1   11.05                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# effect at time = 10\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 10)))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nDiet         3   8314    2771    6.46 0.000989 ***\nResiduals   45  19304     429                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# effect at time = 21\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 21)))\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  57164   19055   4.655 0.00686 **\nResiduals   41 167839    4094                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nWhat do you conclude from the above series of ANOVAs?\nWhat problem is associated with running multiple tests in the way that we have done here?\n\n\n\nOr we may ask, regardless of diet (i.e. disregarding the effect of diet by clumping all chickens together), is time having an effect?\n\nchicks.aov2 &lt;- aov(weight ~ as.factor(Time), data = filter(chicks, Time %in% c(0, 2, 10, 21)))\nsummary(chicks.aov2)\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)    \nas.factor(Time)   3 939259  313086   234.8 &lt;2e-16 ***\nResiduals       190 253352    1333                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nWhat do you conclude from the above ANOVA?\n\n\n\nOr, to save ourselves a lot of time and reduce the coding effort, we may simply run a two-way ANOVA and look at the effects of Diet and Time simultaneously. To specify the different factors we put them in our formula and separate them with a +:\n\nsummary(aov(weight ~ Diet + as.factor(Time), data = filter(chicks, Time %in% c(0, 21))))\n\n                Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nDiet             3  39595   13198   5.987 0.00091 ***\nas.factor(Time)  1 734353  734353 333.120 &lt; 2e-16 ***\nResiduals       90 198402    2204                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nWhat question are we asking with the above line of code? What is the answer? Also, why did we wrap Time in as.factor()?\n\n\n\nIt is also possible to look at what the interaction effect between grouping variables (i.e. in this case the effect of time on diet—does the effect of time depend on which diet we are looking at?), and not just within the individual grouping variables. To do this we replace the + in our formula with *:\n\nsummary(aov(weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(4, 21))))\n\n                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nDiet                  3  40914   13638   6.968 0.000298 ***\nas.factor(Time)       1 582221  582221 297.472  &lt; 2e-16 ***\nDiet:as.factor(Time)  3  25530    8510   4.348 0.006684 ** \nResiduals            86 168322    1957                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nHow do these results differ from the previous set?\n\n\n\nOne may also run a post-hoc Tukey test on these results the same as for a single factor ANOVA:\n\nTukeyHSD(aov(weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21))))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21)))\n\n$Diet\n         diff        lwr       upr     p adj\n2-1  36.18030  -9.301330  81.66194 0.1663037\n3-1  90.63030  45.148670 136.11194 0.0000075\n4-1  62.25253  15.223937 109.28111 0.0045092\n3-2  54.45000   3.696023 105.20398 0.0305957\n4-2  26.07222 -26.072532  78.21698 0.5586643\n4-3 -28.37778 -80.522532  23.76698 0.4863940\n\n$`as.factor(Time)`\n          diff       lwr      upr     p adj\n21-20 8.088223 -17.44017 33.61661 0.5303164\n\n$`Diet:as.factor(Time)`\n                diff        lwr        upr     p adj\n2:20-1:20  35.188235  -40.67378 111.050253 0.8347209\n3:20-1:20  88.488235   12.62622 164.350253 0.0111136\n4:20-1:20  63.477124  -14.99365 141.947897 0.2035951\n1:21-1:20   7.338235  -58.96573  73.642198 0.9999703\n2:21-1:20  44.288235  -31.57378 120.150253 0.6116081\n3:21-1:20  99.888235   24.02622 175.750253 0.0023872\n4:21-1:20  68.143791  -10.32698 146.614563 0.1371181\n3:20-2:20  53.300000  -31.82987 138.429869 0.5234263\n4:20-2:20  28.288889  -59.17374 115.751515 0.9723470\n1:21-2:20 -27.850000 -104.58503  48.885027 0.9486212\n2:21-2:20   9.100000  -76.02987  94.229869 0.9999766\n3:21-2:20  64.700000  -20.42987 149.829869 0.2732059\n4:21-2:20  32.955556  -54.50707 120.418182 0.9377007\n4:20-3:20 -25.011111 -112.47374  62.451515 0.9862822\n1:21-3:20 -81.150000 -157.88503  -4.414973 0.0305283\n2:21-3:20 -44.200000 -129.32987  40.929869 0.7402877\n3:21-3:20  11.400000  -73.72987  96.529869 0.9998919\n4:21-3:20 -20.344444 -107.80707  67.118182 0.9960548\n1:21-4:20 -56.138889 -135.45396  23.176184 0.3619622\n2:21-4:20 -19.188889 -106.65152  68.273738 0.9972631\n3:21-4:20  36.411111  -51.05152 123.873738 0.8984019\n4:21-4:20   4.666667  -85.06809  94.401428 0.9999998\n2:21-1:21  36.950000  -39.78503 113.685027 0.8067041\n3:21-1:21  92.550000   15.81497 169.285027 0.0075185\n4:21-1:21  60.805556  -18.50952 140.120628 0.2629945\n3:21-2:21  55.600000  -29.52987 140.729869 0.4679025\n4:21-2:21  23.855556  -63.60707 111.318182 0.9896157\n4:21-3:21 -31.744444 -119.20707  55.718182 0.9486128\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nYikes! That’s a massive amount of results. What does all of this mean, and why is it so verbose?\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nTo summarise t-tests, single-factor (1-way) and multifactor (2- or 3-way, etc.) ANOVAs:\n\nA t-test is applied to situations where one wants to compare the means of only two groups of a response variable within one categorical independent variable (we say a factor with two levels).\nA 1-way ANOVA also looks at the means of a response variable belonging to one categorical independent variable, but the categorical response variable has more than two levels in it.\nFollowing on from there, a 2-way ANOVA compares the means of response variables belonging to all the levels within two categorical independent variables (e.g. Factor 1 might have three levels, and Factor 2 five levels). In the simplest formulaton, it does so by looking at the main effects, which is the group differences between the three levels of Factor 1 and disregarding the contribution due to the group membership to Factor 2, and also the group differences amongst the levels of Factor 2 but disregarding the group membership of Factor 1. In addition to looking at the main effects, a 2-way ANOVA can also consider the interaction (or combined effect) of Factors 1 and 2 in influencing the means."
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#examples",
    "href": "BCB744/basic_stats/08-anova.html#examples",
    "title": "8. Analysis of Variance (ANOVA)",
    "section": "\n5.3 Examples",
    "text": "5.3 Examples\n\n5.3.1 Snakes!\nThese data could be analysed by a two-way ANOVA without replication, or a repeated measures ANOVA. Here I will analyse it by using a two-way ANOVA without replication.\nPlace and Abramson (2008) placed diamondback rattlesnakes (Crotalus atrox) in a ‘rattlebox,’ a box with a lid that would slide open and shut every 5 minutes. At first, the snake would rattle its tail each time the box opened. After a while, the snake would become habituated to the box opening and stop rattling its tail. They counted the number of box openings until a snake stopped rattling; fewer box openings means the snake was more quickly habituated. They repeated this experiment on each snake on four successive days, which is treated as an influential variable here. Place and Abramson (2008) used 10 snakes, but some of them never became habituated; to simplify this example, data from the six snakes that did become habituated on each day are used.\nFirst, we read in the data, making sure to convert the column named day to a factor. Why? Because ANOVAs work with factor independent variables, while day as it is encoded by default is in fact a continuous variable.\n\nsnakes &lt;- read_csv(\"../../data/snakes.csv\")\nsnakes$day = as.factor(snakes$day)\n\nThe first thing we do is to create some summaries of the data. Refer to the summary statistics Chapter.\n\nsnakes.summary &lt;- snakes %&gt;% \n  group_by(day, snake) %&gt;% \n  summarise(mean_openings = mean(openings),\n            sd_openings = sd(openings)) %&gt;% \n  ungroup()\nsnakes.summary\n\n# A tibble: 24 × 4\n   day   snake mean_openings sd_openings\n   &lt;fct&gt; &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 1     D1               85          NA\n 2 1     D11              40          NA\n 3 1     D12              65          NA\n 4 1     D3              107          NA\n 5 1     D5               61          NA\n 6 1     D8               22          NA\n 7 2     D1               58          NA\n 8 2     D11              45          NA\n 9 2     D12              27          NA\n10 2     D3               51          NA\n# ℹ 14 more rows\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nSomething seems… off. What’s going on here? Please explain this outcome.\n\n\n\nTo fix this problem, let us ignore the grouping by both snake and day.\n\nsnakes.summary &lt;- snakes %&gt;% \n  group_by(day) %&gt;% \n  summarise(mean_openings = mean(openings),\n            sd_openings = sd(openings)) %&gt;% \n  ungroup()\nsnakes.summary\n\n# A tibble: 4 × 3\n  day   mean_openings sd_openings\n  &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 1              63.3        30.5\n2 2              47          12.2\n3 3              34.5        26.0\n4 4              25.3        18.1\n\n\nRmisc::summarySE() offers a convenience function if your feeling less frisky about calculating the summary statistics yourself:\n\nlibrary(Rmisc)\nsnakes.summary2 &lt;- summarySE(data = snakes, measurevar = \"openings\", groupvars = c(\"day\"))\nsnakes.summary2\n\n  day N openings       sd        se       ci\n1   1 6 63.33333 30.45434 12.432931 31.95987\n2   2 6 47.00000 12.21475  4.986649 12.81859\n3   3 6 34.50000 25.95958 10.597956 27.24291\n4   4 6 25.33333 18.08498  7.383164 18.97903\n\n\nNow we turn to some visual data summaries (Figure 3).\n\nggplot(data = snakes, aes(x = day, y = openings)) +\n  geom_segment(data = snakes.summary2, aes(x = day, xend = day,\n                                           y = openings - ci,\n                                           yend = openings + ci,\n                                           colour = day),\n              size = 2.0, linetype = \"solid\", show.legend = FALSE) +\n  geom_boxplot(aes(fill = day), alpha = 0.3, show.legend = FALSE) + \n  geom_jitter(width = 0.05) +\n  theme_pubclean()\n\n\n\n\nFigure 3: Boxplots showing the change in the snakes’ habituation to box opening over time.\n\n\nWhat are our null hypotheses?\n\n\nH0 There is no difference between snakes with respect to the number of openings at which they habituate.\n\nH0 There is no difference between days in terms of the number of openings at which the snakes habituate.\n\nFit the ANOVA model to test these hypotheses:\n\nsnakes.aov &lt;- aov(openings ~ day + snake, data = snakes)\nsummary(snakes.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nday          3   4878  1625.9   3.320 0.0487 *\nsnake        5   3042   608.4   1.242 0.3382  \nResiduals   15   7346   489.7                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow we need to test of the assumptions hold true (i.e. erros are normally distributed and heteroscedastic) (Figure 4). Also, where are the differences (Figure 5)?\n\npar(mfrow = c(1, 2))\n# Checking assumptions...\n# make a histogram of the residuals;\n# they must be normal\nsnakes.res &lt;- residuals(snakes.aov)\nhist(snakes.res, col = \"red\")\n\n# make a plot of residuals and the fitted values;\n# # they must be normal and homoscedastic\nplot(fitted(snakes.aov), residuals(snakes.aov), col = \"red\")\n\n\n\n\nFigure 4: Exploring the assumptions visually.\n\n\n\nsnakes.tukey &lt;- TukeyHSD(snakes.aov, which = \"day\", conf.level = 0.90)\nplot(snakes.tukey, las = 1, col = \"red\")\n\n\n\n\nFigure 5: Exploring the differences between days."
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#wilcoxon-rank-sum-test",
    "href": "BCB744/basic_stats/08-anova.html#wilcoxon-rank-sum-test",
    "title": "8. Analysis of Variance (ANOVA)",
    "section": "\n6.1 Wilcoxon rank sum test",
    "text": "6.1 Wilcoxon rank sum test\nThe non-parametric version of a t-test is a Wilcox rank sum test. To perform this test in R we may again use compare_means() and specify the test we want:\n\ncompare_means(weight ~ Diet, data = filter(chicks, Time == 0, Diet %in% c(1, 2)), method = \"wilcox.test\")\n\n# A tibble: 1 × 8\n  .y.    group1 group2     p p.adj p.format p.signif method  \n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n1 weight 1      2      0.235  0.23 0.23     ns       Wilcoxon\n\n\nWhat do our results show?"
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#kruskall-wallis-rank-sum-test",
    "href": "BCB744/basic_stats/08-anova.html#kruskall-wallis-rank-sum-test",
    "title": "8. Analysis of Variance (ANOVA)",
    "section": "\n6.2 Kruskall-Wallis rank sum test",
    "text": "6.2 Kruskall-Wallis rank sum test\n\n6.2.1 Single factor\nThe non-parametric version of an ANOVA is a Kruskall-Wallis rank sum test. As you may have by now surmised, this may be done with compare_means() as seen below:\n\ncompare_means(weight ~ Diet, data = filter(chicks, Time == 0), method = \"kruskal.test\")\n\n# A tibble: 1 × 6\n  .y.        p p.adj p.format p.signif method        \n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         \n1 weight 0.475  0.48 0.48     ns       Kruskal-Wallis\n\n\nAs with the ANOVA, this first step with the Kruskall-Wallis test is not the last. We must again run a post-hoc test on our results. This time we will need to use pgirmess::kruskalmc(), which means we will need to load a new library.\n\nlibrary(pgirmess)\n\nkruskalmc(weight ~ Diet, data = filter(chicks, Time == 0))\n\nMultiple comparison test after Kruskal-Wallis \nalpha: 0.05 \nComparisons\n    obs.dif critical.dif stat.signif\n1-2    6.95     14.89506       FALSE\n1-3    6.90     14.89506       FALSE\n1-4    4.15     14.89506       FALSE\n2-3    0.05     17.19933       FALSE\n2-4    2.80     17.19933       FALSE\n3-4    2.75     17.19933       FALSE\n\n\nLet’s consult the help file for kruskalmc() to understand what this print-out means.\n\n6.2.2 Multiple factors\nThe water becomes murky quickly when one wants to perform multiple factor non-parametric comparison of means tests. To that end, we will not cover the few existing methods here. Rather, one should avoid the necessity for these types of tests when designing an experiment."
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#the-sa-time-data",
    "href": "BCB744/basic_stats/08-anova.html#the-sa-time-data",
    "title": "8. Analysis of Variance (ANOVA)",
    "section": "\n6.3 The SA time data",
    "text": "6.3 The SA time data\n\nsa_time &lt;- as_tibble(read_csv(\"../../data/snakes.csv\",\n                              col_types = list(col_double(),\n                                               col_double(),\n                                               col_double())))\nsa_time_long &lt;- sa_time %&gt;% \n  gather(key = \"term\", value = \"minutes\") %&gt;% \n  filter(minutes &lt; 300) %&gt;% \n  mutate(term = as.factor(term))\n\nmy_comparisons &lt;- list( c(\"now\", \"now_now\"),\n                        c(\"now_now\", \"just_now\"),\n                        c(\"now\", \"just_now\") )\n\nggboxplot(sa_time_long, x = \"term\", y = \"minutes\",\n          color = \"term\", palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n          add = \"jitter\", shape = \"term\")\n\n\n\n\nFigure 6: Time is not a limited resource in South Africa."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html",
    "href": "BCB744/basic_stats/04-distributions.html",
    "title": "4. Data distributions",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nThe concept of data distributions"
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#bernoulli-and-binomial-distributions",
    "href": "BCB744/basic_stats/04-distributions.html#bernoulli-and-binomial-distributions",
    "title": "4. Data distributions",
    "section": "\n2.1 Bernoulli and binomial distributions",
    "text": "2.1 Bernoulli and binomial distributions\nBernoulli and Binomial distributions are both discrete probability distributions that describe the outcomes of binary events. They are similar but there are also some key differences between the two. In real life examples encountered in ecology and biology we will probably mostly encounter the Binomial distributions. Let us consider each is more detail.\nBernoulli distribution The Bernoulli distribution represents a single binary trial or experiment with only two possible outcomes—‘success’ (usually represented as 1) and ‘failure’ (usually represented as 0). The probability of success is denoted by \\(p\\), while the probability of failure is \\(1 - p\\). A Bernoulli distribution is characterised by only one parameter, \\(p\\), which represents the probability of success for the single trial (Equation 1).\n\n\nThe Bernoulli distribution: \\[\nP(X=k) = \\begin{cases}\n  p, & \\text{if } k=1 \\\\\n  1-p, & \\text{if } k=0\n\\end{cases}\n\\tag{1}\\]\nwhere \\(X\\) is a random variable, \\(k\\) is the outcome (1 for success and 0 for failure), and \\(p\\) is the probability of success.\nBinomial distribution The Binomial distribution represents the sum of outcomes in a fixed number of independent Bernoulli trials with the same probability of success, \\(p\\). It is characterised by two parameters—\\(n\\) (the number of trials) and \\(p\\) (the probability of success in each trial). The Binomial distribution describes the probability of obtaining a specific number of successes (\\(k\\)) in \\(n\\) trials (Equation 2).\n\n\nThe Binomial distribution: \\[P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} \\tag{2}\\]\nwhere \\(X\\) is a random variable, \\(k\\) is the number of successes, \\(n\\) is the total number of trials, and \\(p\\) is the probability of success in each trial.\nThere are several examples of Binomial distributions in ecological and biological contexts. The Binomial distribution is relevant when studying the number of successes in a fixed number of independent trials, each with the same probability of success. A few examples of the Bernoulli distribution in ecological and biological data:\n\nSeed germination Suppose we plant 100 seeds of a particular plant species and wants to know the probability of a certain number of seeds germinating. If the probability of germination for each seed is constant then we can model the number of germinated seeds by a Binomial distribution.\nDisease prevalence An epidemiologist studies the prevalence of a disease within a population. For a random sample of 500 individuals, and with a fixed probability of an individual having the disease, the number of infected individuals in the sample can be modeled using a Binomial distribution.\nSpecies occupancy We do an ecological assessment to determine the occupancy of bird species across 50 habitat patches. If the probability of the species occupying a patch is the same across all patches, the number of patches occupied by the species will follow a Binomial distribution.\nAllele inheritance We want to examine the inheritance of a specific trait following Mendelian inheritance patterns. If the probability of inheriting the dominant allele for a given gene is constant, the number of offspring with the dominant trait in a fixed number of offspring follows the Binomial distribution.\n\nNote that in these examples we assume a fixed probability and independence between trials and this is not always be true in real-world situations."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#negative-binomial-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#negative-binomial-distribution",
    "title": "4. Data distributions",
    "section": "\n2.2 Negative Binomial distribution",
    "text": "2.2 Negative Binomial distribution\nA Negative Binomial random variable, \\(X\\), counts the number of successes in a sequence of independent Bernoulli trials with probability \\(p\\) before \\(r\\) failures occur. This distribution could for example be used to predict the number of heads that result from a series of coin tosses before three tails are observed (Equation 3).\n\n\nThe Negative Binomial distribution: \\[P(X=k) = \\binom{k+r-1}{k} p^r (1-p)^k \\tag{3}\\]\nThe equation describes the probability mass function (PMF) of a Negative Binomial distribution, where \\(X\\) is a random variable, \\(k\\) is the number of failures, \\(r\\) is the number of successes, and \\(p\\) is the probability of success in each trial. The binomial coefficient is denoted by \\(\\binom{k+r-1}{k}\\), which calculates the number of ways to arrange \\(k\\) failures and \\(r\\) successes such that the last trial is a success."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#geometric-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#geometric-distribution",
    "title": "4. Data distributions",
    "section": "\n2.3 Geometric distribution",
    "text": "2.3 Geometric distribution\nA geometric random variable, \\(X\\), represents the number of trials that are required to observe a single success. Each trial is independent and has success probability \\(p\\). As an example, the geometric distribution is useful to model the number of times a die must be tossed in order for a six to be observed (Equation 4).\n\n\nThe Geometric distribution: \\[P(X=k) = p (1-p)^k \\tag{4}\\]\nThe equation represents the PMF of a Geometric distribution, where \\(X\\) is a random variable, \\(k\\) is the number of failures before the first success, and \\(p\\) is the probability of success in each trial. The Geometric distribution can be thought of as a special case of the Negative Binomial distribution with \\(r = 1\\), which models the number of failures before achieving a single success."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#poisson-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#poisson-distribution",
    "title": "4. Data distributions",
    "section": "\n2.4 Poisson distribution",
    "text": "2.4 Poisson distribution\nA Poisson random variable, \\(X\\), tallies the number of events occurring in a fixed interval of time or space, given that these events occur with an average rate \\(\\lambda\\). Poisson distributions can be used to model events such as meteor showers and or number of people entering a shopping mall (Equation 5).\n\n\nThe Poisson distribution: \\[P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\tag{5}\\]\nThe function represents the PMF of a Poisson distribution, where \\(X\\) is a random variable, \\(k\\) is the number of events or occurrences, and \\(\\lambda\\) (lambda) is the average rate of occurrences (events per unit of time or space). The constant \\(e\\) is the base of the natural logarithm, and \\(k!\\) is the factorial of \\(k\\). The Poisson distribution is commonly used to model the number of events occurring within a fixed interval of time or space when events occur independently and at a constant average rate."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#sec-normal",
    "href": "BCB744/basic_stats/04-distributions.html#sec-normal",
    "title": "4. Data distributions",
    "section": "\n3.1 Normal distribution",
    "text": "3.1 Normal distribution\n\n\n\n\nFigure 1: The idealised Normal distribution showing the proportion of data within 1, 2, and 3SD from the mean.\n\nAnother name for this kind of distribution is a Gaussian distribution. A random sample with a Gaussian distribution is normally distributed. These values are independent and identically distributed random variables (i.i.d.), and they have an expected mean given by \\(\\mu\\) (or \\(\\hat{x}\\) in Chapter 3.2.1) and a finite variance given by \\(\\sigma^{2}\\) (or \\(S^{2}\\) in Chapter 3.3.1); if the number of samples drawn from a population is sufficiently large, the estimated mean and SD will be indistinguishable from the population (as per the central limit theorem). It is represented by Equation 6.\n\n\nThe Normal distribution: \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\tag{6}\\]\nwhere \\(x\\) is a continuous random variable, \\(\\mu\\) (mu) is the mean, and \\(\\sigma\\) (sigma) is the standard deviation. The constant factor \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\) ensures that the Probability Density Function (PDF) integrates to 1, and the exponential term is responsible for the characteristic bell-shaped curve of the Normal distribution.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThe Central Limit Theorem (CLT) is a fundamental result in probability theory and statistics, which states that the distribution of the sum (or average) of a large number of independent, identically distributed (IID) random variables approaches a Normal distribution regardless of the shape of the original distribution. So, the CLT asserts that the Normal distribution is the limiting distribution for the sum or average of many random variables, as long as certain conditions are met.\nThe CLT has important implications in statistical analysis, as it provides a basis for making inferences about population parameters using sample statistics. For example, when dealing with large sample sizes, the sampling distribution of the sample mean is approximately normally distributed, even if the underlying population distribution is not normal. This allows us to apply inferential techniques based on the Normal distribution, such as hypothesis testing and constructing confidence intervals, to estimate population parameters using sample data.\nSome conditions must be met for the CLT to be true:\n\n\nThe random variables must be independent The observations should not be influenced by one another.\n\nThe random variables must be identically distributed They must come from the same population with the same mean and variance.\n\nThe number of random variables (sample size) must be sufficiently large Although there is no strict rule for the sample size, a common rule of thumb is that the sample size should be at least 30 for the CLT to be a reasonable approximation."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#uniform-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#uniform-distribution",
    "title": "4. Data distributions",
    "section": "\n3.2 Uniform distribution",
    "text": "3.2 Uniform distribution\nThe continuous uniform distribution is sometime called a rectangular distribution. Simply, it states that all measurements of the same magnitude included with this distribution are equally probable. This is basically random numbers."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#student-t-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#student-t-distribution",
    "title": "4. Data distributions",
    "section": "\n3.3 Student T distribution",
    "text": "3.3 Student T distribution\nThis is a continuous probability distribution that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. It is used in the statistical significance testing between the means of different sets of samples, and not much so in the modelling of natural phenomena."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#chi-squared-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#chi-squared-distribution",
    "title": "4. Data distributions",
    "section": "\n3.4 Chi-squared distribution",
    "text": "3.4 Chi-squared distribution\nMostly used in hypothesis testing, but not to encapsulate the distribution of data drawn to represent natural phenomena."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#exponential-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#exponential-distribution",
    "title": "4. Data distributions",
    "section": "\n3.5 Exponential distribution",
    "text": "3.5 Exponential distribution\nThis is a probability distribution that describes the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate."
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#f-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#f-distribution",
    "title": "4. Data distributions",
    "section": "\n3.6 F distribution",
    "text": "3.6 F distribution"
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#gamma-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#gamma-distribution",
    "title": "4. Data distributions",
    "section": "\n3.7 Gamma distribution",
    "text": "3.7 Gamma distribution"
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#beta-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#beta-distribution",
    "title": "4. Data distributions",
    "section": "\n3.8 Beta distribution",
    "text": "3.8 Beta distribution"
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html",
    "href": "BCB744/basic_stats/01-data-in-R.html",
    "title": "1. Data classes and structures in R",
    "section": "",
    "text": "“That which can be destroyed by the truth should be.”\n— P.C. Hodgell"
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#numeric-variables",
    "href": "BCB744/basic_stats/01-data-in-R.html#numeric-variables",
    "title": "1. Data classes and structures in R",
    "section": "\n2.1 Numeric variables",
    "text": "2.1 Numeric variables\nNumeric data in the context of biostatistics refers to quantitative data that can be expressed in numerical form, typically obtained from field and laboratory measurements, or from field sampling campaigns. Examples of numeric data in biostatistics include the height and mass of a animals, concentrations of nutrients, laboratory test results such as respiration rates, or the number of limpets in a quadrat. Numeric data can be further categorised as discrete or continuous.\n\n2.1.1 Discrete variables\nDiscrete data are whole (integer) numbers that represent counts of items or events. Integer data usually answer the question, “how many?” For example, in the biological and Earth sciences, discrete data is commonly encountered in the form of counts or integers that represent the presence or absence of certain characteristics or events. For example, the number of individuals of a particular species in a population, the number of chromosomes in a cell, or the number of earthquakes occurring in a region within a given time frame. Other examples of discrete data in these sciences include the number of mutations in a gene, the number of cells in a tissue sample, or the number of species present in an ecosystem. These types of data are often analysed using statistical techniques such as frequency distributions, contingency tables, and chi-square tests.\n\n2.1.2 Continuous variables\nContinuous data, on the other hand, are measured on a continuous scale. These usually represent measured quantities such as something’s heat content (temperature, measured in degrees Celsius) or distance (measured in metres or similar), etc. They can be rational numbers including integers and fractions, but typically they have an infinite number of ‘steps’ that depends on rounding (they can even be rounded to whole integers) or considerations such as measurement precision and accuracy. Often, continuous data have upper and lower bounds that depend on the characteristics of the phenomenon being studied or the measurement being taken."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#dates",
    "href": "BCB744/basic_stats/01-data-in-R.html#dates",
    "title": "1. Data classes and structures in R",
    "section": "\n2.2 Dates",
    "text": "2.2 Dates\nWe often encounter date data when dealing with time-related data. For example, in ecological research, data collection may involve recording the date of a particular observation or sampling event, such as the date when a bird was sighted, or when water samples were taken from a stream. The purpose of using date (or time) data in biology and ecology is to enable us to understand and analyse temporal patterns and relationships in their response variables. This can include exploring seasonal trends, understanding the impact of environmental changes over time, or tracking the growth and development of organisms.\nBy analysing date data, we can gain insights into long-term trends and patterns that may not be apparent when looking at the data in aggregate. They can also use this information to make predictions about future trends, develop more effective management strategies, and identify potential areas for further research."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#qualitative-variables",
    "href": "BCB744/basic_stats/01-data-in-R.html#qualitative-variables",
    "title": "1. Data classes and structures in R",
    "section": "\n2.3 Qualitative variables",
    "text": "2.3 Qualitative variables\nQualitative data in the context of statistics used by biologists refers to non-numerical data that are collected from observations, experimental treatment groups, or other sources. They tend to be textual and it are often used to describe characteristics or properties of living organisms, ecosystems, or other biological phenomena. Examples of qualitative data in biology may include the colour of flowers, the type of habitat where an animal is found, the behaviour of animals, or the presence or absence of certain traits or characteristics in a population.\nQualitative data can be further classified into nominal or ordinal data.\n\n2.3.1 Nominal variables\nNominal data are used to describe qualitative variables that do not have any inherent order or ranking. Examples of nominal data in biology may include the type of plant or animal species, or the presence or absence of certain genetic traits. Another term for nominal data is categorical data. Because there are well-defined categories, the number of members belonging to each of the category can be counted. For example, there are three red flowers, 66 purple flowers, and 13 yellow flowers.\n\n2.3.2 Ordinal variables\nOrdinal data refer to a type of data that can be used to describe qualitative categorical variables that have a natural order or ranking. It is used when we need to arrange things in a particular order, such as from worst to best or from least to most. However, the differences between the values cannot be measured or quantified exactly, making them somewhat subjective. Examples of ordinal data include the different stages of development of an organism or the performance of a species to different fertilisers. Ordinal data can be entered as descriptive character strings, and internally, they are translated into an ordered vector of integers in R. For example, we can use a scale of 1 for terrible, 2 for ‘so-so’, 3 for average, 4 for good, and 5 for brilliant."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#binary-variables",
    "href": "BCB744/basic_stats/01-data-in-R.html#binary-variables",
    "title": "1. Data classes and structures in R",
    "section": "\n2.4 Binary variables",
    "text": "2.4 Binary variables\nLife can be boiled down to a series of binary decisions: should I have pizza for dinner, yes or no? Should I go to bed early, TRUE or FALSE? Should I start that new series on Netflix, accept or reject? Am I present or absent? You get the gist… This kind of binary decision-making is known as ‘logical’, and in R they can only take on the values of TRUE or FALSE (remember to mind your case!). In the computing world, logical data are often represented by 1 for TRUE and 0 for FALSE. So basically, your life’s choices can be summarised as a string of 1s and 0s. Who knew it was that simple?\n\n\nWhen it comes down to it, everything in life is either black or white, right or wrong, good or bad. It’s like a cosmic game of “Would You Rather?” — and we’re all just playing along."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#character-variables",
    "href": "BCB744/basic_stats/01-data-in-R.html#character-variables",
    "title": "1. Data classes and structures in R",
    "section": "\n2.5 Character variables",
    "text": "2.5 Character variables\nAs the name implies, these are not numbers. Rather, they are human words that have found their way into R for one reason or another. In biology we most commonly encounter character values when we have a list of things, such as sites or species. These values will often be used as categorical or ordinal data."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#missing-values",
    "href": "BCB744/basic_stats/01-data-in-R.html#missing-values",
    "title": "1. Data classes and structures in R",
    "section": "\n2.6 Missing values",
    "text": "2.6 Missing values\nIt’s unfortunate to admit that one of the most reliable aspects of any biological dataset is the presence of missing data (the presence of something that’s missing?!). It is a stark reminder of the fragility of life. How can we say that something contains missing data? It seems counter intuitive, as if the data were never there in the first place. However, as we remember the principles of tidy data, we see that every observation must be documented in a row, and each column in that row must contain a value. This organisation allows us to create a matrix of data from multiple observations. Since the data are presented in a two-dimensional format, any missing values from an observation will leave a gaping hole in the matrix. We call these ‘missing values.’ It’s a somber reality that even the most meticulous collection of data can be marred by the loss of information."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#complex-numbers",
    "href": "BCB744/basic_stats/01-data-in-R.html#complex-numbers",
    "title": "1. Data classes and structures in R",
    "section": "\n2.7 Complex numbers",
    "text": "2.7 Complex numbers\n\n“And if you gaze long enough into an abyss, the abyss will gaze back into you.”\n— Friedrich Nietzsche\n\nAs we draw to a close on the topic of data types, we cling desperately to the threads of our sanity, hoping against hope that they remain tightly stitched. But let it be known, to those who dare enter further into the realm of data, that beneath the surface lie countless rocks, and around every corner lurk a legion of complex data types, waiting to ensnare the unwary. These shadows of information are as enigmatic as they are perilous, for they challenge the very essence of our understanding. It is not until the final chapter of our journey, when we confront the elusive art of modeling, that we will face these data demons head-on. But fear not, for we shall arm ourselves with the knowledge and techniques acquired on this treacherous path, and with each step forward, we shall move closer to mastering the darkness that awaits us."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#numeric",
    "href": "BCB744/basic_stats/01-data-in-R.html#numeric",
    "title": "1. Data classes and structures in R",
    "section": "\n3.1 numeric\n",
    "text": "3.1 numeric\n\nIn R, the numeric data class represents either integers or floating point (decimal) values. Numerical data are quantitative in nature as they represent things that can be objectively counted, measured, or calculated—the measured variables.\nNumeric data are one of the most common types of data used in statistical and mathematical analysis. In R, numeric data are represented by the class numeric, which includes both integers and floating-point numbers. Numeric data can be used in a variety of operations and calculations, including arithmetic operations, statistical analyses, and visualisations. One important feature of the numeric data class in R is that it supports vectorisation, which allows for efficient and concise operations on large sets of numeric data. Additionally, R provides a wide range of built-in functions for working with numeric data, including functions for calculating basic statistical measures such as mean, median, and standard deviation.\nIn R integer (discrete) data are called int or &lt;int&gt; while continuous data are denoted num or &lt;dbl&gt;.\nExample of integer data Suppose you have a dataset of the number of rats in different storm water drains in a neighbourhood. The number of rats is a discrete variable because it can only take on integer values (you can’t own a fraction of a rat).\nHere’s how you could create a vector of this data in R:\n\n# Create a vector of the number of pets owned by each household\nnum_rats &lt;- c(0, 1, 2, 2, 3, 1, 4, 0, 2, 1, 2, 2, 0, 3, 2, 1, 1, 4, 2, 0)\nnum_rats\n\n [1] 0 1 2 2 3 1 4 0 2 1 2 2 0 3 2 1 1 4 2 0\n\nclass(num_rats)\n\n[1] \"numeric\"\n\n\nIn this example, the data are represented as a vector called num_rats of class numeric (as revealed by class(num_rats)). Each element of the vector represents the number of rats in one storm water drain. For example, the first element of the vector (num_rats[1]) is 0, which means that the first drain in the dataset is free of rats. The fourth element of the vector (num_rats[4]) is 2, indicating that the fourth drain in the dataset is occupied by 2 rats.\nOne can also explicitly create a vector of integer using the as.integer() function:\n\nnum_rats_int &lt;- as.integer(num_rats)\nnum_rats_int\n\n [1] 0 1 2 2 3 1 4 0 2 1 2 2 0 3 2 1 1 4 2 0\n\nclass(num_rats_int)\n\n[1] \"integer\"\n\n\nAbove we coerced the class numeric data to class integer. But we can take floating point numeric and convert them to integers too with the as.integer() function. As we see, the effect is that the whole part of the number is retained and the rest discarded:\n\npies &lt;- pi * seq(1:5)\npies\n\n[1]  3.141593  6.283185  9.424778 12.566371 15.707963\n\nclass(pies)\n\n[1] \"numeric\"\n\nas.integer(pies)\n\n[1]  3  6  9 12 15\n\n\nEffectively, what happened above is more-or-less equivalent to what the floor() function would return:\n\nfloor(pies)\n\n[1]  3  6  9 12 15\n\n\nBe careful when coercing floating point numbers to integers. If rounding is what you expect, this is not what you will get. For rounding, use round() instead:\n\nround(pies, 0)\n\n[1]  3  6  9 13 16\n\n\nExample of continuous data Here are some randomly generated temperature data assigned to an object called temp_data:\n\n# Generate a vector of 50 normally distributed temperature values\ntemp_data &lt;- round(rnorm(n = 50, mean = 15, sd = 3), 2)\ntemp_data\n\n [1] 16.86 17.06 15.84  7.80 16.08 11.60 13.93 10.49 15.38 16.53 16.33 14.20\n[13] 18.19 13.07 19.09 16.29 15.68 16.02 16.89 17.06 19.12 17.65  9.58  9.71\n[25]  8.88 10.78 16.08 12.79 13.41 12.14 13.58 20.53 13.61 20.69 10.30 16.85\n[37] 16.47 10.37 18.80 15.32 18.16  9.26 14.60 18.23  9.57 14.84 11.17 15.65\n[49] 18.83 16.38\n\nclass(temp_data)\n\n[1] \"numeric\""
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#character",
    "href": "BCB744/basic_stats/01-data-in-R.html#character",
    "title": "1. Data classes and structures in R",
    "section": "\n3.2 character\n",
    "text": "3.2 character\n\nIn R, the character data class represents textual data such as words, sentences, and paragraphs. Character data can be created using either single or double quotes, and it can include letters, numbers, and other special characters. In addition, character data can be concatenated using the paste() function or other string manipulation functions.\nOne important feature of the character data class in R is its versatility in working with textual data. For instance, it can be used to store and manipulate text data, including text-based datasets, text-based files, and text-based visualisations. Additionally, R provides a wide range of built-in functions for working with character data, including functions for manipulating strings, searching for patterns, and formatting output. Overall, the character data class in R is a fundamental data type that is critical for working with textual data in a variety of contexts. You will most frequently use character values are often used to represent labels, names, or descriptions."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#factor",
    "href": "BCB744/basic_stats/01-data-in-R.html#factor",
    "title": "1. Data classes and structures in R",
    "section": "\n3.3 factor\n",
    "text": "3.3 factor\n\nIn R, the factor data class is used to represent discrete categorical variables. Factors are often used in statistical analyses to represent class or group belonging. Factor values are categorical data, such as levels or categories of a variable. Factor variables are most commonly also character data, but they can be numeric too if coded correctly as factors. Factor values can be ordered (ordinal) or unordered (categorical or nominal).\nCategorical variables take on a limited number of distinct values, often corresponding to different groups or levels. For example, a categorical variable might represent different colours, size classes, or species. Factors in R are represented as integers with corresponding character levels, where each level corresponds to a distinct category. The levels of a factor can be defined explicitly using the factor() function or automatically using the cut() function. One important feature of the factor data class in R is that it allows for efficient and effective data manipulation and analysis, particularly when working with large datasets. For instance, factors can be used in statistical analyses such as regression models or ANOVA, and they can also be used to create visualisations such as bar or pie graphs. The factor data class in R is a fundamental data type that is critical for representing and working with categorical variables in data analysis and visualisation.\nThe factor data class of data in an R data.frame structure (or in a tibble) is indicated by Factor or &lt;fctr&gt;. Ordered factors are denoted by columns named Ord.factor or &lt;ord&gt;.\nNominal data One example of nominal factor data that ecologists might encounter is the type of vegetation in a particular area, such as ‘grassland’, ‘forest’, or ‘wetland’. Here’s an example of how to generate a vector of nominal data in R using the sample() function:\n\n# Generate a vector of vegetation types\nvegetation &lt;- sample(c(\"grassland\", \"forest\", \"wetland\"), size = 50, replace = TRUE)\n\n# View the vegetation data\nvegetation\n\n [1] \"forest\"    \"grassland\" \"forest\"    \"forest\"    \"forest\"    \"grassland\"\n [7] \"forest\"    \"grassland\" \"wetland\"   \"wetland\"   \"wetland\"   \"forest\"   \n[13] \"wetland\"   \"grassland\" \"grassland\" \"grassland\" \"forest\"    \"forest\"   \n[19] \"grassland\" \"forest\"    \"grassland\" \"wetland\"   \"grassland\" \"grassland\"\n[25] \"grassland\" \"forest\"    \"forest\"    \"wetland\"   \"forest\"    \"forest\"   \n[31] \"wetland\"   \"grassland\" \"wetland\"   \"forest\"    \"grassland\" \"grassland\"\n[37] \"forest\"    \"grassland\" \"grassland\" \"grassland\" \"forest\"    \"forest\"   \n[43] \"grassland\" \"grassland\" \"wetland\"   \"wetland\"   \"grassland\" \"forest\"   \n[49] \"forest\"    \"forest\"   \n\nclass(vegetation)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\nThe sample() function\n\n\n\nNote that the sample() function is not made specifically for nominal data; it can be used on any kind of data class.\n\n\n\n\n\n\n\n\nTask A\n\n\n\n\nProvide some examples of what one can do with the sample() function. Why might we (mostly biologists and ecologists) find it useful?\n\n\n\nOrdinal data Here’s an example vector of ordinal data in R that could be encountered by ecologists:\n\n# Vector of ordinal data representing the successional stage of a forest\nsuccession &lt;- c(\"Early Pioneer\", \"Late Pioneer\",\n                \"Young Forest\", \"Mature Forest\",\n                \"Old Growth\")\nsuccession\n\n[1] \"Early Pioneer\" \"Late Pioneer\"  \"Young Forest\"  \"Mature Forest\"\n[5] \"Old Growth\"   \n\nclass(succession)\n\n[1] \"character\"\n\n# Convert to ordered factor\nsuccession &lt;- factor(succession, ordered = TRUE,\n                     levels = c(\"Early Pioneer\", \"Late Pioneer\",\n                                \"Young Forest\", \"Mature Forest\",\n                                \"Old Growth\"))\nsuccession\n\n[1] Early Pioneer Late Pioneer  Young Forest  Mature Forest Old Growth   \n5 Levels: Early Pioneer &lt; Late Pioneer &lt; Young Forest &lt; ... &lt; Old Growth\n\nclass(succession)\n\n[1] \"ordered\" \"factor\" \n\n\nIn this example, the successional stage of a forest is represented by an ordinal scale with five levels ranging from ‘Early Pioneer’ to ‘Old Growth’. The factor() function is used to convert the vector to an ordered factor, with the ordered argument set to TRUE and the levels argument set to the same order as the original vector. This ensures that the levels are properly represented as an ordered factor."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#logical",
    "href": "BCB744/basic_stats/01-data-in-R.html#logical",
    "title": "1. Data classes and structures in R",
    "section": "\n3.4 logical\n",
    "text": "3.4 logical\n\nIn R, the logical data class represents binary or Boolean data. Logical data are used to represent variables that can take on only two possible values, TRUE or FALSE. In addition to TRUE and FALSE, logical data can also take on the values of NA or NULL, which represent missing or undefined values.\nLogical data can be created using logical operators such as ==, !=, &gt;, &lt;, &gt;=, and &lt;=. Logical data are commonly used in R for data filtering and selection, conditional statements, and logical operations. For example, logical data can be used to filter a dataset to include only observations that meet certain criteria or to perform logical operations such as AND (&) and OR (|). The logical data class in R is a fundamental data type that is critical for representing and working with binary or Boolean variables in data analysis and programming.\nExample logical (binary) data Here’s an example of generating a vector of binary or logical data in R, which represents the presence or absence of a particular species in different ecological sites:\n\n# Generate a vector of 1s and 0s to represent the presence\n# or absence of a species in different ecological sites\nspecies_presence &lt;- sample(c(0,1), 10, replace = TRUE)\nspecies_presence\n\n [1] 0 0 0 1 0 1 0 0 0 0\n\n\nWe can also make a formal logical class data:\n\nspecies_presence_logi &lt;- as.logical(species_presence)\nclass(species_presence_logi)\n\n[1] \"logical\"\n\n\nIn this example, we again use the sample() function to randomly generate a vector of 10 values, each either 0 or 1, to represent the presence or absence of a species in 10 different ecological sites. However, it is often not necessary to coerce to class logical, as we see in the presence-absence datasets we will encounter in BCB743: Quantitative Ecology."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#date",
    "href": "BCB744/basic_stats/01-data-in-R.html#date",
    "title": "1. Data classes and structures in R",
    "section": "\n3.5 date\n",
    "text": "3.5 date\n\nIn R, the POSIXct, POSIXlt, and Date classes are commonly used to represent date and time data. These classes each have unique characteristics that make them useful for different purposes.\nThe POSIXct class is a date/time class that represents dates and times as a numerical value, typically measured in seconds since January 1st, 1970. This class provides a high level of precision, with values accurate to the second. It is useful for performing calculations and data manipulation involving time, such as finding the difference between two dates or adding a certain number of seconds to a given time. An example of how to generate a POSIXct object in R is as follows:\n\nmy_time &lt;- as.POSIXct(\"2022-03-10 12:34:56\")\nclass(my_time)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nmy_time\n\n[1] \"2022-03-10 12:34:56 SAST\"\n\n\nThe POSIXlt class, on the other hand, typically represents dates and times in a more human-readable format. It stores date and time information as a list of named elements, including year, month, day, hour, minute, and second. This format is useful for displaying data in a more understandable way and for extracting specific components of a date or time. An example of how to generate a POSIXlt object in R is as follows:\n\nmy_time &lt;- as.POSIXlt(\"2022-03-10 12:34:56\")\nclass(my_time)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\nmy_time\n\n[1] \"2022-03-10 12:34:56 SAST\"\n\n\nThe Date class is used to represent dates only, without any time information. Dates are typically stored as the number of days since January 1st, 1970. This class provides functions for performing arithmetic operations and comparisons between dates. It is useful for working with time-based data that is only concerned with the date component, such as daily sales or stock prices. An example of how to generate a Date object in R is as follows:\n\nmy_date &lt;- as.Date(\"2022-03-10\")\nclass(my_date)\n\n[1] \"Date\"\n\nmy_date\n\n[1] \"2022-03-10\"\n\n\nTo generate a vector of dates in R with daily intervals, we can use the seq() function to create a sequence of dates, specifying the start and end dates and the time interval. Here’s an example:\n\n# Generate a vector of dates from January 1, 2022 to December 31, 2022\ndates &lt;- seq(as.Date(\"2022-01-01\"), as.Date(\"2022-12-31\"), by = \"day\")\n\n# View the first 10 dates in the vector\nhead(dates, 10)\n\n [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\n [6] \"2022-01-06\" \"2022-01-07\" \"2022-01-08\" \"2022-01-09\" \"2022-01-10\"\n\nclass(dates)\n\n[1] \"Date\"\n\n\nUnderstanding the characteristics of these date and time classes in R is essential for effective data analysis and manipulation in fields where time-based data is a critical component.\nDate and time data in R can be manipulated using various built-in functions and packages such as lubridate and chron. Additionally, date and time data can be visualised using different types of graphs such as time series plots, heatmaps, and Hovmöller diagrams. The date and time data classes in R are essential for working with temporal data and conducting time-related analyses in various biological and environmental datasets."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#missing-values-na",
    "href": "BCB744/basic_stats/01-data-in-R.html#missing-values-na",
    "title": "1. Data classes and structures in R",
    "section": "\n3.6 Missing values, NA\n",
    "text": "3.6 Missing values, NA\n\nMissing values can be encountered in vectors of all data classes. To demonstrate some data that contains missing values, I will generate a data sequence containing 5% missing values. We can use the rnorm() function to generate a sequence of random normal numbers and then randomly assign 5% of the values as missing using the sample() function. The indices of the missing values are stored in missing_indices, and we use them to assign NA to the corresponding elements of the data sequence. Here’s some code to achieve this:\n\n# Set the length of the sequence\nn &lt;- 100\n\n# Generate a sequence of random normal numbers with\n# mean 0 and standard deviation 1\ndata &lt;- rnorm(n, mean = 0, sd = 1)\n\n# Randomly assign 5% of the values as missing\nmissing_indices &lt;- sample(1:n, size = round(0.05*n))\ndata[missing_indices] &lt;- NA\nlength(data)\n\n[1] 100\n\ndata\n\n  [1]  1.009314875 -0.941697671 -0.671938694 -1.272574390 -0.042441620\n  [6] -2.059599862           NA  1.020182427 -0.239880486 -0.760810603\n [11]           NA  0.026114105 -0.212669463 -0.517070692  1.082009721\n [16]  0.887168886  1.934254418  0.429211487  0.631814463 -0.940229262\n [21]  0.568818811  1.104455608  0.210048160 -0.368401421  0.007929404\n [26]  0.138750602 -1.356748650  1.678409626 -1.258383354  0.362573150\n [31]  1.262802032  0.573482375           NA  0.761451110 -1.260455040\n [36]  0.833373865  0.656776728 -0.640594321  0.314466260  0.353808407\n [41]  1.173819283 -1.057222950  0.564910707 -0.650745560 -2.155207999\n [46]  0.503927627  0.421238215 -1.217874507 -0.382529547  0.577719637\n [51] -0.454210823 -0.653807441  0.959678965           NA -0.540770942\n [56]  1.281242350  1.091779521  1.051593498 -0.399086036  0.233647658\n [61] -1.525442400  0.181959970  0.890263360  0.430065128  0.877893338\n [66]  0.740362108  0.885748015 -1.081356162 -0.726064969  0.737690706\n [71] -0.677811054  1.416338516  0.851531676 -0.319692330  0.353957253\n [76] -1.443911800  1.990493573 -1.255412375  0.459288367 -0.643218084\n [81]  0.429254545  0.976279985 -0.819378641  0.514871232  0.047656573\n [86] -1.736963476  0.972327071  0.214110685  1.862696570 -0.597513734\n [91]  1.219337615 -0.128531042  2.564571742  0.232012946  0.465904814\n [96] -0.995107180  0.792048912           NA  1.453465547 -0.068551841\n\n\nTo remove all NAs from the vector of data we can use na.omit():\n\ndata_sans_na &lt;- na.omit(data)\nlength(data_sans_na)\n\n[1] 95\n\ndata_sans_na\n\n [1]  1.009314875 -0.941697671 -0.671938694 -1.272574390 -0.042441620\n [6] -2.059599862  1.020182427 -0.239880486 -0.760810603  0.026114105\n[11] -0.212669463 -0.517070692  1.082009721  0.887168886  1.934254418\n[16]  0.429211487  0.631814463 -0.940229262  0.568818811  1.104455608\n[21]  0.210048160 -0.368401421  0.007929404  0.138750602 -1.356748650\n[26]  1.678409626 -1.258383354  0.362573150  1.262802032  0.573482375\n[31]  0.761451110 -1.260455040  0.833373865  0.656776728 -0.640594321\n[36]  0.314466260  0.353808407  1.173819283 -1.057222950  0.564910707\n[41] -0.650745560 -2.155207999  0.503927627  0.421238215 -1.217874507\n[46] -0.382529547  0.577719637 -0.454210823 -0.653807441  0.959678965\n[51] -0.540770942  1.281242350  1.091779521  1.051593498 -0.399086036\n[56]  0.233647658 -1.525442400  0.181959970  0.890263360  0.430065128\n[61]  0.877893338  0.740362108  0.885748015 -1.081356162 -0.726064969\n[66]  0.737690706 -0.677811054  1.416338516  0.851531676 -0.319692330\n[71]  0.353957253 -1.443911800  1.990493573 -1.255412375  0.459288367\n[76] -0.643218084  0.429254545  0.976279985 -0.819378641  0.514871232\n[81]  0.047656573 -1.736963476  0.972327071  0.214110685  1.862696570\n[86] -0.597513734  1.219337615 -0.128531042  2.564571742  0.232012946\n[91]  0.465904814 -0.995107180  0.792048912  1.453465547 -0.068551841\nattr(,\"na.action\")\n[1]  7 11 33 54 98\nattr(,\"class\")\n[1] \"omit\"\n\n\n\n\n\n\n\n\nDealing with NAs in functions\n\n\n\nMany functions have specific arguments to deal with NAs in data. See for example the na.rm = TRUE argument given to mean(), median(), min(), lm(), etc."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#vector-array-and-matrix",
    "href": "BCB744/basic_stats/01-data-in-R.html#vector-array-and-matrix",
    "title": "1. Data classes and structures in R",
    "section": "\n4.1 vector, array, and matrix\n",
    "text": "4.1 vector, array, and matrix\n\nVectors In R, a vector is a one-dimensional array-like data structure that can hold a sequence of values of the same atomic mode, such as numeric, character, logical values, or Date and times. A vector can be created using the c() function, which stands for ‘combine’ or ‘concatenate,’ and is used to combine a sequence of values into a vector. Vectors can also be created by using the seq() function to generate a sequence of numbers, or the rep() function to repeat a value or sequence of values. Here is an example of a numeric vector:\n\n# create a numeric vector\nmy_vector &lt;- c(1, 2, 3, 4, 5)\n\n# coerce to vector\nmy_vector &lt;- as.vector(c(1, 2, 3, 4, 5))\nclass(my_vector) # but it doesn't change the class from numeric\n\n[1] \"numeric\"\n\n# print the vector\nmy_vector\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n\n\nCoercion to vector\n\n\n\nThe behaviour is such that the output of coercion to vector is that one the atomic modes (the basic data types) is returned.\n\n\nOne of the advantages of using vectors in R is that many of the built-in functions and operations work on vectors, allowing us to easily manipulate and analyse large amounts of data. Additionally, R provides many functions specifically designed for working with vectors, such as mean(), median(), sum(), min(), max(), and many others.\nMatrices A matrix (again, this terminology may be different for other languages), on the other hand, is a special case of an array that has two dimensions (rows and columns). It is also a multi-dimensional data structure that can hold elements of the same data type, but it is specifically designed for handling data in a tabular format. A matrix can be created using the matrix() function in R.\n\n# create a numeric matrix\nmy_matrix &lt;- matrix(1:6, nrow = 2, ncol = 3)\n\n# print the matrix\nmy_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nclass(my_matrix)\n\n[1] \"matrix\" \"array\" \n\n\nWe can query the size or dimensions of the matrix as follows:\n\ndim(my_matrix)\n\n[1] 2 3\n\nncol(my_matrix)\n\n[1] 3\n\nnrow(my_matrix)\n\n[1] 2\n\n\nCoercion of matrices to vectors A matrix can be coerced to a vector:\n\nas.vector(my_matrix)\n\n[1] 1 2 3 4 5 6\n\n\nArrays In R (as opposed to in python or some other languages), an array specifically refers to a multi-dimensional data structure that can hold elements of the same data type. It can have any number of dimensions (1, 2, 3, etc.), and its dimensions can be named. An array can be created using the array() function in R.\n\n# create a 2-dimensional array\nmy_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n# print the array\nmy_array\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27\n\nclass(my_array)\n\n[1] \"array\"\n\n\nWe can figure something out about the size or dimensions of the array:\n\ndim(my_array)\n\n[1] 3 3 3\n\nncol(my_array)\n\n[1] 3\n\nnrow(my_array)\n\n[1] 3\n\n\nCoercion of arrays to vectors The array can be coerced to a vector:\n\nas.vector(my_array)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27\n\n\n\n\n\n\n\n\nTask A\n\n\n\n\nUsing examples (new data), explain how the as.vector() function works when applied to matrices and arrays. How does it decide in what order to string the elements of the matrices and arrays together?\nUse the result produced by as.vector() (your own data) and assemble a new array with a different combination of dimensions from the one initially produced.\n\n\n\nThe key difference between vectors, arrays, and a matrices in R is their dimensions. A vector has one dimension, an array can have any number of dimensions, while a matrix is limited to two dimensions. Additionally, a matrix is often used to store data in a tabular format, while an array is used to store multi-dimensional data in general. A commonly encountered kind of matrix is seen in multivariate statistics is a distance or dissimilarity matrix.\nIn R, vectors, arrays, and matrices share a common characteristic: they do not have row or column names. Therefore, to refer to any element, row, or column, one must use their corresponding index. How?\nAccessing elements, rows, columns, and matrices In R, the square bracket notation is used to access elements, rows, columns, or matrices in arrays. The notation takes the form of [i, j, k, ...], where i, j, k, and so on, represent the indices of the rows, columns, or matrices to be accessed.\nSuppose we have the following array:\n\n\nmy_array &lt;- array(data = round(rnorm(n = 60, mean = 13, sd = 2), 1),\n                  dim = c(5, 4, 3))\nmy_array\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,] 15.4 11.5 11.3 11.4\n[2,] 13.7 14.1 12.9 13.9\n[3,] 11.3 12.1 14.5 11.7\n[4,] 13.1 11.2  9.6 11.5\n[5,]  8.4  9.9 11.8 13.6\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,] 14.0 12.7 12.5 14.2\n[2,] 10.9 11.5 14.4 13.8\n[3,] 14.1 10.6 16.2 12.4\n[4,] 10.2 11.4 11.6 14.8\n[5,] 14.4 14.2 11.2 14.7\n\n, , 3\n\n     [,1] [,2] [,3] [,4]\n[1,] 12.5 13.3 11.7 14.0\n[2,] 14.7 13.5 13.7 13.6\n[3,] 12.3 13.7 12.2 12.5\n[4,] 14.0 14.0 13.1 13.2\n[5,] 13.8 13.2 14.6 16.8\n\ndim(my_array)\n\n[1] 5 4 3\n\n\nThis creates a \\(5\\times4\\times3\\) array with values from 1 to 60.\nWhen working with multidimensional arrays, it is possible to omit some of the indices in the square bracket notation. This results in a subset of the array, which can be thought of as a lower-dimensional array obtained by fixing the omitted dimensions. For example, consider a 3-dimensional array my_array above with dimensions dim(my_array) = c(5,4,3). If we use the notation my_array[1,,], we would obtain a 2-dimensional array with dimensions dim(my_array[1,,]) = c(4,3) obtained by fixing the first index at 1:\n\ndim(my_array[1,,])\n\n[1] 4 3\n\nmy_array[1,,]\n\n     [,1] [,2] [,3]\n[1,] 15.4 14.0 12.5\n[2,] 11.5 12.7 13.3\n[3,] 11.3 12.5 11.7\n[4,] 11.4 14.2 14.0\n\n\nHere are some more examples of how to use square brackets notation with arrays in R:\nTo access a single element in the array, use the notation [i, j, k], where i, j, and k are the indices along each of the three dimensions, which in combination, uniquely identifies each element. Below we return the element in the second row, third column, and first matrix:\n\nmy_array[2, 3, 1]  \n\n[1] 12.9\n\n\nTo access a single row in the array, use the notation [i, , ], where i is the index of the row. This will return the second rows and all of the columns of the first matrix:\n\nmy_array[2,,1]\n\n[1] 13.7 14.1 12.9 13.9\n\n\nTo access a single column in the array, use the notation [ , j, ], where j is the index of the column. Here we will return all the elements in the row of column two and matrix three:\n\nmy_array[ , 2, 3]\n\n[1] 13.3 13.5 13.7 14.0 13.2\n\n\nTo access a single matrix in the array, use the notation [ , , k], where k is the index of the matrix:\n\nmy_array[ , , 2]\n\n     [,1] [,2] [,3] [,4]\n[1,] 14.0 12.7 12.5 14.2\n[2,] 10.9 11.5 14.4 13.8\n[3,] 14.1 10.6 16.2 12.4\n[4,] 10.2 11.4 11.6 14.8\n[5,] 14.4 14.2 11.2 14.7\n\n\nTo obtain a subset of the array, use the notation [i, j, k] with i, j, or k omitted to obtain a lower-dimensional array:\n\nmy_array[1, , ]\n\n     [,1] [,2] [,3]\n[1,] 15.4 14.0 12.5\n[2,] 11.5 12.7 13.3\n[3,] 11.3 12.5 11.7\n[4,] 11.4 14.2 14.0\n\nmy_array[ , 2:3, ]\n\n, , 1\n\n     [,1] [,2]\n[1,] 11.5 11.3\n[2,] 14.1 12.9\n[3,] 12.1 14.5\n[4,] 11.2  9.6\n[5,]  9.9 11.8\n\n, , 2\n\n     [,1] [,2]\n[1,] 12.7 12.5\n[2,] 11.5 14.4\n[3,] 10.6 16.2\n[4,] 11.4 11.6\n[5,] 14.2 11.2\n\n, , 3\n\n     [,1] [,2]\n[1,] 13.3 11.7\n[2,] 13.5 13.7\n[3,] 13.7 12.2\n[4,] 14.0 13.1\n[5,] 13.2 14.6"
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#data.frame",
    "href": "BCB744/basic_stats/01-data-in-R.html#data.frame",
    "title": "1. Data classes and structures in R",
    "section": "\n4.2 data.frame\n",
    "text": "4.2 data.frame\n\nA dataframe is perhaps the most commonly-used ‘container’ for data in R because they are so convenient and serve many purposes. A dataframe is not a data class—more correctly, it is a form of tabular data (like a table in MS Excel), with each vector (a variable or column) comprising the table sharing the same length. What makes a dataframe versatile is that its variables can be any combination of the atomic data types. It may even include list columns (we will not cover list columns in this module). Applying the class() function to a dataframe shows that it blongs to class data.frame.\nHere’s an example of an R data.frame with Date, numeric, and categorical data classes:\n\n# Create a vector of dates\ndates &lt;- as.Date(c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\",\n                   \"2022-01-04\", \"2022-01-05\"))\n\n# Create a vector of numeric data\nnumeric_data &lt;- rnorm(n = 5, mean = 0, sd = 1)\n\n# Create a vector of categorical data\ncategorical_data &lt;- c(\"A\", \"B\", \"C\", \"A\", \"B\")\n\n# Combine the vectors into a data.frame\nmy_dataframe &lt;- data.frame(dates = dates,\n                           numeric_data = numeric_data,\n                           categorical_data = categorical_data)\n\n# Print the dataframe\nmy_dataframe\n\n       dates numeric_data categorical_data\n1 2022-01-01    0.6338448                A\n2 2022-01-02    0.6474752                B\n3 2022-01-03    0.1404013                C\n4 2022-01-04    1.5652625                A\n5 2022-01-05    0.3266074                B\n\nclass(my_dataframe)\n\n[1] \"data.frame\"\n\nstr(my_dataframe)\n\n'data.frame':   5 obs. of  3 variables:\n $ dates           : Date, format: \"2022-01-01\" \"2022-01-02\" ...\n $ numeric_data    : num  0.634 0.647 0.14 1.565 0.327\n $ categorical_data: chr  \"A\" \"B\" \"C\" \"A\" ...\n\nsummary(my_dataframe)\n\n     dates             numeric_data    categorical_data  \n Min.   :2022-01-01   Min.   :0.1404   Length:5          \n 1st Qu.:2022-01-02   1st Qu.:0.3266   Class :character  \n Median :2022-01-03   Median :0.6338   Mode  :character  \n Mean   :2022-01-03   Mean   :0.6627                     \n 3rd Qu.:2022-01-04   3rd Qu.:0.6475                     \n Max.   :2022-01-05   Max.   :1.5653                     \n\n\nDataframes may also have row names:\n\nrownames(my_dataframe) &lt;- paste(rep(\"row\", 5), seq = 1:5)\nmy_dataframe\n\n           dates numeric_data categorical_data\nrow 1 2022-01-01    0.6338448                A\nrow 2 2022-01-02    0.6474752                B\nrow 3 2022-01-03    0.1404013                C\nrow 4 2022-01-04    1.5652625                A\nrow 5 2022-01-05    0.3266074                B\n\n\nTypically we will create a dataframe by reading in data from a .csv file, but it is useful to be able to construct one from scratch."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#tibble",
    "href": "BCB744/basic_stats/01-data-in-R.html#tibble",
    "title": "1. Data classes and structures in R",
    "section": "\n4.3 tibble\n",
    "text": "4.3 tibble\n\nIn R, a dataframe and a tibble are both data structures used to store tabular data. Although tibbles are also dataframes, but they differ subtly in several ways.\n\nA tibble is a relatively new addition to the R language and forms part of the tidyverse suite of packages. They are designed to be more user-friendly than traditional data frames and have several additional features, such as more informative error messages, stricter data input and output rules, and better handling of NA.\nUnlike a dataframe, a tibble never automatically converts strings to factors or changes column names, which can help avoid unexpected behavior when working with the data.\nA tibble does not have row names.\nA tibble has a slightly different and more compact printing method than a dataframe, which makes them easier to read and work with.\nFinally, a tibble has better performance than dataframes for many tasks, especially when working with large datasets.\n\nWhile a dataframe is a core data structure in R, a tibble provides additional functionality and are becoming increasingly popular among R users, particularly those working with tidyverse packages. Applying the class() function to a tibble revelas that it belongs to the classes tbl_df, tbl and data.frame.\nWe can convert our dataframe my_dataframe to a tibble, and present the output with the print() function that applies nicely to tibbles:\n\nlibrary(tidyverse) # we need to load the tidyverse package\nmy_tibble &lt;- as_tibble(my_dataframe)\nclass(my_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(my_tibble)\n\n# A tibble: 5 × 3\n  dates      numeric_data categorical_data\n  &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;           \n1 2022-01-01        0.634 A               \n2 2022-01-02        0.647 B               \n3 2022-01-03        0.140 C               \n4 2022-01-04        1.57  A               \n5 2022-01-05        0.327 B               \n\n\nThis very simple tibble looks identical to a dataframe, but as we start using more complex sets of data you’ll learn to appreciate the small convenience that tibbles offer."
  },
  {
    "objectID": "BCB744/basic_stats/01-data-in-R.html#list",
    "href": "BCB744/basic_stats/01-data-in-R.html#list",
    "title": "1. Data classes and structures in R",
    "section": "\n4.4 list\n",
    "text": "4.4 list\n\nThis is also not actually a data class, but rather another way of representing a collection of objects of different types, all the way from numerical vectors to dataframes. Lists are useful for storing complex data structures and can also be accessed using indexing.\nAs an example, we create another dataframe:\n\ndates &lt;- as.Date(c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\",\n                   \"2022-01-04\", \"2022-01-05\"))\n\n# Create a vector of numeric data\nnumeric_data &lt;- rnorm(n = 5, mean = 1, sd = 1)\n\n# Create a vector of categorical data\ncategorical_data &lt;- c(\"C\", \"D\", \"D\", \"F\", \"A\")\n\n# Combine the vectors into a data.frame\nmy_other_dataframe &lt;- data.frame(dates = dates,\n                                  numeric_data = numeric_data,\n                                  categorical_data = categorical_data)\n\nmy_list &lt;- list(A = my_dataframe,\n                B = my_other_dataframe)\nmy_list\n\n$A\n           dates numeric_data categorical_data\nrow 1 2022-01-01    0.6338448                A\nrow 2 2022-01-02    0.6474752                B\nrow 3 2022-01-03    0.1404013                C\nrow 4 2022-01-04    1.5652625                A\nrow 5 2022-01-05    0.3266074                B\n\n$B\n       dates numeric_data categorical_data\n1 2022-01-01    0.6101902                C\n2 2022-01-02    1.1183232                D\n3 2022-01-03    0.7839694                D\n4 2022-01-04    0.6123027                F\n5 2022-01-05    1.6143559                A\n\nclass(my_list)\n\n[1] \"list\"\n\nstr(my_list)\n\nList of 2\n $ A:'data.frame':  5 obs. of  3 variables:\n  ..$ dates           : Date[1:5], format: \"2022-01-01\" \"2022-01-02\" ...\n  ..$ numeric_data    : num [1:5] 0.634 0.647 0.14 1.565 0.327\n  ..$ categorical_data: chr [1:5] \"A\" \"B\" \"C\" \"A\" ...\n $ B:'data.frame':  5 obs. of  3 variables:\n  ..$ dates           : Date[1:5], format: \"2022-01-01\" \"2022-01-02\" ...\n  ..$ numeric_data    : num [1:5] 0.61 1.118 0.784 0.612 1.614\n  ..$ categorical_data: chr [1:5] \"C\" \"D\" \"D\" \"F\" ...\n\n\nWe can access one of the dataframes is the list as follows:\n\nmy_list[[2]]\n\n       dates numeric_data categorical_data\n1 2022-01-01    0.6101902                C\n2 2022-01-02    1.1183232                D\n3 2022-01-03    0.7839694                D\n4 2022-01-04    0.6123027                F\n5 2022-01-05    1.6143559                A\n\nmy_list[[\"A\"]]\n\n           dates numeric_data categorical_data\nrow 1 2022-01-01    0.6338448                A\nrow 2 2022-01-02    0.6474752                B\nrow 3 2022-01-03    0.1404013                C\nrow 4 2022-01-04    1.5652625                A\nrow 5 2022-01-05    0.3266074                B\n\n\nTo access a variable within one of the elements of the list we can do something like:\n\nmy_list[[\"B\"]]$numeric_data\n\n[1] 0.6101902 1.1183232 0.7839694 0.6123027 1.6143559\n\n\n\n\n\n\n\n\nTask A\n\n\n\n\nFind five datasets that you like the look and content of. They may be some of the datasets built into R (and the various packages you downloaded), or they may be ones you found somewhere else. For each:\n\n\ndescribe the data types (statistical view) of the variables contained within,\nusing the function show in the Chapter, describe their R data classes of each variable, and\nusing the functions shown in the Chapter, describe their data structures."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html",
    "href": "BCB744/basic_stats/09-regressions.html",
    "title": "9. Simple linear regressions",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nThe simple linear regression\nThe model coefficients\nGraphing linear regressions\nConfidence intervals\nPrediction intervals\nModel fit diagnostics"
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#the-intercept",
    "href": "BCB744/basic_stats/09-regressions.html#the-intercept",
    "title": "9. Simple linear regressions",
    "section": "\n5.1 The intercept",
    "text": "5.1 The intercept\nThe intercept (more precisely, the \\(y\\)-intercept, \\(\\alpha\\)) is the best estimate of the starting point of the fitted line on the left hand side of the graph where it crosses the \\(y\\)-axis. You will notice that there is also an estimate for the standard error of the estimate for the intercept.\nThere are several hypothesis tests associated with a simple linear regression. All of them assume that the residual error, \\(\\epsilon\\), in the linear regression model is independent of \\(X\\) (i.e. nothing about the structure of the error term can be inferred based on a knowledge of \\(X\\)), is normally distributed, with zero mean and constant variance. We say the residuals are i.i.d. (independent and identically distributed, which is a fancy way of saying they are random).\nOne of the tests looks at the significance of the intercept, i.e. it tests the H0 that \\(\\alpha=0\\). Is the value of the \\(y\\)-intercept zero? Rejecting this H0 causes the alternate hypothesis of \\(\\alpha \\neq 0\\) to be accepted. This test is automatically performed when fitting a linear model in R and asking for a summary of the regression object, but it is insightful and important to know that the test is simply a one-sample t-test. In the sparrows data, this statistic is in the Coefficients table in the row indicated by (Intercept) under the Pr(&gt;|t|) column."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#the-regression-coefficient",
    "href": "BCB744/basic_stats/09-regressions.html#the-regression-coefficient",
    "title": "9. Simple linear regressions",
    "section": "\n5.2 The regression coefficient",
    "text": "5.2 The regression coefficient\nThe interpretation of the regression coefficient, \\(\\beta\\), is simple. For every one unit of change in the independent variable (here waiting time) there is a corresponding average change in the dependent variable (here the duration of the eruption). This is the slope or gradient, and it may be positive or negative. In the example the slope of the line is denoted by the value 0.27 \\(cm.day^{-1}\\) in the column termed Estimate and in the row called age (the latter name will of course depend on the name of the response column in your dataset). The coefficient of determination (see Section 7.2) multiplies the response variable to produce a prediction of the response based on the slope of the relationship between the response and the predictor. It tells us how much one unit in change of the independent variable determines the corresponding change in the response variable. There is also a standard error for the estimate.\nThe second hypothesis test performed when fitting a linear regression model concerns the regression coefficient. It looks for whether there is a significant relationship (slope) of \\(Y\\) on \\(X\\) by testing the H0 that \\(\\beta=0\\). As before, this is also simply a one-sample t-test. In the regression summary the probability associated with this test is given in the Coefficients table in the column called Pr(&gt;|t|) in the row age. In the sparrows data, the p-value associated with wing is less than 0.05 and we therefore reject the H0 that \\(\\beta=0\\). So, there is a significant linear relationship of eruption duration on the waiting time between eruptions."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#residual-standard-error-rse-and-root-mean-square-error-rmse",
    "href": "BCB744/basic_stats/09-regressions.html#residual-standard-error-rse-and-root-mean-square-error-rmse",
    "title": "9. Simple linear regressions",
    "section": "\n7.1 Residual standard error (RSE) and root mean square error (RMSE)",
    "text": "7.1 Residual standard error (RSE) and root mean square error (RMSE)\nThe residual standard error (RSE) is a measure of the average amount that the response variable deviates from the regression line. It is calculated as the square root of the residual sum of squares divided by the degrees of freedom (Equation 3).\n\n\nThe RSE: \\[RSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2}{n-2}} \\tag{3}\\]\nwhere \\(y_i\\) represents the observed value of the dependent variable for the \\(i\\)-th observation, \\(\\hat{y}_i\\) represents the predicted value of the dependent variable for the \\(i\\)-th observation, and n is the number of observations in the sample.\nThe root mean square error (RMSE) is a similar measure, but it is calculated as the square root of the mean of the squared residuals. It is a measure of the standard deviation of the residuals (Equation 4).\n\n\nThe RMSE: \\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2} \\tag{4}\\] where the model components are as in Equation 3.\nRSE and RMSE are similar but different. There is a small difference in how they are calculated. The RSE takes into account the degrees of freedom which becomes important when models with different numbers of variables are compared. The RMSE is more commonly used in machine learning and data mining, where the focus is on prediction accuracy rather than statistical inference.\nBoth the RSE and RMSE provide information about the amount of error in the model predictions, with smaller values indicating a better fit. However, both may be influenced by outliers or other sources of variability in the data. Use a variety of means to assess the model fit diagnostics."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#sec-coef-det",
    "href": "BCB744/basic_stats/09-regressions.html#sec-coef-det",
    "title": "9. Simple linear regressions",
    "section": "\n7.2 R-squared (R2)",
    "text": "7.2 R-squared (R2)\nThe coefficient of determination, the \\(R^{2}\\), of a linear model is the quotient of the variances of the fitted values, \\(\\hat{y_{i}}\\), and observed values, \\(y_{i}\\), of the dependent variable. If the mean of the dependent variable is \\(\\bar y\\), then the \\(R^{2}\\) is as shown in Equation 5.\n\n\n\n\n\n\nThe R2: \\[R^{2}=\\frac{\\sum(\\hat{Y_{i}} - \\bar{Y})^{2}}{\\sum(Y_{i} - \\bar{Y})^{2}} \\tag{5}\\]\n\n\n\n\n\nFigure 3: A linear regression through random normal data.\n\n\nSimply put, the \\(R^{2}\\) is a measure of the proportion of the variation in the dependent variable that is explained (can be predicted) by the independent variable(s) in the model. It ranges from 0 to 1, with a value of 1 indicating a perfect fit (i.e. a scatter of points to denote the \\(Y\\) vs. \\(X\\) relationship will all fall perfectly on a straight line). It gives us an indication of how well the observed outcome variable is predicted by the observed influential variable, and in the case of a simple linear regression, that the geometric relationship of \\(Y\\) on \\(X\\) is a straight line. For example, in Figure 3 there is absolutely no relationship of \\(y\\) on \\(x\\). Here, the slope is 0.001 and the \\(R^{2}\\) is 0.\nNote, however, that a high \\(R^{2}\\) does not necessarily mean that the model is a good fit; it may also suggest that the model is unduly influenced by outliers or the inclusion of irrelevant variables. Expert knowledge will help with the interpretation of the \\(R^{2}\\).\n\n\nRegressions may take on any relationship, not only a linear one. For example, there are parabolic, hyperbolic, logistic, exponential, etc. relationships of \\(Y\\) on \\(X\\), and here, too, does \\(R^{2}\\) tell us the same thing. If we assume that the samples were representatively drawn from a population (i.e. the sample fully captures the relationship of \\(Y\\) on \\(X\\) that is present in the entire population), the \\(R^{2}\\) will represent the relationship in the population too.\n\nIn the case of our sparrows data, the \\(R^{2}\\) is 0.973, meaning that the proportion of variance explained is 97.3%; the remaining 2.7% is not (yet) accounted for by the linear relationship. Adding more predictors into the regression (i.e. a multiple regression) might consume some of the unexplained variance and increase the overall \\(R^{2}\\).\nSometimes you will also see something called the adjusted \\(R^{2}\\). This is a modified version of \\(R^{2}\\) that takes into account the number of independent variables in the model. It penalises models that include too many variables that do not improve the fit. Generally this is not something to be too concerned with in linear models that have only one independent variable, such as the models seen in this Chapter."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#f-statistic",
    "href": "BCB744/basic_stats/09-regressions.html#f-statistic",
    "title": "9. Simple linear regressions",
    "section": "\n7.3 F-statistic",
    "text": "7.3 F-statistic\nThe F-statistic (or F-value) is another measure of the overall significance of the model. It is used to test whether at least one of the independent variables in the model has a non-zero coefficient, indicating that it has a significant effect on the dependent variable.\nIt is calculated by taking the ratio of the mean square regression (MSR) to the mean square error (MSE) (Equation 6). The MSR measures the variation in the dependent variable that is explained by the independent variables in the model, while the MSE measures the variation in the dependent variable that is not explained by the independent variables.\n\n\nCalculating the F-statistic: \\[MSR = \\frac{\\sum_{i=1}^{n}(\\hat{Y}_i - \\bar{Y})^2}{1}\\] \\[MSE = \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{n-2}\n\\] \\[F = \\frac{MSR}{MSE} \\tag{6}\\]\nwhere the model components are as in Equation 3.\nIf the F-statistic is large and the associated p-value is small (typically less than 0.05), it indicates that at least one of the independent variables in the model has a significant effect on the dependent variable. In other words, the H0 that all the independent variables have zero coefficients can be rejected in favour of the Ha that at least one independent variable has a non-zero coefficient.\nNote that a significant F-statistic does not necessarily mean that all the independent variables in the model are significant. Additional diagnostic tools, such as individual t-tests and residual plots, should be used to determine which independent variables are significant and whether the model is a good fit for the data.\nFortunately, in this Chapter we will encounter linear regressions with only one independent variable. The situation where we deal with multiple independent variables is called multiple regression. We will encounter some multiple regression type models in Quantitative Ecology.\n\n\n\n\n\n\nTask G\n\n\n\n\nExamine the content of the regression model object eruption.lm. Explain the meaning of the recognisable (because they were discussed in the Chapter) components within, and tell us how they relate to the model summary produced by summary(eruption.lm).\nUsing the values inside of the model object, write some R code to show how you can reconstruct the observed values for the dependent variable from the residuals and the fitted values.\nFit a linear regression through the model residuals (use eruptions.lm). Explain your findings.\nSimilarly, fit a linear regression through the the fitted values. Explain."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#plot-of-residuals-vs.-fitted-values",
    "href": "BCB744/basic_stats/09-regressions.html#plot-of-residuals-vs.-fitted-values",
    "title": "9. Simple linear regressions",
    "section": "\n11.1 Plot of residuals vs. fitted values",
    "text": "11.1 Plot of residuals vs. fitted values\nA residual plot shows the residuals (values predicted by the linear model, \\(\\hat{Y}\\), minus the observed values, \\(Y\\), on the y-axis and the independent (\\(X\\)) variable on the x-axis. Points in a residual plot that are randomly dispersed around the horizontal axis indicates a linear regression model that is appropriate for the data. If this simple ‘test’ fails, a non-linear model might be more appropriate, or one might transform the data to normality (assuming that the non-normality of the data is responsible for the non-random dispersal above and below the horizontal line)."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#plot-of-standardised-residuals",
    "href": "BCB744/basic_stats/09-regressions.html#plot-of-standardised-residuals",
    "title": "9. Simple linear regressions",
    "section": "\n11.2 Plot of standardised residuals",
    "text": "11.2 Plot of standardised residuals\nWe may use a plot of the residuals vs. the fitted values, which is helpful for detecting heteroscedasticity—e.g. a systematic change in the spread of residuals over a range of predicted values."
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#normal-probability-plot-of-residuals-normal-q-q-plot",
    "href": "BCB744/basic_stats/09-regressions.html#normal-probability-plot-of-residuals-normal-q-q-plot",
    "title": "9. Simple linear regressions",
    "section": "\n11.3 Normal probability plot of residuals (Normal Q-Q plot)",
    "text": "11.3 Normal probability plot of residuals (Normal Q-Q plot)\nLet see all these plots in action for the sparrows data. The package ggfortify has a convenient function to automagically make all of these graphs:\n\nlibrary(ggfortify)\nautoplot(lm(wing ~ age, data = sparrows), label.size = 3,\n         col = \"red3\", shape = 10, smooth.colour = 'blue3')\n\n\n\n\nFigure 4: Four diagnostic plots testing the assumptions to be met for linear regressions.\n\n\nOne might also use the package gg_diagnose to create all the various (above plus some!) diagnostic plots available for fitted linear models.\nDiagnostic plots will be further explored in the exercises (see below).\n\n\n\n\n\n\n\n\n\nTask G\n\n\n\n\nFind your own two datasets and do a full regression analysis on it. Briefly describe the data and the reason for their existence. Start with setting the appropriate hypotheses. Follow with an EDA, make some exploratory figures, fit the linear model, make a figure with the fitted linear model, provide diagnostic plots to test assumptions, and present the output in a Results section suitable for publication."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html",
    "href": "BCB744/basic_stats/06-assumptions.html",
    "title": "6. Assumptions for parametric statistics",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nRevisiting assumptions\nNormality and homogeneity of variance tests\nRevisiting the non-parametric tests\nLog transformation\nSquare-root transformation\nArcsine transformation\nPower transformation\nLesser-used transformation"
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#tests-for-normality",
    "href": "BCB744/basic_stats/06-assumptions.html#tests-for-normality",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n2.1 Tests for normality",
    "text": "2.1 Tests for normality\n\n\n\n\n\nFigure 1: Histograms showing two randomly generated normal distributions.\n\n\nRemember from Chapter 4 what a normal distribution is/looks like? Let’s have a peek below to remind ourselves (Figure 1).\nWhereas histograms may be a pretty way to check the normality of our data, there are actual statistical tests for this, which is preferable to a visual inspection alone. But remember that you should always visualise your data before performing any statistics on them.\n\n\n\n\n\n\nHypothesis for normailty\n\n\n\n\\(H_{0}\\): The distribution of our data is not different from normal (or, the variable is normally distributed).\n\n\nThe Shapiro-Wilk test is frequently used to assess the normality of a dataset. It is known to have good power and accuracy for detecting departures from normality, even for small sample sizes, and it is also robust to outliers, making it useful for analysing data that may contain extreme values.\nIt tests the H0 that the population from which the sample, \\(x_{1},..., x_{n}\\), was drawn is not significantly different from normal. The test does so by sorting the data from lowest to highest, and a test statistic, \\(W\\), is calculated based on the deviations of the observed values from the expected values under a normal distribution (Equation 1). \\(W\\) is compared to a critical value, based on the sample size and significance level, to determine whether to reject or fail to reject the H0.\n\n\nThe Shapiro-Wilk test: \\[W = \\frac{(\\sum_{i=1}^n a_i x_{(i)})^2}{\\sum_{i=1}^n (x_i - \\overline{x})^2} \\tag{1}\\]\nHere, \\(W\\) represents the Shapiro-Wilk test statistic, \\(a_{i}\\) are coefficients that depend on the sample size and distribution of the data, \\(x_{(i)}\\) represents the \\(i\\)-th order statistic, or the \\(i\\)-th smallest value in the sample, and \\(\\overline{x}\\) represents the sample mean.\nThe Shapiro-Wilk test is available within base R as the function shapiro.test(). If the p-value is above 0.05 we may assume the data to be normally distributed. In order to demonstrate what the output of shapiro.test() looks like we will run it on all of the random data we generated.\n\nshapiro.test(r_dat$dat)\n\n\n    Shapiro-Wilk normality test\n\ndata:  r_dat$dat\nW = 0.9942, p-value = 4.649e-07\n\n\nNote that this shows that the data are not normally distributed. This is because we have incorrectly run this function simultaneously on two different samples of data. To perform this test correctly, and in the tidy way, we need to recognise the grouping structure (Groups A and B) and select only the second piece of information from the shapiro.test() output and ensure that it is presented as a numeric value:\n\n# we use the square bracket notation to select only the *-value;\n# had we used `[1]` we'd have gotten W\nr_dat %&gt;% \n  group_by(sample) %&gt;% \n  summarise(norm_dat = as.numeric(shapiro.test(dat)[2]))\n\n# A tibble: 2 × 2\n  sample norm_dat\n  &lt;chr&gt;     &lt;dbl&gt;\n1 A         0.375\n2 B         0.461\n\n\nNow we see that our two sample sets are indeed normally distributed.\nSeveral other tests are available to test whether our data are consistent with a normal distribution:\n\nKolmogorov-Smirnov test This test is a non-parametric test that compares the empirical distribution of a sample with a hypothesised normal distribution. It is based on the maximum absolute difference between the cumulative distribution function of the sample and the theoretical normal distribution function. This test can also be used to see if one’s own data are consistent with other kinds of data distributions. In R the Kolmogorov-Smirnov test is available as ks.test().\nAnderson-Darling test Similar to the Shapiro-Wilk test, the Anderson-Darling test is used to test the hypothesis that a sample comes from a normal (or any other) distribution. It is based on the squared differences between the empirical distribution function of the sample and the theoretical normal distribution function. This function is not natively available in base R but the function ad.test() is made available in two packages (that I know of), namely, nortest and kSamples. Read the help files—even though the name of the function is the same in the two packages, they are implemented differently.\nLilliefors test This test is a modification of the Kolmogorov-Smirnov test that is specifically designed for small sample sizes. It is based on the maximum difference between the empirical distribution function of the sample and the normal distribution function. Some R packages such as nortest and descTools seem to use Lilliefors synonymously with Kolmogorov-Smirnov. These functions are called lillie.test() and LillieTest(), respectively.\nJarque-Bera test This test is based on the skewness and kurtosis of a sample and tests whether the sample has the skewness and kurtosis expected from a normal distribution. Find it in R as jarque.bera.test() in the DescTools and tseries packages. Again, read the help files as a function with the same name appears in two independent packages and I cannot give assurance that it implemented consistently.\nCramer-Von Mises test The Cramer-Von Mises test is used to assess the goodness of fit of a distribution to a sample of data. The test is based on the cumulative distribution function (CDF) of the sample and the theoretical distribution being tested. See the cvm.test() function in the goftest package.\n\nTake your pick. The Shapiro-Wilk and Kolmogorov-Smirnov tests are the most frequently used normality tests in my experience but be adventurous and use the Cramer-Von Mises test and surprise your supervisor in an interesting way—more than likely, they will not have heard of it before. When you decide, however, do your homework and read about these pros and cons of the tests as they are not all equally robust to all the surprises data can throw at them."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#sec-homogeneity",
    "href": "BCB744/basic_stats/06-assumptions.html#sec-homogeneity",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n2.2 Tests for homogeneity of variances",
    "text": "2.2 Tests for homogeneity of variances\nBesides requiring that our data are normally distributed, we must also ensure that they are homoscedastic. This word means that the scedasticity (variance) of our samples is homogeneous (similar). In practical terms this means that the variance of the samples we are comparing should not be more than two to four times greater than one another. In R, we use the function var() to check the variance in a sample:\n\nr_dat %&gt;% \n  group_by(sample) %&gt;% \n  summarise(sample_var = var(dat))\n\n# A tibble: 2 × 2\n  sample sample_var\n  &lt;chr&gt;       &lt;dbl&gt;\n1 A            8.72\n2 B            3.97\n\n\nAbove we see that the variance of our two samples is homoscedastic because the variance of one is not more than two to four times greater than the other. However, there are formal tests to establish the equality of variances, as we can see in the following hypothesis tests:\n\n\n\n\n\n\nHypotheses for equality of variances\n\n\n\nThe two-sided and one-sided formulations are:\n\\(H_{0}: \\sigma^{2}_{A} = \\sigma^{2}_{B}\\) and \\(H_{a}: \\sigma^{2}_{A} \\ne \\sigma^{2}_{B}\\)\n\\(H_{0}: \\sigma^{2}_{A} \\le \\sigma^{2}_{B}\\) and \\(H_{a}: \\sigma^{2}_{A} \\gt \\sigma^{2}_{B}\\)\n\\(H_{0}: \\sigma^{2}_{A} \\ge \\sigma^{2}_{B}\\) and \\(H_{a}: \\sigma^{2}_{A} \\lt \\sigma^{2}_{B}\\)\nwhere \\(\\sigma^{2}_{A}\\) and \\(\\sigma^{2}_{B}\\) are the variances for samples \\(A\\) and \\(B\\), respectively.\n\n\nThe most commonly used test for equality of variances is Levene’s test. Levene’s test assess the equality of variances between two or more groups in a dataset. The H0 is that the variances of the groups are equal. It is a non-parametric test that does not assume anything about the data’s normality and as such it is more robust than the F-test.\nThe test is commonly used in t-tests and ANOVA to check that the variances of the dependent variable are the same across all levels of the independent variable. Violating this assumption can lead to incorrect conclusions made from the test outcome, such as those resulting from Type I and Type II errors.\nIn Levene’s test, the absolute deviations of the observations from their group medians are calculated, and the test statistic is computed as the ratio of the sum of the deviations to the degrees of freedom (Equation 2). The test statistic follows an F distribution under the H0, and a significant result indicates that the variances of the groups are significantly different.\n\n\nLevene’s test:\n\\[W = \\frac{(N-k)}{(k-1)} \\cdot \\frac{\\sum_{i=1}^k n_i (\\bar{z}_i - \\bar{z})^2}{\\sum_{i=1}^k \\sum_{j=1}^{n_i} (z_{ij} - \\bar{z}_i)^2} \\tag{2}\\]\nwhere \\(W\\) represents the Levene’s test statistic, \\(N\\) is the total sample size, \\(k\\) is the number of groups being compared, \\(n_i\\) is the sample size of the \\(i\\)-th group, \\(z_{ij}\\) is the \\(j\\)-th observation in the \\(i\\)-th group, \\(z_{i}\\) is the mean of the ith group, and \\(\\bar{z}\\) is the overall mean of all groups combined.\nThe test statistic is calculated by comparing the deviations of the observations within each group from their group mean (\\(\\bar{z}_i\\)) to the deviations of the group means from the overall mean (\\(\\bar{z}\\)).\nLevene’s test is considered robust to non-normality and outliers, making it a useful tool for analysing data that do not meet the assumptions of normality, but it can be sensitive to unequal sample sizes and may not be appropriate for very small sample sizes.\nSeveral other statistical tests are available to assess the homogeneity of variances in a dataset:\n\nF-test This test is also known as the variance ratio test. It assumes that the underlying data follows a normal distribution and is designed to test the H0 that the variances of two populations are equal. The test statistic is the ratio of the variances of the two populations. You will often see this test used in the context of an ANOVA to test for homogeneity of variance across groups.\nBartlett’s test This test is similar to Levene’s test and is used to assess the equality of variances across multiple groups. The test is based on the \\(\\chi\\)-squared distribution and assumes that the data are normally distributed. Base R has the bartlett.test() function.\nBrown-Forsythe test This test is a modification of the Levene’s test that uses the absolute deviations of the observations from their respective group medians instead of means. This makes the test more robust to outliers and non-normality. It is available in onewaytests as the function bf.test().\nFligner-Killeen test This is another non-parametric test that uses the medians of the groups instead of the means. It is based on the \\(\\chi\\)-squared distribution and is also robust to non-normality and outliers. The Fligner test is available in Base R as fligner.test().\n\nAs always, supplement your analysis with these checks: i) perform any of the diagnostic plots we covered in the earlier Chapters, or ii) compare the variances and see if they differ by more than a factor of four.\nSee this discussion if you would like to know about some more advanced options when faced with heteroscedastic data."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#log-transformation",
    "href": "BCB744/basic_stats/06-assumptions.html#log-transformation",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n4.1 Log transformation",
    "text": "4.1 Log transformation\nLog transformation is often applied to positively skewed data. It consists of taking the log of each observation. You can use either base-10 logs (log10(x)) or base-\\(e\\) logs, also known as natural logs (log(x)). It makes no difference for a statistical test whether you use base-10 logs or natural logs, because they differ by a constant factor; the base- 10 log of a number is just 2.303…× the natural log of the number. You should specify which log you’re using when you write up the results, as it will affect things like the slope and intercept in a regression. I prefer base-10 logs, because it’s possible to look at them and see the magnitude of the original number: \\(log(1) = 0\\), \\(log(10) = 1\\), \\(log(100) = 2\\), etc.\nThe back transformation is to raise 10 or \\(e\\) to the power of the number; if the mean of your base-10 log-transformed data is 1.43, the back transformed mean is \\(10^{1.43} = 26.9\\) (in R, 10^1.43). If the mean of your base-\\(e\\) log-transformed data is 3.65, the back transformed mean is \\(e^{3.65} = 38.5\\) (in R, exp(3.65)). If you have zeros or negative numbers, you can’t take the log; you should add a constant to each number to make them positive and non-zero (i.e. log10(x) + 1). If you have count data, and some of the counts are zero, the convention is to add 0.5 to each number.\nMany variables in biology have log-normal distributions, meaning that after log-transformation, the values are normally distributed. This is because if you take a bunch of independent factors and multiply them together, the resulting product is log-normal. For example, let’s say you’ve planted a bunch of weed seeds, then 10 years later you see how tall the trees are. The height of an individual tree would be affected by the nitrogen in the soil, the amount of water, amount of sunlight, amount of insect damage, etc. Having more nitrogen might make a tree 10% larger than one with less nitrogen; the right amount of water might make it 30% larger than one with too much or too little water; more sunlight might make it 20% larger; less insect damage might make it 15% larger, etc. Thus the final size of a tree would be a function of nitrogen × water × sunlight × insects, and mathematically, this kind of function turns out to be log-normal."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#arcsine-transformation",
    "href": "BCB744/basic_stats/06-assumptions.html#arcsine-transformation",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n4.2 Arcsine transformation",
    "text": "4.2 Arcsine transformation\nArcsine transformation is commonly used for proportions, which range from 0 to 1, or percentages that go from 0 to 100. Specifically, this transformation is quite useful when the data follow a binomial distribution and have extreme proportions close to 0 or 1.\nA biological example of the type of data suitable for arcsine transformation is the proportion of offspring that survives or the proportion of plants that succumbs to a disease; such data often follow a binomial distribution.\nThis transformation involves of taking the arcsine of the square root of a number (in R, arcsin(sqrt(x))). (The result is given in radians, not degrees, and can range from −π/2 to π/2). The numbers to be arcsine transformed must be in the range 0 to 1. […] the back-transformation is to square the sine of the number (in R, sin(x)^2)."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#square-root-transformation",
    "href": "BCB744/basic_stats/06-assumptions.html#square-root-transformation",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n4.3 Square root transformation",
    "text": "4.3 Square root transformation\nThe square root transformation (in R, sqrt(x)) is often used to stabilise the variance of data that have a non-linear relationship between the mean and variance (heteroscedasticity). It is effective for reducing right-skewness (positively skewed). Taking the square root of each observation has the effect of compressing the data towards zero and reducing the impact of extreme values. It is a monotonic transformation, which means that it preserves the order of the data and does not change the relative rankings of the observations.\nThe square root transformation does not work with negative values, but one could add a constant to each number to make them positive.\nA square root transformation is most frequently applied where the data are counts or frequencies, such as the number of individuals in a population or the number of events in a certain time period. Count data are prone to the variance increasing with the mean due to the discrete nature of the data. In these cases, the data tend to follow a Poisson distribution, which is characterised by a variance that is equal to the mean. The same applies to some environmental data, such as rainfall or wind; these may also exhibit heteroscedasticity due to extreme weather phenomena."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#square-transformation",
    "href": "BCB744/basic_stats/06-assumptions.html#square-transformation",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n4.4 Square transformation",
    "text": "4.4 Square transformation\nAnother transformation available for dealing with heteroscedasticity is the square transformation. As the name suggests, it involves taking the square of each observation in a dataset (x^2). The effect sought is to reduce left skewness.\nThis transformation has the effect of magnifying the differences between values and so increasing the influence of extreme values. However, this can make outliers more prominent and can make it more challenging to interpret the results of statistical analysis.\nThe square transformation is often used in situations where the data are related to areas or volumes, such as the size of cells or the volume of an organ, where the data may follow a nonlinear relationship between the mean and variance."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#cube-transformation",
    "href": "BCB744/basic_stats/06-assumptions.html#cube-transformation",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n4.5 Cube transformation",
    "text": "4.5 Cube transformation\nThis transformation also applies to heteroscedastic data. It is sometimes used with moderately left skewed data. This transformation is more drastic than a square transformation, and the drawback are more severe.\nThe cube transformation is less commonly used than other data transformations such as square-root or log transformation. Use with caution."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#reciprocal-transformation",
    "href": "BCB744/basic_stats/06-assumptions.html#reciprocal-transformation",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n4.6 Reciprocal transformation",
    "text": "4.6 Reciprocal transformation\nIt involves taking the reciprocal or inverse of each observation in a dataset (1/x). It is another variance stabilising transformation and is used with severely positively skewed data."
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#anscombe-transformation",
    "href": "BCB744/basic_stats/06-assumptions.html#anscombe-transformation",
    "title": "6. Assumptions for parametric statistics",
    "section": "\n4.7 Anscombe transformation",
    "text": "4.7 Anscombe transformation\nAnother variance stabilising transformation is the Anscombe transformation, sqrt(max(x+1) - x). It is applied to negatively skewed data. This transformation can be used to shift the data and compress it towards zero, and remove the influence of extreme values. It is a monotonic transformation, which means that it preserves the order of the data and does not change the relative rankings of the observations.\nThe Anscombe transformation is useful when dealing with count or frequency data that have a non-linear relationship between the mean and variance; such data are characteristic of Poisson-distributed count data."
  },
  {
    "objectID": "BCB744/basic_stats/12-confidence.html",
    "href": "BCB744/basic_stats/12-confidence.html",
    "title": "12. Confidence intervals",
    "section": "",
    "text": "1 Introduction\nA confidence interval (CI) tells us within what range we may be certain to find the true mean from which any sample has been taken. If we were to repeatedly sample the same population over and over and calculated a mean every time, the 95% CI indicates the range that 95% of those means would fall into.\n\n2 Calculating confidence intervals\n\nInput &lt;- (\"\nStudent  Grade   Teacher   Score  Rating\na        Gr_1    Vladimir  80     7\nb        Gr_1    Vladimir  90    10\nc        Gr_1    Vladimir 100     9\nd        Gr_1    Vladimir  70     5\ne        Gr_1    Vladimir  60     4\nf        Gr_1    Vladimir  80     8\ng        Gr_10   Vladimir  70     6\nh        Gr_10   Vladimir  50     5\ni        Gr_10   Vladimir  90    10\nj        Gr_10   Vladimir  70     8\nk        Gr_1    Sadam     80     7\nl        Gr_1    Sadam     90     8\nm        Gr_1    Sadam     90     8\nn        Gr_1    Sadam     80     9\no        Gr_10   Sadam     60     5\np        Gr_10   Sadam     80     9\nq        Gr_10   Sadam     70     6\nr        Gr_1    Donald   100    10\ns        Gr_1    Donald    90    10\nt        Gr_1    Donald    80     8\nu        Gr_1    Donald    80     7\nv        Gr_1    Donald    60     7\nw        Gr_10   Donald    60     8\nx        Gr_10   Donald    80    10\ny        Gr_10   Donald    70     7\nz        Gr_10   Donald    70     7\n\")\n\ndata &lt;- read.table(textConnection(Input), header = TRUE)\nsummary(data)\n\n   Student             Grade             Teacher              Score       \n Length:26          Length:26          Length:26          Min.   : 50.00  \n Class :character   Class :character   Class :character   1st Qu.: 70.00  \n Mode  :character   Mode  :character   Mode  :character   Median : 80.00  \n                                                          Mean   : 76.92  \n                                                          3rd Qu.: 87.50  \n                                                          Max.   :100.00  \n     Rating      \n Min.   : 4.000  \n 1st Qu.: 7.000  \n Median : 8.000  \n Mean   : 7.615  \n 3rd Qu.: 9.000  \n Max.   :10.000  \n\n\nThe package rcompanion has a convenient function for estimating the confidence intervals for our sample data. The function is called groupwiseMean() and it has a few options (methods) for estimating the confidence intervals, e.g. the ‘traditional’ way using the t-distribution, and a bootstrapping procedure.\nLet us produce the confidence intervals using the traditional method for the group means:\n\nlibrary(rcompanion)\n# Ungrouped data are indicated with a 1 on the right side of the formula,\n# or the group = NULL argument; so, this produces the overall mean\ngroupwiseMean(Score ~ 1, data = data, conf = 0.95, digits = 3)\n\n   .id  n Mean Conf.level Trad.lower Trad.upper\n1 &lt;NA&gt; 26 76.9       0.95       71.7       82.1\n\n# One-way data\ngroupwiseMean(Score ~ Grade, data = data, conf = 0.95, digits = 3)\n\n  Grade  n Mean Conf.level Trad.lower Trad.upper\n1  Gr_1 15   82       0.95       75.3       88.7\n2 Gr_10 11   70       0.95       62.6       77.4\n\n# Two-way data\ngroupwiseMean(Score ~ Teacher + Grade, data = data, conf = 0.95, digits = 3)\n\n   Teacher Grade n Mean Conf.level Trad.lower Trad.upper\n1   Donald  Gr_1 5   82       0.95       63.6      100.0\n2   Donald Gr_10 4   70       0.95       57.0       83.0\n3    Sadam  Gr_1 4   85       0.95       75.8       94.2\n4    Sadam Gr_10 3   70       0.95       45.2       94.8\n5 Vladimir  Gr_1 6   80       0.95       65.2       94.8\n6 Vladimir Gr_10 4   70       0.95       44.0       96.0\n\n\nNow let us do it through bootstrapping:\n\ngroupwiseMean(Score ~ Grade,\n              data = data,\n              conf = 0.95,\n              digits = 3,\n              R = 10000,\n              boot = TRUE,\n              traditional = FALSE,\n              normal = FALSE,\n              basic = FALSE,\n              percentile = FALSE,\n              bca = TRUE)\n\n  Grade  n Mean Boot.mean Conf.level Bca.lower Bca.upper\n1  Gr_1 15   82        82       0.95      74.7      86.7\n2 Gr_10 11   70        70       0.95      62.7      75.5\n\ngroupwiseMean(Score ~ Teacher + Grade,\n              data = data,\n              conf = 0.95,\n              digits = 3,\n              R = 10000,\n              boot = TRUE,\n              traditional = FALSE,\n              normal = FALSE,\n              basic = FALSE,\n              percentile = FALSE,\n              bca = TRUE)\n\n   Teacher Grade n Mean Boot.mean Conf.level Bca.lower Bca.upper\n1   Donald  Gr_1 5   82      82.1       0.95      68.0      90.0\n2   Donald Gr_10 4   70      70.0       0.95      62.5      75.0\n3    Sadam  Gr_1 4   85      85.0       0.95      80.0      87.5\n4    Sadam Gr_10 3   70      70.0       0.95      60.0      76.7\n5 Vladimir  Gr_1 6   80      80.0       0.95      68.3      88.3\n6 Vladimir Gr_10 4   70      69.9       0.95      55.0      80.0\n\n\nThese upper and lower limits may then be used easily within a figure.\n\n# Load libraries\nlibrary(tidyverse)\n\n# Create dummy data\nr_dat &lt;- data.frame(value = rnorm(n = 20, mean = 10, sd = 2),\n                    sample = rep(\"A\", 20))\n\n# Create basic plot\nggplot(data = r_dat, aes(x = sample, y = value)) +\n  geom_errorbar(aes(ymin = mean(value) - sd(value), ymax = mean(value) + sd(value))) +\n  geom_jitter(colour = \"firebrick1\")\n\n\n\nA very basic figure showing confidence intervals (CI) for a random normal distribution.\n\n\n\n\n3 CI of compared means\nAS stated above, we may also use CI to investigate the difference in means between two or more sample sets of data. We have already seen this in the ANOVA Chapter, but we shall look at it again here with our now expanded understanding of the concept.\n\n# First calculate ANOVA of seapl length of different iris species\niris_aov &lt;- aov(Sepal.Length ~ Species, data = iris)\n\n# Then run a Tukey test\niris_Tukey &lt;- TukeyHSD(iris_aov)\n\n# Lastly use base R to quickly plot the results\nplot(iris_Tukey)\n\n\n\nResults of a post-hoc Tukey test showing the confidence interval for the effect size between each group.\n\n\n\n\nTask 1: Judging from the figure above, which species have significantly different sepal lengths?\n\n\n4 Harrell plots\nThe most complete use of CI that we have seen to date is the Harrell plot. This type of figure shows the distributions of each sample set in the data as boxplots in a lower panel. In the panel above those boxplots it then lays out the results of a post-hoc Tukey test. This very cleanly shows both the raw data as well as high level statistical results of the comparisons of those data. Thanks to the magic of the Internet we may create these figures with a single line of code. This does however require that we load several new libraries.\n\n# The easy creation of these figures has quite a few dependencies\nlibrary(lsmeans)\nlibrary(Hmisc)\nlibrary(broom)\nlibrary(car)\nlibrary(data.table)\nlibrary(cowplot)\nsource(\"../../R/fit_model.R\")\nsource(\"../../R/make_formula_str.R\")\nsource(\"../../R/HarrellPlot.R\")\n\n# Load data\necklonia &lt;- read.csv(\"../../data/ecklonia.csv\")\n\n# Create Harrell Plot\nHarrellPlot(x = \"site\", y = \"stipe_length\", data = ecklonia, short = T)[1]\n\n$gg\n\n\n\n\nHarrell plot showing the distributions of stipe lengths (cm) of the kelp Ecklonia maxima at two different sites in the bottom panel. The top panel shows the confidence interval of the effect of the difference between these two sample sets based on a post-hoc Tukey test.\n\n\n\n\nTask 2: There are a lot of settings for HarrellPlot(), what do some of them do?\n\nThe above figure shows that the CI of the difference between stipe lengths (cm) at the two sites does not cross 0. This means that there is a significant difference between these two sample sets. But let’s run a statistical test anyway to check the results.\n\n# assumptions\necklonia %&gt;% \n  group_by(site) %&gt;% \n  summarise(stipe_length_var = var(stipe_length),\n            stipe_length_Norm = as.numeric(shapiro.test(stipe_length)[2]))\n\n# A tibble: 2 × 3\n  site           stipe_length_var stipe_length_Norm\n  &lt;chr&gt;                     &lt;dbl&gt;             &lt;dbl&gt;\n1 Batsata Rock             14683.            0.0128\n2 Boulders Beach            4970.            0.0589\n\n# We fail both assumptions...\n\n# non-parametric test\nwilcox.test(stipe_length ~ site, data = ecklonia)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  stipe_length by site\nW = 146, p-value = 0.001752\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe results of our Wilcox rank sum test, unsurprisingly, support the output of HarrelPlot().\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {12. {Confidence} Intervals},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/basic_stats/12-confidence.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 12. Confidence intervals. https://tangledbank.netlify.app/BCB744/basic_stats/12-confidence.html."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html",
    "href": "BCB744/basic_stats/03-visualise.html",
    "title": "3. Exploring with figures",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nThe diversity of graphs used to communicate statistical results\nHow to select the right graph for any particular dataset\nAdditional packages available to extend ggplot’s functionality"
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-histograms",
    "href": "BCB744/basic_stats/03-visualise.html#sec-histograms",
    "title": "3. Exploring with figures",
    "section": "\n2.1 Frequency distributions",
    "text": "2.1 Frequency distributions\nFrequency distributions are typically displayed as histograms. Histograms are a type of graph that displays the frequency of occurrences of observations forming a continuous variable. To construct a histogram, the data are divided into intervals, or bins, and the number of occurrences of observations within each bin is tallied. The height of each bar (y-axis) in the histogram represents the number of observations falling within that bin. The x-axis displays the bins, arranged such that the intervals they represent go from small to large on an ordinal scale. The intervals should be chosen carefully, such that they best represent the distribution of the data without being too narrow or too wide. Histograms can be used to quickly assess the distribution of the data, identify any skewness or outliers, and provide a visual representation of the central tendency and variation of the data.\nWe have a choice of absolute (Figure 1 A) and relative (Figure 1 B-C) frequency histograms. In absolute frequency distributions, the sum of all the counts per bin will add up to the total number of obervations. In relative frequency distributions the the frequency of each category is expressed as a proportion or percentage of the total number of observations, and hence the sum of the relative counts per bin is 1. This is useful if two populations being compared have different numbers of observations. There’s also the empirical cumulative distribution function (ECDF) (Figure 1 D) that shows the cumulative proportion of observations that fall below or equal to a certain value. See the Old Faithful data, for example. The eruptions last between 1.6 and 5.1 minutes. So, we create intervals of time spanning these times, and within each count the number of times an event lasts as long as denoted by the intervals. Here we might choose intervals of 1-2 minutes, 2-3 minutes, 3-4 minutes, 4-5 minutes, and 5-6 minutes. The ggplot2 geom_histogram() function automatically creates the bins, but we may specify our own. It is best to explain these principles by example (Figure 1 A-D).\n\n# a normal frequency histogram, with count along y\nhist1 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"'Vanilla' histogram\",\n       x = \"Eruption duration (min)\",\n       y = \"Count\") + theme_pubclean()\n\n# when the binwidth is 1, the density histogram *is* the relative\n# frequency histogram\nhist2 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..density..),\n                 position = 'identity', binwidth = 1,\n                 colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Relative frequency\",\n       x = \"Eruption duration (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\n\n# if binwidth is something other than 1, the relative frequency in\n# a histogram is ..density.. * binwidth\nhist3 &lt;- ggplot(data = faithful, aes(x = waiting)) +\n  geom_histogram(aes(y = 0.5 * ..density..),\n                 position = 'identity', binwidth = 0.5,\n                 colour = \"salmon\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Relative frequency\",\n       x = \"Waiting time (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\n# ECDF\nhist4 &lt;- ggplot(data = faithful, aes(x = eruptions)) + \n  stat_ecdf() +\n  labs(title = \"ECDF\",\n       x = \"Eruption duration (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\nggarrange(hist1, hist2, hist3, hist4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\nFigure 1: Example histograms for the Old Faithful data. A) A default frequency histogram with the count of eruption times falling within the specified bins. B) A relative frequency histogram with bins adjusted to a width of 1 minute intervals; here, the sum of counts within each of the four bins is 1. C) Another relative frequency histogram, but with the bins adjusted to each be 0.5 minute increments; again the sum of counts represented by each bin is equal to 1.\n\n\nAs we see above, ggplot2 can automatically construct a frequency histogram with the geom_histogram() function. We can also manually create a frequency distribution with the cut() function.\n\n\n\n\n\n\nTask C\n\n\n\n\nStarting with the cut() function, recreate Figure 1 A-C manually.\n\n\n\nWhat if we have continuous data belonging with multiple categories? The iris dataset provides a nice collection of measurements that we may use to demonstrate a grouped frequency histogram. These data are size measurements (cm) of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of three species of Iris. The species are Iris setosa, I. versicolor, and I. virginica. The figures are shown in Figure 2.\n\n# first we make long data\niris.long &lt;- iris %&gt;% \n  gather(key = \"variable\", value = \"size\", -Species)\n\nggplot(data = iris.long, aes(x = size)) +\n  geom_histogram(position = \"dodge\", # ommitting this creates a stacked histogram\n                 colour = NA, bins = 20,\n                 aes(fill = Species)) +\n  facet_wrap(~variable) +\n  labs(title = \"Iris data\",\n       subtitle = \"Grouped frequency histogram\",\n       x = \"Size (cm)\",\n       y = \"Count\") +\n  theme_pubclean()\n\n\n\n\nFigure 2: Grouped histograms for the four Iris variables."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-bargraphs",
    "href": "BCB744/basic_stats/03-visualise.html#sec-bargraphs",
    "title": "3. Exploring with figures",
    "section": "\n2.2 Bar graphs",
    "text": "2.2 Bar graphs\nBar graphs are popular among biologists and ecologists. Often used to represent discrete categories or groups, bar graphs provide a visual representation of mean values for each category, thus allowing us to identify trends, patterns, and differences across data sets or experimental treatments. In complex biological systems, such as population dynamics, species abundance, or ecological niches, bar graphs offer a clear and concise way to depict the interactions and variations among different elements. Importantly, bar graphs may also include some indication of variation, such as error bars (a term that also applies when the variation statistic used is the standard deviation) or other visual cues to denote the range of variation within the data, such as confidence intervals. This additional layer of information not only highlights the variability inherent in biological and ecological data but also aids in the interpretation of results and the overall understanding of the phenomena under investigation. Note that it is not incorrect to plot the median in bar graphs, but bat graphs is typically reserved for displaying the mean. For plotting the median, see Section 2.3, below.\nA naïve application of bar graphs is to indicate the number of observations within several groups. Although this can be presented numerically in tabular form, sometimes one might want to create a bar or pie graph of the number of occurrences in a collection of non-overlapping classes or categories. Both the data and graphical displays will be demonstrated here.\nThe first case is of a variation of frequency distribution histograms, but here showing the raw counts per each of the categories that are represented in the data—unlike ‘true’ frequency histograms in Section 2.1 that divide data into bins, this one takes a cruder approach. The count within each of the categories sums to the sample size, \\(n\\). In the second case, we may want to report those data as proportions. Here we show the frequency proportion in a collection of non-overlapping categories. For example, we have a sample size of 12 (\\(n=12\\)). In this sample, two are coloured blue, six red, and five purple. The relative proportions are \\(2/12=0.1666667\\) blue, \\(6/12=0.5\\) red, and \\(5/12=0.4166667\\) purple. The important thing to note here is that the relative proportions sum to 1, i.e. \\(0.1666667+0.5+0.4166667=1\\). These data may be presented as a table or as a graph.\nIn Figure 3 I demonstrate the numerical and graphical summaries using the built-in iris data (I’d not do this in real life, it’s silly; just write it out in the text of the Methods section):\n\n# the numerical summary produced by a piped series of functions;\n# create a summary of the data (i.e. number of replicates per species)\n# used for (A), (B) and (C), below\niris.cnt &lt;- iris %&gt;%\n  count(Species) %&gt;% # automagically creates a column, n, with the counts\n  mutate(prop = n / sum(n)) # creates the relative proportion of each species\n\n\n# a stacked bar graph with the cumulative sum of observations\nplt1 &lt;- ggplot(data = iris.cnt, aes(x = \"\", y = n, fill = Species)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  labs(title = \"Stacked bar graph\", subtitle = \"cumulative sum\",\n       x = NULL, y = \"Count\") +\n  theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\n# a stacked bar graph with the relative proportions of observations\nplt2 &lt;- ggplot(data = iris.cnt, aes(x = \"\", y = prop, fill = Species)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  scale_y_continuous(breaks = c(0.00, 0.33, 0.66, 1.00)) +\n  labs(title = \"Stacked bar graph\", subtitle = \"relative proportions\",\n       x = NULL, y = \"Proportion\") +\n  theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\n# a basic pie chart\nplt3 &lt;- plt1 + coord_polar(\"y\", start = 0) +\n  labs(title = \"Friends don't let...\", subtitle = \"...friends make pie charts\",\n       x = NULL, y = NULL) +\n  scale_fill_brewer(palette = \"Blues\") +\n  theme_minimal()\n# if you seriously want a pie chart, rather use the base R function, `pie()`\n\n# here now a bar graph...\n# the default mapping of `geom_bar` is `stat = count`, which is a\n# bar for each fo the categories (`Species`), with `count` along y\nplt4 &lt;- ggplot(data = iris, aes(x = Species, fill = Species)) +\n  geom_bar(show.legend = FALSE) +\n  labs(title = \"Side-by-side bars\", subtitle = \"n per species\", y = \"Count\") +\n theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\nggarrange(plt1, plt2, plt3, plt4, nrow = 2, ncol = 2, labels = \"AUTO\")\n\n\n\n\nFigure 3: Examples of histograms for the built-in Iris data. A) A default frequency histogram showing the count of samples for each of the three species. B) A relative frequency histogram of the same data; here, the sum of counts of samples available for each of the three species is 1. C) A boring pie chart. D) A frequency histogram of raw data counts shown as a series of side-by-side bars.\n\n\nNow I’ll demonstrate more realistic bar graphs. We stay with the iris data (Figure 4):\n\niris |&gt;\n  pivot_longer(cols = Sepal.Length:Petal.Width,\n               names_to = \"variable\",\n               values_to = \"size\") |&gt; \n  group_by(Species, variable) |&gt; \n  summarise(mean = round(mean(size), 1),\n            sd = round(sd(size), 1), .groups = \"drop\") |&gt; \n  ggplot(aes(x = Species, y = mean)) +\n  geom_bar(position = position_dodge(), stat = \"identity\", \n           col = \"black\", fill = \"salmon\", alpha = 0.4) +\n  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),\n                width = .2) +\n  facet_wrap(~variable,\n             scales = \"free\") +\n  ylab(\"Size (mm)\") +\n  theme_minimal()\n\n\n\n\nFigure 4: Bar graphs indicating the mean size (± SD) for various flower features of three species of Iris."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-boxplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-boxplots",
    "title": "3. Exploring with figures",
    "section": "\n2.3 Box plots",
    "text": "2.3 Box plots\nA box plot provides a graphical summary of the distribution of data. They allow us to compare the medians, quartiles, and ranges of the data for multiple groups, and identify any differences or similarities in the distributions. For example, box plots can be used to compare the body size distributions of different species, or to compare the reproductive output of different populations. Additionally, box plots can be used to identify outliers and other anomalies in the data, which may be indicative of underlying ecological processes or environmental factors.\nBox plots plots are traditionally used to display data that are not normally distributed, but I like to use them for any old data, even normal data. I prefer these over the old-fashioned bar graphs (seen in Section 2.2). As a variation of the basic box-and-whisker plot, I also quite like to superimpose a jittered scatter plot of the raw data on each bar.\nI create a simple example using the msleep dataset (Figure 5). Additional examples are provided in Chapter 2.\n\nmsleep |&gt; \n  ggplot(aes(x = vore, y = sleep_total)) + \n  geom_boxplot(colour = \"black\", fill = \"salmon\", alpha = 0.4,\n               outlier.color = \"red3\", outlier.fill = \"red\",\n               outlier.alpha = 1.0, outlier.size = 2.2) +\n  geom_jitter(width = 0.10, fill = \"blue\", alpha = 0.5,\n              col = \"navy\", shape = 21, size = 2.2) +\n  labs(x = \"'-vore'\",\n       y = \"Sleep duration (hr)\") +\n  theme_pubclean()\n\n\n\n\nFigure 5: Box-plot summarising the amount of sleep required by different ‘vores’.\n\n\nBox plots are sometimes called box-and-whisker plots. The keen eye can glance the ‘shape’ of the data distribution; they provide an alternative view to that given by the frequency distribution. There is a lot of information in these graphs, so let’s see what’s there. From the geom_boxplot documentation, which says it best (type ?geom_boxplot):\n\n“The lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles).”\n“The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called ‘outlying’ points and are plotted individually.”\n“In a notched box plot, the notches extend 1.58 * IQR / sqrt(n). This gives a roughly 95% confidence interval for comparing medians.”\n\nHere be more examples (Figure 6), this time of notched box plots:\n\nlibrary(ggsci) # for nice colours\n\nggplot(data = iris.long, aes(x = Species, y = size)) +\n  geom_boxplot(alpha = 0.4, notch = TRUE) +\n  geom_jitter(width = 0.1, shape = 21, fill = NA,\n              alpha = 0.4, aes(colour = as.factor(Species))) +\n  facet_wrap(~variable, nrow = 2) +\n  scale_color_npg() +\n  labs(y = \"Size (cm)\") +\n  guides(colour = FALSE) +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\n\n\n\nFigure 6: A panelled collection of box plots, one for each of the four variables, with a scatterplot to indicate the spread of the raw data points."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-densityplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-densityplots",
    "title": "3. Exploring with figures",
    "section": "\n2.4 Density plots",
    "text": "2.4 Density plots\nOften when we are displaying a distribution of data we are interested in the ‘shape’ of the data more than the actual count of values in a specific category, as shown by a standard histogram. When one wishes to more organically visualise the frequency of values in a sample set a density graphs is used. These may also be thought of as smooth histograms. These work well with histograms and rug plots, as we may see in the figure below. It is important to note with density plots that they show the relative density of the distribution along the \\(y\\)-axis, and not the counts of the data. This can of course be changed, as seen below, but is not the default setting. Sometimes it can be informative to see how different the count and density distributions appear.\nFigure 7 shows examples af density plots:\n\n# a normal density plot\ndens1 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"A vanilla density plot\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a density and rug plot combo\ndens2 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  geom_rug(colour = \"red\") +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"A density and rug plot\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a relative frequency histogram overlayed with a density plot\ndens3 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..density..),\n                 position = 'identity', binwidth = 1,\n                 colour = \"black\", fill = \"turquoise\", alpha = 0.6) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"Relative frequency with density\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a normal frequency histogram with density overlayed\n# note that the density curve must be adjusted by\n# the number of data points times the bin width\ndens4 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..count..),\n                 binwidth = 0.2, colour = \"black\", fill = \"turquoise\", alpha = 0.6) +\n  geom_density(aes(y = ..density.. * nrow(datasets::faithful) * 0.2), position = \"identity\",\n               colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"Frequency with density\",\n       x = \"Eruption duration (min)\",\n       y = \"Count\") + theme_pubr()\n\nggarrange(dens1, dens2, dens3, dens4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\nFigure 7: A bevy of density graphs option based on the iris data. A) A lone density graph. B) A density graph accompanied by a rug plot. C) A histogram with a density graph overlay. D) A ridge plot."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-violinplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-violinplots",
    "title": "3. Exploring with figures",
    "section": "\n2.5 Violin plots",
    "text": "2.5 Violin plots\nWe may combine the box plot and density graph concepts into a new figure type. They can become quite snooty and display more information in more informative ways than vanilla box plots. These are known as violin plots and are very useful when we want to show the distribution of multiple categories of a continuous variate alongside one another.\nViolin plots show the same information as box plots but take things one step further by allowing the shape of the box plot to also show the distribution of the continuous data within the sample sets. They show not only central tendencies (like median) but also the full distribution, including possible multimodal or skewed characteristics.\nOne needs to install additional packages to make then, such as the package ggstatplot. This package offers many non-traditional options for graphical statistical summaries. Here, the violin plot includes the following features:\n\nViolins The vertical, symmetrical, and mirrored shapes represent the estimated probability density of the data at different values. The wider the violin at a given point, the higher the density of data at that value.\nBox plot A box plot can be embedded within the violin plot to show the median, quartiles, and the possible outliers.\nStatistical annotations The violin plots offered by ggstatplot accommodate various statistical annotations such as mean, median, confidence intervals, or p-values, depending on the your needs.\n\nWe will use the iris data below to highlight the different types of violin plots one may use (Figure 8):\n\nlibrary(ggstatsplot)\nset.seed(123) # for reproducibility\n\n# plot\nggstatsplot::ggbetweenstats(\n  data = iris,\n  x = Species,\n  y = Sepal.Length,\n  ylab = \"Sepal length (cm)\",\n  title = \"Distribution of sepal length across the three *Iris* species\"\n)\n\n\n\n\nFigure 8: Examples of violin plots made for the Iris data.\n\n\nHere’s another verson of the iris data analysed with violin plots (Figure 9):\n\nvio1 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_violin() + \n  labs(title = \"Iris data\",\n       subtitle = \"Basic violin plot\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# A violin plot showing the quartiles as lines\nvio2 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_violin(show.legend = FALSE, draw_quantiles = c(0.25, 0.5, 0.75)) + \n  labs(title = \"Iris data\",\n       subtitle = \"Violin plot with quartiles\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# Box plots nested within violin plots\nvio3 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, colour = Species)) +\n  geom_violin(fill = \"grey70\") + \n  geom_boxplot(width = 0.1, colour = \"grey30\", fill = \"white\") +\n  labs(title = \"Iris data\",\n       subtitle = \"Box plots nested within violin plots\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# Boxes in violins with the raw data jittered about\nvio4 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, colour = Species)) +\n  geom_violin(fill = \"grey70\") + \n  geom_boxplot(width = 0.1, colour = \"black\", fill = \"white\") +\n  geom_jitter(shape = 1, width = 0.1, colour = \"red\", alpha = 0.7, fill = NA) +\n  labs(title = \"Iris data\",\n       subtitle = \"Violins, boxes, and jittered data\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\nggarrange(vio1, vio2, vio3, vio4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\nFigure 9: More variations of violin plots applied to the Iris data.\n\n\nThe ggpubr package also provides many convenience functions for the drawing of publication quality graphs, including violin plots."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-scatterplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-scatterplots",
    "title": "3. Exploring with figures",
    "section": "\n3.1 Scatter plots",
    "text": "3.1 Scatter plots\nThe relationship between two continuous variables is typically displayed with scatter plots. In a scatter plot, each data point is represented by a dot or other symbol plotted on a Cartesian coordinate system, with one variable mapped to the \\(x\\)-axis and the other to the \\(y\\)-axis. One may choose to fit a best fit line through these points, but displaying the scatter of points is typically enough. In scatter plots, the points are not connected by lines, and the use of discrete points causes us to not assume a specific order or continuity in the data between ‘consecutive’ points on the graph. Also, a scatter plot typically does not require that the \\(x\\)-axis is independent.\nThe most basic use of scatter plots is the following:\n\nExploratory data analysis Scatter plots are useful in the initial exploration of data sets. They help us identify patterns and relationships that might warrant further investigation using more advanced statistical techniques.\nIdentifying trends One can identify whether there is a positive, negative, or no apparent trend between the two variables by observing the overall pattern (slope) of an imaginary or real line fitted to the data points. The detection of trends is something we will encounter in Chapter 9 on Simple linear regressions.\nIdentifying correlations Scatter plots can be used to visually assess the correlation between two variables. A strong positive correlation will result in data points forming a line or curve sloping upward, while a strong negative correlation will result in data points forming a line or curve sloping downward. A weak or no correlation will result in a more scattered and less structured pattern. We will discover more about this in Chapter 10 on Correlation.\nAssessing clustering Scatter plots can reveal natural groupings or clusters of data points, which can be helpful in understanding the structure of the data or identifying potential subgroups for further analysis.\n\nAll of these applications of scatter plots are shown in Figure 10. In Figure 10 I show the relationship between two (matched) continuous variables. The statistical strength of the relationship can be indicated by a correlation (no causal relationship implied as is the case here) or a regression (when a causal link of \\(x\\) on \\(y\\) is demonstrated), and the grouping structure is clearly indicated with colour.\n\nplt1 &lt;- ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +\n  geom_point() +\n  labs(x = \"Petal length (cm)\", y = \"Petal width (cm)\") +\n  theme(legend.position = c(0.22, 0.75)) +\n  scale_color_fivethirtyeight() +\n  scale_fill_fivethirtyeight() +\n  theme_minimal()\n\nplt2 &lt;- ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, show.legend = FALSE) +\n  scale_color_fivethirtyeight() +\n  scale_fill_fivethirtyeight() +\n  labs(x = \"Petal length (cm)\", y = \"Petal width (cm)\") +\n  theme_minimal()\n\nggarrange(plt1, plt2, ncol = 2, nrow = 1, labels = \"AUTO\",\n          common.legend = TRUE)\n\n\n\n\nFigure 10: Examples of scatterplots made for the Iris data. A) A default scatter plot showing the relationship between petal length and width. B) The same as (A) but with a correlation line added.\n\n\nScatter plots may also indicate some of the following properties of our datasets, which make them useful as a diagnostic tool in inferential data analysis, specifically when it comes to assessing assumptions about our data:\n\nDetecting outliers Outliers are data points that deviate significantly from the overall pattern of the data. Scatter plots can help identify such points that might warrant further investigation or indicate problems in data collection.\nAssessing linearity Scatter plots can reveal whether the relationship between two variables is linear or nonlinear. A linear relationship will result in data points forming a straight line, while a nonlinear relationship will result in data points forming a curve or more complex pattern.\n\nWe will encounter these uses in later Chapters dealing with inferential statistics."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-linegraph",
    "href": "BCB744/basic_stats/03-visualise.html#sec-linegraph",
    "title": "3. Exploring with figures",
    "section": "\n3.2 Line graphs",
    "text": "3.2 Line graphs\nA line graph connects data points with lines, typically emphasising a continuous relationship or a sequence over time or some other continuous scale. The \\(x\\)-axis often represents time (or another independent variable), while the \\(y\\)-axis represents the other variable (usually the dependent variable). Line graphs are particularly useful for tracking changes, trends, or patterns over time and for comparing multiple data series. They suggest a more explicit connection between data points, making it easier to observe fluctuations and the overall direction of the data.\nWe typically encounter line graphs in visual displays of time-series. One might include a point for each observation in time, but it may be omitted. The important thing to note is that a line connects each consecutive observation to the next, indicating the continuity of time. It is a useful tool for exploring trends, patterns, and seasonality in data. For example, a time-series plot can be used to visualise the seasonal trends in temperature over an annual cycle (Figure 11). In this example, points are not used at all, and I opt instead for a stepped line that suggests continuity and yet maintain a ‘discrete’ measure per month (i.e. ignoring the higher frequency daily and finer scale variations within a month).\n\nlibrary(lubridate)\nread_csv(\"../../data/SACTN_SAWS.csv\") |&gt; \n  mutate(month = month(date)) |&gt; \n  group_by(site, month) |&gt; \n  dplyr::summarise(temp = mean(temp, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = month, y = temp)) +\n  geom_step(colour = \"red4\") +\n  scale_x_continuous(breaks = c(1, 3, 5, 7, 9, 11)) +\n  xlab(\"Month\") + ylab(\"Temperature (°C)\") +\n  facet_wrap(~site, ncol = 5) +\n  theme_minimal()\n\n\n\n\nFigure 11: A time series plot showing the monthly climatology for several sites around South Africa. The specific kind of line drawn here forms a stepped graph."
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-heatmaps",
    "href": "BCB744/basic_stats/03-visualise.html#sec-heatmaps",
    "title": "3. Exploring with figures",
    "section": "\n3.3 Heatmaps and Hovmöller Diagrams",
    "text": "3.3 Heatmaps and Hovmöller Diagrams\nWe can extend the time series line graph to two dimensions. A heatmap is a raster representation of data where the values in a matrix are represented as colours. We will see some heatmaps in Chapter 10 on Correlations. A special kind of heatmap is a calendar heatmap, which is a visualisation technique that uses a calendar layout to show patterns in data over time. For example, a calendar heatmap can be used to show the daily time series or climatologies of temperature or some other environmental variable that varies seasonally (Figure 12).\n\n# Load the function to the local through Paul Bleicher's GitHub page\nsource(\"https://raw.githubusercontent.com/iascchen/VisHealth/master/R/calendarHeat.R\")\n\ntemps &lt;- heatwaveR::sst_WA |&gt; \n  filter(t &gt;= \"2010-01-01\" & t &lt;= \"2019-12-31\") |&gt; \n  mutate(weekday = wday(t),\n         weekday_f = wday(t, label = TRUE),\n         week = week(t),\n         month = month(t, label = TRUE),\n         year = year(t)) |&gt; \n  group_by(year, month) |&gt; \n  mutate(monthweek = 1 + week - min(week))\n\nggplot(temps, aes(monthweek, weekday_f, fill = temp)) + \n  geom_tile(colour = \"white\") +\n  facet_grid(year(t) ~ month) +\n  scale_x_continuous(breaks = c(1, 3, 5)) +\n  scale_y_discrete(breaks = c(\"Sun\", \"Wed\", \"Sat\")) +\n  scale_fill_viridis_c() +\n  xlab(\"Week of Month\") +\n  ylab(\"\") +\n  ggtitle(\"Time-Series Calendar Heatmap: Western Australia SST\") +\n  labs(fill = \"[°C]\")\n\n\n\n\nFigure 12: A calendar heatmap showing a timeseries of SST for Western Australia. The infamous marine heatwave that resulted in a new field of study on extreme temperatures can be seen in the summer of 2011.\n\n\nA special kind of heatmap is used in Ocean and Atmospheric Science is the Hovmöller Diagram (see Figure 13), where we have one continuous spatial covariate along one axis (e.g. latitude or longitude) and time along the other axis on a two-dimensional graph. These diagrams were originally developed by Swedish meteorologist Ernest Hovmöller. By mapping oceanographic variables such as sea surface temperature, salinity, or ocean currents, Hovmöller Diagrams allow us to track the progression of phenomena like El Niño and La Niña events, or to examine the migration of ocean eddies and gyres.\nA variation of Hovmöller Diagrams is the horizon plot (?@fig-horizonplot), which shows the same kind of information (and more) but in a more visually impactful format, in my opinion. I provide more information on horizon plots in my vignette, where I also demonstrate their application to the visualisation of extreme temperature events.\n\n\n\n\nlibrary(data.table)\nlibrary(colorspace)\n\nNWA &lt;- fread(\"../../data/NWA_Hovmoller.csv\")\n\n# calculate anomalies\nNWA |&gt; \n  mutate(anom = zonal_sst - mean(zonal_sst)) |&gt; \n  ggplot(aes(x = t, y = lat, fill = anom)) +\n  geom_tile(colour = \"transparent\") +\n  scale_fill_binned_diverging(palette = \"Blue-Red 3\", n_interp = 21) +\n  # scale_fill_viridis_c() +\n  xlab(\"\") + ylab(\"Latitude [°N]\") + labs(fill = \"[°C]\") +\n  theme_minimal()\n\n\n\n\nFigure 13: A Hovmöller Diagram of zonally averaged SST for a region off Northwest Africa in the Canary upwelling system. A variation of this figure appears in the vignette and shows the timeline of marine heatwaves and cold spells in the region.\n\n\n\nlibrary(ggHoriPlot)\n\ncutpoints &lt;- NWA  %&gt;% \n  mutate(\n    outlier = between(\n      zonal_sst, \n      quantile(zonal_sst, 0.25, na.rm = TRUE)-\n        1.5*IQR(zonal_sst, na.rm = TRUE),\n      quantile(zonal_sst, 0.75, na.rm = TRUE)+\n        1.5*IQR(zonal_sst, na.rm=TRUE))) %&gt;% \n  filter(outlier)\n\n# The origin\nori &lt;- round(sum(range(cutpoints$zonal_sst))/2, 2)\n\n# The horizon scale cutpoints\nsca &lt;- round(seq(range(cutpoints$zonal_sst)[1], \n                 range(cutpoints$zonal_sst)[2], \n                 length.out = 7)[-4], 2)\n\nNWA %&gt;% ggplot() +\n  geom_horizon(aes(t,\n                   zonal_sst,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = TRUE) +\n  facet_grid(lat~.) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand=c(0,0),\n               date_breaks = \"1 year\",\n               date_labels = \"%Y\") +\n  xlab('Year') +\n  ggtitle('Canary current system zonal SST')\n\nggplot(temps, aes(monthweek, weekday_f, fill = temp)) + \n  geom_tile(colour = \"white\") +\n  facet_grid(year(t) ~ month) +\n  scale_x_continuous(breaks = c(1, 3, 5)) +\n  scale_y_discrete(breaks = c(\"Sun\", \"Wed\", \"Sat\")) +\n  scale_fill_viridis_c() +\n  xlab(\"Week of Month\") +\n  ylab(\"\") +\n  ggtitle(\"Time-Series Calendar Heatmap: Western Australia SST\") +\n  labs(fill = \"[°C]\")\n\n\n\n\nFigure 14: Zonally average time series of SST in the Canary current system displayed as a horizon plot.\n\n\n\n\nFigure 15: Zonally average time series of SST in the Canary current system displayed as a horizon plot.\n\n\n\n\n\n\n\n\n\nTask C\n\n\n\n\nFollowing on from Task B 9-10, create the necessary accompanying figures that support your EDA using descriptive statistics.\n\nEnsure your presentation is professional and adhere to the standards required by scientific publications. Also, state the major aims of your analysis and the patterns you seek (you may include this with Task B 9-10 and simply refer that it in this answer). Using the combined findings from the EDA and the figures produced here, discuss the findings in a Results section."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html",
    "href": "BCB744/basic_stats/07-t_tests.html",
    "title": "7. t-tests",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nOne-sample t-tests\nTwo-sample s t-tests\nPaired t-tests\nComparison of proportions"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#two-sided-one-sample-t-test",
    "href": "BCB744/basic_stats/07-t_tests.html#two-sided-one-sample-t-test",
    "title": "7. t-tests",
    "section": "\n3.1 Two-sided one-sample t-test",
    "text": "3.1 Two-sided one-sample t-test\n\n\n\n\n\n\nHypothesis for two-sided one-sample t-test\n\n\n\n\\(H_{0}: \\bar{x} = \\mu_{0}\\) and \\(H_{a}: \\bar{x} \\ne \\mu_{0}\\)\nThis is the same as:\n\\(H_{0}: \\bar{x} - \\mu_{0} = 0\\) and \\(H_{a}: \\bar{x} - \\mu_{0} \\ne 0\\)\nHere \\(\\bar{x}\\) is the population mean and \\(\\mu_{0}\\) the hypothesised mean to which \\(\\bar{x}\\) is being compared. In this example we have a two-sided one-sample t-test.\nAs stated above, \\(H_{0}\\) does not make a distinction between whether it expects \\(\\bar{x}\\) to be \\(\\lt\\) or \\(\\gt\\) \\(\\mu_{0}\\).\n\n\nGenerally when we use a t-test it will be a two-sample t-test (see below). Occasionally, however, we may have only one set of observations (random samples taken to represent a population) whose mean, \\(\\bar{x}\\), we wish to compare against a known population mean, \\(\\mu_{0}\\), which had been established a priori (Equation 1). In R’s t.test() function, the default setting is for a two-sided one-sample t-test—that is, we don’t care if our \\(H_{a}\\) is significantly less than \\(\\mu_{0}\\) or if it is significantly greater than \\(\\mu_{0}\\).\n\n\nThe one-sample t-test:\n\\[t = \\frac{\\overline{x} - \\mu}{s / \\sqrt{n}} \\tag{1}\\]\nwhere \\(t\\) is the calculated \\(t\\)-value, \\(\\overline{x}\\) is the sample mean, \\(\\mu\\) is the hypothesised population mean, \\(s\\) is the sample standard deviation, and \\(n\\) the sample size.\n\n# create a single sample of random normal data\nset.seed(666)\nr_one &lt;- data.frame(dat = rnorm(n = 20, mean = 20, sd = 5),\n                    sample = \"A\")\n\n\n\n\n\n\n# compare random data against a population mean of 20\nt.test(r_one$dat, mu = 20)\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = 0.0048653, df = 19, p-value = 0.9962\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 16.91306 23.10133\nsample estimates:\nmean of x \n 20.00719 \n\n# compare random data against a population mean of 30\nt.test(r_one$dat, mu = 30)\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = -6.7596, df = 19, p-value = 1.858e-06\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 16.91306 23.10133\nsample estimates:\nmean of x \n 20.00719 \n\n\nWhat do the results of these two different tests show? Let’s visualise these data to get a better understanding (Figure 1).\n\nggplot(data = r_one, aes(y = dat, x = sample)) +\n  geom_boxplot(fill = \"indianred\", notch = TRUE,\n               alpha = 0.3, colour = \"black\") +\n  # population  mean (mu) = 20\n  geom_hline(yintercept = 20, colour = \"dodgerblue2\", \n             size = 0.9) +\n  # population  mean (mu) = 30\n  geom_hline(yintercept = 30, colour = \"indianred2\", \n             size = 0.9) +\n  labs(y = \"Value\", x = NULL) +\n  coord_flip() +\n  theme_pubclean()\n\n\n\n\nFigure 1: Boxplot of random normal data with. A hypothetical population mean of 20 is shown as a blue line, with the red line showing a mean of 30.\n\n\nThe boxplot shows the distribution of our random data against two potential population means. Does this help now to illustrate the results of our one-sample t-tests?"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sided-one-sample-t-tests",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sided-one-sample-t-tests",
    "title": "7. t-tests",
    "section": "\n3.2 One-sided one-sample t-tests",
    "text": "3.2 One-sided one-sample t-tests\n\n\n\n\n\n\nHypothesis for one-sided one-sample t-test\n\n\n\nFor example, when we are concerned that our sample mean, \\(\\bar{x}\\), should be less than the a priori established value, \\(\\mu_{0}\\):\n\\(H_{0}: \\bar{x} \\ge \\mu_{0}\\) and \\(H_{a}: \\bar{x} \\lt \\mu_{0}\\)\nOnly one of the two options is shown.\n\n\nRemember that a normal distribution has two tails. As indicated already, when we are testing for significance we are generally looking for a result that sits in the far end of either of these tails. Occasionally, however, we may want to know if the result is specifically in one of the two tails. Explicitly the leading or trailing tail. For example, is the mean value of our sample population, \\(\\bar{x}\\), significantly greater than the value \\(\\mu_{0}\\)? Or, is \\(\\bar{x}\\) less than the value \\(\\mu_{0}\\)? This t-test is called a one-sided one-sample t-tests. To specify this in R we must add an argument as seen below:\n\n# check against the trailing tail\nt.test(r_one$dat, mu = 30, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = -6.7596, df = 19, p-value = 9.292e-07\nalternative hypothesis: true mean is less than 30\n95 percent confidence interval:\n     -Inf 22.56339\nsample estimates:\nmean of x \n 20.00719 \n\n# check against the leading tail\nt.test(r_one$dat, mu = 30, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = -6.7596, df = 19, p-value = 1\nalternative hypothesis: true mean is greater than 30\n95 percent confidence interval:\n 17.451    Inf\nsample estimates:\nmean of x \n 20.00719 \n\n\nAre these the results we would have expected? Why does the second test not return a significant result?\n\n\n\n\n\n\nTask E\n\n\n\n\nCreate a visualisation to graphically demonstrate the outcome of this t-test."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#two-sided-two-sample-t-test",
    "href": "BCB744/basic_stats/07-t_tests.html#two-sided-two-sample-t-test",
    "title": "7. t-tests",
    "section": "\n4.1 Two-sided two-sample t-test",
    "text": "4.1 Two-sided two-sample t-test\n\n\n\n\n\n\nHypothesis for two-sided two-sample t-test\n\n\n\n\\(H_{0}: \\bar{A} = \\bar{B}\\) and \\(H_{a}: \\bar{A} \\ne \\bar{B}\\)\nwhere \\(\\bar{A}\\) is the population mean of the first sample and \\(\\bar{B}\\) the population mean of the second sample. In this example we have a two-sided two-sample t-test, which is the default in R’s t.test() function.\nAs stated above, \\(H_{0}\\) does not make a distinction between whether it expects the difference between \\(\\bar{A}\\) and \\(\\bar{B}\\) to be greater than or less than 0.\n\n\nA two-sample t-test is used when we have samples from two different (independent) populations whose means, \\(\\bar{A}\\) and \\(\\bar{B}\\), we would like to compare against one another. Sometimes it is called an independent sample t-test. Specifically, it tests whether the difference between the means of two samples is zero. Note that again we make no distinction between whether it is more interesting that the difference is greater than zero or less zero—as long as there is a difference between \\(\\bar{A}\\) and \\(\\bar{B}\\). This test is called a two-sided two sample t-test and it is the most common use of a t-test.\nThere are two varieties of t-tests. In the case of samples whose variances do not differ, we perform a Student’s t-test. Equation 2 shows how to calculate the t-statistic for Student’s t-test. The other case is if we have unequal variances in \\(\\bar{A}\\) and \\(\\bar{B}\\) (established with the Levene’s test for equality of variances; see Chapter 6); here, we perform Welch’s t-test as written in Equation 4. Welch’s t-test is the default in R’s t.test() function.\n\n\nStudent’s t-test: \\[t=\\frac{\\bar{A}-\\bar{B}}{\\sqrt{\\frac{S^{2}}{n}+\\frac{S^{2}}{m}}} \\tag{2}\\]\n\\(\\bar{A}\\) and \\(\\bar{B}\\) are the means for groups \\(A\\) and \\(B\\), respectively; \\(n\\) and \\(m\\) are the sample sizes of the two sets of samples, respectively; and \\(S^{2}\\) is the pooled variance, which is calculated as per Equation 3:\n\\[S^{2}=\\frac{(n-1)S_{A}^{2}+(m-1)S_{B}^{2} }{n+m-2} \\tag{3}\\]\nThe degrees of freedom, d.f., in the equation for the shared variance is \\(n_{A}+m_{B}-2\\).\n\nWelch’s t-test: \\[t=\\frac{\\bar{A}-\\bar{B}}{\\sqrt{\\frac{S^{2}_{A}}{n}+\\frac{S^{2}_{B}}{m}}} \\tag{4}\\]\nHere, \\(S_{A}\\) and \\(S_{B}\\) are the variances of groups \\(A\\) and \\(B\\), respectively (see Section X). The d.f. to use with Welch’s t-test is obtained using the Welch–Satterthwaite equation (Equation 5):\n\\[d.f. = \\frac{\\left( \\frac{S^{2}_{A}}{n}+\\frac{S^{2}_{B}}{m} \\right)^{2}}{\\left( \\frac{S^{4}_{A}}{n-1} + \\frac{S^{4}_{B}}{m-1} \\right)} \\tag{5}\\]\n\nWhat do we do with this t-statistic? In the olden days we had to calculate the t-statistics and the d.f. by hand. These two values, the d.f. and t-value had to be read off a table of pre-calculated t-values, probabilities and degrees of freedom as in here. Luckily, the t-test function nowadays does this all automagically. But if you are feeling nostalgic over times that you have sadly never experienced, please calculate the t-statistic and the d.f. yourself and give the table a go. In fact, an excessive later in this Chapter will give you an opportunity to do so.\nBack to the present day and the wonders of modern technology. Let’s generate some new random normal data and test to see if the data belonging to the two groups differ significantly from one-another. First, we apply the t-test function as usual:\n\n# random normal data\nset.seed(666)\nr_two &lt;- data.frame(dat = c(rnorm(n = 20, mean = 4, sd = 1),\n                            rnorm(n = 20, mean = 5, sd = 1)),\n                    sample = c(rep(\"A\", 20), rep(\"B\", 20)))\n\n# perform t-test\n# note how we set the `var.equal` argument to TRUE because we know \n# our data has the same SD (they are simulated as such!)\nt.test(dat ~ sample, data = r_two, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  dat by sample\nt = -1.9544, df = 38, p-value = 0.05805\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.51699175  0.02670136\nsample estimates:\nmean in group A mean in group B \n       4.001438        4.746584 \n\n# if the variances are not equal, simply set `var.equal` to false\n# and a Welch's t-test will be performed\n\nThe first argument we see in t.test() is dat ~ sample. Usually in R when we see a ~ (tilde) we are creating what is known as a formula. A formula tells R how it should look for interactions between data and factors. For example Y ~ X reads: \\(Y\\) as a function of \\(X\\). In our code above we see dat ~ sample. This means we are telling R that the t-test we want it to perform is when the dat column is a function of the sample column. In plain English we are dividing up the dat column into the two different samples we have, and then running a t-test on these samples. Another way of stating this is that the value of dat depends on the grouping it belong to (A or B). We will see this same formula notation cropping up later under ANOVAs, linear models, etc.\n\n\n\n\n\n\nTask E\n\n\n\n\nCreate a visualisation to graphically demonstrate the outcome of this t-test."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sided-two-sample-t-test",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sided-two-sample-t-test",
    "title": "7. t-tests",
    "section": "\n4.2 One-sided two-sample t-test",
    "text": "4.2 One-sided two-sample t-test\n\n\n\n\n\n\nHypothesis for one-sided two-sample t-test\n\n\n\nFor example, when we are concerned that the sample mean of the first population, \\(\\bar{A}\\), should be greater than that of the second, \\(\\bar{B}\\):\n\\(H_{0}: \\bar{A} \\le \\bar{B}\\) and \\(H_{a}: \\bar{A} \\gt \\bar{B}\\)\nOnly one of the two options is shown.\n\n\nJust as with the one-sample t-tests above, we may also specify which tail of the distribution we are interested in when we compare the means of our two samples. This is a one-sided two-sample t-test, and here too we have the Student’s t-test and Welch’s t-test varieties. We do so by providing the same arguments as previously:\n\n# is the mean of sample B smaller than that of sample A?\ncompare_means(dat ~ sample, data = r_two, method = \"t.test\", var.equal = TRUE, alternative = \"less\")\n\n# A tibble: 1 × 8\n  .y.   group1 group2     p p.adj p.format p.signif method\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1 dat   A      B      0.971  0.97 0.97     ns       T-test\n\n# is the mean of sample B greater than that of sample A?\ncompare_means(dat ~ sample, data = r_two, method = \"t.test\", var.equal = TRUE, alternative = \"greater\")\n\n# A tibble: 1 × 8\n  .y.   group1 group2      p p.adj p.format p.signif method\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1 dat   A      B      0.0290 0.029 0.029    *        T-test\n\n\nWhat do these results show? Is this surprising?\n\n\n\n\n\n\nTask E\n\n\n\n\nRepeat the above analyses using the old-fashioned t.test() function.\nRepeat the above analysis using the even more old-fashioned Equation 2. Show the code and talk us through the step you followed to read the p-values off the table of t-statistics."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sample-and-two-sample-tests",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sample-and-two-sample-tests",
    "title": "7. t-tests",
    "section": "\n6.1 One-sample and two-sample tests",
    "text": "6.1 One-sample and two-sample tests\nAs with t-tests, proportion tests may also be based on one sample, or two. If we have only one sample we must specify the total number of trials as well as what the expected population probability of success is. Because these are individual values, and not matrices, we will show what this would look like without using any objects but will rather give each argument within prop.test() a single exact value. In the arguments within prop.test(), x denotes the number of successes recorded, n shows the total number of individual trials performed, and p is the expected probability. It is easiest to consider this as though it were a series of 100 coin tosses.\n\n# When the probability matches the population\nprop.test(x = 45, n = 100, p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  45 out of 100, null probability 0.5\nX-squared = 0.81, df = 1, p-value = 0.3681\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.3514281 0.5524574\nsample estimates:\n   p \n0.45 \n\n# When it doesn't\nprop.test(x = 33, n = 100, p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  33 out of 100, null probability 0.5\nX-squared = 10.89, df = 1, p-value = 0.0009668\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2411558 0.4320901\nsample estimates:\n   p \n0.33 \n\n\nIf we have two samples that we would like to compare against one another we enter them into the function as follows:\n\n# NB: Note that the `mosquito` data are a matrix, NOT a data.frame\nprop.test(mosquito)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.05882\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.253309811  0.003309811\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n\nDo mosquito’s bite Jack and Jill at different proportions?\n\n\n\n\n\n\nTask E\n\n\n\n\nDivide the class into two groups, Group A and Group B. In each group, collect data on 100 coin tosses. The intention is to compare the coin tosses across Groups A and B. State your hypothesis. Test it. Discuss."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sided-and-two-sided-tests",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sided-and-two-sided-tests",
    "title": "7. t-tests",
    "section": "\n6.2 One-sided and two-sided tests",
    "text": "6.2 One-sided and two-sided tests\nAs with all other tests that compare values, proportion tests may be specified as either one or two-sided. Just to be clear, the default setting for prop.test(), like everything else, is a two-sided test. See code below to confirm that the results are identical with or without the added argument:\n\n# Default\nprop.test(mosquito)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.05882\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.253309811  0.003309811\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n# Explicitly state two-sided test\nprop.test(mosquito, alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.05882\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.253309811  0.003309811\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n\nShould we want to specify only one of the tails to be considered, we do so precisely the same as with t-tests. Below are examples of what this code would look like:\n\n# Jack is bit less than Jill\nprop.test(mosquito, alternative = \"less\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.02941\nalternative hypothesis: less\n95 percent confidence interval:\n -1.00000000 -0.01597923\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n# Jack is bit more than Jill\nprop.test(mosquito, alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.9706\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.2340208  1.0000000\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n\nDo these results differ from the two-sided test? What is different?"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#loading-data",
    "href": "BCB744/basic_stats/07-t_tests.html#loading-data",
    "title": "7. t-tests",
    "section": "\n7.1 Loading data",
    "text": "7.1 Loading data\nBefore we can run any analyses we will need to load our data. We are also going to convert these data from their wide format into a long format because this is more useful for the rest of our workflow.\n\necklonia &lt;- read_csv(\"../../data/ecklonia.csv\") %&gt;% \n  gather(key = \"variable\", value = \"value\", -species, -site, -ID)"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#visualising-data",
    "href": "BCB744/basic_stats/07-t_tests.html#visualising-data",
    "title": "7. t-tests",
    "section": "\n7.2 Visualising data",
    "text": "7.2 Visualising data\nWith our data loaded, let’s visualise them in order to ensure that these are indeed the data we are after (Figure 2). Visualising the data will also help us to formulate a hypothesis.\n\nggplot(data = ecklonia, aes(x = variable, y = value, fill = site)) +\n  geom_boxplot(colour = \"black\", fill = \"dodgerblue4\", alpha = 0.4) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nFigure 2: Boxplots showing differences in morphometric properties of the kelp Ecklonia maxima at two sites in False Bay.\n\n\nThe first thing we should notice from the figure above is that our different measurements are on very different scales. This makes comparing all of our data visually rather challenging. Even given this complication, one should readily be able to make out that the measurement values at Batsata Rock appear to be greater than at Boulders Beach. Within the framework of the scientific process, that is what we would call an ‘observation’, and is the first step towards formulating a hypothesis. The next step is to refine our observation into a hypothesis. By what measurement are the kelps greater at one site than the other?"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#formulating-a-hypothesis",
    "href": "BCB744/basic_stats/07-t_tests.html#formulating-a-hypothesis",
    "title": "7. t-tests",
    "section": "\n7.3 Formulating a hypothesis",
    "text": "7.3 Formulating a hypothesis\nLooking at the figure above it appears that for almost all measurements of length, Batsata Rock far exceeds that of Boulders Beach however, the stipe masses between the two sites appear to be more similar. Let’s pull out just this variable and create a new boxplot (Figure 3).\n\n# filter the data\necklonia_sub &lt;- ecklonia %&gt;% \n  filter(variable == \"stipe_mass\")\n\n# then create a new figure\nggplot(data = ecklonia_sub, aes(x = variable, y = value, fill = site)) +\n  geom_boxplot(colour = \"black\", alpha = 0.4) +\n  coord_flip() +\n  labs(y = \"Stipe mass (kg)\", x = \"\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  theme_minimal()\n\n\n\n\nFigure 3: Boxplots showing the difference in stipe mass (kg) of the kelp Ecklonia maxima at two sites in False Bay.\n\n\nNow we have a more interesting comparison at hand. The question I think of when I look at these data is “Are the stipe masses at Batsata Rock greater than at Boulders Beach?”. The hypothesis necessary to answer this question would look like this:\n\n\nH0: Stipe mass at Batsata Rock is not greater than at Boulders Beach.\n\nHa: Stipe mass at Batsata Rock is greater than at Boulders Beach.\n\nOr more formally:\n\n\\(H_{0}: \\bar{A} \\leq \\bar{B}\\)\n\n\\(H_{a}: \\bar{A} &gt; \\bar{B}\\).\n\nWhich test must we use for this hypothesis?"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#choosing-a-test",
    "href": "BCB744/basic_stats/07-t_tests.html#choosing-a-test",
    "title": "7. t-tests",
    "section": "\n7.4 Choosing a test",
    "text": "7.4 Choosing a test\nBefore we can pick the correct statistical test for our hypothesis, we need to be clear on what it is we are asking. Starting with the data being used is usually a good first step. As we may see in the above figure, we have two sample sets that we are comparing. Therefore, unsurprisingly, we will likely be using a t-test. But we’re not done yet. How is it that we are comparing these two sample sets? Remember from the examples above that there are multiple different ways to compare two sets of data. For our hypothesis we want to see if the stipe mass at Batsata Rock is greater than the stipe mass at Boulders Beach, not just that they are different. Because of this we will need a one-sided t-test. But wait, there’s more! We’ve zeroed in on which sort of test would be appropriate for our hypothesis, but before we run it we need to check our assumptions."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#checking-assumptions",
    "href": "BCB744/basic_stats/07-t_tests.html#checking-assumptions",
    "title": "7. t-tests",
    "section": "\n7.5 Checking assumptions",
    "text": "7.5 Checking assumptions\nIn case we forgot, here are the assumptions for a t-test:\n\nthe dependent variable must be continuous,\nthe observations in the groups being compared are independent of each other,\nthe data are normally distributed, and\nthat the data are homoscedastic, and in particular, that there are no outliers.\n\nWe know that the first two assumptions are met because our data are measurements of mass at two different sites. Before we can run our one-sided t-test we must meet the last two assumptions. Lucky us, we have a function tat will do that automagically.\nPlease refer to Chapter 6 to see what to do if the assumptions fail.\n\necklonia_sub %&gt;% \n  group_by(site) %&gt;% \n  summarise(stipe_mass_var = two_assum(value)[1],\n            stipe_mass_norm = two_assum(value)[2])\n\n# A tibble: 2 × 3\n  site           stipe_mass_var stipe_mass_norm\n  &lt;chr&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\n1 Batsata Rock             2.00           0.813\n2 Boulders Beach           2.64           0.527\n\n\nLovely. The variances are equal and the data are normal. On to the next step."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#running-an-analysis",
    "href": "BCB744/basic_stats/07-t_tests.html#running-an-analysis",
    "title": "7. t-tests",
    "section": "\n7.6 Running an analysis",
    "text": "7.6 Running an analysis\nWith our assumptions checked, we may now analyse our data. We’ll see below how to do this with both of the functions we’ve learned in this chapter for comparing means of two sample sets.\n\nt.test(value ~ site, data = ecklonia_sub, var.equal = TRUE, alternative = \"greater\")\n\n\n    Two Sample t-test\n\ndata:  value by site\nt = 1.8741, df = 24, p-value = 0.03657\nalternative hypothesis: true difference in means between group Batsata Rock and group Boulders Beach is greater than 0\n95 percent confidence interval:\n 0.09752735        Inf\nsample estimates:\n  mean in group Batsata Rock mean in group Boulders Beach \n                    6.116154                     4.996154"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#interpreting-the-results",
    "href": "BCB744/basic_stats/07-t_tests.html#interpreting-the-results",
    "title": "7. t-tests",
    "section": "\n7.7 Interpreting the results",
    "text": "7.7 Interpreting the results\nWe may reject the null hypothesis that the stipe mass of kelps at Batsata Rock is not greater than at Boulders Beach if our t-test returns a p-value \\(\\leq\\) 0.05. We must also pay attention to some of the other results from our t-test, specifically the t-value (t) and the degrees of freedom (df) as these are also needed when we are writing up our results. From all of the information above, we may accept the alternative hypothesis. But how do we write that up?"
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#drawing-conclusions",
    "href": "BCB744/basic_stats/07-t_tests.html#drawing-conclusions",
    "title": "7. t-tests",
    "section": "\n7.8 Drawing conclusions",
    "text": "7.8 Drawing conclusions\nThere are many ways to present ones findings. Style, without too much flourish, is encouraged as long as certain necessary pieces of information are provided. The sentence below is a very minimalist example of how one may conclude this mini research project. A more thorough explanation would be desirable.\n\nThe stipe mass (kg) of the kelp Ecklonia maxima was found to be significantly greater at Batsata Rock than at Boulders Beach (p = 0.03, t = 1.87, df = 24)."
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#going-further",
    "href": "BCB744/basic_stats/07-t_tests.html#going-further",
    "title": "7. t-tests",
    "section": "\n7.9 Going further",
    "text": "7.9 Going further\nBut why though? As is often the case in life, and science is no exception, answers to our questions just create even more questions! Why would the mass of kelp stipes at one locations in the same body of water and only a kilometre or so apart be significantly different? It looks like we are going to need to design a new experiment… Masters thesis anyone?"
  },
  {
    "objectID": "BCB744/basic_stats/11-glance.html",
    "href": "BCB744/basic_stats/11-glance.html",
    "title": "11. Non-parametric statistical tests at a glance",
    "section": "",
    "text": "In Chapters 7, 8, 9, and 10 we have seen t-tests, ANOVAs, simple linear regressions, and correlations. These tests may be substituted with non-parametric tests if our assumptions about our data fail us. The most commonly encountered non-parametric methods include the following:\n\nWilcoxon rank-sum test The test is used when the two samples being compared are related, meaning that each observation in one sample is paired with a corresponding observation in the other sample. The test is designed to detect whether there is a difference between the paired observations. Specifically, the Wilcoxon signed-rank test ranks the absolute differences between the pairs of observations, and then compares the sum of the ranks for positive differences to the sum of the ranks for negative differences. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the null hypothesis that there is no difference between the paired observations. Use the Wilcoxon test as a non-parametric substitute for a paired sample t-test. See wilcox.test().\nMann-Whitney \\(U\\) test This test is used when the two samples being compared are independent, meaning that there is no pairing between observations in the two samples. The test is designed to detect whether there is a difference between the two groups based on the ranks of the observations. Specifically, the Mann-Whitney \\(U\\) test ranks all observations from both samples, combines the ranks across the two samples, and calculates a test statistic (\\(U\\)) that indicates whether one sample tends to have higher ranks than the other sample. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference between the two groups. Use this test in stead of a one- or two-sample t-test when assumptions of normality or homoscedasticity are not met. See wilcox.test().\nKruskal-Wallis test The Kruskal-Wallis test is a non-parametric statistical test used to compare three or more independent groups on a continuous outcome variable. The test is designed to detect whether there is a difference in the medians of the groups. The Kruskal-Wallis test works by ranking all the observations from all the groups, then calculating a test statistic (\\(H\\)) that measures the degree of difference in the ranked values between the groups. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference in the medians of the groups. The Kruskal-Wallis test is often used as a non-parametric alternative to the one-way ANOVA. See kruskal.test().\nFriedman test This test is a non-parametric statistical test used to compare three or more related groups on a continuous outcome variable. The test is designed to detect whether there is a difference in the medians of the groups. The Friedman test works by ranking all the observations within each group, then calculating a test statistic (\\(\\chi^2\\)) that measures the degree of difference in the ranked values between the groups. The test produces a \\(p\\)-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference in the medians of the groups. The Friedman test is often used as a non-parametric alternative to the repeated measures ANOVA. You can use the friedman_test() in the rstatix package or the friedman.test() in Base R.\n\nTables 1 and 2 summarise common parametric and non-parametric statistical tests, along with a brief explanation of each test and the most common R function used to perform the test. Non-parametric tests are robust alternatives to parametric tests when the assumptions of the parametric test are not met. Also provided is additional information on the nature of the independent (IV) and dependent variables (DV) for each test.\n\nTable 1: When our data are normal with equal variances across groups, choose the suitable parametric test\n\n\n\n\n\n\n\n\n\n\nStatistical Test\nExplanation\nVariables\nR Function\nNon-Parametric Substitute\n\n\n\n\nParametric Tests\n\n\n\n\n\n\nPaired-sample t-test\nTests if the difference in means between paired samples is significantly different from zero. Assumes normality and equal variances.\nContinuous (DV)\nt.test(..., var.equal = TRUE)\nWilcoxon signed-rank test\n\n\nStudent’s t-test\nTests if the means of two independent groups are significantly different. Assumes normality and equal variances.\nContinuous (DV) and categorical (IV)\nt.test(..., var.equal = TRUE)\nMann-Whitney U test\n\n\nWelch’s t-test (unequal variances)\nUse this test when data are normal but variances differ between the two groups. It can be used for paired- and two-sample t-tests.\nContinuous (DV) and categorical (IV)\nt.test()\nMann-Whitney U test or Wilcoxon signed-rank test\n\n\nANOVA (one-way ANOVA only; ANOVAs with interactions do not have non-parametric tests)\nTests if the means of three or more independent groups are significantly different. Assumes normality, equal variances, and independence.\nContinuous (DV) and categorical (IV)\naov()\nKruskal-Wallis test\n\n\nANOVA with Welch’s approximation of variances\nTests if the means of three or more independent groups are significantly different. Assumes normality but variances may differ.\nContinuous (DV) and categorical (IV)\noneway.test()\nKruskal-Wallis test\n\n\nRegression Analysis\nModels the relationship between two continuous variables. Assumes linearity, normality, and equal variances of errors.\nContinuous (DV) and continuous (IV)\nlm()\nN/A\n\n\nPearson Correlation\nMeasures the strength and direction of the linear relationship between two continuous variables. Assumes normality and linearity.\nContinuous (DV) and continuous (IV)\ncor.test()\nSpearman’s \\(\\rho\\) or Kendall’s \\(\\tau\\) rank correlation\n\n\n\n\nTable 2: Should the data not be normal and/or are heteroscedastic, substitute the parametric test with a non-parametric option.\n\n\n\n\n\n\n\n\n\n\n\nStatistical Test\nExplanation\nVariables\nR Function\nParametric Equivalent\n\n\n\n\nNon-Parametric Tests\n\n\n\n\n\n\nWilcoxon signed-rank test\nTests if the medians of two related samples are significantly different. Does not assume normality.\nContinuous (DV)\nwilcox.test()\nPaired-sample t-test\n\n\nMann-Whitney U test\nTests if the medians of two independent groups are significantly different. Does not assume normality or equal variances.\nContinuous (DV) and categorical (IV)\nwilcox.test()\nStudent’s t-test\n\n\nKruskal-Wallis test\nTests if the medians of three or more independent groups are significantly different. Does not assume normality or equal variances.\nContinuous (DV) and categorical (IV)\nkruskal.test()\nANOVA, or ANOVA with Welch’s approximation of variances\n\n\nFriedman test\nTests if the medians of three or more related samples are significantly different. Does not assume normality.\nContinuous (DV) and categorical (IV)\nfriedman.test()\nRepeated measures ANOVA\n\n\nSpearman’s rank correlation\nMeasures the strength and direction of the monotonic relationship between two continuous variables. Does not assume normality or linearity.\nContinuous (DV) and continuous (IV)\ncor.test(method = \"spearman\")\nPearson correlation\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {11. {Non-parametric} Statistical Tests at a Glance},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/basic_stats/11-glance.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 11. Non-parametric statistical tests at a glance. https://tangledbank.netlify.app/BCB744/basic_stats/11-glance.html."
  },
  {
    "objectID": "BCB744/BCB744_index.html",
    "href": "BCB744/BCB744_index.html",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "",
    "text": "“Most people use statistics like a drunk man uses a lamppost; more for support than illumination.”\n— Andrew Lang\nWelcome to the pages for BCB744. This page provides the syllabus and teaching policies for the module, and it serves is a starting point accessing all the theory, instruction, and data."
  },
  {
    "objectID": "BCB744/BCB744_index.html#core-theoretical-content-and-philosophical-framework-of-the-intro-r-workshop",
    "href": "BCB744/BCB744_index.html#core-theoretical-content-and-philosophical-framework-of-the-intro-r-workshop",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n5.1 Core theoretical content and philosophical framework of the Intro R Workshop",
    "text": "5.1 Core theoretical content and philosophical framework of the Intro R Workshop\nThe Intro R Workshop focuses on the functionality offered by the tidyverse suite of packages. I designed the Workshop to introduce students to a powerful set of tools for data manipulation, exploration, and visualisation. The tidyverse is a collection of R packages that work together to provide a cohesive set of functions for working with data. This course will cover the most popular packages in the tidyverse, including tidyr for data reshaping, dplyr for data ‘wrangling’, and ggplot2 for data visualisation. Students will learn how to clean, transform, and visualise data, as well as how to use these tools to build reproducible and informative data analysis pipelines. With a focus on practical application and hands-on exercises, students will gain the skills and knowledge needed to effectively use the tidyverse in their own data analysis projects.\nOne of the key heuristic devices I use throughout is to focus on figures, particularly maps. Maps can be a powerful tool for teaching coding to novices because they combine both aesthetics and information in a visually compelling format. Maps are not only beautiful, but they also provide insights and context that can be difficult to grasp through numbers or text alone. By working with code to create their own maps, students can learn programming concepts and techniques while also developing their visual literacy skills. This engaging and interactive approach to learning coding can help to demystify the subject and make it more accessible and enjoyable for beginners."
  },
  {
    "objectID": "BCB744/BCB744_index.html#core-statistical-content-of-the-biostatistics-workshop",
    "href": "BCB744/BCB744_index.html#core-statistical-content-of-the-biostatistics-workshop",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n5.2 Core statistical content of the Biostatistics Workshop",
    "text": "5.2 Core statistical content of the Biostatistics Workshop\nIn the biological and ecological sciences, statistical methods play a crucial role in analysing and interpreting data. Some of the basic statistical methods used in these fields include:\n\nDescriptive statistics These methods are used to summarise and describe the basic features of a dataset, such as the mean, median, and standard deviation.\nInferential statistics These methods allow researchers to make predictions and inferences about a population based on a sample of data. Common inferential statistical techniques include t-tests, ANOVA, and regression analysis.\nNon-parametric statistics Non-parametric methods are called for when the data do not meet the assumptions of parametric statistics, and are often used in ecological and biological research where normality assumptions may not hold. Examples of non-parametric techniques include Wilcoxon rank-sum test and Kruskal-Wallis test.\n\nThese basic statistical methods provide biologists and ecologists with valuable tools to test hypotheses, draw conclusions, and make informed decisions based on their data."
  },
  {
    "objectID": "BCB744/BCB744_index.html#daily-tasks",
    "href": "BCB744/BCB744_index.html#daily-tasks",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n16.1 Daily Tasks",
    "text": "16.1 Daily Tasks\nIntro R and Biostatistics require that we work with real-world datasets. To this end, a series of Daily Tasks involving real data is a required part of the BCB744. These are interwoven into the daily Chapters, and your feedback is required the following day. The Daily Tasks are part of the CA.\nWhen assessing the tasks, we will pay attention to the following criteria:\n\nContent (10%):\n\nQuestions answered in order\nAnnotations (meta-data in file header, commnets about code, ideas, and approach)\n\n\nCode formatting and correctness (45%):\n\nApplication of R code conventions, e.g. spaces around &lt;-, after #, after ,, etc.\nNew line for each dplyr function (lines end in %&gt;%) or ggplot layer (lines end in +)\n\n\nFigures (45%):\n\nSensible use of themes / colours\nPublication quality\nInformative and complete titles, axes labels, legends, etc."
  },
  {
    "objectID": "BCB744/BCB744_index.html#sec-summative",
    "href": "BCB744/BCB744_index.html#sec-summative",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n16.2 Summative Tasks",
    "text": "16.2 Summative Tasks\nAt the end of Intro R and Biostatistics, the more demanding Summative Tasks 1 and 2 will be required. As the table above indicates, the weighting of the Summative Tasks is more than that of the Daily Tasks but they also differ slightly between the two parts of the module. These assessments will take place over a few days and you may work at home or on campus. Like the Daily Tasks, the Summative Tasks are also open book assessments. The Summative Tasks form part of the CA and will prepare you for the Intro R Assessment (at the end of Intro R) and the Integrative Assessment (after Biostatistics; see ‘Final Assessment’, below).\nWe will assess the Summative Tasks as per the assessment breakdown provided under the Daily Tasks."
  },
  {
    "objectID": "BCB744/BCB744_index.html#intro-r-assessment",
    "href": "BCB744/BCB744_index.html#intro-r-assessment",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n16.3 Intro R Assessment",
    "text": "16.3 Intro R Assessment\nThe Intro R Assessment will, as the name suggests, be about the material covered in the Introduction to R section of the work only. It forms part of the CA.\nThe Intro R Assessment will be graded with the following in expectations in mind:\n\nContent (20%):\n\nQuestions answered in order\nA written explanation of approach included for each question\nAppropriate formatting of text, for example, fonts not larger than necessary, headings used properly, etc. Be sensible and tasteful.\n\n\nCode formatting, structure, and correctness (50%):\n\nUse Tidyverse code\nNo more than ~80 characters of code per line (pay particular attention to the comments)\nApplication of R code conventions, e.g. spaces around &lt;-, after #, after ,, etc.\nNew line for each dplyr function (lines end in %&gt;%) or ggplot layer (lines end in +)\nProper indentation of pipes and ggplot() layers\nAll chunks labelled without spaces\nNo unwanted / commented out code left behind in the document\n\n\nFigures (30%):\n\nSensible use of themes / colours\nPublication quality\nInformative and complete titles, axes labels, legends, etc.\nNo redundant features or aesthetics"
  },
  {
    "objectID": "BCB744/BCB744_index.html#integrative-final-assessment",
    "href": "BCB744/BCB744_index.html#integrative-final-assessment",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n16.4 Integrative (Final) Assessment",
    "text": "16.4 Integrative (Final) Assessment\nThe Integrative Assessment is the Final Assessment. As such, it will test your skills broadly across both Intro R and Biostatistics, and hence its weighting causes it to contribute more towards your final grade as it falls outside of the pool of marks that form your CA. The Integrative Assessment may be up to five days in duration. It will involve the analysis of real world data. Some of the Questions might expect (as per Question) that you write 1) statements of aims, objectives, and hypotheses; 2) the full and detailed methods followed by analyses together with all code, 3) full reporting of results in a manner suited for peer reviewed publications; 4) graphical support highlighting the patterns observed (again with the code), and 5) a discussion if and when required. The weighting of marks to these various sections is:\n\nAims, objectives, and hypotheses: 5%\nMethods and analyses: 45%\nResults: 20%\nGraphs: 15%\nDiscussion: 15%\n\nOther Questions might be shorter in nature, designed to specifically test important aspects of BCB744. Such Question might be worth anything from 10 to 50 marks, depending on the nature of the Questions.\nThe Integrative Assessment is also open book. Go home. Look at the questions. Answer them at home. Submit them by the deadline."
  },
  {
    "objectID": "BCB744/BCB744_index.html#submission-of-tasks-assessments-and-the-final-assessment",
    "href": "BCB744/BCB744_index.html#submission-of-tasks-assessments-and-the-final-assessment",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n16.5 Submission of Tasks, Assessments, and the Final Assessment",
    "text": "16.5 Submission of Tasks, Assessments, and the Final Assessment\nA statement such as the one below accompanies every assignment—pay attention, as failing to observe this instruction may result in a loss of marks (i.e. if an assignment remains ungraded because the owner of the material cannot be identified):\nSubmit a Quarto script wherein you provide answers to Questions by no later than 8:00 the following data (or the Monday in cases when assignments were given on Fridays). Label the script as follows (e.g.): BCB744_AJ_Smit_Task_A.qmd."
  },
  {
    "objectID": "BCB744/BCB744_index.html#late-submission-of-tasks",
    "href": "BCB744/BCB744_index.html#late-submission-of-tasks",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n16.6 Late submission of Tasks",
    "text": "16.6 Late submission of Tasks\nLate assignments will be penalised 10% per day and will not be accepted more than 48 hours late, unless evidence such as a doctor’s note, a death certificate, or another documented emergency can be provided. If you know in advance that a submission will be late, please discuss this and seek prior approval. This policy is based on the idea that in order to learn how to translate your human thoughts into computer language (coding) you should be working with them at multiple times each week—ideally daily. Time has been allocated in class for working on assignments and students are expected to continue to work on the assignments outside of class. Successfully completing (and passing) this module requires that you finish assignments based on what we have covered in class by the following class period. Work diligently from the onset so that even if something unexpected happens at the last minute you should already be close to done. This approach also allows rapid feedback to be provided to you, which can only be accomplished by returning assignments quickly and punctually."
  },
  {
    "objectID": "BCB744/BCB744_index.html#help-via-bcb744-issues-on-github",
    "href": "BCB744/BCB744_index.html#help-via-bcb744-issues-on-github",
    "title": "BCB744: Introduction to R, and Biostatistics",
    "section": "\n17.1 Help via BCB744 Issues on GitHub",
    "text": "17.1 Help via BCB744 Issues on GitHub\nAll discussion for the BCB744 and BCB743 workshops will be held in the Issues of this repository. Please post all content-related questions there, and use email only for personal matters. Note that this is a public repository, so be professional in your writing here (grammar, etc.).\nTo start a new thread, create a New issue. Tag your peers using their handle—@ajsmit, for example—to get their attention.\nOnce a question has been answered, the issue will be closed, so lots of good answers might end up in closed issues. Don’t forget to look there when looking for answers—you can use the Search feature on this repository to find answers that might have been offered by the same or similar problem experienced by someone else in the past.\nGuidelines for posting questions:\n\nFirst search existing issues (open or closed) for answers. If the question has already been answered, you’re done! If there is an open issue, feel free to contribute to it. Or feel free to open a closed issue if you believe the answer is not satisfactory.\nGive your issue an informative title.\n\nGood: “Error: could not find function”ggplot””\nBad: “My code does not work!” Note that you can edit an issue’s title after it’s been posted.\n\n\nFormat your questions nicely using markdown and code formatting. Preview your issue prior to posting.\nAs I explained above, your peers and I will more sympathetic to your cause if you can show all the things you have tried as you, yourself, tried to fix the issue first.\nInclude code and example data so the person trying to help you have something to work with (and which results in the error, perhaps)\nWhere appropriate, provide links to specific files, or even lines within them, in the body of your issue. This will help your peers understand your question. Note that only the teaching team will have access to private repos.\n(Optional) Tag someone or some group of people. Start by typing their GitHub username prefixed with the @ symbol. Of course this supposes that each of you have a GitHub account and username.\nHit Submit new issue when you’re ready to post."
  },
  {
    "objectID": "BCB744/intro_r/16-base_r.html",
    "href": "BCB744/intro_r/16-base_r.html",
    "title": "16. Base R primer",
    "section": "",
    "text": "Please note that the following chapter departs from the syntax employed by the tidyverse, as utilised throughout this workshop, in favour of the base R syntax. This may be changed in the future, but has been left for now in order to better highlight the fundamental machinations of the R language, upon which the tidyverse is based.\n\n1 Dataframes\nThe ‘workhorse’ data-containing structures you will use extensively in R are called dataframes. In fact, almost all of the work you do in R will be done directly with dataframes or will involve converting data into a dataframe. A dataframe is used for storing data as tables, with a table defined by a collection of vectors of similar or dissimilar data types but all of the same length. Don’t worry if any of those terms are unknown or daunting. We will cover them in detail just now. But first we need to see what a dataframe looks like in order to provide context for all of the parts they consist of. After we have covered all of the terms used for data in R we will learn some methods of creating our own dataframes.\nTo load a dataframe into R is quite simple when the data are already in the ‘.Rdata’ format. Let’s load a small dataframe that was prepared for this class and see. The file extension ‘.Rdata’ does not mean necessarily that the data are in a dataframe (table) format. This file extensions is actually a form of data compression unique to R and could hold anything from a single letter to the results of a complex species distribution model. For the following line of code to work we must make sure we are in the ‘Intro_R_Workshop’ project.\n\nload(\"../../data/intro_data.Rdata\")\n\nUpon loading the data frame we see in the Environment tab that there is a little blue circle next to our object. If we click on that we see a summary of each column. First it says what the data type for that column is and then shows the first several values therein.\nIf you click on the ‘intro_data’ word in your Environment tab it will open it in your Source Editor and allow you to click on the columns to organise them by ascending or descending order. Note that this does not change the dataframe, it is only a visual aid.\n\n2 Basic data types\nThere are several basic R data types that you frequently encounter in daily work. These include but are not limited to numeric, integer, logical, character, factor and date classes. All of these data types are present in our ‘intro_data’ dataframe for us to see practical examples. We will create our own examples as we go along.\n\n3 Numeric\nNumeric data with decimal values are called numeric in R. It is the default computational data type. If we look at our data frame we see that the following columns are numeric: lon, lat, NA.perc, mean, min and max. What sort of data are these?\nLet’s create our own numeric object by assigning a decimal value to a variable x as follows, x will be of numeric type:\n\nx &lt;- 1.2 # assign 1.2 to x\nx # print the value of x\n\n[1] 1.2\n\nclass(x) # what is the class of x?\n\n[1] \"numeric\"\n\n\nFurthermore, even if we assign a number to a variable k that doesn’t have a decimal place, it is still being saved as a numeric value:\n\nk &lt;- 1\nk\n\n[1] 1\n\nclass(k)\n\n[1] \"numeric\"\n\n\nIf we want to really be certain that k is or is not an integer we use is.integer():\n\nis.integer(k) # is k an integer?\n\n[1] FALSE\n\n\n\n4 Integer\nAn integer in R is a numeric value that does not have a decimal place. It may only be a round whole number. Integers are often used for count data and when converting qualitative data to numbers for data analysis. In our dataframe we may see that we have two integer columns: depth and length. Why are these integers?\nIn order to create your own integer variable(s) in R, we use the as.integer(). We can be assured that y is indeed an integer by checking with is.integer():\n\ny &lt;- as.integer(13)\ny\n\n[1] 13\n\nclass(y)\n\n[1] \"integer\"\n\nis.integer(y) # is it an integer?\n\n[1] TRUE\n\n\nIf we really have to, we can coerce a numeric value into an integer with the same as.integer() function:\n\nz &lt;- as.integer(pi)\nz\n\n[1] 3\n\nclass(z)\n\n[1] \"integer\"\n\nis.integer(z) # is it an integer?\n\n[1] TRUE\n\n\n\n5 Logical\nThere are several logic values in R. We are mostly going to be concerned with the two main values we will be encountering: TRUE and FALSE. Note that all letters must be upper case. In our dataframe we see that only the ‘thermo’ column is logical. This column tells us whether or not the data were collected with a thermometer or not.\nLogical values (TRUE or FALSE) are often created via comparison between variables:\n\nx &lt;- 1; y &lt;- 2 # sample values\nz &lt;- x &gt; y\nz\n\n[1] FALSE\n\nclass(z)\n\n[1] \"logical\"\n\n\nIn order to perform logical operations we mostly use & (and), | (or), and ! (negation):\n\nu &lt;- TRUE; v &lt;- FALSE; w &lt;- TRUE; x &lt;- FALSE\nu & v\n\n[1] FALSE\n\nu & w\n\n[1] TRUE\n\nv & x\n\n[1] FALSE\n\nu | v\n\n[1] TRUE\n\n!u\n\n[1] FALSE\n\n\nAlthough these logical operators can be immensely useful in more advanced R programming, we will not go into too much detail in this introductory course. For more information on the logical operators, see the R help material:\n\nhelp(\"&\")\n\nOne final thing to note about logic in R is that it can be useful to perform arithmetic on logical values. TRUE has the value 1, while FALSE has value 0:\n\nas.integer(TRUE) # the numeric value of TRUE\n\n[1] 1\n\nas.integer(FALSE) # the numeric value of FALSE\n\n[1] 0\n\nsum(as.integer(intro_data$thermo))\n\n[1] 10\n\n\nWhat is this telling us?\n\n6 Character\nIn our dataframe we see that only the ‘src’ column has the character values. This column is showing us which government body etc. collected the data in that row. At the use of a very familiar word, character, one may think this data type must be the most straightforward. This is not necessarily so as character values are used to represent string values in R. Because computers do not understand text the same way we do, they tend to handle this information differently. This allows us to do some pretty wild stuff with character values, but we won’t be getting into that in this course as it quickly becomes very technical and generally speaking isn’t very useful in a daily application.\nIf however we wanted to convert an object to a character value we would do so with as.character():\n\nd &lt;- as.character(pi)\nclass(d)\n\n[1] \"character\"\n\n\nThis can be useful if you have data that you want to be characters, but for one reason or another R has decided to make it a different data type.\nIf you want to join two character objects they can be concatenated with the paste() function:\n\na &lt;- \"fluffy\"; b &lt;- \"bunny\"\npaste(a, b)\n\n[1] \"fluffy bunny\"\n\npaste(a, b, sep = \"-\")\n\n[1] \"fluffy-bunny\"\n\n\nMore functions for string manipulation can be found in the R documentation — type help(\"sub\") at the command prompt. You may also wish to install Hadley Wickham’s nifty stringr package for more cool ways to work with character strings.\n\n7 Factor\nFactor values are somewhat difficult to explain and often even more difficult to understand. Factor values appear the same as character values when we look at them in a spreadsheet. But they are not the same. This will lead to much wailing and gnashing of teeth. So why then do factors exist and why would we use them? Factors allow us to numerically order names non-alphabetically, for example. This then allows one to order a list of research sites in geographical order.\nWe will see many examples of factors during this course but for now look at the ‘site’ column in our dataframe. If we click on this column a couple of times we see that it reorders all the data based on ascending or descending order of the sites. But that order is not alphabetical, it is based on the levels within the factor column. Each factor value in a column is assigned a level integer value (e.g. 1, 2, 3, 4, etc.). If multiple values in a factor column are the same, they receive the same level value as well.\nIf we want to see what the levels within a factor column are we use levels():\n\nlevels(intro_data$site)\n\n [1] \"Port Nolloth\"  \"St Helena Bay\" \"Saldanha Bay\"  \"Muizenberg\"   \n [5] \"Cape Agulhas\"  \"Mossel Bay\"    \"Tsitsikamma\"   \"Humewood\"     \n [9] \"Hamburg\"       \"Durban\"        \"Richards Bay\"  \"Sodwana\"      \n\n\nWe will discuss in the next session what that $ means. But for now, are you able to see what the pattern is in the levels of the site listing?\nIf we want to create our own factors we will use as.factor():\n\nf &lt;- as.factor(letters[1:5])\nlevels(f)\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\nAnd if we want to change the order of our factor levels we use factor():\n\nf &lt;- factor(f, levels = c(\"b\", \"a\", \"c\", \"e\", \"d\"))\nlevels(f)\n\n[1] \"b\" \"a\" \"c\" \"e\" \"d\"\n\n\nAnother reason for using factors to re-order our data, as we shall see tomorrow, is that this allows us to control the order in which values are plotted.\n\n8 Dates\nSee the next chapter about dates.\n\n\nDates.\n\n\n9 Vectors\nA vector, by definition, is a one-dimensional sequence of data elements of the same basic type (class). Members in a vector are officially called components. Basically, a vector is a column. Indeed, a dataframe is nothing more than a collection of vectors stuck together. If we wanted to create a vector from our dataframe we would do this:\n\nlonely_vector &lt;- intro_data$NA.perc\n\nNotice that we may not click on the object lonely_vector in our Environment tab. This is because it is no longer two-dimensional. If we want to visualise the data we need to enter it into the console or run it from our script:\n\nlonely_vector\n\n [1]  6 41 32  4 28 26  8  3  6 67 38 16\n\n\nLet’s create some vectors of our own:\n\nprimes1 &lt;- c(3, 5, 7)\nprimes1\n\n[1] 3 5 7\n\nclass(primes1)\n\n[1] \"numeric\"\n\np1 &lt;- pi\np2 &lt;- 5\np3 &lt;- 7\n\nprimes2 &lt;- c(p1, p2, p3)\nprimes2\n\n[1] 3.141593 5.000000 7.000000\n\nclass(primes2)\n\n[1] \"numeric\"\n\nis.numeric(primes2)\n\n[1] TRUE\n\nis.integer(primes2) # integers coerced into floating point numbers\n\n[1] FALSE\n\n\nWe can also have vectors of logical values or character strings, and we can use the function length() to see how many components each has:\n\ntf &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE)\ntf\n\n[1]  TRUE FALSE  TRUE FALSE FALSE\n\nlength(tf)\n\n[1] 5\n\ncs &lt;- c(\"Mary\", \"has\", \"a\", \"silly\", \"lamb\")\ncs\n\n[1] \"Mary\"  \"has\"   \"a\"     \"silly\" \"lamb\" \n\nlength(cs)\n\n[1] 5\n\n\nOf course one would seldom enter data into R using the c() (combine) function, but it is useful for short calculations. More often than not one would import data from Excel (urgh!) or something more reputable. The kinds of data one can read into R are remarkable. We will get to that later on.\nWe can also combine vectors in many ways, and the simplest way is the append one after the other:\n\nprimes12 &lt;- c(primes1, primes2)\nprimes12\n\n[1] 3.000000 5.000000 7.000000 3.141593 5.000000 7.000000\n\nnonSense &lt;- c(primes12, cs)\nnonSense\n\n [1] \"3\"                \"5\"                \"7\"                \"3.14159265358979\"\n [5] \"5\"                \"7\"                \"Mary\"             \"has\"             \n [9] \"a\"                \"silly\"            \"lamb\"            \n\nclass(nonSense)\n\n[1] \"character\"\n\n\nIn the code fragment above, notice how the numeric values are being coerced into character strings when the two vectors of dissimilar class are combined. This is necessary so as to maintain the same primitive data type for members in the same vector.\n\n10 Vector indices\nWhat if we want to extract one or a few components from the vector? Easy… We retrieve values in a vector by declaring an index inside a single square bracket [] operator. For example, the following shows how to retrieve a vector component. Since the vector index is 1-based (i.e. the first component in a vector is numbered 1), we use the index position 7 for retrieving the seventh member:\n\nnonSense[7] # find the seventh component in the vector\n\n[1] \"Mary\"\n\n# or combine them in interesting ways...\npaste(nonSense[7], nonSense[8], nonSense[4], nonSense[10], \"bunnies\", sep = \" \")\n\n[1] \"Mary has 3.14159265358979 silly bunnies\"\n\n\nIf the index given is negative, it will remove the value whose position has the same absolute value as the negative index. For example, the following creates a vector slice with the third member removed. However, if an index is out-of-range, a missing value will be reported via the symbol NA:\n\na &lt;- c(2, 6, 3, 8, 13)\na\n\n[1]  2  6  3  8 13\n\na[-3]\n\n[1]  2  6  8 13\n\na[10]\n\n[1] NA\n\n\n\n11 Vector creation\nR has many funky ways of creating vectors. This process is important to understand because we will need to build on it to create our own dataframes. Here are some examples of vector creation:\n\nseq(1:10) # assign them to a variable if you want to...\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from = 0, to = 100, by = 10)\n\n [1]   0  10  20  30  40  50  60  70  80  90 100\n\nseq(0, 100, len = 10) # one may omit from and to\n\n [1]   0.00000  11.11111  22.22222  33.33333  44.44444  55.55556  66.66667\n [8]  77.77778  88.88889 100.00000\n\nseq(1, 9, by = pi)\n\n[1] 1.000000 4.141593 7.283185\n\nrep(13, times = 13)\n\n [1] 13 13 13 13 13 13 13 13 13 13 13 13 13\n\nrep(seq(1:5), times = 6)\n\n [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\na &lt;- rnorm(20, mean = 13, sd = 0.13) # random numbers with known mean and sd\nrep(a, 5) # one may omit the times argument\n\n  [1] 12.83704 13.19724 12.97919 12.70365 12.87187 12.98340 13.36055 12.95754\n  [9] 12.78420 13.22739 12.90640 12.85689 13.11575 13.28997 12.91806 12.81904\n [17] 13.34069 12.89680 12.91343 13.01366 12.83704 13.19724 12.97919 12.70365\n [25] 12.87187 12.98340 13.36055 12.95754 12.78420 13.22739 12.90640 12.85689\n [33] 13.11575 13.28997 12.91806 12.81904 13.34069 12.89680 12.91343 13.01366\n [41] 12.83704 13.19724 12.97919 12.70365 12.87187 12.98340 13.36055 12.95754\n [49] 12.78420 13.22739 12.90640 12.85689 13.11575 13.28997 12.91806 12.81904\n [57] 13.34069 12.89680 12.91343 13.01366 12.83704 13.19724 12.97919 12.70365\n [65] 12.87187 12.98340 13.36055 12.95754 12.78420 13.22739 12.90640 12.85689\n [73] 13.11575 13.28997 12.91806 12.81904 13.34069 12.89680 12.91343 13.01366\n [81] 12.83704 13.19724 12.97919 12.70365 12.87187 12.98340 13.36055 12.95754\n [89] 12.78420 13.22739 12.90640 12.85689 13.11575 13.28997 12.91806 12.81904\n [97] 13.34069 12.89680 12.91343 13.01366\n\nrep(c(\"A\", \"B\", \"C\"), 3)\n\n[1] \"A\" \"B\" \"C\" \"A\" \"B\" \"C\" \"A\" \"B\" \"C\"\n\nrep(c(\"A\", \"B\", \"C\"), each = 3)\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\nx &lt;- c(\"01-31-1960\", \"02-13-1960\", \"06-23-1977\", \"01-01-2013\")\nclass(x)\n\n[1] \"character\"\n\nz &lt;- as.Date(x, \"%m-%d-%Y\")\nclass(z) # introducing the date class\n\n[1] \"Date\"\n\nseq(as.Date(\"2013-12-30\"), as.Date(\"2014-01-04\"), by = \"days\")\n\n[1] \"2013-12-30\" \"2013-12-31\" \"2014-01-01\" \"2014-01-02\" \"2014-01-03\"\n[6] \"2014-01-04\"\n\nseq(as.Date(\"2013-12-01\"), as.Date(\"2016-01-31\"), by = \"months\")\n\n [1] \"2013-12-01\" \"2014-01-01\" \"2014-02-01\" \"2014-03-01\" \"2014-04-01\"\n [6] \"2014-05-01\" \"2014-06-01\" \"2014-07-01\" \"2014-08-01\" \"2014-09-01\"\n[11] \"2014-10-01\" \"2014-11-01\" \"2014-12-01\" \"2015-01-01\" \"2015-02-01\"\n[16] \"2015-03-01\" \"2015-04-01\" \"2015-05-01\" \"2015-06-01\" \"2015-07-01\"\n[21] \"2015-08-01\" \"2015-09-01\" \"2015-10-01\" \"2015-11-01\" \"2015-12-01\"\n[26] \"2016-01-01\"\n\nseq(as.Date(\"2000/1/1\"), by = \"month\", length.out = 12)\n\n [1] \"2000-01-01\" \"2000-02-01\" \"2000-03-01\" \"2000-04-01\" \"2000-05-01\"\n [6] \"2000-06-01\" \"2000-07-01\" \"2000-08-01\" \"2000-09-01\" \"2000-10-01\"\n[11] \"2000-11-01\" \"2000-12-01\"\n\n# and many more...\n\n\n12 Vector arithmetic\nArithmetic operations of vectors are performed component-by-component, i.e., componentwise. For example, suppose we have vectors a and b:\n\na &lt;- c(1, 3, 5, 7)\nb &lt;- c(1, 2, 4, 8)\n\nThen we multiply a by 5…\n\na * 5\n\n[1]  5 15 25 35\n\n\n… and see that each component of a is multiplied by 5. In other words, the shorter vector (here 5) is recycled. Now multiply a with b…\n\na * b\n\n[1]  1  6 20 56\n\n\n…and we see that the components in one vector matches those in the other one-for-one. Similarly for subtraction, addition and division, we get new vectors via componentwise operations. Try this here now a few times with your own vectors.\nBut what if one vector is somewhat shorter than the other? The recycling rule comes into play. If two vectors are of unequal length, the shorter one will be recycled in order to match the longer vector. For example, the following vectors u and v have different lengths, and their sum is computed by recycling values of the shorter vector u:\n\nv &lt;- rep(2, len = 13)\nu &lt;- rep(c(1, 20), len = 5)\nv + u\n\nWarning in v + u: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  3 22  3 22  3  3 22  3 22  3  3 22  3\n\n\n\n13 Dataframe creation\nThe most rudimentary way to create a dataframe is to create several vectors and then assemble them into a dataframe using cbind() — this is a function that combines by column. For instance:\n\n# create three vectors of different types\nvec1 &lt;- rep(c(\"A\", \"B\", \"C\"), each = 5) # a character vector (a facctor)\nvec2 &lt;- seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                 length.out = length(vec1)) # date vector\nvec3 &lt;- rnorm(n = length(vec1), mean = 0, sd = 0.35) # numeric vector\n# now assemble dataframe\ndf1 &lt;- cbind(vec1, vec2, vec3)\nhead(df1)\n\n     vec1 vec2   vec3                \n[1,] \"A\"  \"4018\" \"0.314152909047363\" \n[2,] \"A\"  \"4019\" \"0.175236151519713\" \n[3,] \"A\"  \"4020\" \"0.189164777566461\" \n[4,] \"A\"  \"4021\" \"0.276716733829274\" \n[5,] \"A\"  \"4022\" \"-0.431054528778207\"\n[6,] \"B\"  \"4023\" \"-0.217602250496364\"\n\n\nAnother way to achieve the same thing is to use the data.frame() function that will allow you to achieve all of the above steps at once. Here is the example:\n\ndf2 &lt;- data.frame(vec1 = rep(c(\"A\", \"B\", \"C\"), each = 5),\n                  vec2 = seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                                  length.out = length(vec1)),\n                  vec3 = rnorm(n = length(vec1), mean = 2, sd = 0.75))\nhead(df2, 2)\n\n\n\n\nvec1\nvec2\nvec3\n\n\n\nA\n1981-01-01\n1.712485\n\n\nA\n1981-01-02\n3.314969\n\n\n\n\n\n\nWhat about the names of the dataframe that you just created? Are you happy that they are descriptive enough? If you aren’t, don’t fear. There are several different ways in which we can change it. We can assign the existing separate vectors vec1, vec2 and vec3 to more user-friendly names using the data.frame() function, like this:\n\ndf1 &lt;- data.frame(level = vec1,\n                  sample.date = vec2,\n                  measurement = vec3)\nhead(df1, 2)\n\n\n\n\nlevel\nsample.date\nmeasurement\n\n\n\nA\n1981-01-01\n0.3141529\n\n\nA\n1981-01-02\n0.1752362\n\n\n\n\n\n\nAnother way is to change the name after you have created the dataframe using the colnames() assignment function, as in:\n\ncolnames(df2) &lt;- c(\"level\", \"sample.date\", \"measurement\")\nhead(df2, 2)\n\n\n\n\nlevel\nsample.date\nmeasurement\n\n\n\nA\n1981-01-01\n1.712485\n\n\nA\n1981-01-02\n3.314969\n\n\n\n\n\nnames(df2)\n\n[1] \"level\"       \"sample.date\" \"measurement\"\n\n\nDataframes are very versatile and we can do many operations on them. A common requirement is to add a column to a dataframe that contains the outcome of some calculation. We could create a new column in the dataframe ‘on the fly’, as in:\n\ndf2.1 &lt;- df1 # copy the dataframe\ndf2.1$meas.anom &lt;- df1$measurement - mean(df1$measurement)\ndf2.1$meas.diff &lt;- df2.1$measurement - df2.1$meas.anom\nhead(df2.1, 2)\n\n\n\n\nlevel\nsample.date\nmeasurement\nmeas.anom\nmeas.diff\n\n\n\nA\n1981-01-01\n0.3141529\n0.1603506\n0.1538023\n\n\nA\n1981-01-02\n0.1752362\n0.0214338\n0.1538023\n\n\n\n\n\n\nWe can also combine dataframes in different ways. Perhaps you have two (or more) dataframe that conform to the same layout, i.e. they have the same number of columns (although the length of the dataframes may differ), they have the same data type in those columns and the names of those columns are the same. Also, the order of the columns must be identical in all the dataframes. Two separate dataframe with the same structure may, for example, result from two identical experiments that were repeated at different times. We can then stack one on top (e.g. combine our experiments) of the other using the row bind function rbind(), as in:\n\nnrow(df1) # check the number of rows first\n\n[1] 15\n\nnrow(df2)\n\n[1] 15\n\ndf3 &lt;- rbind(df1, df2)\nnrow(df3) # number of rows in the combined dataframe\n\n[1] 30\n\nhead(df3, 2)\n\n\n\n\nlevel\nsample.date\nmeasurement\n\n\n\nA\n1981-01-01\n0.3141529\n\n\nA\n1981-01-02\n0.1752362\n\n\n\n\n\n\nBut now how do we know how the portions of the stacked dataframe relate to the experiments that resulted in the data in the first place? There is no label to distinguish one experiment from the other. We can fix this by adding a new column to the stacked dataframe that contains the coding for the two experiments. We can achieve it like this:\n\ndf3$exp.no &lt;- rep(c(\"exp1\", \"exp2\"), each = nrow(df1))\nhead(df3, 2)\n\n\n\n\nlevel\nsample.date\nmeasurement\nexp.no\n\n\n\nA\n1981-01-01\n0.3141529\nexp1\n\n\nA\n1981-01-02\n0.1752362\nexp1\n\n\n\n\n\ntail(df3, 2)\n\n\n\n\n\nlevel\nsample.date\nmeasurement\nexp.no\n\n\n\n29\nC\n1981-01-14\n2.907408\nexp2\n\n\n30\nC\n1981-01-15\n1.020527\nexp2\n\n\n\n\n\n\nWe can combine dataframes in another way — that is, bind columns side-by-side using the function cbind(). We used it before to place vectors of the same length next to each other to create a dataframe. This function is similar to rbind(), but where rbind() fusses over the names of the columns, cbind() does not. What does concern cbind(), however, is that the number of rows in the two (or more) dataframes that will be ‘glued’ side-by-side is the same. Try it yourself with your own dataframes.\n\n14 Dataframe indices\nRemember that weird $ symbol we saw a little while ago? That symbol tells R that you want to see a column (vector) within a dataframe. For example, if we wanted to perform an operation on only one column in intro_data in order to ascertain the mean depth (m) of sampling:\n\nround(mean(intro_data$depth),2)\n\n[1] 1.33\n\n\nIf we want to subset only specific values in a dataframe, as we have seen how to do with vectors, we need to consider that we are now working with two dimensions and not one. We still use [] but now we must do a little extra. If we want to see how long the time series for Sodwana is we could do this in several ways, here are the three most common in an improving order:\n\n# Subset a dataframe using [,]\nintro_data[12,9]\n\n[1] 4606\n\n# Subset only one column using []\nintro_data$length[12]\n\n[1] 4606\n\n# Subset from one column using logic for another column\nintro_data$length[intro_data$site == \"Sodwana\"]\n\n[1] 4606\n\n\nThe important thing to remember here is that when one needs to use a comma when subsetting, the row number is always on the left, and the column number is always on the right. Rows then columns! Tattoo that onto your brain. Or fore-arm if you are the adventurous type. We will go into the subsetting and analysis of dataframes in much more detail in the following session.\nOne must keep in mind that data in R can become substantially more complex than what we have covered, and the software also distinguishes several other kinds of data ‘containers’: in addition to vectors and dataframes, we also have lists, matrices, time series and arrays. The more complex ones, such as arrays, may have more dimensions than the two (rows along dimension 1, columns along dimension 2) that most people are familiar with. We will not delve into these here as they are bit more advanced than the goals of this course.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {16. {Base} {R} Primer},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/16-base_r.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 16. Base R primer. https://tangledbank.netlify.app/BCB744/intro_r/16-base_r.html."
  },
  {
    "objectID": "BCB744/intro_r/14-recap.html",
    "href": "BCB744/intro_r/14-recap.html",
    "title": "14. Recap",
    "section": "",
    "text": "“Everyone should have their mind blown once a day.”\n— Neil deGrasse Tyson\n\n\n“Somewhere, something incredible is waiting to be known.”\n— Carl Sagan\n\nOver the past four days we have covered quite a bit of ground. By now it is our hope that after having participated in this workshop you will feel confident enough using R to branch out on your own and begin applying what you have learned to your own research.\nAbove all, remember the tidy principles you have leaned here and endeavour to apply them to all facets of your work. The more uniformly tidy your work becomes, the more compounding benefits you will begin to notice.\n\n1 The future\nThe content we have covered in this workshop is only the beginning. We have looked down upon the tidyverse, it’s multitudinous spiralling arms stretching out away from us in all directions. The next step is to begin to investigate the specific branches of the R tree of knowledge that interest us most. Or are most relevant to our work. The following list contains some further suggestions for workshops that are available:\n\nR for biologists\nR for environmental science\nR for oceanographers\nAdvanced visualisations\nMultivariate analysis\nSpecies distribution modelling\nReproducible research\nBasic stats\n\nFor further information or inquiries about additional training please contact Robert Schlegel: robwschlegel@gmail.com .\n\n2 Today\nFor the rest of today we will now open the floor to questions and suggestions that we may work through as a group.\n\n3 Session info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\nR&gt; character(0)\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {14. {Recap}},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/14-recap.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 14. Recap. https://tangledbank.netlify.app/BCB744/intro_r/14-recap.html."
  },
  {
    "objectID": "BCB744/intro_r/15-functions.html",
    "href": "BCB744/intro_r/15-functions.html",
    "title": "15. Functions by chapter",
    "section": "",
    "text": "…incomplete…\n\n\nThere are several operators you can use to help build expressions as shown in Table \\(\\ref{tab:operators}\\).\n\n\n\nSome example functions covered so far are presented in Table \\(\\ref{tab:functions}\\).\nSome summary functions are presented in Table \\(\\ref{tab:summaries}\\)."
  },
  {
    "objectID": "BCB744/intro_r/15-functions.html#useful-information",
    "href": "BCB744/intro_r/15-functions.html#useful-information",
    "title": "15. Functions by chapter",
    "section": "",
    "text": "…incomplete…\n\n\nThere are several operators you can use to help build expressions as shown in Table \\(\\ref{tab:operators}\\).\n\n\n\nSome example functions covered so far are presented in Table \\(\\ref{tab:functions}\\).\nSome summary functions are presented in Table \\(\\ref{tab:summaries}\\)."
  },
  {
    "objectID": "BCB744/intro_r/11-tidy.html",
    "href": "BCB744/intro_r/11-tidy.html",
    "title": "11. Tidy data",
    "section": "",
    "text": "“Order and simplification are the first steps toward the mastery of a subject.”\n— Thomas Mann\nThe Tidyverse is a collection of R packages designed for data manipulation, exploration, and visualisation. It is based on a philosophy of ‘tidy data,’ which is a standardised way of organising data. The purpose of these packages is to make working with data more efficient. The core Tidyverse packages were created by Hadley Wickham, but over the last few years other individuals have added some packages to the collective, which has significantly expanded our data analytical capabilities through improved ease of use and efficiency. All packages that are built on tidy principles provide the use of a consistent set of tools across a wide range of data analysis tasks. The core Tidyverse packages can be loaded collectively by calling the tidyverse package, as we have seen throughout this workshop. The packages making up the Tidyverse are shown in Figure 1.\nlibrary(tidyverse)"
  },
  {
    "objectID": "BCB744/intro_r/11-tidy.html#pivot_longer",
    "href": "BCB744/intro_r/11-tidy.html#pivot_longer",
    "title": "11. Tidy data",
    "section": "\n3.1 pivot_longer()\n",
    "text": "3.1 pivot_longer()\n\nThe R function pivot_longer() is a useful tool for transforming data from wide to long format. It belongs to the tidyr package (loaded with tidyverse) and allows you to reshape your data frame by gathering multiple columns into key-value pairs. Specifically, pivot_longer() takes in a data frame and allows you to select a set of columns that you would like to pivot into longer format, while specifying the names of the key and value columns that you want to create. The resulting data frame will have a new row for each unique combination of key and value pairs. This function is particularly useful when you need to reshape your data in order to carry out certain analyses or visualisations.\nLet’s have a look now at SACTN2 for an example of what wide data look like, and how to fix it.\nIn SACTN2 we can see that the src column has been removed and that the temperatures are placed in columns that denote the collecting source. This may at first seem like a reasonable way to organise these data, but it is not tidy because the collecting source is one variable, and so should not take up more than one column (i.e. there are multiple observations per row). We need to gather these source columns together into one column so that the seperate measurements (observations) can conform to the one observation per row rule. We do this by telling pivot_longer() the names of the columns we want to squish together. We then tell it the name of the key (names_to) column. This is the column that will contain all of the old column names we are gathering. In this case we will call it source. The last piece of this puzzle is the value (values_to) column. This is where we decide what the name of the column will be for measurements we are gathering up. In this case we will name it temperature, because we are gathering up the temperature values that were incorrectly spread out by the source of the measurements.\n\nSACTN2_tidy &lt;- pivot_longer(SACTN2, cols = c(\"DEA\", \"KZNSB\", \"SAWS\"),\n                            names_to = \"src\",\n                            values_to = \"temp\")\n\n\n\n\n\n\n\nTask G\n\n\n\n\nUsing the tidy data (SACTN2_tidy) and untidy data (SACTN2), create line graphs, one for each of DEA, SAWS, and KZNSB, showing a time series of temperature. For SACTN2_tidy and SACTN2, make sure you have a column of three figures (ncol = 1). Use the fewest number of lines of code possible.\n\nYou should end up with two graphs, each with three panels.\nPlease also refer to the Summative End-of-Intro-R Task material which must be submitted as the final assignment in the Intro R portion of BCB744."
  },
  {
    "objectID": "BCB744/intro_r/11-tidy.html#pivot_wider",
    "href": "BCB744/intro_r/11-tidy.html#pivot_wider",
    "title": "11. Tidy data",
    "section": "\n3.2 pivot_wider()\n",
    "text": "3.2 pivot_wider()\n\nThe function pivot_wider() is a tool for transforming data from long to wide format. It is the counterpart to the pivot_longer() function. pivot_wider() allows you to take a set of columns containing key-value pairs and convert them into a wider format, where each unique key value becomes a separate column in the resulting data frame. You can also specify a set of value columns that you want to spread across the new columns created by the key values. With pivot_wider(), you can quickly transform your data from long format into a more intuitive, wide format that is easier to work with in some applications.\nShould ones data be too long for a particular application (typically a non-Tidyverse application) or your liking, meaning when individual observations are spread across multiple rows, we will need to use pivot_wider() to rectify the situation. This is generally the case when we have two or more variables stored within the same column, as we may see in SACTN3. This is not terribly common as it would require someone to put quite a bit of time into making a dataframe this way. But never say never. To spread data to become wider we first tell R what the name of the column is that contains more than one variable, in this case the var column. We then tell R what the name of the column is that contains the values that need to be spread, in this case the val column.\n\nSACTN3_tidy1 &lt;- SACTN3 %&gt;% \n  pivot_wider(names_from = \"var\", values_from = \"val\")"
  },
  {
    "objectID": "BCB744/intro_r/11-tidy.html#separate",
    "href": "BCB744/intro_r/11-tidy.html#separate",
    "title": "11. Tidy data",
    "section": "\n4.1 Separate",
    "text": "4.1 Separate\nIf we look at SACTN4a we see that we no longer have a site and src column. Rather these have been replaced by an index column. This is an efficient way to store these data, but it is not tidy because the site and source of each observation have now been combined into one column (variable). Remember, tidy data calls for each of the things known about the data to be its own variable. To re-create our site and src columns we must separate the index column. There are two options: separate_wider_delim() and separate_wider_position(). What does each do? First we give R the name of the column we want to separate, in this case index. Next we must say what the names of the new columns will be. Remember that because we are creating new column names we feed these into R within inverted commas. Lastly we should tell R where to separate the index column. If we look at the data we may see that the values we want to split up are separated with / (including a space), so that is what we give to R.\n\nSACTN4a_tidy &lt;- SACTN4a |&gt; \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \")"
  },
  {
    "objectID": "BCB744/intro_r/11-tidy.html#separating-dates-using-mutate",
    "href": "BCB744/intro_r/11-tidy.html#separating-dates-using-mutate",
    "title": "11. Tidy data",
    "section": "\n4.2 Separating dates using mutate()\n",
    "text": "4.2 Separating dates using mutate()\n\nAlthough the date column represents an example of a date type (a kind of data in its own right), one might also want to split this column into its constituent parts, i.e. create separate columns for day, month, and year. In this case we can spread these components of the date vector into three columns using the mutate() function and some functions in the lubridate package (contained within tidyverse).\n\nSACTN_tidy2 &lt;- SACTN4a %&gt;% \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \") %&gt;% \n  mutate(day = lubridate::day(date),\n         month = lubridate::month(date),\n         year = lubridate::year(date))\n\nNote that when the date is split into component parts the data are no longer tidy (see below)."
  },
  {
    "objectID": "BCB744/intro_r/11-tidy.html#unite",
    "href": "BCB744/intro_r/11-tidy.html#unite",
    "title": "11. Tidy data",
    "section": "\n4.3 Unite",
    "text": "4.3 Unite\nIt is not uncommon that field/lab instruments split values across multiple columns while they are making recordings. I see this most often with date values. Often the year, month, and day values are given in different columns. There are uses for the data in this way, though it is not terribly tidy. We usually want the date of any observation to be shown in just one column. If we look at SACTN4b we will see that there is a year, month, and day column. To unite() them we must first tell R what we want the united column to be labelled, in this case we will use date. We then list the columns to be united, her this is year, month, and day. Lastly we must specify if we want the united values to have a separator between them. The standard separator for date values is ‘-’.\n\nSACTN4b_tidy &lt;- SACTN4b |&gt; \n  unite(year, month, day, col = \"date\", sep = \"-\")"
  },
  {
    "objectID": "BCB744/intro_r/07-mapping.html",
    "href": "BCB744/intro_r/07-mapping.html",
    "title": "7. Mapping with ggplot2",
    "section": "",
    "text": "“There’s no map to human behaviour.”\n— Bjork\n\n\n“Here be dragons.”\n— Unknown\n\nYesterday we learned how to create ggplot2 figures, change their aesthetics, labels, colour palettes, and facet/arrange them. Now we are going to look at how to create maps.\nMost of the work that we perform as environmental/biological scientists involves going out to a location and sampling information there. Sometimes only once, and sometimes over a period of time. All of these different sampling methods lend themselves to different types of figures. One of those, collection of data at different points, is best shown with maps. As we will see over the course of Day 3, creating maps in ggplot2 is very straight forward and is extensively supported. For that reason we are going to have plenty of time to also learn how to do some more advanced things. Our goal in this chapter is to produce the figure below.\n\n\nToday’s goal.\n\n\n1 Using prepared data\nBefore we begin let’s go ahead and load the packages we will need, as well as the several dataframes required to make the final product.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\n\n# Load data\nload(\"../../data/south_africa_coast.Rdata\")\nload(\"../../data/sa_provinces.RData\")\nload(\"../../data/rast_annual.Rdata\")\nload(\"../../data/MUR.Rdata\")\nload(\"../../data/MUR_low_res.RData\")\n\n# Choose which SST product you would like to use\nsst &lt;- MUR_low_res\n# OR\nsst &lt;- MUR\n\n# The colour palette we will use for ocean temperature\ncols11 &lt;- c(\"#004dcd\", \"#0068db\", \"#007ddb\", \"#008dcf\", \"#009bbc\",\n            \"#00a7a9\", \"#1bb298\", \"#6cba8f\", \"#9ac290\", \"#bec99a\")\n\n\n2 A new concept?\nThe idea of creating a map in R may be daunting to some, but remember that a basic map is nothing more than a simple figure with an x and y axis. We tend to think of maps as different from other scientific figures, whereas in reality they are created the exact same way. Let’s compare a dot plot of the chicken weight data against a dot plot of the coastline of South Africa.\nChicken dots:\n\nggplot(data = ChickWeight, aes(x = Time, y = weight)) +\n  geom_point()\n\n\n\nDot plot of chicken weight data.\n\n\n\nSouth African coast dots:\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_point()\n\n\n\nDot plot off South African coast.\n\n\n\nDoes that look familiar? Notice how the x and y axis tick labels look the same as any map you would see in an atlas. This is because they are. But this isn’t a great way to create a map. Rather it is better to represent the land mass with a polygon. With ggplot2 this is a simple task.\n\n3 Land mask\nNow that we have seen that a map is nothing more than a bunch of dots and shapes on specific points along the x and y axes we are going to look at the steps we would take to build a more complex map. Don’t worry if this seems daunting at first. We are going to take this step by step and ensure that each step is made clear along the way. The first step is to create a polygon. Note that we create an aesthetic argument inside of geom_polygon() and not ggplot() because some of the steps we will take later on will not accept the group aesthetic. Remember, whatever aesthetic arguments we put inside of ggplot() will be inserted for us into all of our other geom_...() lines of code.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) # The land mask\n\n\n\nThe map of South Africa. Now with province borders!\n\n\n\n\n4 Borders\nThe first thing we will add is the province borders as seen in Figure @ref(fig:map-goal). Notice how we only add one more line of code to do this.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) # The province borders\n\n\n\nThe map of South Africa. Now with province borders!\n\n\n\n\n5 Force lon/lat extent\nUnfortunately when we added our borders it increased the plotting area of our map past what we would like. To correct that we will need to explicitly state the borders we want.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) + \n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0) # Force lon/lat extent\n\n\n\nThe map, but with the extra bits snipped off.\n\n\n\n\n6 Ocean temperature\nThis is starting to look pretty fancy, but it would be nicer if there was some colour involved. So let’s add the ocean temperature. Again, this will only require one more line of code. Starting to see a pattern? But what is different this time and why?\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) + # The ocean temperatures\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\nOcean temperature (°C) visualised as an ice cream spill.\n\n\n\nThat looks… odd. Why do the colours look like someone melted a big bucket of ice cream in the ocean? This is because the colours you see in this figure are the default colours for discrete values in ggplot2. If we want to change them we may do so easily by adding yet one more line of code.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) + # Set the colour palette\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\nOcean temperatures (°C) around South Africa.\n\n\n\nThere’s a colour palette that would make Jacques Cousteau swoon. When we set the colour palette for a figure in ggplot2 we must use that colour palette for all other instances of those types of values, too. What this means is that any other discrete values that will be filled in, like the ocean colour above, must use the same colour palette (there are some technical exceptions to this rule that we will not cover in this course). We normally want ggplot2 to use consistent colour palettes anyway, but it is important to note that this constraint exists. Let’s see what we mean. Next we will add the coastal pixels to our figure with one more line of code. We won’t change anything else. Note how ggplot2 changes the colour of the coastal pixels to match the ocean colour automatically.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  geom_tile(data = rast_annual, aes(x = lon, y = lat, fill = bins), \n            colour = \"white\", size = 0.1) + # The coastal temperature values\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\nMap of South Africa showing in situ temeperatures (°C) as pixels along the coast.\n\n\n\n\n7 Final touches\nWe used geom_tile() instead of geom_rast() to add the coastal pixels above so that we could add those little white boxes around them. This figure is looking pretty great now. And it only took a few rows of code to put it all together! The last step is to add several more lines of code that will control for all of the little things we want to change about the appearance of the figure. Each little thing that is changed below is annotated for your convenience.\n\nfinal_map &lt;- ggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  geom_tile(data = rast_annual, aes(x = lon, y = lat, fill = bins), \n            colour = \"white\", size = 0.1) +\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0) +\n  scale_x_continuous(position = \"top\") + # Put x axis labels on top of figure\n  theme(axis.title = element_blank(), # Remove the axis labels\n        legend.text = element_text(size = 7), # Change text size in legend\n        legend.title = element_text(size = 7), # Change legend title text size\n        legend.key.height = unit(0.3, \"cm\"), # Change size of legend\n        legend.background = element_rect(colour = \"white\"), # Add legend background\n        legend.justification = c(1, 0), # Change position of legend\n        legend.position = c(0.55, 0.4) # Fine tune position of legend\n        )\nfinal_map\n\n\n\nThe cleaned up map of South Africa. Resplendent with coastal and ocean temperatures (°C).\n\n\n\nThat is a very clean looking map so let’s go ahead and save it on our computers.\n\nggsave(plot = final_map, \"figures/map_complete.pdf\", height = 6, width = 9)\n\n\n8 Session info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\nR&gt;    ggpubr lubridate   forcats   stringr     dplyr     purrr     readr     tidyr \nR&gt;   \"0.6.0\"   \"1.9.2\"   \"1.0.0\"   \"1.5.0\"   \"1.1.1\"   \"1.0.1\"   \"2.1.4\"   \"1.3.0\" \nR&gt;    tibble   ggplot2 tidyverse \nR&gt;   \"3.2.1\"   \"3.4.1\"   \"2.0.0\"\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {7. {Mapping} with Ggplot2},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/07-mapping.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 7. Mapping with ggplot2. https://tangledbank.netlify.app/BCB744/intro_r/07-mapping.html."
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html",
    "href": "BCB744/intro_r/01-RStudio.html",
    "title": "1. R and RStudio",
    "section": "",
    "text": "“Strange events permit themselves the luxury of occurring.”\n— Charlie Chan\nIn this Lecture we will cover:"
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#general-settings",
    "href": "BCB744/intro_r/01-RStudio.html#general-settings",
    "title": "1. R and RStudio",
    "section": "\n3.1 General settings",
    "text": "3.1 General settings\nBefore we start using RStudio (which is a code editor and environment that runs R) let’s first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’) and select the tick boxes as shown in the figure below.\n\n\nRStudio preferences"
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#customising-appearance",
    "href": "BCB744/intro_r/01-RStudio.html#customising-appearance",
    "title": "1. R and RStudio",
    "section": "\n3.2 Customising appearance",
    "text": "3.2 Customising appearance\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g. Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code.\n\n\nAppearance settings"
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#configuring-panes",
    "href": "BCB744/intro_r/01-RStudio.html#configuring-panes",
    "title": "1. R and RStudio",
    "section": "\n3.3 Configuring panes",
    "text": "3.3 Configuring panes\nYou cannot rearrange panes (see below) in RStudio by dragging them, but you can alter their position via the Pane Layout tab in the ‘Tools’/‘Global Options’ (‘RStudio’/‘Preferences’ – for Mac). You may arrange the panes as you would prefer; however, we recommend that during the duration of this workshop you leave them in the default layout.\n\n\nRearranging the panes"
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#source-editor",
    "href": "BCB744/intro_r/01-RStudio.html#source-editor",
    "title": "1. R and RStudio",
    "section": "\n6.1 Source Editor",
    "text": "6.1 Source Editor\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it isn’t already)\nNow double click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (for Windows machines; for Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon or Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18 and re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now… Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nThe # symbol\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done."
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#console",
    "href": "BCB744/intro_r/01-RStudio.html#console",
    "title": "1. R and RStudio",
    "section": "\n6.2 Console",
    "text": "6.2 Console\nThis is where you can type code that executes immediately. This is also known as the command line. Throughout the notes, we will represent code for you to execute in R as a different font.\n\n\n\n\n\n\nType it in!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really shouldn’t. The first reason is that you might unwittingly copy invisible PDF formatting errors into R, which will make the code fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your own errors. This is an invaluable way of learning and taking shortcuts now will only hurt you in the long run.\n\n\nEntering code in the command line is intuitive and easy. For example, we can use R as a calculator by typing into the Console (and pressing Enter after each line). Note that the output for every line of calculation (e.g. 6 * 3) is indicated by R&gt;, as we see here:\n\n6 * 3\n\nR&gt; [1] 18\n\n5 + 4\n\nR&gt; [1] 9\n\n2 ^ 3\n\nR&gt; [1] 8\n\n\nNote that spaces are optional around simple calculations, but I encourage their use to adhere to the R style guidelines.\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\nR&gt; [1] 9\n\n\nTo type the assignment operator (&lt;-) push the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\n\n\n\n\n\n\nTask A\n\n\n\n\nWhat are the values after each hashed statement in the following? Use the RStudio Console to determine these values:\n\n\nmass &lt;- 48 \nmass &lt;- mass * 2.0 # mass? \nage &lt;- 42\nage &lt;- age - 17 # age?\nmass_index &lt;- mass / age # mass_index?\n\n\nUse R to calculate some simple mathematical expressions entered. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y Display z in the console.\n\n\n\nWe can create a vector in R by using the combine c() function:\n\napples &lt;- c(5.3, 3.8, 4.5)\n\nA vector is a one-dimensional array (i.e., a list of numbers), and this is the simplest form of data used in R (you can think of a single value in R as just a very short vector). We’ll talk about more complex (and therefore more powerful) types of data structures as we go along.\nIf you want to display the value of apples type:\n\napples\n\nR&gt; [1] 5.3 3.8 4.5\n\n\nFinally, there are default functions in R for nearly all basic statistical analyses, including mean() and sd() (standard deviation):\n\nmean(apples)\n\nR&gt; [1] 4.533333\n\nsd(apples)\n\nR&gt; [1] 0.7505553\n\n\n\n\n\n\n\n\nVariable names\n\n\n\nIt is best not to use c as the name of a value or array. Why? What other words might not be good to use?\n\n\nOr try this:\n\nround(sd(apples), 2)\n\nR&gt; [1] 0.75\n\n\n\n\n\n\n\n\nTask A\n\n\n\n\nWhat did we do above? What can you conclude from those functions?\n\n\n\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters app and then the Tab key. What happens?\nThe code completion feature also provides brief inline help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google (see the code in: BONUS/mapping_yourself.Rmd). On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl+Up to review the list (Cmd+Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane (see below).\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source pane using the buttons at the top-right or by double-clicking the title bar)"
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#environment-and-history-panes",
    "href": "BCB744/intro_r/01-RStudio.html#environment-and-history-panes",
    "title": "1. R and RStudio",
    "section": "\n6.3 Environment and History panes",
    "text": "6.3 Environment and History panes\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values and functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\nR&gt; [1] \"a\"        \"apples\"   \"b\"        \"pkgs_lst\" \"url\"\n\n\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?"
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#files-plots-packages-help-and-viewer-panes",
    "href": "BCB744/intro_r/01-RStudio.html#files-plots-packages-help-and-viewer-panes",
    "title": "1. R and RStudio",
    "section": "\n6.4 Files, Plots, Packages, Help, and Viewer panes",
    "text": "6.4 Files, Plots, Packages, Help, and Viewer panes\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where graphics you create will appear. The Packages tab shows you the packages that are installed and those that can be installed (more on this just now). The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console.\nMethods of getting help from the Console include…\n\n?mean\n\n…or:\n\nhelp(mean)\n\nWe will go into this in more detail in the next session.\nTo reproduced Figure @ref(fig:ggplot2-1) in the Plot tab, simply copy and paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.01)\ny &lt;- 2 * sin(2 * pi * (x - 1/4))\nggplot() +\n  geom_point(aes(x = x, y = y), shape = 21, col = \"salmon\", fill = \"white\")\n\n\n\nA plot assembled with ggplot2.\n\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R script wherein you provide answers to the Task questions by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated and labelled Rmarkdown file which outlines the graphs and all calculations (as necessary).\nPlease label the Rmarkdown and resulting HTML files as follows:\n\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Task_A.Rmd, and\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Task_A.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Tasks on iKamva when ready."
  },
  {
    "objectID": "BCB744/intro_r/12-tidier.html",
    "href": "BCB744/intro_r/12-tidier.html",
    "title": "12. Tidier data",
    "section": "",
    "text": "“Knowing where things are, and why, is essential to rational decision making.”\n— Jack Dangermond\n\n\n“The mind commands the body and it obeys. The mind orders itself and meets resistance.”\n— Frank Herbert, Dune\n\nOn Day 1 already we walked ourselves through a tidy workflow. We saw how to import data, how to manipulate it, run a quick analysis or two, and create figures. In the previous session we filled in the missing piece of the workflow by also learning how to tidy up our data within R. For the remainder of today we will be revisiting the ‘transform’ portion of the tidy workflow. In this session we are going to go into more depth on what we learned in Day 1, and in the last session we will learn some new tricks. Over these two sessions we will also become more comfortable with the pipe command %&gt;%, while practising writing tidy code.\nThere are five primary data transformation functions that we will focus on here:\n\nArrange observations (rows) with arrange()\n\nFilter observations (rows) with filter()\n\nSelect variables (columns) with select()\n\nCreate new variables (columns) with mutate()\n\nSummarise variables (columns) with summarise()\n\n\nWe will use the full South African Coastal Temperature Network dataset for these exercises. Before we begin however we will need to cover two new concepts.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load the data from a .RData file\nload(\"../../data/SACTNmonthly_v4.0.RData\")\n\n# Copy the data as a dataframe with a shorter name\nSACTN &lt;- SACTNmonthly_v4.0\n\n# Remove the original\nrm(SACTNmonthly_v4.0)\n\n\n1 Comparison operators\nThe assignment operator (&lt;-) is a symbol that we use to assign some bit of code to an object in our environment. Likewise, comparison operators are symbols we use to compare different objects. This is how we tell R how to decide to do many different things. We will see these symbols often out in the ‘real world’ so let’s spend a moment now getting to know them better. Most of these should be very familiar to us:\n\nGreater than: &gt;\n\nGreater than or equal to: &gt;=\n\nLess than: &lt;\n\nLess than or equal to: &lt;=\n\nEqual to: ==\n\nNot equal to: !=\n\n\nIt is important here to note that == is for comparisons and = is for maths. They are not interchangeable, as we may see in the following code chunk. This is one of the more common mistakes one makes when writing code. Luckily the error message this creates should provide us with the clues we need to figure out that we have made this specific mistake.\n\nSACTN %&gt;% \n  filter(site = \"Amanzimtoti\")\n\nR&gt; Error in `filter()`:\nR&gt; ! We detected a named input.\nR&gt; ℹ This usually means that you've used `=` instead of `==`.\nR&gt; ℹ Did you mean `site == \"Amanzimtoti\"`?\n\n\n\n2 Logical operators\nComparison operators are used to make direct comparisons between specific things, but logical operators are used more broadly when making logical arguments. Logic is central to most computing so it is worth taking the time to cover these symbols explicitly here. R makes use of the same Boolean logic symbols as many other platforms, including Google, so some (or all) of these will likely be familiar. We will generally only use three:\n\nand: &\n\nor: |\n\nnot: !\n\n\nWhen writing a line of tidy code we tend to use these logical operator to combine two or more arguments that use comparison operators. For example, the following code chunk uses the filter() function to find all temperatures recorded at Pollock Beach during December or January. Don’t worry if the following line of code is difficult to piece out, but make sure you can locate which symbols are comparison operators and which are logical operators. Please note that for purposes of brevity all of the outputs in this section are limited to ten lines, but when one runs these code chunks on ones own computer they will be much longer.\n\nSACTN %&gt;% \n  filter(site == \"Pollock Beach\", month(date) == 12 | month(date) == 1)\n\n\n\n\n\n\nsite\nsrc\ndate\ntemp\ndepth\ntype\n\n\n\nPollock Beach\nSAWS\n1999-12-01\n19.95000\n0\nthermo\n\n\nPollock Beach\nSAWS\n2000-01-01\n19.03333\n0\nthermo\n\n\nPollock Beach\nSAWS\n2000-12-01\n19.20000\n0\nthermo\n\n\nPollock Beach\nSAWS\n2001-01-01\n18.32667\n0\nthermo\n\n\nPollock Beach\nSAWS\n2001-12-01\n20.59032\n0\nthermo\n\n\nPollock Beach\nSAWS\n2002-01-01\n21.47097\n0\nthermo\n\n\nPollock Beach\nSAWS\n2002-12-01\n19.78065\n0\nthermo\n\n\nPollock Beach\nSAWS\n2003-01-01\n20.64516\n0\nthermo\n\n\nPollock Beach\nSAWS\n2003-12-01\n20.48710\n0\nthermo\n\n\nPollock Beach\nSAWS\n2004-01-01\n21.34839\n0\nthermo\n\n\n\n\n\n\nWe will look at the interplay between comparison and logical operators in more depth in the following session after we have reacquainted ourselves with the main transformation functions we need to know.\n\n3 Arrange observations (rows) with arrange()\n\nFirst up in our greatest hits reunion tour is the function arrange(). This very simply arranges the observations (rows) in a dataframe based on the variables (columns) it is given. If we are concerned with ties in the ordering of our data we provide additional columns to arrange(). The importance of the columns for arranging the rows is given in order from left to right.\n\nSACTN %&gt;% \n  arrange(depth, temp)\n\n\n\n\n\n\nsite\nsrc\ndate\ntemp\ndepth\ntype\n\n\n\nSea Point\nSAWS\n1990-07-01\n9.635484\n0\nthermo\n\n\nMuizenberg\nSAWS\n1984-07-01\n9.708333\n0\nthermo\n\n\nDoringbaai\nSAWS\n2000-12-01\n9.772727\n0\nthermo\n\n\nHondeklipbaai\nSAWS\n2003-06-01\n9.775000\n0\nthermo\n\n\nSea Point\nSAWS\n1984-06-01\n10.000000\n0\nthermo\n\n\nMuizenberg\nSAWS\n1992-07-01\n10.193548\n0\nthermo\n\n\nHondeklipbaai\nSAWS\n2005-07-01\n10.333333\n0\nthermo\n\n\nHondeklipbaai\nSAWS\n2003-07-01\n10.340909\n0\nthermo\n\n\nSea Point\nSAWS\n2000-12-01\n10.380645\n0\nthermo\n\n\nMuizenberg\nSAWS\n1984-08-01\n10.387097\n0\nthermo\n\n\n\n\n\n\nIf we would rather arrange our data in descending order, as is perhaps more often the case, we simply wrap the column name we are arranging by with the desc() function as shown below.\n\nSACTN %&gt;% \n  arrange(desc(temp))\n\n\n\n\n\n\nsite\nsrc\ndate\ntemp\ndepth\ntype\n\n\n\nSodwana\nDEA\n2000-02-01\n28.34648\n18\nUTR\n\n\nSodwana\nDEA\n1999-03-01\n28.04890\n18\nUTR\n\n\nSodwana\nDEA\n1998-03-01\n27.87781\n18\nUTR\n\n\nSodwana\nDEA\n1998-02-01\n27.76452\n18\nUTR\n\n\nSodwana\nDEA\n1996-02-01\n27.73637\n18\nUTR\n\n\nSodwana\nDEA\n2000-03-01\n27.52637\n18\nUTR\n\n\nSodwana\nDEA\n2000-01-01\n27.52291\n18\nUTR\n\n\nLeadsmanshoal\nEKZNW\n2007-02-01\n27.48132\n10\nUTR\n\n\nSodwana\nEKZNW\n2005-01-01\n27.45619\n12\nUTR\n\n\nSodwana\nEKZNW\n2007-02-01\n27.44054\n12\nUTR\n\n\n\n\n\n\nIt must also be noted that when arranging data in this way, any rows with NA values will be sent to the bottom of the dataframe. This is not always ideal and so must be kept in mind.\n\n4 Filter observations (rows) with filter()\n\nWhen simply arranging data is not enough, and we need to remove rows of data we do not want, filter() is the tool to use. For example, we can select all monthly temperatures recorded at the site Humewood during the year 1990 with the following code chunk:\n\nSACTN %&gt;% \n  filter(site == \"Humewood\", year(date) == 1990)\n\n\n\n\n\n\nsite\nsrc\ndate\ntemp\ndepth\ntype\n\n\n\nHumewood\nSAWS\n1990-01-01\n21.87097\n0\nthermo\n\n\nHumewood\nSAWS\n1990-02-01\n18.64286\n0\nthermo\n\n\nHumewood\nSAWS\n1990-03-01\n18.61290\n0\nthermo\n\n\nHumewood\nSAWS\n1990-04-01\n17.30000\n0\nthermo\n\n\nHumewood\nSAWS\n1990-05-01\n16.35484\n0\nthermo\n\n\nHumewood\nSAWS\n1990-06-01\n15.93333\n0\nthermo\n\n\nHumewood\nSAWS\n1990-07-01\n15.70968\n0\nthermo\n\n\nHumewood\nSAWS\n1990-08-01\n16.09677\n0\nthermo\n\n\nHumewood\nSAWS\n1990-09-01\n16.41667\n0\nthermo\n\n\nHumewood\nSAWS\n1990-10-01\n17.14194\n0\nthermo\n\n\n\n\n\n\nRemember to use the assignment operator (&lt;-, keyboard shortcut alt -) if one wants to create an object in the environment with the new results.\n\nhumewood_90s &lt;- SACTN %&gt;% \n  filter(site == \"Humewood\", year(date) %in% seq(1990, 1999, 1))\n\nIt must be mentioned that filter() also automatically removes any rows in the filtering column that contain NA values. Should one want to keep rows that contain missing values, insert the is.na() function into the line of code in question. To illustrate this let’s filter the temperatures for the Port Nolloth data collected by the DEA that were at or below 11°C OR were missing values. We’ll put each argument on a separate line to help keep things clear. Note how R automatically indents the last line in this chunk to help remind us that they are in fact part of the same argument. Also note how I have put the last bracket at the end of this argument on it’s own line. This is not required, but I like to do so as it is a very common mistake to forget the last bracket.\n\nSACTN %&gt;% \n  filter(site == \"Port Nolloth\", # First give the site to filter\n         src == \"DEA\", # Then specify the source\n         temp &lt;= 11 | # Temperatures at or below 11°C OR\n           is.na(temp) # Include missing values\n         )\n\n\n5 Select variables (columns) withselect()\n\nWhen one loads a dataset that contains more columns than will be useful or required it is preferable to shave off the excess. We do this with the select() function. In the following four examples we are going to remove the depth and type columns. There are many ways to do this and none are technically better or faster. So it is up to the user to find a favourite technique.\n\n# Select columns individually by name\nSACTN %&gt;% \n  select(site, src, date, temp)\n\n# Select all columns between site and temp like a sequence\nSACTN %&gt;% \n  select(site:temp)\n\n# Select all columns except those stated individually\nSACTN %&gt;% \n  select(-date, -depth)\n\n# Select all columns except those within a given sequence\n  # Note that the '-' goes outside of a new set of brackets\n  # that are wrapped around the sequence of columns to remove\nSACTN %&gt;% \n  select(-(date:depth))\n\nWe may also use select() to reorder the columns in a dataframe. In this case the inclusion of the everything() function may be a useful shortcut as illustrated below.\n\n# Change up order by specifying individual columns\nSACTN %&gt;% \n  select(temp, src, date, site)\n\n# Use the everything function to grab all columns \n# not already specified\nSACTN %&gt;% \n  select(type, src, everything())\n\n# Or go bananas and use all of the rules at once\n  # Remember, when dealing with tidy data,\n  # everything may be interchanged\nSACTN %&gt;% \n  select(temp:type, everything(), -src)\n\n\n6 Create new variables (columns) with mutate()\n\nWhen one is performing data analysis/statistics in R this is likely because it is necessary to create some new values that did not exist in the raw data. The previous three functions we looked at (arrange(), filter(), select()) will prepare us to create new data, but do not do so themselves. This is when we need to use mutate(). We must however be very mindful that mutate() is only useful if we want to create new variables (columns) that are a function of one or more existing columns. This means that any column we create with mutate() will always have the same number of rows as the dataframe we are working with. In order to create a new column we must first tell R what the name of the column will be, in this case let’s create a column named kelvin. The second step is to then tell R what to put in the new column. AS you may have guessed, we are going to convert the temp column into Kelvin (°K) by adding 273.15 to every row.\n\nSACTN %&gt;% \n  mutate(kelvin = temp + 273.15))\n\n\n\n\n\n\nsite\nsrc\ndate\ntemp\ndepth\ntype\nkelvin\n\n\n\nPort Nolloth\nDEA\n1991-02-01\n11.47029\n5\nUTR\n284.6203\n\n\nPort Nolloth\nDEA\n1991-03-01\n11.99409\n5\nUTR\n285.1441\n\n\nPort Nolloth\nDEA\n1991-04-01\n11.95556\n5\nUTR\n285.1056\n\n\nPort Nolloth\nDEA\n1991-05-01\n11.86183\n5\nUTR\n285.0118\n\n\nPort Nolloth\nDEA\n1991-06-01\n12.20722\n5\nUTR\n285.3572\n\n\nPort Nolloth\nDEA\n1991-07-01\n12.53810\n5\nUTR\n285.6881\n\n\nPort Nolloth\nDEA\n1991-08-01\n11.25202\n5\nUTR\n284.4020\n\n\nPort Nolloth\nDEA\n1991-09-01\n11.29208\n5\nUTR\n284.4421\n\n\nPort Nolloth\nDEA\n1991-10-01\n11.37661\n5\nUTR\n284.5266\n\n\nPort Nolloth\nDEA\n1991-11-01\n10.98208\n5\nUTR\n284.1321\n\n\n\n\n\n\nThis is a very basic example and mutate() is capable of much more than simple addition. We will get into some more exciting examples during the next session.\n\n7 Summarise variables (columns) with summarise()\n\nFinally this brings us to the last tool for this section. To create new columns we use mutate(), but to calculate any sort of summary/statistic from a column that will return fewer rows than the dataframe has we will use summarise(). This makes summarise() much more powerful than the other functions in this section, but because it is able to do more, it can also be more unpredictable, making it’s use potentially more challenging. We will almost always end op using this function in our work flows however so it behoves us to become well acquainted with it. The following chunk very simply calculates the overall mean temperature for the entire SACTN.\n\nSACTN %&gt;% \n  summarise(mean_temp = mean(temp, na.rm = TRUE))\n\n\n\n\nmean_temp\n\n\n19.26955\n\n\n\n\n\nNote how the above chunk created a new dataframe. This is done because it cannot add this one result to the previous dataframe due to the mismatch in the number of rows. Were we to want to create additional columns with other summaries we may do so within the same summarise() function. These multiple summaries are displayed on individual lines in the following chunk to help keep things clear.\n\nSACTN %&gt;% \n  summarise(mean_temp = mean(temp, na.rm = TRUE),\n            sd_temp = sd(temp, na.rm = TRUE),\n            min_temp = min(temp, na.rm = TRUE),\n            max_temp = max(temp, na.rm = TRUE)\n            )\n\n\n\n\nmean_temp\nsd_temp\nmin_temp\nmax_temp\n\n\n19.26955\n3.682122\n9.136322\n28.34648\n\n\n\n\n\nCreating summaries of the entire SACTN dataset in this way is not appropriate as we should not be combining time series from such different parts of the coast. In order to calculate summaries within variables we will need to learn how to use group_by(), which in turn will first require us to learn how to chain multiple functions together within a pipe (%&gt;%). That is how we will begin the next session for today. Finishing with several tips on how to make our data the tidiest that it may be.\n\n8 Session info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\nR&gt; lubridate   forcats   stringr     dplyr     purrr     readr     tidyr    tibble \nR&gt;   \"1.9.2\"   \"1.0.0\"   \"1.5.0\"   \"1.1.0\"   \"1.0.1\"   \"2.1.4\"   \"1.3.0\"   \"3.1.8\" \nR&gt;   ggplot2 tidyverse \nR&gt;   \"3.4.1\"   \"2.0.0\"\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {12. {Tidier} Data},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/12-tidier.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 12. Tidier data. https://tangledbank.netlify.app/BCB744/intro_r/12-tidier.html."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html",
    "href": "BCB744/intro_r/03-workflow.html",
    "title": "3. R workflows",
    "section": "",
    "text": "“A dream doesn’t become reality through magic; it takes sweat, determination and hard work.”\n— Colin Powell"
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#preparing-data-for-r",
    "href": "BCB744/intro_r/03-workflow.html#preparing-data-for-r",
    "title": "3. R workflows",
    "section": "\n2.1 Preparing data for R",
    "text": "2.1 Preparing data for R\nImporting data can actually take longer than the statistical analysis itself! In order to avoid as much frustration as possible it is important to remember that for R to be able to analyse your data they need to be in a consistent format, with each variable in a column and each sample in a row. The format within each variable (column) needs to be consistent and is commonly one of the following types: a continuous numeric variable (e.g., fish length (m): 0.133, 0.145); a factor or categorical variable (e.g., Month: Jan, Feb or 1, 2, …, 12); a nominal variable (e.g., algal colour: red, green, brown); or a logical variable (i.e., TRUE or FALSE). You can also use other more specific formats such as dates and times, and more general text formats.\nWe will learn more about working with data in R — specifically, we will teach you about the tidyverse principles and the distinction between long and wide format data in more detail on Day 4. For most of our work in R we require our data to be in the long format, but Excel users (poor things!) are more familiar with data stored in the wide format. For now let’s bring some data into R and not worry too much about the data being tidy."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#converting-data",
    "href": "BCB744/intro_r/03-workflow.html#converting-data",
    "title": "3. R workflows",
    "section": "\n2.2 Converting data",
    "text": "2.2 Converting data\nBefore we can read in the Laminaria dataset provided for the following exercises, we need to convert the Excel file supplied into a .csv file. Open ‘laminaria.xlsx’ in Excel, then select ‘Save As’ from the File menu. In the ‘Format’ drop-down menu, select the option called ‘Comma Separated Values’, then hit ‘Save’. You’ll get a warning that formatting will be removed and that only one sheet will be exported; simply ‘Continue’. Your working directory should now contain a file called laminaria.csv."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#importing-data",
    "href": "BCB744/intro_r/03-workflow.html#importing-data",
    "title": "3. R workflows",
    "section": "\n2.3 Importing data",
    "text": "2.3 Importing data\nThe easiest way to import data into R is by changing your working directory to be the same as the file path where the file(s) are you want to load. A file path is effectively an address. In most operating systems, if you open the folder where your files are you may click on the navigation bar and it will show you the complete file path. Many people develop the nasty habit of squirling away their files within folders within folders within folders within folders… within folders within folders. Please don’t do that.\nThe concept of file paths is either one that you are familiar with, or you’ve never heard of before. There tends to be little middle ground. Happily, RStudio allows us to circumvent this issue. We do this by using the Intro_R_Workshop.Rproj that you may find in the files downloaded for this workshop. If you have not already switched to the Intro_R_Workshop.Rproj as outlined in Chapter 2, click on the project button in the top right corner your RStudio window. Then navigate to where you saved Intro_R_Workshop.Rproj and select it. Notice that your RStudio has changed a bit and all of the objects you may have previously created in your environment have been removed and any tabs in the source editor pane have been closed. That is fine for now, but it may mean you need to re-open the Day_1.R script you just created.\nOnce we have the working directory set, either by doing it manually with setwd() or by loading a project, R will now know where to look for the files we want to read. The function read_csv() is the most convenient way to read in raw data. There are several other ways to read in data, but for the purposes of this workshop we’ll stick to this one, for now. To find out what it does, we will go to its help entry in the usual way (i.e. ?read_csv).\nAll R Help items are in the same format. A short Description (of what it does), Usage, Arguments (the different inputs it requires), Details (of what it does), Value (what it returns) and Examples. Arguments (the parameters that are passed to the function) are the lifeblood of any function, as this is how you provide information to R. You do not need to specify all arguments, as most have appropriate default values for your requirements, and others might not be needed for your particular case.\n\n\n\n\n\n\nData formats\n\n\n\nR has pedantic requirements for naming variables. It is safest to not use spaces, special characters (e.g., commas, semicolons, any of the shift characters above the numbers), or function names (e.g., mean). One can use ‘camelCase’, such as myFirstVariable, or simply separate the ‘parts’ of the variable name using an underscore such as in my_first_variable. Always make sure to use meaningful names; eventually you will learn to find a balance between meaningfulness and something short that’s easy enough to retype repeatedly (although R’s ability to use tab completion helps with not having to type long names to often).\n\n\n\n\n\n\n\n\nImport\n\n\n\nread_csv() is simply a ‘wrapper’ (i.e., a command that modifies) a more basic command called read_delim(), which itself allows you to read in many types of files besides .csv. To find out more, type ?read_delim()."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#loading-a-file",
    "href": "BCB744/intro_r/03-workflow.html#loading-a-file",
    "title": "3. R workflows",
    "section": "\n2.4 Loading a file",
    "text": "2.4 Loading a file\nTo load the laminaria.csv file we created, and assign it to an object name in R, we will use the read_csv() function from the tidyverse package, so let’s make sure it is activated.\n\nlibrary(tidyverse)\n\nDepending on the version of Excel you are using, or perhaps the settings within it, the ‘laminaria.csv’ file you created may be corrupted in different ways. Generally Excel likes to replace the , between columns in our .csv files with ;. This may seem like a triviality but sadly it is not. Lucky for use, the tidyverse knows about this problem and they have made a plan. Please open your ‘laminaria.csv’ file and look at which character is being used to separate columns. If it is , then we will load the data with read_csv(). If the columns are separated with ; we will use read_csv2().\n\n# Run this if 'laminaria.csv` has columns separated by ','\nlaminaria &lt;- read_csv(\"../../data/laminaria.csv\")\n# Run this if 'laminaria.csv` has columns separated by ';'\nlaminaria &lt;- read_csv2(\"../../data/laminaria.csv\")\n\nIf one clicks on the newly created laminaria object in the Environment pane it will open a new panel that shows the information as a spreadsheet. To go back to your script click the appropriate tab in the Source Editor pane. With these data loaded we may now perform analyses on them.\nAt any point when working in R, you can see exactly what objects are in memory in several ways. First, you can look at the Environment tab in RStudio, then Workspace Browser. Alternatively you can type either of the following:\n\nls()\n# or\nobjects()\n\nYou can delete an object from memory by specifying the rm() function with the name of the object:\n\nrm(laminaria)\n\nThis will of course delete our variable, so we will import it in again using whichever of the following two lines of code matched our Excel situation.\n\nlaminaria &lt;- read.csv(\"../../data/laminaria.csv\")\n\n\n\n\n\n\n\nManaging variables\n\n\n\nIt is good practice to remove variables from memory that you are not using, especially if they are large."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#examine-your-data",
    "href": "BCB744/intro_r/03-workflow.html#examine-your-data",
    "title": "3. R workflows",
    "section": "\n3.1 Examine your data",
    "text": "3.1 Examine your data\nOnce the data are in R, you need to check there are no glaring errors. It is useful to call up the first few lines of the dataframe using the function head(). Try it yourself by typing:\n\nhead(laminaria)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nsite\nInd\nblade_weight\nblade_length\nblade_thickness\nstipe_mass\nstipe_length\nstipe_diameter\ndigits\nthallus_mass\ntotal_length\n\n\n\nWC\nKommetjie\n2\n1.90\n160\n2.00\n1.50\n120\n56.0\n12\n3000\n256\n\n\nWC\nKommetjie\n3\n1.50\n120\n1.40\n2.25\n149\n68.5\n12\n3750\n269\n\n\nWC\nKommetjie\n4\n0.55\n110\n1.50\n1.15\n97\n69.0\n13\n1700\n207\n\n\nWC\nKommetjie\n5\n1.00\n159\n1.50\n2.60\n167\n60.0\n8\n3600\n326\n\n\nWC\nKommetjie\n6\n2.30\n149\n2.00\nNA\n146\n73.0\n15\n5100\n295\n\n\nWC\nKommetjie\n7\n1.60\n107\n1.75\n2.90\n161\n63.0\n17\n4500\n268\n\n\n\n\n\n\nThis lists the first six lines of each of the variables in the dataframe as a table. You can similarly retrieve the last six lines of a dataframe by an identical call to the function tail(). Of course, this works better when you have fewer than 10 or so variables (columns); for larger data sets, things can get a little messy. If you want more or fewer rows in your head or tail, tell R how many rows it is you want by adding this information to your function call. Try typing:\n\nhead(laminaria, n = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nsite\nInd\nblade_weight\nblade_length\nblade_thickness\nstipe_mass\nstipe_length\nstipe_diameter\ndigits\nthallus_mass\ntotal_length\n\n\n\nWC\nKommetjie\n2\n1.90\n160\n2.0\n1.50\n120\n56.0\n12\n3000\n256\n\n\nWC\nKommetjie\n3\n1.50\n120\n1.4\n2.25\n149\n68.5\n12\n3750\n269\n\n\nWC\nKommetjie\n4\n0.55\n110\n1.5\n1.15\n97\n69.0\n13\n1700\n207\n\n\n\n\n\ntail(laminaria, n = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nsite\nInd\nblade_weight\nblade_length\nblade_thickness\nstipe_mass\nstipe_length\nstipe_diameter\ndigits\nthallus_mass\ntotal_length\n\n\n\nWC\nRocky Bank\n12\n2.1\n194\n1.4\n3.75\n183\n38.20\n12\n5850\n377\n\n\nWC\nRocky Bank\n13\n1.3\n160\n1.9\n2.45\n155\n50.93\n16\n3550\n297\n\n\n\n\n\n\nYou can also check the structure of your data by using the glimpse() function:\n\nglimpse(laminaria)\n\nR&gt; Rows: 140\nR&gt; Columns: 12\nR&gt; $ region          &lt;chr&gt; \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", …\nR&gt; $ site            &lt;chr&gt; \"Kommetjie\", \"Kommetjie\", \"Kommetjie\", \"Kommetjie\", \"K…\nR&gt; $ Ind             &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 10, 11, 1, 3, 4, 5, 6, 7, 8, 9, 1…\nR&gt; $ blade_weight    &lt;dbl&gt; 1.90, 1.50, 0.55, 1.00, 2.30, 1.60, 0.65, 0.95, 2.30, …\nR&gt; $ blade_length    &lt;dbl&gt; 160, 120, 110, 159, 149, 107, 104, 111, 178, 145, 146,…\nR&gt; $ blade_thickness &lt;dbl&gt; 2.00, 1.40, 1.50, 1.50, 2.00, 1.75, 2.00, 1.25, 2.50, …\nR&gt; $ stipe_mass      &lt;dbl&gt; 1.50, 2.25, 1.15, 2.60, NA, 2.90, 0.75, 1.60, 4.20, 0.…\nR&gt; $ stipe_length    &lt;dbl&gt; 120, 149, 97, 167, 146, 161, 110, 136, 176, 82, 118, 1…\nR&gt; $ stipe_diameter  &lt;dbl&gt; 56.0, 68.5, 69.0, 60.0, 73.0, 63.0, 51.0, 56.0, 76.0, …\nR&gt; $ digits          &lt;dbl&gt; 12, 12, 13, 8, 15, 17, 11, 11, 8, 19, 20, 23, 20, 24, …\nR&gt; $ thallus_mass    &lt;dbl&gt; 3000, 3750, 1700, 3600, 5100, 4500, 1400, 2550, 6500, …\nR&gt; $ total_length    &lt;dbl&gt; 256, 269, 207, 326, 295, 268, 214, 247, 354, 227, 264,…\n\n\nThis very handy function lists the variables in your dataframe by name, tells you what sorts of data are contained in each variable (e.g., continuous number, discrete factor) and provides an indication of the actual contents of each.\nIf we wanted only the names of the variables (columns) in the dataframe, we could use:\n\nnames(laminaria)\n\nR&gt;  [1] \"region\"          \"site\"            \"Ind\"             \"blade_weight\"   \nR&gt;  [5] \"blade_length\"    \"blade_thickness\" \"stipe_mass\"      \"stipe_length\"   \nR&gt;  [9] \"stipe_diameter\"  \"digits\"          \"thallus_mass\"    \"total_length\"\n\n\nAnother option, but by no means the only one remaining, is to install a library called skimr and to use thew skim() function:\n\nlibrary(skimr)\nskim(iris) # using built-in `iris` data\n\n\nData summary\n\n\nName\niris\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃"
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#tidyverse-sneak-peek",
    "href": "BCB744/intro_r/03-workflow.html#tidyverse-sneak-peek",
    "title": "3. R workflows",
    "section": "\n3.2 Tidyverse sneak peek",
    "text": "3.2 Tidyverse sneak peek\nBefore we begin to manipulate our data further we need to briefly introduce ourselves to the tidyverse. And no introduction can be complete within learning about the pipe command, %&gt;%. We may type this by pushing the following keys together: ctrl-shift-m. The pipe (%&gt;%) allows us to perform calculations sequentially, which helps us to avoid making errors.\nThe pipe works best in tandem with the following five common functions:\n\nArrange observations (rows) with arrange()\n\nFilter observations (rows) with filter()\n\nSelect variables (columns) with select()\n\nCreate new variables (columns) with mutate()\n\nSummarise variables (columns) with summarise()\n\n\nWe will cover these functions in more detail on Day 4. For now we will ease ourselves into the code with some simple examples."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#subsetting",
    "href": "BCB744/intro_r/03-workflow.html#subsetting",
    "title": "3. R workflows",
    "section": "\n3.3 Subsetting",
    "text": "3.3 Subsetting\nNow let’s have a look at specific parts of the data. You will likely need to do this in almost every script you write. If we want to refer to a variable, we specify the dataframe then the column name within the select() function. In your script type:\n\nlaminaria %&gt;% # Tell R which dataframe we are using\n  select(site, total_length) # Select only specific columns\n\nIf we want to only select values from specific columns we insert one more line of code.\n\nlaminaria %&gt;% \n  select(site, total_length) %&gt;% # Select specific columns first\n  slice(56:78)\n# what does the '56:78' do? Change some numbers and run the code again. What happens?\n\nIf we wanted to select only the rows of data belonging to the Kommetjie site, we could type:\n\nlaminaria %&gt;%\n  filter(site == \"Kommetjie\")\n\nThe function filter() has two arguments: the first is a dataframe (we specify laminaria in the previous line and the pipe supplies this for us) and the second is an expression that relates to which rows of a particular variable we want to include. Here we include all rows for Kommetjie and we find that in the variable site. It returns a subset that is actually a dataframe itself; it is in the same form as the original dataframe. We could assign that subset of the full dataframe to a new dataframe if we wanted to.\n\nlam_kom &lt;- laminaria %&gt;% \n  filter(site == \"Kommetjie\")\n\n\n\n\n\n\n\nTask B\n\n\n\n\nIn the script you have started, create a new named dataframe containing only kelps from two of the sites. Check that the new dataframe has the correct values in it. What purpose can the naming of a newly-created dataframe serve?"
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#basic-stats",
    "href": "BCB744/intro_r/03-workflow.html#basic-stats",
    "title": "3. R workflows",
    "section": "\n3.4 Basic stats",
    "text": "3.4 Basic stats\nStraight out of the box it is possible in R to perform a broad range of statistical calculations on a dataframe. If we wanted to know how many samples we have at Kommetjie, we simply type the following:\n\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(site == \"Kommetjie\") %&gt;% # Filter out only records from Kommetjie\n  nrow() # Count the number of remaining rows\n\nOr, if we want to select only the row with the greatest total length:\n\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(total_length == max(total_length)) # Select row with max total length\n\n\n\n\n\n\n\nTask B\n\n\n\n\nUsing pipes, subset the Laminaria data to include regions where the blade thickness is thicker than 5 cm and retain only the columns site, region, blade weight and blade thickness. Now exit RStudio. Pretend it is three days later and revisit your analysis. Calculate the number of entries at Kommetjie and find the row with the greatest length. Do this now.\n\n\n\nImagine doing this daily as our analysis grows in complexity. It will very soon become quite repetitive if each day you had to retype all these lines of code. And now, six weeks into the research and attendant statistical analysis, you discover that there were some mistakes and some of the raw data were incorrect. Now everything would have to be repeated by retyping it at the command prompt. Or worse still (and bad for repetitive strain injury) doing all of it in SPSS and remembering which buttons to click and then re-clicking them. A pain. Let’s avoid that altogether and do it the right way by writing an R script to automate and annotate all of this.\n\n\n\n\n\n\nDealing with missing data\n\n\n\nThe .csv file format is usually the most robust for reading data into R. Where you have missing data (blanks), the .csv format separates these by commas. However, there can be problems with blanks if you read in a space-delimited format file. If you are having trouble reading in missing data as blanks, try replacing them in your spreadsheet with NA, the missing data code in R. In Excel, highlight the area of the spreadsheet that includes all the cells you need to fill with NA. Do an Edit/Replace… and leave the ‘Find what:’ textbox blank and in the ‘Replace with:’ textbox enter NA, the missing value code. Once imported into R, the NA values will be recognised as missing data.\n\n\nSo far we have calculated the mean and standard deviation of some data in the Laminaria data set. If you have not, please append those lines of code to the end of your script. You can run individual lines of code by highlighting them and pressing ctrl-Enter (cmd-Enter on a Mac). Do this.\nYour file will now look similar to this one, but of course you will have added your own notes and comments as you went along:\n\n# Day_1.R\n# Reads in some data about Laminaria collected along the Cape Peninsula\n# do various data manipulations, analyses and graphs\n# AJ Smit\n# 9 January 2020\n\n# Find the current working directory (it will be correct if a project was\n# created as instructed earlier)\ngetwd()\n\n# If the directory is wrong because you chose not to use an Rworkspace (project),\n# set your directory manually to where the script will be saved and where the data\n# are located\n# setwd(\"&lt;insert_path_here&gt;\")\n\n# Load libraries\nlibrary(tidyverse)\n\n# Load the data\nlaminaria &lt;- read_csv(\"../../data/laminaria.csv\")\n\n# Examine the data\nhead(laminaria, 5) # First five lines\ntail(laminaria, 2) # Last two lines\nglimpse(laminaria) # A more thorough summary\nnames(laminaria) # THe names of the columns\n\n# Subsetting data\nlaminaria %&gt;% # Tell R which dataframe to use\n  select(site, total_length) %&gt;% # Select specific columns\n  slice(56:78) # Select specific rows\n\n# How many data points do we have at Kommetjie?\nlaminaria %&gt;%\n  filter(site == \"Kommetjie\") %&gt;%\n  nrow()\n\n# The row with the greatest length\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(total_length == max(total_length)) # Select row with max total length\n\nMaking sure all the latest edits in your R script have been saved, close your R session. Pretend this is now 2019 and you need to revisit the analysis. Open the file you created in 2017 in RStudio. All you need to do now is highlight the file’s entire contents and hit ctrl-Enter.\n\n\n\n\n\n\nStick with .csv files\n\n\n\nThere are packages in R to read in Excel spreadsheets (e.g., .xlsx), but remember there are likely to be problems reading in formulae, graphs, macros and multiple worksheets. We recommend exporting data deliberately to .csv files (which are also commonly used in other programs). This not only avoids complications, but also allows you to unambiguously identify the data you based your analysis on. This last statement should give you the hint that it is good practice to name your .csv slightly differently each time you export it from Excel, perhaps by appending a reference to the date it was exported.\n\n\n\n\n\n\n\n\nRemember…\n\n\n\nFriends don’t let friends use Excel."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#summary-statistics-by-variable",
    "href": "BCB744/intro_r/03-workflow.html#summary-statistics-by-variable",
    "title": "3. R workflows",
    "section": "\n4.1 Summary statistics by variable",
    "text": "4.1 Summary statistics by variable\nThis is all very convenient, but we may want to ask R specifically for just the mean of a particular variable. In this case, we simply need to tell R which summary statistic we are interested in, and to specify the variable to apply it to using summarise(). Try typing:\n\nlaminaria %&gt;% # Chose the dataframe\n  summarise(avg_bld_wdt = mean(blade_length)) # Calculate mean blade length\n\nOr, if we wanted to know the mean and standard deviation for the total lengths of all the plants across all sites, do:\n\nlaminaria %&gt;% # Tell R that we want to use the 'laminaria' dataframe\n  summarise(avg_stp_ln = mean(total_length), # Create a summary of the mean of the total lengths\n            sd_stp_ln = sd(total_length)) # Create a summary of the sd of the total lengths\n\nOf course, the mean and standard deviation are not the only summary statistic that R can calculate. Try max(), min(), median(), range(), sd() and var(). Do they return the values you expected? Now try:\n\nlaminaria %&gt;% \n  summarise(avg_stp_ms = mean(stipe_mass))\n\nThe answer probably isn’t what you would expect. Why not? Sometimes, you need to tell R how you want it to deal with missing data. In this case, you have NAs in the named variable, and R takes the cautious approach of giving you the answer of NA, meaning that there are missing values here. This may not seem useful, but as the programmer, you can tell R to respond differently, and it will. Simply append an argument to your function call, and you will get a different response. Type:\n\nlaminaria %&gt;% \n  summarise(avg_stp_ms = mean(stipe_mass, na.rm = T))\n\nThe na.rm argument tells R to remove (or more correctly ‘strip’) NAs from the data string before calculating the mean. It now returns the correct answer. Although needing to deal explicitly with missing values in this way can be a bit painful, it does make you more aware of missing data, what the analyses in R are doing, and makes you decide explicitly how you will treat missing data."
  },
  {
    "objectID": "BCB744/intro_r/03-workflow.html#more-complex-calculations",
    "href": "BCB744/intro_r/03-workflow.html#more-complex-calculations",
    "title": "3. R workflows",
    "section": "\n4.2 More complex calculations",
    "text": "4.2 More complex calculations\nLet’s say you want to calculate something that is not standard in R, say the standard error of the mean for a variable, rather than just the corresponding standard deviation. How can this be done?\nThe trick is to remember that R is a calculator, so we can use it to do maths, even complex maths (which we won’t do). The formula for standard error is:\n\\[se = \\frac{var}{\\sqrt{n}}\\]\nWe know that the variance is given by var(), so all we need to do is figure out how to get n and calculate a square root. The simplest way to determine the number of elements in a variable is a call to the function nrow(), as we saw previously. We may therefore calculate standard error with one chunk of code, step by step, using the pipe. Furthermore, by using group_by() we may calculate the standard error for all sites in one go.\n\nlaminaria %&gt;% # Select 'laminaria'\n  group_by(site) %&gt;% # Group the dataframe by site\n  summarise(var_bl = var(blade_length), # Calculate variance\n            n_bl = n()) %&gt;%  # Count number of values\n  mutate(se_bl = sqrt(var_bl / n_bl)) # Calculate se\n\nWhen calculating the mean, we specified that R should strip the NAs, using the argument na.rm = TRUE. In the example above, we didn’t have NAs in the variable of interest. What happens if we do?\nUnfortunately, the call to the function nrow() has no arguments telling R how to treat NAs; instead, they are simply treated as elements of the variable and are therefore counted. The easiest way to resolve this problem is to strip out NAs in advance of any calculations. Try typing:\n\nlaminaria %&gt;% \n  select(stipe_mass) %&gt;% \n  summarise(n = n())\n\nthen:\n\nlaminaria %&gt;% \n  select(stipe_mass) %&gt;% \n  na.omit() %&gt;% \n  summarise(n = n())\n\nYou will notice that the function na.omit() removes NAs from the variable that is specified as its argument.\n\n\n\n\n\n\nTask B\n\n\n\n\nUsing this new information, calculate the mean stipe mass and the corresponding standard error.\nCreate a new data frame from the laminaria dataset that meets the following criteria: contains only the site column and a new column called total_length_half containing values that are half of the total_length. In this total_length_half column, there are no NAs and all values are less than 100. Hint: think about how the commands should be ordered to produce this data frame!\nUse group_by() and summarise() to find the mean, min, and max blade_length for each site. Also add the number of observations (hint: see ?n).\nWhat was the heaviest stipe measured in each site? Return the columns site, region, and stipe_length."
  },
  {
    "objectID": "BCB744/intro_r/05-faceting.html",
    "href": "BCB744/intro_r/05-faceting.html",
    "title": "5. Faceting figures",
    "section": "",
    "text": "“But let the mind beware, that though the flesh be bugged, the circumstances of existence are pretty glorious.”\n— Kurt Vonnegut, Player Piano\nSo far we have only looked at single panel figures. But as you may have guessed by now, ggplot2 is capable of creating any sort of data visualisation that a human mind could conceive. This may seem like a grandiose assertion, but we’ll see if we can’t convince you of it by the end of this course. For now however, let’s just take our understanding of the usability of ggplot2 two steps further by first learning how to facet a single figure, and then stitch different types of figures together into a grid. In order to aid us in this process we will make use of an additional package, ggpubr. The purpose of this package is to provide a bevy of additional tools that researchers commonly make use of in order to produce publication quality figures. Note that library(ggpubr) will not work on your computer if you have not yet installed the package.\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)"
  },
  {
    "objectID": "BCB744/intro_r/05-faceting.html#line-graph",
    "href": "BCB744/intro_r/05-faceting.html#line-graph",
    "title": "5. Faceting figures",
    "section": "\n2.1 Line graph",
    "text": "2.1 Line graph\n\nline_1 &lt;- ggplot(data = ChickWeight, aes(x = Time, y = weight, colour = Diet)) +\n  geom_point() +\n  geom_line(aes(group = Chick)) +\n  labs(x = \"Days\", y = \"Mass (g)\") +\n  theme_minimal()\nline_1\n\n\n\nLine graph for the progression of chicken weights (g) over time (days) based on four different diets."
  },
  {
    "objectID": "BCB744/intro_r/05-faceting.html#smooth-gam-model",
    "href": "BCB744/intro_r/05-faceting.html#smooth-gam-model",
    "title": "5. Faceting figures",
    "section": "\n2.2 Smooth (GAM) model",
    "text": "2.2 Smooth (GAM) model\n\nlm_1 &lt;- ggplot(data = ChickWeight, aes(x = Time, y = weight, colour = Diet)) +\n  geom_point() +\n  geom_smooth(method = \"gam\") +\n  labs(x = \"Days\", y = \"Mass (g)\") +\n  theme_minimal()\nlm_1\n\n\n\nLinear models for the progression of chicken weights (g) over time (days) based on four different diets."
  },
  {
    "objectID": "BCB744/intro_r/05-faceting.html#histogram",
    "href": "BCB744/intro_r/05-faceting.html#histogram",
    "title": "5. Faceting figures",
    "section": "\n2.3 Histogram",
    "text": "2.3 Histogram\n\n# Note that we are using 'ChickLast', not 'ChickWeight'\nhistogram_1 &lt;- ggplot(data = ChickLast, aes(x = weight)) +\n  geom_histogram(aes(fill = Diet), position = \"dodge\", binwidth = 100) +\n  labs(x = \"Final Mass (g)\", y = \"Count\") +\n  theme_minimal()\nhistogram_1\n\n\n\nHistogram showing final chicken weights (g) by diet."
  },
  {
    "objectID": "BCB744/intro_r/05-faceting.html#boxplot",
    "href": "BCB744/intro_r/05-faceting.html#boxplot",
    "title": "5. Faceting figures",
    "section": "\n2.4 Boxplot",
    "text": "2.4 Boxplot\n\n# Note that we are using 'ChickLast', not 'ChickWeight'\nbox_1 &lt;- ggplot(data = ChickLast, aes(x = Diet, y = weight)) +\n  geom_boxplot(aes(fill = Diet)) +\n  labs(x = \"Diet\", y = \"Final Mass (g)\") +\n  theme_minimal()\nbox_1\n\n\n\nViolin plot showing the distribution of final chicken weights (g) by diet."
  },
  {
    "objectID": "BCB744/intro_r/06-brewing.html",
    "href": "BCB744/intro_r/06-brewing.html",
    "title": "6. Brewing colours",
    "section": "",
    "text": "“Every portrait that is painted with feeling is a portrait of the artist, not of the sitter.”\n— Oscar Wilde\n\n\n“If you could say it in words, there would be no reason to paint.”\n— Edward Hopper\n\nNow that we have seen the basics of ggplot2, let’s take a moment to delve further into the beauty of our figures. It may sound vain at first, but the colour palette of a figure is actually very important. This is for two main reasons. The first being that a consistent colour palette looks more professional. But most importantly it is necessary to have a good colour palette because it makes the information in our figures easier to understand. The communication of information to others is central to good science.\n\n1 R Data\nBefore we get going on our figures, we first need to learn more about the built in data that R has. The base R program that we all have loaded on our computers already comes with heaps of example dataframes that we may use for practice. We don’t need to load our own data. Additionally, whenever we install a new package (and by now we’ve already installed dozens) it usually comes with several new dataframes. There are many ways to look at the data that we have available from our packages. Below we show two of the many options.\n\n# To create a list of ALL available data\n  # Not really recommended as the output is overwhelming\ndata(package = .packages(all.available = TRUE))\n\n# To look for datasets within a single known package\n  # type the name of the package followed by '::'\n  # This tells R you want to look in the specified package\n  # When the autocomplete bubble comes up you may scroll\n  # through it with the up and down arrows\n  # Look for objects that have a mini spreadsheet icon\n  # These are the datasets\n\n# Try typing the following code and see what happens...\ndatasets::\n\nWe have an amazing amount of data available to us. So the challenge is not to find a dataframe that works for us, but to just decide on one. My preferred method is to read the short descriptions of the dataframes and pick the one that sounds the funniest. But please use whatever method makes the most sense to you. One note of caution, in R there are generally two different forms of data: wide OR long. We will see in detail what this means on Day 4, and what to do about it. For now we just need to know that ggplot2 works much better with long data. To look at a dataframe of interest we use the same method we would use to look up a help file for a function.\nOver the years I’ve installed so many packages on my computer that it is difficult to chose a dataframe. The package boot has some particularly interesting dataframes with a biological focus. Please install this now to access to these data. I have decided to load the urine dataframe here. Note that library(boot) will not work on your computer if you have not installed the package yet. With these data we will now make a scatterplot with two of the variables, while changing the colour of the dots with a third variable.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(boot)\n\n# Load data\nurine &lt;- boot::urine\n\n# Look at help file for more info\n# ?urine\n\n# Create a quick scatterplot\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond))\n\n\n\n\nAnd now we have a scatterplot that is showing the relationship between the osmolarity and pH of urine, with the conductivity of those urine samples shown in shades of blue. What is important to note here is that the colour scale is continuous. How can we now this by looking at the figure? Let’s look at the same figure but use a discrete variable for colouring.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r)))\n\n\n\n\nWhat is the first thing you notice about the difference in the colours? Why did we use as.factor() for the colour aesthetic for our points? What happens if we don’t use this? Try it now.\n\n2 RColorBrewer\nCentral to the purpose of ggplot2 is the creation of beautiful figures. For this reason there are many built in functions that we may use in order to have precise control over the colours we use, as well as additional packages that extend our options even further. The RColorBrewer package should have been installed on your computer and activated automatically when we installed and activated the tidyverse. We will use this package for its lovely colour palettes. Let’s spruce up the previous continuous colour scale figure now.\n\n# The continuous colour scale figure\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond)) +\n  scale_colour_distiller() # Change the continuous variable colour palette\n\n\n\n\nDoes this look different? If so, how? The second page of the colour cheat sheet we included in the course material shows some different colour brewer palettes. Let’s look at how to use those here.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond)) +\n  scale_colour_distiller(palette = \"Spectral\")\n\n\n\n\nDoes that help us to see a pattern in the data? What do we see? Does it look like there are any significant relationships here? How would we test that?\nIf we want to use colour brewer with a discrete variable we use a slightly different function.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r))) +\n  scale_colour_brewer() # This is the different function\n\n\n\n\nThe default colour scale here is not helpful at all. So let’s pick a better one. If we look at our cheat sheet we will see a list of different continuous and discrete colour scales. All we need to do is copy and paste one of these names into our colour brewer function with inverted commas.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r))) +\n  scale_colour_brewer(palette = \"Set1\") # Here I used \"Set1\", but use what you like\n\n\n\n\n\n3 Make your own palettes\nThis is all well and good. But didn’t we claim that this should give us complete control over our colours? So far it looks like it has just given us a few more palettes to use. And that’s nice, but it’s not ‘infinite choices’. That is where the Internet comes to our rescue. There are many places we may go to for support in this regard. The following links, in descending order, are very useful. And fun!\n\nhttp://tristen.ca/hcl-picker/#/hlc/6/0.95/48B4B6/345363\nhttp://tools.medialab.sciences-po.fr/iwanthue/index.php\nhttp://jsfiddle.net/d6wXV/6/embedded/result/\n\nI find the first link the easiest to use. But the second and third links are better at generating discrete colour palettes. Take several minutes playing with the different websites and decide for yourself which one(s) you like.\n\n4 Use your own palettes\nNow that we’ve had some time to play around with the colour generators let’s look at how to use them with our figures. I’ve used the first web link to create a list of five colours. I then copy and pasted them into the code below, separating them with commas and placing them inside of c() and inverted commas. Be certain that you insert commas and inverted commas as necessary or you will get errors. Note also that we are using a new function to use our custom palette.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond)) +\n  scale_colour_gradientn(colours = c(\"#A5A94D\", \"#6FB16F\", \"#45B19B\",\n                                    \"#59A9BE\", \"#9699C4\", \"#CA86AD\"))\n\n\n\n\nIf we want to use our custom colour palettes with a discrete colour scale we use a different function as seen in the code below. While we are at it, let’s also see how to correct the title of the legend and its text labels. Sometimes the default output is not what we want for our final figure. Especially if we are going to be publishing it. Also note in the following code chunk that rather than using hexadecimal character strings to represent colours in our custom palette, we are simply writing in the human name for the colours we want. This will work for the continuous colour palettes above, too.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r))) +\n  scale_colour_manual(values = c(\"pink\", \"maroon\"), # How to use custom palette\n                     labels = c(\"no\", \"yes\")) + # How to change the legend text\n  labs(colour = \"crystals\") # How to change the legend title\n\n\n\n\nSo now we have seen how to control the colours palettes in our figures. I know it is a but much. Four new functions just to change some colours! That’s a bummer. Don’t forget that one of the main benefits of R is that all of your code is written down, annotated and saved. You don’t need to remember which button to click to change the colours, you just need to remember where you saved the code that you will need. And that’s pretty great in my opinion.\n\n\n\n\n\n\nTask E\n\n\n\nToday we learned the basics of ggplot2, how to facet, how to brew colours, and how to plot some basic summary statistics. Sjog, that’s a lot of stuff to remember… which is why we will now spend the rest of Day 2 putting our new found skills to use.\nPlease group up as you see fit to produce your very own ggplot2 figures. We’ve not yet learned how to manipulate/tidy up our data so it may be challenging to grab any ol’ dataset and make a plan with it. But try! Explore some of the other built-in datasets and find two or three you like. Or use your own data!\nThe goal by the end of today is to have created four figures and join them together via faceting and the options offered by ggarrange(). We will be walking the room to help with any issues that may arise.\nMake sure the above assignment is included within a Quarto file rendered to .html. Include some textual information to inform the reader of the intent of the plots and what patterns are visible.\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nSubmit a R (Quarto) script wherein you provide answers to the Task questions by no later than 8:00 tomorrow.\nProvide a neat and thoroughly annotated and labelled Rmarkdown file which outlines the graphs and all calculations (as necessary).\nPlease label the Rmarkdown and resulting HTML files as follows:\n\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Task_E.qmd, and\nBCB744_&lt;first_name&gt;_&lt;last_name&gt;_Task_E.html\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).\nSubmit your Tasks on iKamva when ready.\n\n\n\n5 Session info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\nR&gt;       boot  lubridate    forcats    stringr      dplyr      purrr      readr \nR&gt; \"1.3-28.1\"    \"1.9.2\"    \"1.0.0\"    \"1.5.0\"    \"1.1.0\"    \"1.0.1\"    \"2.1.4\" \nR&gt;      tidyr     tibble    ggplot2  tidyverse \nR&gt;    \"1.3.0\"    \"3.1.8\"    \"3.4.1\"    \"2.0.0\"\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {6. {Brewing} Colours},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/06-brewing.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 6. Brewing colours. https://tangledbank.netlify.app/BCB744/intro_r/06-brewing.html."
  },
  {
    "objectID": "BCB744/intro_r/10-mapping_quakes.html",
    "href": "BCB744/intro_r/10-mapping_quakes.html",
    "title": "10. The Fiji Earthquake Data",
    "section": "",
    "text": "Here I will plot the built-in earthquake dataset (datasets::quakes).\n\n\n\n\n\n\nGlobal earthquake distribution\n\n\n\nFor a global map of earthquakes, see my plot of the Kaggle earthquake data.\n\n\nTwo new concepts will be introduced in the Chapter:\n\nGeographic Coordinate Systems\nProjected Coordinate Systems\n\nThese are specified to the mapping / plotting functions via the Coordinate Reference System (CRS) through functionality built into the sf package.\nYou will also learn how to deal with polygons that cross the dateline (0° wrapping back onto 360°) or the anti-meridian (-180° folding back onto +180°).\n\n1 Load packages and data\nI will use the Natural Earth data and manipulate it with the sf package functions. This package integrates well with the tidyverse.\nTHIS PAGE IS NOT DSPLAYING THE OUTPUT OF CODE AT PRESENT DUE TO AN UPDATE OF THE SF PACKAGE THAT CAUSED IT TO BREAK. I WILL ENABLE THE OUTPUT AGAIN ONCE THE AUTHORS HAVE CORRECTED THEIR CODE\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthhires)\n\n\n2 Load the Natural Earth map\nThe rnaturalearth package has the ne_counties() function which we use to load borders of all the countries in the world. I load the medium resolution dataset and make sure the data are of class sf, i.e. a simple features collection.\n\nsf_use_s2(FALSE)\n\nworld &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3)  \nhead(world[c('continent')])\n\nR&gt; head(world[c('continent')])\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.4537 ymin: -55.9185 xmax: 140.9776 ymax: 7.35578\nCRS:           +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n      continent                       geometry\n0          Asia MULTIPOLYGON (((117.7036 4....\n1          Asia MULTIPOLYGON (((117.7036 4....\n2 South America MULTIPOLYGON (((-69.51009 -...\n3 South America MULTIPOLYGON (((-69.51009 -...\n4 South America MULTIPOLYGON (((-69.51009 -...\n5 South America MULTIPOLYGON (((-67.28475 -...\nNote that for the Natural Earth data the coordinate reference system (CRS) is in the dataset are longitude / latitude coordinates in degrees in a CRS called WGS84. This is the World Geodetic System 1984 commonly used in most GPS devices. The default unit of this CRS is in degrees longitude and latitude.\nFor more information on CRS, see:\n\nThe PROJ system. The proj-string specified with every map can be used in sf; it can be retrieved using st_crs() and one can transform between various projections uing st_transform(). PROJ is written in C++ and loaded automagically with sf.\nThe EPSG coordinate codes, which provide a convenient shortcut to the longer proj-string. Navigating the a page for a projection—WGS84 known as EPSG:4326—gives one the various relevant details, and the proj-string can be located in the PROJ.4 tab under Exports.\n\nMore information about the map data is available with the head(world) function (as seen above), namely that the longitude goes from -180° (180° west of the prime meridian) to +180° (180° east of the prime meridian). This means that the anti-meridian cuts some of the polygons in the Pacific along the line where -180° wraps back onto +180°, and this can be problematic for maps of the Pacific. We will get to this later. The latitude goes from -90° (90° south of the equator) to +90° (90° north of the equator).\nA very quick map looks like this:\n\nggplot() +\n  geom_sf(data = world, colour = \"black\", fill = \"grey70\")\n\n\n\n\nWe see above that Africa is centrally positioned. However, I want to focus on the western Pacific region. I am also going to apply a new projection (ESRI:53077, the Natural Earth projection) to it. The ‘rotation’ is accomplished by setting lon_0=170 in the proj-string.\n\nNE_proj &lt;- \"+proj=natearth +lon_0=170 \"\n\nworld_0 &lt;- world |&gt; \n  st_transform(NE_proj)\n\nggplot() +\n  geom_sf(data = world_0, colour = \"black\", fill = \"grey70\")\n\n\n\n\nThe western Pacific is now focal, but the map looks strange to say the least. This is due to the break at the anti-meridian which causes the polygons to join up in strange and unexpected ways when the central longitude in the map is not displayed at exactly 0° (in my reprojection I made the focus on 170°E and it became the center). I can fix it using st_break_antimeridian() but to do so I must start with unprojected data, and only then apply this function.\n\nworld_1 &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3) |&gt; \n  st_break_antimeridian(lon_0 = 170) |&gt; \n  st_transform(NE_proj)\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n3 Zooming in\nThere are three options for focusing in on a particular area of the map (zooming in):\n\nselecting only certain areas of interest from the spatial dataset (e.g. only certain countries / continent(s) / etc.);\ncropping the geometries in the spatial dataset using sf_crop(); and\nrestricting the display window via coord_sf().\n\nI will look at each in some detail.\n\n4 Selecting areas of interest\nI am interested only in the ‘continent’ called Oceania, which includes the Pacific Islands, Australia, and New Zealand. More correctly, it a geographical region and not a continent. It is comprised of Australasia, Melanesia, Micronesia, and Polynesia which span the the Eastern and Western Hemispheres.\n\noceania &lt;- world_1[world_1$continent == 'Oceania',]\nggplot() +\n  geom_sf(data = oceania, colour = \"black\", fill = \"grey70\")\n\n\n\n\nZooming in on only the group of nations included with Oceania reveals another problem. This is, only part of New Guinea is displayed: Papua New Guinea appears on the map but the western side of New Guinea, called Western New Guinea, is absent. This is because Papua New Guinea is part of Micronesia whereas West New Guinea is part of Indonesia (part of the South-eastern Asia region).\nThere is no easy way to select the countries that constitute Australasia, Melanesia, Micronesia, Indonesia, and Polynesia—unless I create an exhaustive list of these small island nations. But I can use the countrycode package to insert an attribute with the geographic regional classification of the United Nations in the world_1 map. Now all the constituent countries belonging to these regions will be correctly classified to the UN regional classification scheme. Note that I also collapse the countries into their continents by merging all polygons belonging to the same continent (the group_by() and summarise() functions)—I do this because I don’t want to display individual countries.\n\nlibrary(countrycode)\n\nworld_1$region &lt;- countrycode(world_1$iso_a3, origin = \"iso3c\",\n  destination = \"un.regionsub.name\")\n\nsw_pacific &lt;- world_1 |&gt; \n  filter(region %in% c(\"Australia and New Zealand\", \"Melanesia\", \"Micronesia\",\n    \"Indonesia\", \"Polynesia\", \"South-eastern Asia\")) |&gt; \n  group_by(continent) |&gt;\n  summarise()\n\nggplot() +\n  geom_sf(data = sw_pacific, colour = \"black\", fill = \"grey70\")\n\n\n\n\nAs we can see above, by including the South-eastern Asia region I complete the mass of land that is New Guinea.\n\n5 Cropping\nThe above map is good but not perfect. I have in mind zooming in a bit more into the region around Fiji where the earthquake monitoring network is located. One way to do this is to crop the extent of the study region using a bounding box whose boundaries are defined by the extent of the quakes datapoints.\nTo start, I use the earthquake data and extract from there the study domain and increase the edges all round by a given margin—this is so that the stations are not plotted right on the maps’ edges.\nImportant! The coordinates in the quakes data are provided in WGS84, so I need to first specify them as such and then transform them to the same coordinate system used by the map.\n\nquakes &lt;- as_tibble(datasets::quakes)\nmargin &lt;- 15.0\nxmin &lt;- min(quakes$long) - margin; xmax &lt;- max(quakes$long) + margin\nymin &lt;- min(quakes$lat) - margin; ymax &lt;- max(quakes$lat) + margin\n\nWGS84_proj &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\nbbox &lt;- st_sfc(st_point(c(xmin, ymin)), st_point(c(xmax, ymax)),\n                         crs = WGS84_proj)\nbbox_trans &lt;- st_transform(bbox, NE_proj)\n\nsw_pacific_cropped &lt;- sw_pacific |&gt; \n  st_crop(bbox_trans)\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"black\", fill = \"grey70\")\n\n\n\n\nThat looks decent enough, but there’s another way to accomplish the same.\n\n6 Setting the mapping limits in ggplot2\n\nThe third approach to get closer to the region of interest is to use the full map extent (the world) loaded at the beginning, but to set the limits of the view window within the coord_sf() function in ggplot().\nDo do this, I start with the bbox_sf_trans object, which was obtained by first specifying the coordinates marking the extent of the map in the WGS84 coordinate system and then transforming them to Natural Earth. We can extract the transformed limits with st_coordinates() and supply them to the map.\n\nbbox_trans_coord &lt;- st_coordinates(bbox_trans)\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\") +\n  coord_sf(xlim = bbox_trans_coord[,'X'], ylim = bbox_trans_coord[,'Y'],\n    expand = FALSE)\n\n\n\n\nGreat! This works well. Note that the coordinates on the graticule are in degrees longitude and latitude, the default for WGS84. By setting datum = NE_proj ensures the graticule is displayed in Natural Earth coordinate system units, which is meters. This might look strange at first, but it is not wrong.\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\") +\n  coord_sf(xlim = bbox_trans_coord[,'X'], ylim = bbox_trans_coord[,'Y'],\n    expand = FALSE, datum = NE_proj)\n\n\n\n\n\n7 Adding the quakes data as points\nIn order to plot the quakes data, I need to create a sf object from the quakes data. When converting the dataframe to class sf, I must also assign to it the CRS associated of the original dataset. This would be WGS84. Afterwards I will transform it to the Natural Earth CRS.\n\nquakes_sf &lt;- quakes |&gt; \n  st_as_sf(coords = c(\"long\", \"lat\"),\n    crs = WGS84_proj)\nquakes_sf_trans &lt;- st_transform(quakes_sf, NE_proj)\nhead(quakes_sf_trans)\n\nR&gt; head(quakes_sf_trans)\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1047820 ymin: -2923232 xmax: 1361900 ymax: -2017755\nCRS:           +proj=natearth +lon_0=170 \n# A tibble: 6 × 4\n  depth   mag stations           geometry\n  &lt;int&gt; &lt;dbl&gt;    &lt;int&gt;        &lt;POINT [m]&gt;\n1   562   4.8       41 (1104307 -2293735)\n2   650   4.2       15 (1047820 -2316276)\n3    42   5.4       43 (1323082 -2923232)\n4   626   4.1       19 (1113132 -2017755)\n5   649   4         11 (1136619 -2293735)\n6   195   4         12 (1361900 -2210349)\nI am going to make a map of the Fiji region and plot the spatial location of the earthquakes, and scale the points indicating the earthquakes’ magnitude by their intensity (mag).\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"black\", fill = \"grey70\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = mag, size = mag),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.4) +\n  scale_colour_continuous(type = \"viridis\") +\n  guides(size = \"none\") +\n  coord_sf(expand = FALSE) +\n  labs(x = NULL, y = NULL,\n    title = \"The Fiji Earthquake Data\",\n    subtitle = \"Natural Earth\")\n\n\n\n\nNow I apply a more appropriate CRS to the map. The WGS 84 / NIWA Albers projection (EPSG:9191) is suitable for the southwestern Pacific Ocean and Southern Ocean areas surrounding New Zealand.\n\nNIWA_Albers_proj &lt;- \"+proj=aea +lat_0=-22 +lon_0=175 +lat_1=-20 +lat_2=-30 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs\"\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"indianred\", fill = \"beige\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = mag, size = mag),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.6) +\n  scale_colour_continuous(type = \"viridis\") +\n  guides(size = \"none\",\n    colour = guide_colourbar(\"Magnitude\")) +\n  coord_sf(expand = FALSE,\n    crs = NIWA_Albers_proj) +\n  labs(x = NULL, y = NULL,\n    title = \"The Fiji Earthquake Data\",\n    subtitle = \"WGS 84 / NIWA Albers\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {10. {The} {Fiji} {Earthquake} {Data}},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/10-mapping_quakes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 10. The Fiji Earthquake Data. https://tangledbank.netlify.app/BCB744/intro_r/10-mapping_quakes.html."
  },
  {
    "objectID": "BCB744/intro_r/17-dates.html",
    "href": "BCB744/intro_r/17-dates.html",
    "title": "17. Dates",
    "section": "",
    "text": "This script covers some of the more common issues we may face while dealing with dates.\n\n\nDates\n\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(zoo)\n\n# Load data\nsad_dates &lt;- read.csv(\"../../data/sad_dates.csv\")\n\n\n1 Date details\nLook at strip time format for guidance\n\n?strptime\n\nCheck the local time zone\n\nSys.timezone(location = TRUE)\n\nR&gt; [1] \"Africa/Johannesburg\"\n\n\n\n2 Creating daily dates\nCreate date columns out of the mangled date data we have loaded.\n\n# Create good date column\nnew_dates &lt;- sad_dates %&gt;%\n  mutate(new_good = as.Date(good))\n\n# Correct bad date column\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_bad = as.Date(bad, format = \"%m/%d/%y\"))\n\n# Correct ugly date column\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_ugly = seq(as.Date(\"1998-01-13\"), as.Date(\"1998-01-21\"), by = \"day\"))\n\n\n3 Creating hourly dates\nIf we want to create date values out of data that have hourly values (or smaller), we must create ‘POSIXct’ valus because ‘Date’ values may not have a finer temporal resolution than one day.\n\n# Correcting good time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_good_hours = as.POSIXct(good_hours, tz = \"Africa/Mbabane\"))\n\n# Correcting bad time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_bad_hours = as.POSIXct(bad_hours, format = \"%Y-%m-%d %I:%M:%S %p\", tz = \"Africa/Mbabane\"))\n\n# Correcting bad time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_ugly_hours = seq(as.POSIXct(\"1998-01-13 09:00:00\", tz = \"Africa/Mbabane\"),\n                              as.POSIXct(\"1998-01-13 17:00:00\", tz = \"Africa/Mbabane\"), by = \"hour\"))\n\nBut shouldn’t there be a function that loads dates correctly?\n\n4 Importing dates in one step\nWhy yes, yes there is. read_csv() is the way to go.\n\nsmart_dates &lt;- read_csv(\"../../data/sad_dates.csv\")\n\nBut why does it matter that we correct the values to dates? For starters, it affects the way our plots look/work. Let’s create some random numbers for plotting and see how these compare against our date values when we create figures.\n\n# Generate random number\nsmart_dates$numbers &lt;- rnorm(9, 2, 10)\n\n# Scatterplot with correct dates\nggplot(smart_dates, aes(x = good, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n# Scatterplot with incorrect dates\nggplot(smart_dates, aes(x = bad, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n# OR\nggplot(smart_dates, aes(x = ugly, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\nIf the dates are formatted correctly it also allows us to do schnazy things with the data.\n\nsmart_dates$good[4]+32\n\nR&gt; [1] \"1998-02-17\"\n\nsmart_dates$good[9]-smart_dates$good[3]\n\nR&gt; Time difference of 6 days\n\nas.Date(smart_dates$good[9]:smart_dates$good[3])\n\nR&gt; [1] \"1998-01-21\" \"1998-01-20\" \"1998-01-19\" \"1998-01-18\" \"1998-01-17\"\nR&gt; [6] \"1998-01-16\" \"1998-01-15\"\n\nsmart_dates$good[9]-10247\n\nR&gt; [1] \"1970-01-01\"\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {17. {Dates}},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/17-dates.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 17. Dates. https://tangledbank.netlify.app/BCB744/intro_r/17-dates.html."
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html",
    "href": "BCB744/intro_r/13-tidiest.html",
    "title": "13. Tidiest data",
    "section": "",
    "text": "“Conducting data analysis is like drinking a fine wine. It is important to swirl and sniff the wine, to unpack the complex bouquet and to appreciate the experience. Gulping the wine doesn’t work.”\n— Daniel B. Wright\nIn the previous session we covered the five main transformation functions one would use in a typical tidy workflow. But to really unlock their power we need to learn how to use them with group_by(). This is how we may calculate statistics based on the different grouping variables within our data, such as sites or species or soil types, for example. Let’s begin by loading the tidyverse package and the SACTN data if we haven’t already.\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# load the data from a .RData file\nload(\"../../data/SACTNmonthly_v4.0.RData\")\n\n# Copy the data as a dataframe with a shorter name\nSACTN &lt;- SACTNmonthly_v4.0\n\n# Remove the original\nrm(SACTNmonthly_v4.0)"
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html#grouping-by-multiple-variables",
    "href": "BCB744/intro_r/13-tidiest.html#grouping-by-multiple-variables",
    "title": "13. Tidiest data",
    "section": "\n1.1 Grouping by multiple variables",
    "text": "1.1 Grouping by multiple variables\nAs one may have guessed by now, grouping is not confined to a single column. One may use any number of columns to perform elaborate grouping measures. Let’s look at some ways of doing this with the SACTN data.\n\n# Create groupings based on temperatures and depth\nSACTN_temp_group &lt;- SACTN %&gt;% \n  group_by(round(temp), depth)\n\n# Create groupings based on source and date\nSACTN_src_group &lt;- SACTN %&gt;% \n  group_by(src, date)\n\n# Create groupings based on date and depth\nSACTN_date_group &lt;- SACTN %&gt;% \n  group_by(date, depth)\n\nNow that we’ve created some grouped dataframes, let’s think of some ways to summarise these data."
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html#ungrouping",
    "href": "BCB744/intro_r/13-tidiest.html#ungrouping",
    "title": "13. Tidiest data",
    "section": "\n1.2 Ungrouping",
    "text": "1.2 Ungrouping\nOnce we level up our tidyverse skills we will routinely be grouping variables while calculating statistics. This then poses the problem of losing track of which dataframes are grouped and which aren’t. Happily, to remove any grouping we just use ungroup(). No arguments required, just the empty function by itself. Too easy.\n\nSACTN_ungroup &lt;- SACTN_date_group %&gt;% \n  ungroup()"
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html#rename-variables-columns-with-rename",
    "href": "BCB744/intro_r/13-tidiest.html#rename-variables-columns-with-rename",
    "title": "13. Tidiest data",
    "section": "\n6.1 Rename variables (columns) with rename()\n",
    "text": "6.1 Rename variables (columns) with rename()\n\nWe have seen that we select columns in a dataframe with select(), but if we want to rename columns we have to use, you guessed it, rename(). This functions works by first telling R the new name you would like, and then the existing name of the column to be changed. This is perhaps a bit back to front, but such is life on occasion.\n\nSACTN %&gt;% \n  rename(source = src)\n\n\n\n\n\n\nsite\nsource\ndate\ntemp\ndepth\ntype\n\n\n\nPort Nolloth\nDEA\n1991-02-01\n11.47029\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-03-01\n11.99409\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-04-01\n11.95556\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-05-01\n11.86183\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-06-01\n12.20722\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-07-01\n12.53810\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-08-01\n11.25202\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-09-01\n11.29208\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-10-01\n11.37661\n5\nUTR\n\n\nPort Nolloth\nDEA\n1991-11-01\n10.98208\n5\nUTR"
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html#create-a-new-dataframe-for-a-newly-created-variable-column-with-transmute",
    "href": "BCB744/intro_r/13-tidiest.html#create-a-new-dataframe-for-a-newly-created-variable-column-with-transmute",
    "title": "13. Tidiest data",
    "section": "\n6.2 Create a new dataframe for a newly created variable (column) with transmute()\n",
    "text": "6.2 Create a new dataframe for a newly created variable (column) with transmute()\n\nIf for whatever reason one wanted to create a new variable (column), as one would do with mutate(), but one does not want to keep the dataframe from which the new column was created, the function to use is transmute().\n\nSACTN %&gt;% \n  transmute(kelvin = temp + 273.15)\n\n\n\nR&gt;  [1] 284.6203 285.1441 285.1056 285.0118 285.3572 285.6881 284.4020 284.4421\nR&gt;  [9] 284.5266 284.1321\n\n\nThis makes a bit more sense when paired with group_by() as it will pull over the grouping variables into the new dataframe. Note that when it does this for us automatically it will provide a message in the console.\n\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  transmute(kelvin = temp + 273.15)\n\n\n\n\n\n\nsite\nsrc\nkelvin\n\n\n\nPort Nolloth\nDEA\n284.6203\n\n\nPort Nolloth\nDEA\n285.1441\n\n\nPort Nolloth\nDEA\n285.1056\n\n\nPort Nolloth\nDEA\n285.0118\n\n\nPort Nolloth\nDEA\n285.3572\n\n\nPort Nolloth\nDEA\n285.6881\n\n\nPort Nolloth\nDEA\n284.4020\n\n\nPort Nolloth\nDEA\n284.4421\n\n\nPort Nolloth\nDEA\n284.5266\n\n\nPort Nolloth\nDEA\n284.1321"
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html#count-observations-rows-with-n",
    "href": "BCB744/intro_r/13-tidiest.html#count-observations-rows-with-n",
    "title": "13. Tidiest data",
    "section": "\n6.3 Count observations (rows) with n()\n",
    "text": "6.3 Count observations (rows) with n()\n\nWe have already seen this function sneak it’s way into a few of the code chunks in the previous session. We use n() to count any grouped variable automatically. It is not able to be given any arguments, so we must organise our dataframe in order to satisfy it’s needs. It is the diva function of the tidyverse; however, it is terribly useful as we usually want to know how many observations our summary stats are based. First we will run some stats and create a figure without documenting n. Then we will include n and see how that changes our conclusions.\n\n SACTN_n &lt;- SACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %&gt;% \n  arrange(mean_temp) %&gt;% \n  ungroup() %&gt;% \n  select(mean_temp) %&gt;% \n  unique()\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point() +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\n\n\nDot plot showing range of mean temperatures for the time series in the SACTN dataset.\n\n\n\nThis looks like a pretty linear distribution of temperatures within the SACTN dataset. But now let’s change the size of the dots to show how frequently each of these mean temperatures is occurring.\n\n SACTN_n &lt;- SACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %&gt;% \n  ungroup() %&gt;% \n  select(mean_temp) %&gt;% \n  group_by(mean_temp) %&gt;% \n  summarise(count = n())\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point(aes(size = count)) +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\n\n\nDot plot showing range of mean temperatures for the time series in the SACTN dataset with the size of each dote showing the number of occurences of each mean.\n\n\n\nWe see now when we include the count (n) of the different mean temperatures that this distribution is not so even. There appear to be humps around 17°C and 22°C. Of course, we’ve created dot plots here just to illustrate this point. In reality if one were interested in a distribution like this one would use a histogram, or better yet, a density polygon.\n\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))\n            ) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(x = mean_temp)) +\n  geom_density(fill = \"seagreen\", alpha = 0.6) +\n  labs(x = \"Temperature (°C)\")\n\n\n\nFrequency distribution of mean temperature for each time series in the SACTN dataset."
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html#select-observations-rows-by-number-with-slice",
    "href": "BCB744/intro_r/13-tidiest.html#select-observations-rows-by-number-with-slice",
    "title": "13. Tidiest data",
    "section": "\n6.4 Select observations (rows) by number with slice()\n",
    "text": "6.4 Select observations (rows) by number with slice()\n\nIf one wants to select only specific rows of a dataframe, rather than using some variable like we do for filter(), we use slice(). The function expects us to provide it with a series of integers as seen in the following code chunk. Try playing around with these values and see what happens\n\n# Slice a seqeunce of rows\nSACTN %&gt;% \n  slice(10010:10020)\n\n# Slice specific rows\nSACTN %&gt;%\n  slice(c(1,8,19,24,3,400))\n\n# Slice all rows except these\nSACTN %&gt;% \n  slice(-(c(1,8,4)))\n\n# Slice all rows except a sequence\nSACTN %&gt;% \n  slice(-(1:1000))\n\nIt is discouraged to use slice to remove or select specific rows of data as this does not discriminate against any possible future changes in ones data. Meaning that if at some point in the future new data are added to a dataset, re-running this code will likely no longer be selecting the correct rows. This is why filter() is a main function, and slice() is not. This auxiliary function can however still be quite useful when combined with arrange.\n\n# The top 5 variable sites as measured by SD\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(sd_temp = sd(temp, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(sd_temp)) %&gt;% \n  slice(1:5)\n\n\n\n\nsite\nsrc\nsd_temp\n\n\n\nMuizenberg\nSAWS\n2.759564\n\n\nStilbaai\nSAWS\n2.721391\n\n\nMossel Bay\nSAWS\n2.647028\n\n\nDe Hoop\nDAFF\n2.514565\n\n\nMossel Bay\nDEA\n2.505965"
  },
  {
    "objectID": "BCB744/intro_r/13-tidiest.html#summary-functions",
    "href": "BCB744/intro_r/13-tidiest.html#summary-functions",
    "title": "13. Tidiest data",
    "section": "\n6.5 Summary functions",
    "text": "6.5 Summary functions\nThere is a near endless sea of possibilities when one starts to become comfortable with writing R code. We have seen several summary functions used thus far. Mostly in straightforward ways. But that is one of the fun things about R, the only limits to what we may create are within our mind, not the program. Here is just one example of a creative way to answer a straightforward question: ‘What is the proportion of recordings above 15°C per source?’. Note how we may refer to columns we have created within the same chunk. There is no need to save the intermediate dataframes if we choose not to.\n\nSACTN %&gt;% \n  na.omit() %&gt;% \n  group_by(src) %&gt;%\n  summarise(count = n(), \n            count_15 = sum(temp &gt; 15)) %&gt;% \n  mutate(prop_15 = count_15/count) %&gt;% \n  arrange(prop_15)\n\n\n\n\nsrc\ncount\ncount_15\nprop_15\n\n\n\nDAFF\n641\n246\n0.3837754\n\n\nSAWS\n8636\n4882\n0.5653080\n\n\nUWC\n12\n7\n0.5833333\n\n\nDEA\n2087\n1388\n0.6650695\n\n\nSAEON\n596\n573\n0.9614094\n\n\nEKZNW\n369\n369\n1.0000000\n\n\nKZNSB\n15313\n15313\n1.0000000"
  },
  {
    "objectID": "BCB744/intro_r/08-mapping_style.html",
    "href": "BCB744/intro_r/08-mapping_style.html",
    "title": "8. Mapping with style",
    "section": "",
    "text": "“How beautiful the world was when one looked at it without searching, just looked, simply and innocently.”\n— Hermann Hesse, Siddartha\n\n\n“You can’t judge a book by it’s cover but you can sure sell a bunch of books if you have a good one.”\n— Jayce O’Neal\n\nNow that we have learned the basics of creating a beautiful map in ggplot2 it is time to look at some of the more particular things we will need to make our maps extra stylish. There are also a few more things we need to learn how to do before our maps can be truly publication quality.\nIf we have not yet loaded the tidyverse let’s do so.\nTHIS PAGE IS NOT DSPLAYING THE OUTPUT OF CODE AT PRESENT DUE TO AN UPDATE OF THE SF PACKAGE THAT CAUSED IT TO BREAK. I WILL ENABLE THE OUTPUT AGAIN ONCE THE AUTHORS HAVE CORRECTED THEIR CODE\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggsn)\n\n# Load Africa map\nload(\"../../data/africa_map.RData\")\n\n\n1 Default maps\nIn order to access the default maps included with the tidyverse we will use the function borders().\n\nggplot() +\n  borders(col = \"black\", fill = \"cornsilk\", size = 0.2) + # The global shape file\n  coord_equal() # Equal sizing for lon/lat \n\n\n\nThe built in global shape file.\n\n\n\nJikes! It’s as simple as that to load a map of the whole planet. Usually we are not going to want to make a map of the entire planet, so let’s see how to focus on just the area around South Africa.\n\nsa_1 &lt;- ggplot() +\n  borders(size = 0.2, fill = \"cornsilk\", colour = \"black\") +\n  coord_equal(xlim = c(12, 36), ylim = c(-38, -22), expand = 0) # Force lon/lat extent\nsa_1\n\n\n\nA better way to get the map of South Africa.\n\n\n\nThat is a very tidy looking map of South(ern) Africa without needing to load any files.\n\n2 Specific labels\nA map is almost always going to need some labels and other visual cues. We saw in the previous section how to add site labels. The following code chunk shows how this differs if we want to add just one label at a time. This can be useful if each label needs to be different from all other labels for whatever reason. We may also see that the text labels we are creating have \\n in them. When R sees these two characters together like this it reads this as an instruction to return down a line. Let’s run the code to make sure we see what this means.\n\nsa_2 &lt;- sa_1 +\n  annotate(\"text\", label = \"Atlantic\\nOcean\", \n           x = 15.1, y = -32.0, \n           size = 5.0, \n           angle = 30, \n           colour = \"navy\") +\n  annotate(\"text\", label = \"Indian\\nOcean\", \n           x = 33.2, y = -34.2, \n           size = 5.0, \n           angle = 330, \n           colour = \"red4\")\nsa_2\n\n\n\nMap of southern Africa with specific labels.\n\n\n\n\n3 Scale bars\nWith our fancy labels added, let’s insert a scale bar next. There is no default scale bar function in the tidyverse, which is why we have loaded the ggsn package. This package is devoted to adding scale bars and North arrows to ggplot2 figures. There are heaps of options so we’ll just focus on one of them for now. It is a bit finicky so to get it looking exactly how we want it requires some guessing and checking. Please feel free to play around with the coordinates below. We may see the list of available North arrow shapes by running northSymbols().\n\nsa_3 &lt;- sa_2 +\n  scalebar(x.min = 22, x.max = 26, y.min = -36, y.max = -35, # Set location of bar\n           dist = 200, dist_unit = \"km\", height = 0.3, st.dist = 0.8, st.size = 4, # Set particulars\n           transform = TRUE, border.size = 0.2, model = \"WGS84\") + # Set appearance\n  north(x.min = 22.5, x.max = 25.5, y.min = -33, y.max = -31, # Set location of symbol\n        scale = 1.2, symbol = 16)\nsa_3\n\n\n\nMap of southern Africa with labels and a scale bar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Session info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\nR&gt;      ggsn    scales lubridate   forcats   stringr     dplyr     purrr     readr \nR&gt;   \"0.5.0\"   \"1.2.1\"   \"1.9.2\"   \"1.0.0\"   \"1.5.0\"   \"1.1.1\"   \"1.0.1\"   \"2.1.4\" \nR&gt;     tidyr    tibble   ggplot2 tidyverse \nR&gt;   \"1.3.0\"   \"3.2.1\"   \"3.4.1\"   \"2.0.0\"\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {8. {Mapping} with Style},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/08-mapping_style.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 8. Mapping with style. https://tangledbank.netlify.app/BCB744/intro_r/08-mapping_style.html."
  },
  {
    "objectID": "BCB744/intro_r/09-mapping_rnaturalearth.html",
    "href": "BCB744/intro_r/09-mapping_rnaturalearth.html",
    "title": "9. Mapping with Natural Earth and the sf package",
    "section": "",
    "text": "“The only thing Google has failed to do, so far, is fail.”\n— John Battelle\n\n\n“I’m afraid that if you look at a thing long enough, it loses all of its meaning.”\n— Andy Warhol\n\n\n1 Web resources about R for Spatial Applications\nNow that we are upgrading to better, more powerful maps, you’ll need to refer to industrial-strength documentation for detailed help. Please refer to links below for information about the vast array of functions available for spatial computations and graphics.\n\n\n\n\n\n\nWeb resources about spatial methods in R\n\n\n\n\n\nAUTHOR\nTITLE\n\n\n\nSpatial R\n\n\n\nEdzer Pebesma\nSimple Features for R\n\n\nEdzer Pebesma, Roger Bivand\nSpatial Data Science with applications in R\n\n\nRobin Lovelace et al.\nGeocomputation with R\n\n\nManuel Gimond\nIntro to GIS and Spatial Analysis\n\n\nWasser et al.\nIntroduction to Geospatial Raster and Vector Data with R\n\n\nTaro Mieno\nR as GIS for Economists\n\n\n\n\n\nTHIS PAGE IS NOT DSPLAYING THE OUTPUT OF CODE AT PRESENT DUE TO AN UPDATE OF THE SF PACKAGE THAT CAUSED IT TO BREAK. I WILL ENABLE THE OUTPUT AGAIN ONCE THE AUTHORS HAVE CORRECTED THEIR CODE\n\n2 The sf package\nThe sf package in R is a package for handling and processing spatial data. In recent years it has become the de facto package to use for many mapping application, replacing older packages such as sp and including the C libraries GEOS 3, GDAL, and PROJ. It provides classes for storing and manipulating simple feature geometries, and functions for working with spatial data. ‘Simple features’ refer to a standardised way of encoding vector data, including points, lines, and polygons, that are widely used in geographic information systems (GIS).\nThe sf package was created to provide a fast and efficient way to work with vector data in R, and it is designed to integrate with other packages in the tidyverse, such as dplyr and ggplot2, allowing for seamless processing and visualisation of spatial data. The package provides a variety of functions for data import, transformation, manipulation, and analysis, making it a valuable tool for working with spatial data in R.\nIn addition to its core functionality, the sf package also provides a set of methods for converting between different data representations, such as data frames, matrices, and lists, making it a versatile tool for working with spatial data in a variety of formats.\nWhile sf works with vector data, raster data require the well-known but old raster package, or its modern replacements terra and stars. I will not work with raster data in this Chapter.\n\n3 Maps with rnaturalearth\n\nNatural Earth is a public domain map dataset that provides high-quality, general-purpose base maps for the world at various scales. It was designed to be a visually pleasing alternative to other public domain datasets, and its creators aim to provide the data in a form that is useful for a wide range of applications and to make it easy to use and integrate with other data.\nThe dataset includes a variety of geographic features, including coastlines, rivers, lakes, and political boundaries, as well as cultural features like cities, roads, and railways. The data are available in several different formats, including vector and raster, and it can be used with a variety of software, including GIS and mapping applications. Within R we can access these map layers using the rnaturalearth package.\nOne of the key benefits of Natural Earth is its public domain status, which means that anyone can use and distribute the data without restrictions or licensing fees. This makes it an ideal choice for organizations, governments, and individuals who need high-quality base maps for their projects but may not have the resources or expertise to create them from scratch. I am not convinced that students actually read this. The first person to send me a WhatsApp mentioning the phrase “Know your maps” will get a Lindt chocolate.\nIn addition to its public domain status, Natural Earth is also regularly updated with new data to ensure that the maps remain accurate and up-to-date. This makes it a valuable resource for anyone who needs reliable and up-to-date geographic data.\n\n4 Install packages and set things up\n\n# install.packages(\"rnaturalearth\", \"rnaturalearthdata\", \"sf\")\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n# for the buffer to work as I expect, swith off\n# the functions for spherical geometry:\nsf_use_s2(FALSE)\n\nFirst, I define the extent of the map region:\n\n# the full map extent:\nxmin &lt;- 12; ymin &lt;- -36.5; xmax &lt;- 40.5; ymax &lt;- -10\nxlim &lt;- c(xmin, xmax); ylim &lt;- c(ymin, ymax)\n\n# make a bounding box for cropping:\nbbox &lt;- st_bbox(c(xmin = xmin, ymin = ymin,\n  xmax = xmax, ymax = ymax))\n\n# might be useful for zooming into a smaller region (False Bay and \n# the Cape Peninsula):\nxlim_zoom &lt;- c(17.8, 19); ylim_zoom &lt;- c(-34.5, -33.2)\n\n\n5 Load the data and make maps\n\n# load the countries:\nsafrica_countries &lt;- ne_countries(returnclass = 'sf',\n  continent = \"Africa\",\n  country = c(\"South Africa\", \"Mozambique\",\n    \"Namibia\", \"Zimbabwe\", \"Botswana\",\n    \"Lesotho\", \"Eswatini\"),\n  scale = \"large\")\n\nLet us see what is inside the safrica_countries object:\n\nclass(safrica_countries)\n\nR&gt; [1] \"sf\"         \"data.frame\"\n\n# safrica_countries\n\nAs you can see, it is a data.frame and tbl (tibble), amongst other classes, and so we can apply many of the tidyverse functions to it, including select(), filter(), summarise() and so on. The class() argument additionally indicates that it has some simple features properties, so some functions provided by the sf package also becomes available to use. We can see some of these functions in action, below.\n\n\n\n\n\n\nThe sf class\n\n\n\nsf indicates that the object is of class simple features. In sf language, what would be called columns (variables) in normal tidyverse speak becomes known as attributes—these are the properties of the map features, with the features being the types of geometrical representations of geographical objects.\n\n\nLet us plot the entire safrica_countries object to see all the attributes of all of the features. This kind of figure a called a choropleth map:\n\nplot(safrica_countries)\n\n\n\n\nWe probably don’t want to plot all of them. Let us select one:\n\nplot(safrica_countries[\"sovereignt\"])\n\n\n\n\nWe might achieve the same in a more familiar way:\n\nsafrica_countries |&gt; \n  select(sovereignt) |&gt; \n  plot()\n\n\n\n\nOr we may want to plot the estimate of the population size, which is contained in the attribute pop_est:\n\nsafrica_countries |&gt; \n  select(pop_est) |&gt; \n  plot()\n\n\n\n\nThe names of the countries are in the rows down the safrica_countries object, and so they become accessible with filter(). Let us only plot some attribute for South Africa:\n\nsafrica_countries |&gt; \n  dplyr::filter(sovereignt == \"South Africa\") |&gt; \n  select(sovereignt) |&gt; \n  plot()\n\n\n\n\nWe can continue to add additional operations to create a new map:\n\nsafrica_countries_new &lt;- safrica_countries |&gt; \n  group_by(continent) |&gt; \n  summarise() |&gt; \n  st_crop(bbox) |&gt;\n  st_combine()\n\nplot(safrica_countries_new)\n\n\n\n\n\n\n\n\n\n\nTask F\n\n\n\n\nPlease explain in English what the code above does, line by line.\n\n\n\nSo far we have relied on the base R plot function made for the simple features. We can also plot the map in ggplot using a more familiar and more customisable interface:\n\nggplot() +\n  geom_sf(data = safrica_countries,\n    colour = \"indianred\", fill = \"beige\") +\n  coord_sf(xlim = xlim,\n           ylim = ylim)\n\n\n\n\nNow we can layer another feature:\n\nbuffer &lt;- safrica_countries_new %&gt;%\n  st_buffer(0.4)\n\nggplot() +\n  geom_sf(data = buffer, fill = \"lightblue\", col = \"transparent\") +\n  geom_sf(data = safrica_countries, colour = \"indianred\", fill = \"beige\") +\n  theme_minimal()\n\n\n\n\n\n6 Example\nHere are examples that use the built-in Fiji earthquake data or the Kaggle earthquake data.\n\n\n\n\n\n\nTask F\n\n\n\nWhat’s going on in the above figure?\n\nWhy are we now again plotting the full, original map extent to the south?\nWhat does st_buffer(0.4) do?\nWith the above map, zoom into False Bay and the Cape Peninsula.\nCreate your own map of your favourite country (not South Africa).\n\n\nMake sure that the country is displayed inside of the continent where it is located, and ensure that the border of your country stands out from the surrounding countries.\nIndicate on the map using points the names of three places you want to visit there.\n\nCreate the most beautiful graphs you can.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nSuccessfully completing on of the options available in this task will earn you a bonus of up to 8 or 10% onto your CA mark.\nYou have until 31 March 2023 to complete it.\n\nA map that is worthy of display will become a large format poster to display in the BCB Department. Your name displayed next to it will immortalise you for continued fame and glory amongst future BCB students.\nThe winner of each category of map (hypometric and non-hypsometric) will also get a box of Lindt chocolate.\n\nOption 1 [up to 10% bonus]: Create a hypsometric map based on these examples\n\nThe maps show the locations of linefish catches along the SA coast as per a DFFE dataset. I do not expect that you add these data points as you don’t have access to this dataset. However, the location of the 58 coastal sections indicated by circles can be plotted using the data provided here. You are also welcome to create a map of any topographically-interesting region on Earth, but be sure to include a few data points of some kind to draw our attention to some interesting features or statistics. Be creative!\nSince I think a few of you might actually accomplish this, best add a few improvements to it to make your map even better than mine and stand out from that of your peers. There can be only one winner in each category, and the best one wins (although everyone can benefit from the bonus marks).\nWarning: You’ll need a fairly beefy computer to accomplish this task.\nOption 2 [up to 8% bonus]: Create an artistic map of your choice\nAlternatively, if you cannot access a powerful computer, for a bonus of up to 8% onto your CAM, create any (non-hypsometric) map of your choice of any region on Earth. Make something that you would be proud to display as a large format poster. The map may draw attention to an interesting regional geophysical, ecological, or socio-ecological (etc.) phenomena, or it may simply showcase your unique (but tasteful!) artistic ability. Show me some examples of what you wish to create before you start to avoid wasting your time on something too simple or entirely tasteless. There are many examples of beautiful maps on the internet that you may use as source of inspiration.\nWhichever option you choose, please also submit your code together with the final product in a well-described Quarto .html document. Explain each step of the way and describe the rationale for the approach you take.\nGood luck!\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2021,\n  author = {Smit, AJ},\n  title = {9. {Mapping} with {Natural} {Earth} and the **Sf** Package},\n  date = {2021-01-01},\n  url = {https://tangledbank.netlify.app/BCB744/intro_r/09-mapping_rnaturalearth.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2021) 9. Mapping with Natural Earth and the **sf** package. https://tangledbank.netlify.app/BCB744/intro_r/09-mapping_rnaturalearth.html."
  },
  {
    "objectID": "BCB744/intro_r/04-graphics.html",
    "href": "BCB744/intro_r/04-graphics.html",
    "title": "4. Graphics with ggplot2",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.”\n— John Tukey"
  },
  {
    "objectID": "BCB744/intro_r/04-graphics.html#geom_-the-pipe-or-and-the-sign",
    "href": "BCB744/intro_r/04-graphics.html#geom_-the-pipe-or-and-the-sign",
    "title": "4. Graphics with ggplot2",
    "section": "\n3.1 geom_*(), the pipe (%>% or |>), and the + sign",
    "text": "3.1 geom_*(), the pipe (%&gt;% or |&gt;), and the + sign\nAs part of the tidyverse (as we saw briefly on Day 1, and will go into in depth on Day 4), the ggplot2 package endeavours to use a clean, easy for humans to understand syntax that relies heavily on functions that do what they say. For example, the function geom_point() makes points on a figure. Need a line plot? geom_line() is the way to go! Need both at the same time? No problem. In ggplot2 we may seamlessly merge a nearly limitless number of objects together to create startlingly sophisticated figures. Before we go over the code below, it is very important to note the use of the + signs. This is different from the pipe symbol (|&gt; or %&gt;%) used elsewhere in the tidyverse. The + sign indicates that one set of geometric features is added to another, each building on top of what came before. In other words, we add one geometry on top of the next, and in such a way we can arrive at complex graphical representations of data. Effectively, each line of code represents one new geometric feature with its own aesthetic appearance of the figure. It is designed this way so as to make it easier for the human eye to read through the code.\n\n\n\n\n\n\n+ signs in ggplot() code\n\n\n\nOne may see below that the code naturally indents itself if the previous line ended with a + sign. This is because R knows that the top line is the parent line and the indented lines are it’s children. This is a concept that will come up again when we learn about tidying data. What we need to know now is that a block of code that has + signs, like the one below, must be run together. As long as lines of code end in +, R will assume that you want to keep adding lines of code (more geometric features). If we are not mindful of what we are doing we may tell R to do something it cannot and we will see in the console that R keeps expecting more + signs. If this happens, click inside the console window and push the esc button to cancel the chain of code you are trying to enter."
  },
  {
    "objectID": "BCB744/intro_r/04-graphics.html#aes",
    "href": "BCB744/intro_r/04-graphics.html#aes",
    "title": "4. Graphics with ggplot2",
    "section": "\n3.2 aes()\n",
    "text": "3.2 aes()\n\nAnother recurring function within the parent ggplot() function or the associated geom_*() is aes(). The aes() function in ggplot2 is used to specify the mapping between variables in a dataframe and visual properties of a plot. aes() stands for ‘aesthetic,’ which refers to the visual elements of a plot, such as colour, size, shape, etc. In ggplot2, the aesthetics of a plot are defined inside the aes() function, which is passed as an argument to the base ggplot() function or its associated geometry.\nFor example, if you have a dataframe with two variables x and y, you can create a scatterplot of x against y by calling ggplot(data, aes(x, y)) + geom_point(). The aes(x, y) function maps the variables (columns) in the dataframe to the x and y positions of the points in the scatterplot. Similarly, we can map variables in the dataframe to aesthetic properties of the geometric features, such as colour (e.g. a colour might be more internse as the magnitude of the values in a column increase), size (larger symbols for bigger values), transparency, etc."
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html",
    "href": "BCB744/intro_r/02-working-with-data.html",
    "title": "2. Working with data and code",
    "section": "",
    "text": "“The plural of anecdote is not data.”\n— Roger Brinner\nIn this Chapter we will cover:"
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#comma-separated-value-files",
    "href": "BCB744/intro_r/02-working-with-data.html#comma-separated-value-files",
    "title": "2. Working with data and code",
    "section": "\n2.1 Comma separated value files",
    "text": "2.1 Comma separated value files\nCSV stands for ‘Comma Separated Value’. A CSV file is a simple text file that stores data in a tabular format, with each row representing a record and each column representing a field of data. In a CSV file, each data value is separated by a comma (or sometimes another delimiter such as a semicolon or tab), and each row is terminated by a new line. But we should not assume that CSV files adhere to the tidy principles!\nCSV files are widely used in data analysis and can be opened and edited by most spreadsheet software, such as MS Excel and Google Sheets. Being comprised of plain text (ASCII), they are often used to import and export data between different applications or systems, as they provide a standardised format that can be easily parsed by software.\nCSV files are easy to create and use, and they have the advantage of being lightweight and easy to read and write by both humans and machines. However, they can be limited in their ability to represent complex data structures or to handle large amounts of data efficiently. Additionally, if our data contain certain kinds of special characters, this can cause problems with parsing the file correctly.\nWe will most frequently use the functions read.csv() or readr::read_csv() (and related forms) for reading in CSV data. We can write CSV files to disk with the write.csv() or readr::write_csv() commands. For very large datasets that might take a long time to read in or save, data.table::fread() or data.table::fwrite() are faster alternatives to the aforementioned base R or tidyverse options. Even faster options are feather::read_feather() and feather::write_feather(); although feather saves tabular data, the format is not actually an ASCII CSV, however.\n\n\n\n\n\n\nASCII files\n\n\n\nASCII stands for “American Standard Code for Information Interchange”. An ASCII file is a plain text file that contains ASCII characters. ASCII is a character encoding standard that assigns a unique numeric code to each character, including letters, numbers, punctuation, and other symbols commonly used in the English language.\nASCII files are the most basic type of text file and are supported by virtually all operating systems and applications. We can create and edit ASCII files using any text editor, such as Notepad, TextEdit, or VS Code. ASCII files are typically used for storing and sharing simple text-based information, such as program source code, configuration files, and other types of data that do not require special formatting or rich media content.\nASCII files are limited in their ability to represent non-English characters or symbols that are not included in the ASCII character set. To handle these types of characters, other character encoding standards such as UTF-8 or Unicode are used. However, ASCII files remain an important and widely used format for storing and sharing simple text-based data."
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#tab-separated-value-files",
    "href": "BCB744/intro_r/02-working-with-data.html#tab-separated-value-files",
    "title": "2. Working with data and code",
    "section": "\n2.2 Tab separated value files",
    "text": "2.2 Tab separated value files\nThe primary difference between a ‘tab-separated value’ (TSV) file and a ‘comma-separated values’ (CSV) file lies in the delimiter used to separate data fields. Both file formats are plain text ASCII files used to store data in a tabular format, but they employ different characters to distinguish individual fields within each row.\nIn a TSV file, the fields are separated by tab characters (represented as \\t in many programming languages). This format is particularly useful when dealing with data that includes commas within the values, as it avoids potential conflicts and parsing issues.\nWe already saw that in a CSV file, the fields are separated by commas. CSV files are more common and widely supported than TSV files. However, they can present difficulties when the data itself contains commas, potentially causing confusion between actual field separators and commas within the data. To mitigate this issue, values containing commas are often enclosed in quotation marks.\nLike CSV files, TSV can also be imported into and exported from spreadsheet software like Excel, or read and manipulated using programming languages like Python, R, and many others. The choice between TSV and CSV largely depends on the nature of the data and personal preferences, but it’s crucial to be aware of the delimiter used in order to accurately parse the files. The same functions that read or write CSV files in R can be used for TSV, but one has to set the arguments sep = \"\\t\" or delim = \"\\t\" for the functions read.csv() and read_csv() respectively.\n\n\n\n\n\n\nMissing values and CSV and TSV files\n\n\n\nWhere we have missing data (blanks), the CSV format separates these by commas with empty field in-between. However, there can be problems with blanks if we read in a space-delimited format file. If we are having trouble reading in missing data as blanks, try replacing them in the spreadsheet with NA, the missing data code in R. In Excel, highlight the area of the spreadsheet that includes all the cells we need to fill with NA. Do an ‘Edit/Replace…’ and leave the ‘Find what:’ textbox blank and in the ‘Replace with:’ textbox enter NA. Once imported into R, the NA values will be recognised as missing data."
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#microsoft-excel-files",
    "href": "BCB744/intro_r/02-working-with-data.html#microsoft-excel-files",
    "title": "2. Working with data and code",
    "section": "\n2.3 Microsoft Excel files",
    "text": "2.3 Microsoft Excel files\nMicrosoft Excel files are a type of file format that is used to store data in a tabular form, much like CSV files. However, Excel files are proprietary and are specifically designed to work with Excel software. Excel files can contain more advanced formatting features such as colours, fonts, and formulas, which make them a popular choice for people who like embellishments. But, as much as I dislike Excel as a software for data analysis, Excel files are definitely a good option for data entry.\nUsing MS Excel for data analysis can be a terrible idea for a number of reasons:\n\nCompatibility Excel files may not be compatible with all data science tools and programming languages. For example, R cannot read Excel files directly.\nData integrity Excel files can be prone to errors and inconsistencies in the data. For example, if a user changes a formula or formatting, it could affect the entire dataset. Also, it is possible for Excel to change the data types of certain columns, or to mix the class of data within a column, which can cause issues with data processing and analysis.\nFile size Excel files can quickly become very large when dealing with large datasets, which can lead to performance issues and storage problems.\nVersion control Excel files can make it difficult to keep track of changes and versions of the data, particularly when multiple people are working on the same file.\n\nIn contrast, CSV files are a simple, lightweight, and widely supported file format that can be easily used with most data science tools and programming languages. CSV files are also less prone to errors and inconsistencies than Excel files, making them a more reliable choice for data science tasks.\nSo, while Excel files may be useful for certain tasks such as initial data entry, they are generally not recommended for use in data science due to their potential for errors (see box “Well-known Excel errors”), incompatibility, and other issues. I recommend exporting data deliberately to CSV files. This not only avoids complications, but also allows us to unambiguously identify the data we based our analysis on. This last statement should give us the hint that it is good practice to name our .csv slightly differently each time we export it from Excel, perhaps by appending a reference to the date it was exported. Also, for those of us who use commas in Excel as the decimal separator, or to separate 1000s, undo these features now.\n\n\n\n\n\n\nWell-known Excel errors\n\n\n\nExcel is a widely used spreadsheet application, but it has been responsible for several serious errors in data analysis, science, and data science. Some of these errors include:\n\nGene name errors In 2016, researchers discovered that Excel automatically converted gene symbols to dates or floating-point numbers. For example, gene symbols like SEPT2 (Septin 2) were converted to “2-Sep” and gene symbols like MARCH1 (Membrane Associated Ring-CH-Type Finger 1) were converted to “1-Mar”. This led to errors and inconsistencies in genetic data, affecting nearly 20% of published papers in leading genomic journals.\nReinhart-Rogoff controversy In 2010, economists Carmen Reinhart and Kenneth Rogoff published a paper arguing that high levels of public debt were associated with lower economic growth. Their findings influenced policy decisions worldwide. However, in 2013, other researchers found that Reinhart and Rogoff’s results were affected by an Excel spreadsheet error that excluded some data points, causing them to overstate the relationship between debt and growth.\nLondon Whale incident In 2012, JPMorgan Chase, a leading financial institution, suffered a trading loss of over $6 billion, partially due to an Excel error. The bank’s model for calculating the risk of their trades, implemented in Excel, used incorrect formulas that significantly underestimated the risk involved. The event, which became known as the “London Whale” incident, highlighted the potential consequences of relying on Excel for complex financial models.\nTruncation of large numbers Excel can handle only a limited number of digits for large numbers, truncating any value that exceeds this limit. This truncation has lead to a loss of precision and inaccurate calculations in scientific and data analysis contexts, where exact values were important.\nIssues with floating-point arithmetic Excel uses floating-point arithmetic, which can cause rounding errors and imprecise results when working with very large or very small numbers. These inaccuracies can lead to incorrect conclusions or predictions in data analysis and scientific research.\n\nThese errors underscore the importance of using specialised tools and software, such as R or Python, for data analysis and scientific research. While Excel can be useful for basic tasks and smaller datasets, its limitations can lead to serious consequences in more complex situations."
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#rdata-files",
    "href": "BCB744/intro_r/02-working-with-data.html#rdata-files",
    "title": "2. Working with data and code",
    "section": "\n2.4 Rdata files",
    "text": "2.4 Rdata files\nRdata files are a file format used by the R programming language to store data objects. These files can contain any type of R object, such as vectors, matrices, dataframes, lists, and more. Rdata files are binary files, which means they are not human-readable like text files such as CSV files. Binary R data files have a .rda or .Rdata file extension and can be created or read using the save() and load(), respectively, functions in R.\nRdata files are convenient for a number of reasons:\n\nEfficient storage Rdata files can be more compact (they can be compressed) and efficient than other file formats, such as CSV files, because they are stored in a binary format. This means they take up less disk space and can be read and written to faster.\nEasy access to R objects Rdata files make it easy to save and load R objects, which can be useful for preserving data objects for future analysis or sharing them with others. This is especially useful for complex datasets or objects that would be difficult to recreate.\nPreserve metadata Rdata files can preserve metadata such as variable names, row and column names, and other attributes of R objects. This makes it easier to work with the data objects in the future without having to recreate this metadata.\nConvenient for reproducibility Rdata files can be used to save and load data objects as part of a reproducible research workflow. This can help ensure that data objects are preserved and can be easily accessed in the future, even if the data sources or code have changed.\n\nOn the downside, they can only be used within R, making them a less than ideal proposition when you intend sharing your data with colleagues who sadly do not use R."
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#other-binary-files",
    "href": "BCB744/intro_r/02-working-with-data.html#other-binary-files",
    "title": "2. Working with data and code",
    "section": "\n2.5 Other binary files",
    "text": "2.5 Other binary files\nAs a biostatistician, you may encounter several other binary data files in your work. Such binary data files may be software-specific and can be used to store large datasets or data objects that are not easily represented in a text format. For example, a binary data file might contain a large matrix or array of numeric data that would be difficult to store in a text file. Binary data files can also be used to store images, audio files, and other types of data that are not represented as text.\nOne common type of binary data file that you may encounter as a statistician is a SAS data file. SAS is a statistical software package that is widely used in data analysis, and SAS data files are a binary format used to store datasets in SAS. These files typically have a .sas7bdat file extension and contain metadata such as variable names and formats in addition to the data itself. Another type of binary data file you may encounter is a binary .mat data file, which is a file format used to store Matlab data.\nWhen working with binary data files, it is important to be aware of the specific format of the file and the tools and software needed to read and manipulate the data. Some statistical software packages may have built-in functions for reading and writing certain types of binary data files, while others may require additional libraries or packages."
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#netcdf-grib-and-hdf-files",
    "href": "BCB744/intro_r/02-working-with-data.html#netcdf-grib-and-hdf-files",
    "title": "2. Working with data and code",
    "section": "\n2.6 NetCDF, Grib, and HDF files",
    "text": "2.6 NetCDF, Grib, and HDF files\nNetCDF, HDF, and GRIB are all file formats commonly used in the scientific and research communities to store and share large and complex datasets. While CSV files are a simple and widely used format, they can become impractical for large datasets with complex structures or metadata. Here’s a brief overview of each file format:\n\nNetCDF (Network Common Data Form) is a binary file format that is designed for storing and sharing scientific data. It can store multidimensional arrays and metadata, such as variable names and units, in a self-describing format. NetCDF files are commonly used in fields such as atmospheric science, oceanography, and climate modelling.\nHDF (Hierarchical Data Format) is a file format that is designed to store and organise large and complex data structures. It can store a wide variety of data types, including multidimensional arrays, tables, and hierarchical data. HDF files are commonly used in fields such as remote sensing, astronomy, and engineering.\nGRIB (GRIdded Binary) is a binary file format used to store meteorological and oceanographic data. It can store gridded data, such as atmospheric or oceanic model output, in a compact and efficient binary format. GRIB files are commonly used by weather forecasting agencies and research organisations.\n\nCompared to CSV files, these file formats offer several benefits for storing and sharing complex datasets:\n\nSupport for multidimensional arrays These file formats can store and handle multidimensional arrays, which cannot be represented in a CSV file.\nEfficient storage Binary file formats can be more compact and efficient than text-based formats such as CSV files, which can save disk space and make it easier to share and transfer large datasets.\nMemory use efficiency NetCDF, GRIB, and HDF files are better for memory use efficiency compared to CSV files because they can store multidimensional arrays and metadata in a compact binary format, which can save disk space and memory when working with large and complex datasets. Also, they do not have to be read into memory all at once.\nSelf-describing metadata These file formats can include metadata, such as variable names and units, which are self-describing and can be easily accessed and understood by other researchers and software.\nSupport for compression Binary file formats can support compression, which can further reduce file size and make it easier to share and transfer large datasets.\n\nThe various efficiencies mention above may be offset by them being quite challenging to work with, and as such novices might experience steep learning curves."
  },
  {
    "objectID": "BDC334/03-biodiversity1.html",
    "href": "BDC334/03-biodiversity1.html",
    "title": "3. Quantifying Biodiversity",
    "section": "",
    "text": "Biodiversity The variability among living organisms from all sources including, inter alia, terrestrial, marine and other aquatic ecosystems and the ecological complexes of which they are part; this includes diversity within species, between species and of ecosystems.\n— International Union for the Conservation of Nature (IUCN), Convention on Biological Diversity\nThe IUCN definition considers a diversity of diversity concepts. This module looks at diversity only at the species level (species diversity). However, we can also approach macroecological problems from phylogenetic and functional (and other) diversity concepts of view. Functional and phylogenetic diversity ideas will be introduced in the BDC743 module Quantitative Ecology."
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#the-south-african-seaweed-data",
    "href": "BDC334/03-biodiversity1.html#the-south-african-seaweed-data",
    "title": "3. Quantifying Biodiversity",
    "section": "\n1.1 The South African seaweed data",
    "text": "1.1 The South African seaweed data\nIn these examples, we will use the seaweed data of Smit et al. (2017). Please make sure that you read this paper. An additional file describing the background to the data is available here (Figure 1).\n\n\nFigure 1: The coastal sections and associated seawater temperature profile associated with the study by Smit et al. (2017).\n\nOne of the datasets, \\(Y\\) (in the file SeaweedSpp.csv), comprises updated distribution records of 847 macroalgal species within each of 58 × 50 km-long sections of the South African coast (Bolton and Stegenga 2002). The dataset captures ca. 90% of the known seaweed flora of South Africa, but excludes some very small and/or very rare species for which data are insufficient. The data are from verifiable literature sources and John Bolton and Rob Anderson’s collections, assembled from information collected by teams of phycologists over three decades (Bolton 1986; Stegenga et al. 1997; Bolton and Stegenga 2002; De Clerck et al. 2005). Another file, \\(E\\) (in env.csv), is a dataset of in situ coastal seawater temperatures derived from daily measurements over 40 years (Smit et al. 2013)."
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#setting-up-the-analysis-environment",
    "href": "BDC334/03-biodiversity1.html#setting-up-the-analysis-environment",
    "title": "3. Quantifying Biodiversity",
    "section": "\n1.2 Setting up the analysis environment",
    "text": "1.2 Setting up the analysis environment\nWe will use R, so first, we must find, install and load various packages. Some packages will be available on CRAN and can be accessed and installed the usual way, but you will need to download others from R Forge.\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(betapart)\nlibrary(BiodiversityR) # this package may at times be problematic to install"
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#a-look-at-the-data",
    "href": "BDC334/03-biodiversity1.html#a-look-at-the-data",
    "title": "3. Quantifying Biodiversity",
    "section": "\n1.3 A look at the data",
    "text": "1.3 A look at the data\nLet’s load the data and see how it is structured:\n\nspp &lt;- read.csv('../data/seaweed/SeaweedSpp.csv')\nspp &lt;- dplyr::select(spp, -1)\n\n# Lets look at the data:\ndim(spp)\n\n[1]  58 847\n\n\nWe see that our dataset has 58 rows and 847 columns. What is in the columns and rows? Start with the first five rows and five columns:\n\nspp[1:5, 1:5]\n\n  ACECAL ACEMOE ACRVIR AROSP1 ANAWRI\n1      0      0      0      0      0\n2      0      0      0      0      0\n3      0      0      0      0      0\n4      0      0      0      0      0\n5      0      0      0      0      0\n\n\nNow the last five rows and five columns:\n\nspp[(nrow(spp) - 5):nrow(spp), (ncol(spp) - 5):ncol(spp)]\n\n   WOMKWA WOMPAC WRAARG WRAPUR WURMIN ZONSEM\n53      0      0      1      0      0      0\n54      0      0      1      0      0      0\n55      0      0      1      0      0      0\n56      0      1      1      0      1      0\n57      1      0      1      0      1      0\n58      0      0      1      0      1      0\n\n\nSo, each row corresponds to a site (i.e. each of the coastal sections), and each column contains a species. We arrange the species alphabetically and use a six-letter code to identify them."
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#alpha-diversity",
    "href": "BDC334/03-biodiversity1.html#alpha-diversity",
    "title": "3. Quantifying Biodiversity",
    "section": "\n3.1 Alpha diversity",
    "text": "3.1 Alpha diversity\nWe can represent \\(\\alpha\\)-diversity in three ways:\n\nas species richness, \\(S\\);\nas a univariate diversity index, such as the \\(\\alpha\\) parameter of Fisher’s log-series, Shannon diversity, \\(H'\\), or Simpson’s diversity, \\(\\lambda\\);\nSpecies evenness, e.g. Pielou’s evenness, \\(J\\); or\nas a pairwise dissimilarity index, e.g. Bray-Curtis, Jaccard, or Sørensen dissimilarities—see Koleff et al. (2003) for many more; also see ?vegdist. However, discussing pairwise dissimilarities with \\(\\beta\\)-diversity makes more sense.\n\nWe will work through each in turn.\n\n3.1.1 Species richness, \\(S\\)\n\nFirst, is species richness, which we denote by the symbol \\(S\\). In the seaweed biodiversity data—because we view each coastal section as the local scale (the smallest unit of sampling)—I count the number of species within each of the sections.\nThe preferred option for calculating species richness is the specnumber() function in vegan:\n\nspecnumber(spp, MARGIN = 1)\n\n [1] 138 139 139 140 143 143 143 145 149 148 159 162 208 147 168 204 269 276 280\n[20] 265 265 283 269 279 281 295 290 290 299 295 311 317 298 299 301 315 308 327\n[39] 340 315 315 302 311 280 300 282 283 321 319 319 330 293 291 292 294 313 333\n[58] 316\n\n\nThe data output is easier to understand if we display it as a tibble():\n\n# Use 'MARGIN = 1' to calculate the number of species within each row (site)\nspp_richness &lt;- tibble(section = 1:58,\n                       richness = specnumber(spp, MARGIN = 1))\nhead(spp_richness)\n\n# A tibble: 6 × 2\n  section richness\n    &lt;int&gt;    &lt;int&gt;\n1       1      138\n2       2      139\n3       3      139\n4       4      140\n5       5      143\n6       6      143\n\n\nThe diversityresult() function in the BiodiversityR package can do the same (sometimes this package is difficult to install due to various software dependencies that might be required for the package to load properly— do not be sad if this method does not work):\n\nspp_richness &lt;- diversityresult(spp, index = 'richness',\n                                method = 'each site')\n# spp_richness\n\nNow we make a plot seen in Figure 2:\n\nggplot(data = spp_richness, (aes(x = 1:58, y = richness))) +\n  geom_line(size = 1.2, colour = \"indianred\") +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Species richness\") +\n  theme_linedraw()\n\n\n\nFigure 2: The seaweed species richness, \\(S\\), within each of the coastal sections along the shore of South Africa.\n\n\n\nIn other instances, it makes more sense to calculate the mean species richness of all the sampling units (e.g. quadrats) taken inside the ecosystem of interest. You will have to decide based on your data.\nThe mean species richness is:\n\nround(mean(spp_richness$richness), 0)\n\n[1] 259\n\n\n\n3.1.2 Univariate diversity indices\nThe second way we can express \\(\\alpha\\)-diversity is to use one of the univariate diversity indices such as Fisher’s \\(\\alpha\\), Shannon’s \\(H'\\) or Simpson’s \\(\\lambda\\).\nThe choice of which indices to use should be informed by the extent to which one wants to emphasise richness or evenness. Species richness, \\(S\\), does not consider evenness as it is all about richness (obviously). Simpson’s \\(\\lambda\\) emphasises evenness a lot more. Shannon’s \\(H'\\) is somewhere in the middle.\nSimpson’s \\(\\lambda\\), or simply the Simpson index, is calculated as:\n\\[\\displaystyle \\lambda = \\sum_{i=1}^{S} p_{i}^{2}\\] where \\(S\\) is the species richness and \\(p_{i}\\) is the relative abundance of the \\(i\\)th species.\nShannon’s \\(H'\\) is sometimes called Shannon’s diversity, the Shannon-Wiener index, the Shannon-Weaver index, or the Shannon entropy. It is calculated as:\n\\[H' = -\\sum_{i=1}^{S} p_{i} \\ln p_{i}\\] where \\(p_{i}\\) is the proportion of individuals belonging to the \\(i\\)th species, and \\(S\\) is the species richness.\nFisher’s \\(\\alpha\\) estimates the \\(\\alpha\\) parameter of Fisher’s logarithmic series (see functions fisher.alpha() and fisherfit()). The estimation is possible only for actual counts (i.e. integers) of individuals, so it will not work for per cent cover, biomass, and other measures that real numbers can express. We will get to this function later under Fisher’s logarithmic series.\nExcept for Fisher’s-\\(\\alpha\\), we cannot calculate these for the seaweed data because, in order to do so, we require abundance data—the seaweed data are presence-absence only. Let us load a fictitious dataset of the diversity of three different communities of plants, with each community corresponding to a different light environment (dim, mid, and high light):\n\nlight &lt;- read.csv(\"../data/light_levels.csv\")\nlight\n\n        Site    A    B    C    D    E    F\n1  low_light 0.75 0.62 0.24 0.33 0.21 0.14\n2  mid_light 0.38 0.15 0.52 0.57 0.28 0.29\n3 high_light 0.08 0.15 0.18 0.52 0.54 0.56\n\n\nWe can see above that instead of having data with 1s and 0s for presence-absence, here we have some values that indicate the relative amounts of each of the species in the three light environments. We calculate species richness (as before), and also the Shannon and Simpson indices using vegan’s diversity() function:\n\nlight_div &lt;- tibble(\n  site = c(\"low_light\", \"mid_light\", \"high_light\"),\n  richness = specnumber(light[, 2:7], MARGIN = 1),\n  shannon = round(diversity(light[, 2:7], MARGIN = 1, index = \"shannon\"), 2),\n  simpson = round(diversity(light[, 2:7], MARGIN = 1, index = \"simpson\"), 2)\n)\nlight_div\n\n# A tibble: 3 × 4\n  site       richness shannon simpson\n  &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 low_light         6    1.62    0.78\n2 mid_light         6    1.71    0.81\n3 high_light        6    1.59    0.77\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Species evenness\nEvenness refers to the shape of a species abundance distribution.\nOne index for evenness is Pielou’s evenness, \\(J\\):\n\\[J = \\frac{H^{\\prime}} {log(S)}\\]\nwhere \\(H'\\) is Shannon’s diversity index, and \\(S\\) the number of species (i.e. \\(S\\)).\nTo calculate Pielou’s evenness index for the light data, we can do this:\n\nH &lt;- diversity(light[, 2:7], MARGIN = 1, index = \"shannon\")\n\nJ &lt;- H/log(specnumber(light[, 2:7]))\nround(J, 2)\n\n[1] 0.91 0.95 0.89\n\n\n\n3.1.4 Dissimilarity indices\nIn this Section, we will cover the dissimilarity indices, special cases of diversity indices that use pairwise comparisons between sampling units, habitats, or ecosystems. We can express both \\(\\alpha\\)- and \\(\\beta\\)-diversity as dissimilarity indices, but let us deal with \\(\\alpha\\)-diversity first.\nSpecies dissimilarities result in pairwise matrices similar to the pairwise correlation or Euclidian distance matrices we have seen in Lab 1. In Lab 2b you will have also learned how to calculate these ecological distances in R. These dissimilarity indices are multivariate and compare between sites, sections, plots, etc., and must therefore not be confused with the univariate diversity indices.\nWe use the Bray-Curtis and Jaccard indices with abundance data and the Sørensen dissimilarity with presence-absence data. The seaweed dataset is a presence-absence dataset that necessitates using the Sørensen index. The interpretation of the resulting square (number of rows = number of columns) dissimilarity matrices is the same regardless of whether we calculate it for an abundance or presence-absence dataset. The values in the matrix range from 0 to 1. A 0 means that the pair of sites we compare is identical (all species in common) but 1 means they are completely different (no species in common). In the square dissimilarity matrix, the diagonal is 0, which essentially (and obviously) means that any site is identical to itself. Elsewhere the values will range from 0 to 1. Since this is a pairwise calculation (each site compared to every other site), our seaweed dataset will contain (58 × (58 - 1))/2 = 1653 values, each one ranging from 0 to 1.\nThe first step involves the species table, \\(Y\\). First, we compute the Sørensen dissimilarity index, \\(\\beta_{\\text{sør}}\\), to compare the dissimilarity of all pairs of coastal sections using presence-absence data. The dissimilarity in species composition between two sections is calculated from three parameters, viz., b and c, which represent the number of species unique to each of the two sites, and a, the number of species in common between them. It is given by:\n\\[\\beta_\\text{sør}=\\frac{b+c}{2a+b+c}\\]\nThe vegan function vegdist() provides access to the dissimilarity indices. We calculate the Sørensen dissimilarity index:\n\nsor &lt;- vegdist(spp, binary = TRUE) # makes the lower triangle matrix\nsor_df &lt;- round(as.matrix(sor), 4)\ndim(sor_df)\n\n[1] 58 58\n\nsor_df[1:10, 1:10] # the first 10 rows and columns\n\n        1      2      3      4      5      6      7      8      9     10\n1  0.0000 0.0036 0.0036 0.0072 0.0249 0.0391 0.0391 0.0459 0.0592 0.0629\n2  0.0036 0.0000 0.0000 0.0036 0.0213 0.0355 0.0355 0.0423 0.0556 0.0592\n3  0.0036 0.0000 0.0000 0.0036 0.0213 0.0355 0.0355 0.0423 0.0556 0.0592\n4  0.0072 0.0036 0.0036 0.0000 0.0177 0.0318 0.0318 0.0386 0.0519 0.0556\n5  0.0249 0.0213 0.0213 0.0177 0.0000 0.0140 0.0140 0.0208 0.0342 0.0378\n6  0.0391 0.0355 0.0355 0.0318 0.0140 0.0000 0.0000 0.0069 0.0205 0.0241\n7  0.0391 0.0355 0.0355 0.0318 0.0140 0.0000 0.0000 0.0069 0.0205 0.0241\n8  0.0459 0.0423 0.0423 0.0386 0.0208 0.0069 0.0069 0.0000 0.0136 0.0171\n9  0.0592 0.0556 0.0556 0.0519 0.0342 0.0205 0.0205 0.0136 0.0000 0.0034\n10 0.0629 0.0592 0.0592 0.0556 0.0378 0.0241 0.0241 0.0171 0.0034 0.0000\n\n\nWhat we see above is a square dissimilarity matrix. The most important characteristics of the matrix are:\n\nwhereas the raw species data, \\(Y\\), is rectangular (number rows ≠ number columns), the dissimilarity matrix is square (number rows = number columns);\nthe diagonal is filled with 0;\nthe matrix is symmetrical—it is comprised of symetrical upper and lower triangles.\n\nCreate a data.frame suitable for plotting:\n\nsor_df &lt;- data.frame(round(as.matrix(sor), 4))\n\n\n\n\n\n\n\nLab 3\n\n\n\n(To be reviewed by BCB743 student but not for marks)\nThese questions concern matrices produced from species data using any of the indices available in vegdist():\n\nWhy is the matrix square, and what determines the number of rows/columns?\nWhat is the meaning of the diagonal?\nWhat is the meaning of the non-diagonal elements?\nReferring to the seaweed species data specifically, take the data in row 1 or column 1 and create a line graph showing these values as a function of the section number.\n\n\nAnswer:\n\n\nggplot(data = sor_df, (aes(x = 1:58, y = X1))) +\n  geom_line(size = 1.2, colour = \"indianred\") +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Total beta-diversity\") +\n  theme_linedraw()\n\n\n\n\n\nProvide a mechanistic (ecological) explanation for why this figure takes the shape that it does. Which community assembly process does this hint at?\n\n\n\nDissimilarity indices will be unpacked in more detail under contemporary views of \\(\\beta\\)-diversity."
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#gamma-diversity",
    "href": "BDC334/03-biodiversity1.html#gamma-diversity",
    "title": "3. Quantifying Biodiversity",
    "section": "\n3.2 Gamma diversity",
    "text": "3.2 Gamma diversity\nStaying with the seaweed data, \\(Y\\), let us now look at \\(\\gamma\\)-diversity—this would be the total number of species along the South African coastline in all 58 coastal sections. Since each column represents one species, and the dataset contains data collected at each of the 58 sites (the number of rows), we can do:\n\n# the number of columns gives the total number of species in this example:\nncol(spp)\n\n[1] 847\n\n\nWe can also use:\n\ndiversityresult(spp, index = 'richness', method = 'pooled')\n\n       richness\npooled      846\n\n\n\n\n\n\n\n\n\n\n\nLab 3 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nWhy is there a difference between the two?\nWhich is correct?\n\n\n\nThink before you calculate \\(\\gamma\\)-diversity for your own data as it might not be as simple as here!"
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#beta-diversity",
    "href": "BDC334/03-biodiversity1.html#beta-diversity",
    "title": "3. Quantifying Biodiversity",
    "section": "\n3.3 Beta-diversity",
    "text": "3.3 Beta-diversity\n\n3.3.1 Whittaker’s concept of \\(\\beta\\)-diversity\nThe first measure of \\(\\beta\\)-diversity comes from Whittaker (1960) and is called true \\(\\beta\\)-diversity. In this instance, divide the \\(\\gamma\\)-diversity for the region by the \\(\\alpha\\)-diversity for a specific coastal section. We can calculate it all at once for the whole dataset and make a graph (Figure 3):\n\ntrue_beta &lt;- data.frame(\n  beta = specnumber(spp, MARGIN = 1) / ncol(spp),\n  section_no = c(1:58)\n)\n# true_beta\nggplot(data = true_beta, (aes(x = section_no, y = beta))) +\n  geom_line(size = 1.2, colour = \"indianred\") +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"True beta-diversity\") +\n  theme_linedraw()\n\n\n\nFigure 3: Whittaker’s true β-diversity shown in the seaweed data.\n\n\n\nThe second measure of \\(\\beta\\)-diversity is absolute species turnover, and to calculate this, we subtract \\(\\alpha\\)-diversity for each section from the region’s \\(\\gamma\\)-diversity (Figure 4):\n\nabs_beta &lt;- data.frame(\n  beta = ncol(spp) - specnumber(spp, MARGIN = 1),\n  section_no = c(1:58)\n)\n# abs_beta\nggplot(data = abs_beta, (aes(x = section_no, y = beta))) +\n  geom_line(size = 1.2, colour = \"indianred\") +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Absolute beta-diversity\") +\n  theme_linedraw()\n\n\n\nFigure 4: Whittaker’s absolute species turnover shown in action in the seaweed data.\n\n\n\n\n3.3.2 Contemporary definitions \\(\\beta\\)-diversity\nContemporary views of \\(\\beta\\)-diversity are available by Nekola and White (1999), Baselga (2010), and Anderson et al. (2011). Nowadays, we see \\(\\beta\\)-diversity is a concept that describes how species assemblages (communities) measured within the ecosystem of interest vary from place to place, e.g. between the various transects or quadrats used to sample the ecosystem. \\(\\beta\\)-diversity results from habitat heterogeneity (along gradients or randomly). We have already seen two concepts of \\(\\beta\\)-diversity, viz. true \\(\\beta\\)-diversity and absolute species turnover—both rely on knowledge of species richness at local (a measure of \\(\\alpha\\)-diversity) and regional (\\(\\gamma\\)-diversity) scales. However, more insight into species assembly processes can be extracted when we view \\(\\beta\\)-diversity as a dissimilarity index. In this view, we will see that there are two processes by which \\(\\beta\\)-diversity might be affected (i.e. in which the patterning of communities over landscapes might arise). These offer glimpses into mechanistic influences on how ecosystems are structured.\nProcess 1: If a region comprises the species A, B, C, …, M (i.e. \\(\\gamma\\)-diversity is 13), a subset of the regional flora captured by one quadrat might be species A, D, E. In another quadrat species A, D, F might be present. \\(\\alpha\\)-diversity is three in both instances, and heterogeneity (and hence \\(\\beta\\)-diversity) results from the fact that the first quadrat has species E, but the other has species F. In other words, here, we have the same number of species in both quadrats, but only two of the species are the same. The process responsible for this form of \\(\\beta\\)-diversity is species turnover, \\(\\beta_\\text{sim}\\). Turnover refers to processes that cause communities to differ due to species being lost or gained from section to section, i.e. the species composition changes between sections without corresponding changes in \\(\\alpha\\)-diversity. The function beta() in the R package BAT calls this form of \\(\\beta\\) diversity replacement and use the symbol \\(\\beta_{repl}\\) (Cardoso et al. (2015)).\nProcess 2: Consider again species A, B, C, …, M. Now we have a quadrat with species A, B, C, D, G, H (\\(\\alpha\\)-diversity is six) but another quadrat has a subset of these species, e.g. only species A, B, G (\\(\\alpha\\)-diversity three). Here, \\(\\beta\\)-diversity is high even though the quadrats share some species, but the number of species differs among the quadrats (i.e. from place to place) due to one quadrat capturing only a subset of species present in the other. This form of \\(\\beta\\)-diversity is called nestedness-resultant \\(\\beta\\)-diversity, \\(\\beta_\\text{sne}\\), and it refers to processes that cause species to be gained or lost, and the community with the lowest \\(\\alpha\\)-diversity is a subset of the richer community. The function beta() in the R package BAT calls this form of \\(\\beta\\) diversity *richness difference** and uses the symbol \\(\\beta_{rich}\\) (Cardoso et al. (2015)).\nThe above two examples show that \\(\\beta\\)-diversity is coupled not only with the identity of the species in the quadrats but also \\(\\alpha\\)-diversity—with species richness in particular.\nWe express \\(\\beta\\)-diversity as nestedness-resultant, \\(\\beta_\\text{sne}\\), and turnover, \\(\\beta_\\text{sim}\\), components to be able to distinguish between these two processes. It allows us to make inferences about the two possible drivers of \\(\\beta\\)-diversity. Turnover refers to processes that cause communities to differ due to species being lost or gained from section to section, i.e. the species composition changes between sections without corresponding changes in \\(\\alpha\\)-diversity. The nestedness-resultant component implies processes that cause species to be gained or lost without replacement, and the community with the lowest \\(\\alpha\\)-diversity is a subset of the richer community.\nAccording to Nekola and White (1999) on p. 868, there are two causes of ecological distance decay. ‘Ecological’ is key to the first cause—it is environmental filtering results in a decrease in similarity as the distance between sites increases. We sometimes call this the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography.\nThe second cause of distance decay sensu Nekola and White (1999) involves aspects of the spatial configuration, context of the habitats, and some temporal considerations. Here, the evolutionary differences between species—specifically around those traits that affect their ability to disperse—are more at play and are the primary influences of distance decay rates that might vary between species. Let us first consider some properties of a hypothetical homogeneous landscape. The landscape creates some impediment (resistance) to the propagation of some species (hypothetically species A, B, and C) across its surface, but which are less effective in impeding others (D, E, and F). For argument’s sake, all species (A, …, F) share similar environmental tolerances to the prevailing environmental conditions, so one can argue that the niche difference model (environmental filtering) does not explain distributional patterns. Given a particular founding or disturbance event, species D, E, and F will, in a relatively shorter period, be able to become evenly distributed (relatively similar abundances everywhere) across this landscape. However, the less vagile (in terms of dispersal ability), species A, B, and C will develop a steeper gradient of decreasing species abundances away from the founding populations (resulting from, for example, adaptive radiation). They will require more time to become homogeneously dispersed across the landscape. In this regard, historical events set up striking distributional patterns that can be mistaken for gradients, which exist because insufficient time has passed to ensure complete dispersal. Studying the influence of such past events is called ‘historical biogeography.’ In reality, landscapes are seldom homogeneous in their spatial template (e.g. there are hills and valleys), and variable dispersal mechanisms and abilities will interact with this heterogeneous landscape to form interesting patterns of communities. The ecologist will have an exciting time figuring out the relative importance of actual gradients vs those that result from evolved traits that affect their dispersal ability and interact with the environment. I have not said anything about ‘neutral theories’ (but which are seen in the \\(\\beta_\\text{sne}\\) form of \\(\\beta\\)-diversity as in Smit et al. 2017), nor biological interactions that might affect community structure.\nHow do we calculate the turnover and nestedness-resultant components of \\(\\beta\\)-diversity? The betapart package (Baselga et al. 2022) comes to the rescue. We decompose the dissimilarity into the \\(\\beta_\\text{sim}\\) and \\(\\beta_\\text{sne}\\) components (Baselga 2010) using the betapart.core() and betapart.pair() functions. The outcomes of this partitioning calculation are placed into the matrices \\(Y1\\) and \\(Y2\\). These data can then be analysed further—e.g. we can apply a principal components analysis (PCA) or another multivariate analysis on \\(Y\\) to find the major patterns in the community data—we will do this in a later section.\n\n# Decompose total Sørensen dissimilarity into turnover and nestedness-resultant\n# components:\nY.core &lt;- betapart.core(spp)\nY.pair &lt;- beta.pair(Y.core, index.family = \"sor\")\n\n# Let Y1 be the turnover component (beta-sim):\nY1 &lt;- data.frame(round(as.matrix(Y.pair$beta.sim), 3))\n\n# Let Y2 be the nestedness-resultant component (beta-sne):\nY2 &lt;- data.frame(round(as.matrix(Y.pair$beta.sne), 3))\n\nA portion of the turnover component matrix:\n\nY1[1:10, 1:10]\n\n      X1    X2    X3    X4    X5    X6    X7    X8    X9   X10\n1  0.000 0.000 0.000 0.000 0.007 0.022 0.022 0.022 0.022 0.029\n2  0.000 0.000 0.000 0.000 0.007 0.022 0.022 0.022 0.022 0.029\n3  0.000 0.000 0.000 0.000 0.007 0.022 0.022 0.022 0.022 0.029\n4  0.000 0.000 0.000 0.000 0.007 0.021 0.021 0.021 0.021 0.029\n5  0.007 0.007 0.007 0.007 0.000 0.014 0.014 0.014 0.014 0.021\n6  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.007\n7  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.007\n8  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.007\n9  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.000\n10 0.029 0.029 0.029 0.029 0.021 0.007 0.007 0.007 0.000 0.000\n\n\nA portion of the nestedness-resultant matrix:\n\nY2[1:10, 1:10]\n\n      X1    X2    X3    X4    X5    X6    X7    X8    X9   X10\n1  0.000 0.004 0.004 0.007 0.018 0.017 0.017 0.024 0.037 0.034\n2  0.004 0.000 0.000 0.004 0.014 0.014 0.014 0.021 0.034 0.030\n3  0.004 0.000 0.000 0.004 0.014 0.014 0.014 0.021 0.034 0.030\n4  0.007 0.004 0.004 0.000 0.011 0.010 0.010 0.017 0.030 0.027\n5  0.018 0.014 0.014 0.011 0.000 0.000 0.000 0.007 0.020 0.017\n6  0.017 0.014 0.014 0.010 0.000 0.000 0.000 0.007 0.021 0.017\n7  0.017 0.014 0.014 0.010 0.000 0.000 0.000 0.007 0.021 0.017\n8  0.024 0.021 0.021 0.017 0.007 0.007 0.007 0.000 0.014 0.010\n9  0.037 0.034 0.034 0.030 0.020 0.021 0.021 0.014 0.000 0.003\n10 0.034 0.030 0.030 0.027 0.017 0.017 0.017 0.010 0.003 0.000\n\n\nA portion of the nestedness-resultant matrix reformatted as a tibble()2:\n\nY2_tib &lt;- as_tibble(Y2)\nhead(Y2_tib)\n\n# A tibble: 6 × 58\n     X1    X2    X3    X4    X5    X6    X7    X8    X9   X10   X11   X12   X13\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0     0.004 0.004 0.007 0.018 0.017 0.017 0.024 0.037 0.034 0.069 0.078 0.196\n2 0.004 0     0     0.004 0.014 0.014 0.014 0.021 0.034 0.03  0.065 0.074 0.193\n3 0.004 0     0     0.004 0.014 0.014 0.014 0.021 0.034 0.03  0.065 0.074 0.193\n4 0.007 0.004 0.004 0     0.011 0.01  0.01  0.017 0.03  0.027 0.062 0.071 0.19 \n5 0.018 0.014 0.014 0.011 0     0     0     0.007 0.02  0.017 0.052 0.061 0.181\n6 0.017 0.014 0.014 0.01  0     0     0     0.007 0.021 0.017 0.053 0.062 0.184\n# ℹ 45 more variables: X14 &lt;dbl&gt;, X15 &lt;dbl&gt;, X16 &lt;dbl&gt;, X17 &lt;dbl&gt;, X18 &lt;dbl&gt;,\n#   X19 &lt;dbl&gt;, X20 &lt;dbl&gt;, X21 &lt;dbl&gt;, X22 &lt;dbl&gt;, X23 &lt;dbl&gt;, X24 &lt;dbl&gt;,\n#   X25 &lt;dbl&gt;, X26 &lt;dbl&gt;, X27 &lt;dbl&gt;, X28 &lt;dbl&gt;, X29 &lt;dbl&gt;, X30 &lt;dbl&gt;,\n#   X31 &lt;dbl&gt;, X32 &lt;dbl&gt;, X33 &lt;dbl&gt;, X34 &lt;dbl&gt;, X35 &lt;dbl&gt;, X36 &lt;dbl&gt;,\n#   X37 &lt;dbl&gt;, X38 &lt;dbl&gt;, X39 &lt;dbl&gt;, X40 &lt;dbl&gt;, X41 &lt;dbl&gt;, X42 &lt;dbl&gt;,\n#   X43 &lt;dbl&gt;, X44 &lt;dbl&gt;, X45 &lt;dbl&gt;, X46 &lt;dbl&gt;, X47 &lt;dbl&gt;, X48 &lt;dbl&gt;,\n#   X49 &lt;dbl&gt;, X50 &lt;dbl&gt;, X51 &lt;dbl&gt;, X52 &lt;dbl&gt;, X53 &lt;dbl&gt;, X54 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nLab 3 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nPlot species turnover as a function of Section number, and provide a mechanistic explanation for the pattern observed.\n\n\nBased on an assessment of literature on the topic, provide a discussion of nestedness-resultant \\(\\beta\\)-diversity. Use either a marine or terrestrial example to explain this mode of structuring biodiversity (i.e. assembly of species into a community).\n\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nThe Lab 3 assignment on Species Data was discussed on Monday 15 August and is due at 07:00 on Monday 22 August 2022.\nProvide a neat and thoroughly annotated R file which can recreate all the graphs and all calculations. Written answers must be typed in the same file as comments.\nPlease label the R file as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_3.R\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named R documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%)."
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#references",
    "href": "BDC334/03-biodiversity1.html#references",
    "title": "3. Quantifying Biodiversity",
    "section": "\n3.4 References",
    "text": "3.4 References\n\n\nAnderson MJ, Crist TO, Chase JM, Vellend M, Inouye BD, Freestone AL, Sanders NJ, Cornell HV, Comita LS, Davies KF, others (2011) Navigating the multiple meanings of \\(\\beta\\) diversity: A roadmap for the practicing ecologist. Ecology Letters 14:19–28.\n\n\nBaselga A (2010) Partitioning the turnover and nestedness components of beta diversity. Global Ecology and Biogeography 19:134–143.\n\n\nBaselga A, Orme D, Villeger S, De Bortoli J, Leprieur F, Logez M (2022) betapart: Partitioning Beta Diversity into Turnover and Nestedness Components.\n\n\nBolton J (1986) Marine phytogeography of the Benguela upwelling region on the west coast of southern africa: A temperature dependent approach.\n\n\nBolton J, Stegenga H (2002) Seaweed species diversity in South Africa. South African Journal of Marine Science 24:9–18.\n\n\nCardoso P, Rigal F, Carvalho JC (2015) BAT–biodiversity assessment tools, an r package for the measurement and estimation of alpha and beta taxon, phylogenetic and functional diversity. Methods in Ecology and Evolution 6:232–236.\n\n\nDe Clerck O, Bolton J, Anderson R, Coppejans E, Bolton J, Anderson R (2005) Guide to the seaweeds of KwaZulu-Natal.\n\n\nJurasinski G, Retzer V, Beierkuhnlein C (2009) Inventory, differentiation, and proportional diversity: A consistent terminology for quantifying species diversity. Oecologia 159:15–26.\n\n\nKoleff P, Gaston KJ, Lennon JJ (2003) Measuring beta diversity for presence–absence data. Journal of Animal Ecology 72:367–382.\n\n\nNekola JC, White PS (1999) The distance decay of similarity in biogeography and ecology. Journal of Biogeography 26:867–878.\n\n\nOksanen J, Simpson GL, Blanchet FG, Kindt R, Legendre P, Minchin PR, O’Hara RB, Solymos P, Stevens MHH, Szoecs E, Wagner H, Barbour M, Bedward M, Bolker B, Borcard D, Carvalho G, Chirico M, De Caceres M, Durand S, Evangelista HBA, FitzJohn R, Friendly M, Furneaux B, Hannigan G, Hill MO, Lahti L, McGlinn D, Ouellette M-H, Ribeiro Cunha E, Smith T, Stier A, Ter Braak CJF, Weedon J (2022) vegan: Community Ecology Package.\n\n\nSmit AJ, Roberts M, Anderson RJ, Dufois F, Dudley SF, Bornman TG, Olbers J, Bolton JJ (2013) A coastal seawater temperature dataset for biogeographical studies: large biases between in situ and remotely-sensed data sets around the coast of South Africa. PLoS One 8:e81944.\n\n\nSmit AJ, Bolton JJ, Anderson RJ (2017) Seaweeds in two oceans: Beta-diversity. Frontiers in Marine Science 4:404.\n\n\nStegenga H, Bolton JJ, Anderson RJ (1997) Seaweeds of the South African west coast.\n\n\nWhittaker RH (1960) Vegetation of the Siskiyou mountains, Oregon and California. Ecological Monographs 30:279–338.\n\n\nWhittaker RH (1972) Evolution and measurement of species diversity. Taxon 21:213–251."
  },
  {
    "objectID": "BDC334/03-biodiversity1.html#footnotes",
    "href": "BDC334/03-biodiversity1.html#footnotes",
    "title": "3. Quantifying Biodiversity",
    "section": "Footnotes",
    "text": "Footnotes\n\nI am by no means an advocate for veganism.↩︎\nNote that the rows are no longer numbered in the tibble view, but it can easily be recreated by seq(1:58).↩︎"
  },
  {
    "objectID": "BDC334/02b-env_dist.html",
    "href": "BDC334/02b-env_dist.html",
    "title": "2b. Environmental Distance",
    "section": "",
    "text": "“It’s not that I’m so smart, it’s just that I stay with problems longer.”\n— Albert Einstein"
  },
  {
    "objectID": "BDC334/02b-env_dist.html#set-up-the-analysis-environment",
    "href": "BDC334/02b-env_dist.html#set-up-the-analysis-environment",
    "title": "2b. Environmental Distance",
    "section": "\n1 Set up the analysis environment",
    "text": "1 Set up the analysis environment\n\nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(geodist) # to calculate geographic distances between lats/lons\nlibrary(ggpubr) # to arrange the multipanel graphs"
  },
  {
    "objectID": "BDC334/02b-env_dist.html#revisiting-euclidian-distance",
    "href": "BDC334/02b-env_dist.html#revisiting-euclidian-distance",
    "title": "2b. Environmental Distance",
    "section": "\n2 Revisiting Euclidian distance",
    "text": "2 Revisiting Euclidian distance\nThe toy data have arbitrary columns to demonstrate the Euclidian distance calculation:\n\\[ d(a,b) = \\sqrt{(a_x - b_x)^2 + (a_y - b_y)^2 + (a_z - b_z)^2} \\]\nThe distance is found between every pair of sites named a to g whose locations are marked by the ‘coordinates’ \\(x\\), \\(y\\), and \\(z\\)—i.e. this is an example of 3-dimensional data (a space or volume, as opposed to 2D data situated on a \\(x\\), \\(y\\) place). We might also call each coordinate a ‘variable’ (sometimes called a ‘dimension’) and hence we have multivariate or multidimensional data.\nLet’s load the dataset and find the size of the dataframe:\n\nxyz &lt;- read.csv(\"../data/Euclidian_distance_demo_data_xyz.csv\")\ndim(xyz)\n\n[1] 7 4\n\n\nThere are seven rows and four columns.\nThe data look like:\n\nxyz\n\n  site x y z\n1    a 4 1 3\n2    b 5 5 5\n3    c 6 6 4\n4    d 1 4 9\n5    e 2 3 8\n6    f 8 3 1\n7    g 9 1 5\n\n\nThe first column contains the site names and it must be excluded from subsequent calculations. The remaining three columns will be used below.\nCalculate the Euclidian distance using vegan’s vegdist() function and view the lower triangle with the diagonal:\n\nxyz_euc &lt;- round(vegdist(xyz[, 2:4], method = \"euclidian\",\n                         upper = FALSE, diag = TRUE), 4)\n# selected only cols 2, 3 and 4\nxyz_euc\n\n        1       2       3       4       5       6       7\n1  0.0000                                                \n2  4.5826  0.0000                                        \n3  5.4772  1.7321  0.0000                                \n4  7.3485  5.7446  7.3485  0.0000                        \n5  5.7446  4.6904  6.4031  1.7321  0.0000                \n6  4.8990  5.3852  4.6904 10.6771  9.2195  0.0000        \n7  5.3852  5.6569  5.9161  9.4340  7.8740  4.5826  0.0000\n\n\nConvert to a dataframe and view it:\n\nxyz_df &lt;- as.data.frame(as.matrix(xyz_euc))\nxyz_df\n\n       1      2      3       4      5       6      7\n1 0.0000 4.5826 5.4772  7.3485 5.7446  4.8990 5.3852\n2 4.5826 0.0000 1.7321  5.7446 4.6904  5.3852 5.6569\n3 5.4772 1.7321 0.0000  7.3485 6.4031  4.6904 5.9161\n4 7.3485 5.7446 7.3485  0.0000 1.7321 10.6771 9.4340\n5 5.7446 4.6904 6.4031  1.7321 0.0000  9.2195 7.8740\n6 4.8990 5.3852 4.6904 10.6771 9.2195  0.0000 4.5826\n7 5.3852 5.6569 5.9161  9.4340 7.8740  4.5826 0.0000\n\n\nDistance matrices have the same properties as dissimilarity matrices, i.e.:\n\nThe distance matrix is square (number rows = number columns).\nThe diagonal is filled with 0.\nThe matrix is symmetrical—it is comprised of symmetrical upper and lower triangles.\n\nIn terms of the meaning of the cell values, their interpretation is also analogous with that of the species dissimilarities. A value of 0 means the properties of the sites (or sections, plots, transects, quadrats, etc.) in terms of their environmental conditions are identical (this is always the case the the diagonal). The larger the number (which may be &gt;1) the more different sites are in terms of their environmental conditions.\nSince each column, \\(x\\), \\(y\\), and \\(z\\), is a variable, we can substitute them for actual variables or properties of the environment within which species are present. Let’s load such data (again fictitious):\n\nenv_fict &lt;- read.csv(\"../data/Euclidian_distance_demo_data_env.csv\")\nhead(env_fict, 2) # print first two rows only\n\n  site temperature depth light\n1    a           4     1     3\n2    b           5     5     5\n\n\nThese are the same data as in Euclidian_distance_demo_data_xyz.csv but I simply renamed the columns to names of the variables temperature, depth, and light intensity. I won’t repeat the analysis here as the output remains the same.\nNow apply vegdist() as before. The resultant distances are called ‘environmental distances’.\nLet us now use some real data."
  },
  {
    "objectID": "BDC334/02b-env_dist.html#a-look-at-the-seaweed-environmental-data",
    "href": "BDC334/02b-env_dist.html#a-look-at-the-seaweed-environmental-data",
    "title": "2b. Environmental Distance",
    "section": "\n3 A look at the seaweed environmental data",
    "text": "3 A look at the seaweed environmental data\nThese data accompany the analysis of the South African seaweed flora (Smit et al. 2017).\n\nload(\"../data/seaweed/SeaweedEnv.RData\")\n\n# lets look at the data\ndim(env)\n\n[1] 58 18\n\n\nWe see that the data have 58 rows and 18 columns… the same number of rows as the seaweed.csv data. What is in the first five rows?\n\nround(env[1:5, 1:5], 4)\n\n  febMean  febMax  febMed  febX95 febRange\n1 13.0012 18.7204 12.6600 16.8097   6.0703\n2 13.3795 18.6190 13.1839 17.0724   5.8893\n3 13.3616 17.8646 13.2319 16.6111   5.4314\n4 13.2897 17.1207 13.1028 16.1214   5.0490\n5 12.8113 16.3783 12.4003 15.5324   4.9779\n\n\nAnd the last five rows?\n\nround(env[(nrow(env) - 5):nrow(env), (ncol(env) - 5):ncol(env)], 4)\n\n   annRange  febSD  augSD annChl augChl febChl\n53   4.3707 1.0423 0.7735 4.3420 4.3923 4.6902\n54   4.3358 1.1556 0.9104 1.6469 2.2654 1.6930\n55   4.4104 1.1988 0.8427 0.2325 0.6001 0.5422\n56   4.6089 1.1909 0.6631 0.1321 0.4766 0.3464\n57   4.9693 1.1429 0.4994 0.1339 0.5845 0.3185\n58   5.5743 1.0000 0.3494 0.1486 0.7363 0.4165\n\n\nSo, each of the rows corresponds to a site (i.e. each of the coastal sections), and the columns each contains an environmental variable. The names of the environmental variables are:\n\ncolnames(env)\n\n [1] \"febMean\"  \"febMax\"   \"febMed\"   \"febX95\"   \"febRange\" \"augMean\" \n [7] \"augMin\"   \"augMed\"   \"augX5\"    \"augRange\" \"annMean\"  \"annSD\"   \n[13] \"annRange\" \"febSD\"    \"augSD\"    \"annChl\"   \"augChl\"   \"febChl\"  \n\n\nAs we have seen, there are 18 variables (or dimensions). These data are truly multidimensional in a way that far exceeds our brains’ limited ability to spatially visualise. For mathematicians these data define an 18-dimensional space, but all we can do is visualise 3-dimensions.\nWe select only some of the thermal variables; the rest are collinear with some of the ones I import:\n\n  env1 &lt;- dplyr::select(env, febMean, febRange, febSD, augMean,\n                    augRange, augSD, annMean, annRange, annSD)\n\nLet us make a quick graph of annMean as a function of distance along the coast (Figure 1).\n\nggplot(env1, aes(x = 1:58, y = annMean)) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  labs(x = \"Coastal section (west to east)\",\n       y = \"Temperature (°C)\") +\n  theme_linedraw()\n\n\n\nFigure 1: Line plot showing the trend in the mean annual seawater temperature along the coast from the west at Section 1 to Section 58 in the East."
  },
  {
    "objectID": "BDC334/02b-env_dist.html#z-scores",
    "href": "BDC334/02b-env_dist.html#z-scores",
    "title": "2b. Environmental Distance",
    "section": "\n4 z-scores",
    "text": "4 z-scores\nHere we need to do something new that was not necessary with the toy data. We calculate z-scores, and the process is called ‘standardisation’. Standardisation is necessary when the variables are measured in different units—e.g. the unit for temperature is °C whereas Ch-a is measured in mg Chl-a/m3.\n\nE1 &lt;- round(decostand(env1, method = \"standardize\"), 4)\nE1[1:5, 1:5]\n\n  febMean febRange   febSD augMean augRange\n1 -1.4915  -0.0443 -0.2713 -1.3765  -0.4735\n2 -1.4014  -0.1432 -0.1084 -1.4339  -0.0700\n3 -1.4057  -0.3932 -0.1720 -1.5269   0.0248\n4 -1.4228  -0.6020 -0.3121 -1.5797  -0.0508\n5 -1.5368  -0.6408 -0.4096 -1.5464  -0.0983\n\n\nFor comparison with the previous plot showing the raw data, let us now plot the standardised annMean data (Figure 2).\n\nggplot(E1, aes(x = 1:58, y = annMean)) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  labs(x = \"Coastal section (west to east)\",\n       y = \"Standardised temperature\")+\n  theme_linedraw()\n\n\n\nFigure 2: Line plot showing the trend in the standardised mean annual seawater temperature along the coast from the west at Section 1 to Section 58 in the East."
  },
  {
    "objectID": "BDC334/02b-env_dist.html#euclidian-distance",
    "href": "BDC334/02b-env_dist.html#euclidian-distance",
    "title": "2b. Environmental Distance",
    "section": "\n5 Euclidian distance",
    "text": "5 Euclidian distance\n\nE1_euc &lt;- round(vegdist(E1, method = \"euclidian\", upper = TRUE), 4)\nE1_df &lt;- as.data.frame(as.matrix(E1_euc))\nE1_df[1:10, 1:10]\n\n        1      2      3      4      5      6      7      8      9     10\n1  0.0000 0.7040 1.0006 1.1132 0.9902 0.9124 0.7849 0.7957 2.7901 2.0327\n2  0.7040 0.0000 0.3769 0.6126 0.6553 0.7726 0.6291 0.5565 2.2733 1.7509\n3  1.0006 0.3769 0.0000 0.2818 0.4729 0.7594 0.7164 0.7939 2.2692 1.8055\n4  1.1132 0.6126 0.2818 0.0000 0.3662 0.7566 0.7911 0.9708 2.4523 1.9019\n5  0.9902 0.6553 0.4729 0.3662 0.0000 0.4094 0.5261 0.9860 2.4847 2.1376\n6  0.9124 0.7726 0.7594 0.7566 0.4094 0.0000 0.2862 1.0129 2.4449 2.3483\n7  0.7849 0.6291 0.7164 0.7911 0.5261 0.2862 0.0000 0.7678 2.3035 2.1656\n8  0.7957 0.5565 0.7939 0.9708 0.9860 1.0129 0.7678 0.0000 2.2251 1.5609\n9  2.7901 2.2733 2.2692 2.4523 2.4847 2.4449 2.3035 2.2251 0.0000 2.8476\n10 2.0327 1.7509 1.8055 1.9019 2.1376 2.3483 2.1656 1.5609 2.8476 0.0000\n\n\nWe already know how to read this matrix. Let’s plot it as a function of the coastal section’s number (Figure 3).\n\nggplot(data = E1_df, (aes(x = 1:58, y = `1`))) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Environmental distance\")+\n  theme_linedraw()\n\n\n\nFigure 3: Line plot showing the trend in environmental distance along the coast from the west at Section 1 to Section 58 in the East.\n\n\n\n\n\n\n\n\n\nLab 2\n\n\n\n(To be reviewed by BCB743 student but not for marks)\nUse the Doubs River environmental data for this exercise.\n\nStandardise these data using R and display a portion of the resultant standardised data file.\nDiscuss why standardisation was necessary for these data. Use the content of the actual ‘raw’ data file in your discussion.\nUsing R, calculate the Euclidian distances for these data and display a portion of the resultant distance matrix.\nDiscuss the ecological conclusions you are able to draw from these Euclidian distances. Provide a few graphs to substantiate your answer.\n\n\n\nWe will explore distance and dissimilarity matrices in more detail in later sections."
  },
  {
    "objectID": "BDC334/02b-env_dist.html#pairwise-correlations",
    "href": "BDC334/02b-env_dist.html#pairwise-correlations",
    "title": "2b. Environmental Distance",
    "section": "\n6 Pairwise correlations",
    "text": "6 Pairwise correlations\nIt is easy to calculate pairwise correlation matrices for the above data:\n\nenv1_cor &lt;- round(cor(env1), 2)\nenv1_cor\n\n         febMean febRange febSD augMean augRange augSD annMean annRange annSD\nfebMean     1.00    -0.27 -0.28    0.90    -0.10 -0.16    0.98     0.74  0.41\nfebRange   -0.27     1.00  0.79   -0.32     0.14  0.14   -0.29    -0.08  0.48\nfebSD      -0.28     0.79  1.00   -0.16     0.35  0.46   -0.26    -0.33  0.31\naugMean     0.90    -0.32 -0.16    1.00    -0.01 -0.05    0.96     0.37  0.13\naugRange   -0.10     0.14  0.35   -0.01     1.00  0.91   -0.10    -0.20  0.06\naugSD      -0.16     0.14  0.46   -0.05     0.91  1.00   -0.17    -0.27  0.08\nannMean     0.98    -0.29 -0.26    0.96    -0.10 -0.17    1.00     0.60  0.29\nannRange    0.74    -0.08 -0.33    0.37    -0.20 -0.27    0.60     1.00  0.68\nannSD       0.41     0.48  0.31    0.13     0.06  0.08    0.29     0.68  1.00\n\n\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nExplain in s short (1/3 page paragraph) what is meant by ‘environmental distance’.\nDescribe to your grandmother how to interpret the above correlation matrix, and also mention what the major conclusions are that can be drawn from studying the matrix. Add a mechanistic explanation to demonstrate to her what your thought processes are for reaching your conclusion.\nExplain why the same general trend is seen in the raw or standardised environmental data for annMean (Figure 1 and 2) and that of environmental distance (Figure 3)."
  },
  {
    "objectID": "BDC334/02b-env_dist.html#euclidian-distance-of-geographical-data",
    "href": "BDC334/02b-env_dist.html#euclidian-distance-of-geographical-data",
    "title": "2b. Environmental Distance",
    "section": "\n7 Euclidian distance of geographical data",
    "text": "7 Euclidian distance of geographical data\nWhen we calculate Euclidian distances between geographic lat/lon coordinate, the relationship between sections will be the same (but scaled) as actual geographic distances.\n\ngeo &lt;- read.csv(\"../data/seaweed/SeaweedSites.csv\")\ndim(geo)\n\n[1] 58  2\n\n\n\nhead(geo)\n\n   Latitude Longitude\n1 -28.98450  16.72429\n2 -29.38053  16.94238\n3 -29.83253  17.08194\n4 -30.26426  17.25928\n5 -30.67874  17.47638\n6 -31.08580  17.72167\n\n\n\nCalculate geographic distances (in meters) between coordinate pairs (Figure 4).\n\ndists &lt;- geodist(geo, paired = TRUE, measure = \"geodesic\")\ndists_df &lt;- as.data.frame(as.matrix(dists))\ncolnames(dists_df) &lt;- seq(1:58)\ndists_df[1:5, 1:5]\n\n          1         2         3         4         5\n1      0.00  48752.45 100201.82 151021.75 201380.00\n2  48752.45      0.00  51894.01 102638.03 152849.90\n3 100201.82  51894.01      0.00  50822.71 101197.22\n4 151021.75 102638.03  50822.71      0.00  50457.53\n5 201380.00 152849.90 101197.22  50457.53      0.00\n\n\n\nplt1 &lt;- ggplot(data = dists_df, (aes(x = 1:58, y = `1`/1000))) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Distance (km)\") +\n  ggtitle(\"Actual geographic distance\")+\n  theme_linedraw()\n\n\ndists_euc &lt;- vegdist(geo, method = \"euclidian\")\ndists_euc_df &lt;- round(as.data.frame(as.matrix(dists_euc)), 4)\ndists_euc_df[1:5, 1:5]\n\n       1      2      3      4      5\n1 0.0000 0.4521 0.9204 1.3871 1.8537\n2 0.4521 0.0000 0.4731 0.9388 1.4037\n3 0.9204 0.4731 0.0000 0.4667 0.9336\n4 1.3871 0.9388 0.4667 0.0000 0.4679\n5 1.8537 1.4037 0.9336 0.4679 0.0000\n\n\n\nplt2 &lt;- ggplot(data = dists_euc_df, (aes(x = 1:58, y = `1`))) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Euclidian distance\") +\n  ggtitle(\"Euclidian distance\")+\n  theme_linedraw()\n\nggarrange(plt1, plt2, ncol = 2)\n\n\n\nFigure 4: Line plots showing the relationship between Euclidian and geographical distance.\n\n\n\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nDo a full analysis of the Doubs River environmental data using Euclidian distances and correlations. Demonstrate graphically any clear spatial patterns that you might find, and offer a full suite of mechanistic explanations for the patterns you see. It is sufficient to submit a fully annotated R script (not a MS Word or Excel file).\n\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nThe Lab 2 assignment on Ecological Data was discussed on Monday 8 August and is due at 07:00 on Monday 15 August 2022.|\nProvide a neat and thoroughly annotated R file which can recreate all the graphs and all calculations. Written answers must be typed in the same file as comments.\nPlease label the R file as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_2.R\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named R documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%)."
  },
  {
    "objectID": "BDC334/02b-env_dist.html#references",
    "href": "BDC334/02b-env_dist.html#references",
    "title": "2b. Environmental Distance",
    "section": "\n8 References",
    "text": "8 References\n\n\nSmit AJ, Bolton JJ, Anderson RJ (2017) Seaweeds in two oceans: Beta-diversity. Frontiers in Marine Science 4:404."
  },
  {
    "objectID": "BDC334/01-introduction.html",
    "href": "BDC334/01-introduction.html",
    "title": "1. Ecological Data",
    "section": "",
    "text": "“A scientific man ought to have no wishes, no affections, – a mere heart of stone.”\n— Charles Darwin"
  },
  {
    "objectID": "BDC334/01-introduction.html#about-macroecology",
    "href": "BDC334/01-introduction.html#about-macroecology",
    "title": "1. Ecological Data",
    "section": "\n1 About Macroecology",
    "text": "1 About Macroecology\nThis course is about community ecology across different spatial and temporal scales. Community ecology underpins the vast fields of biodiversity and biogeography and concerns spatial scales from square meters to all of Earth. We can look at historical, contemporary, and future processes implicated in shaping the distribution of life on our planet.\nEcologists tend to analyse how multiple environmental factors act as drivers that influence the distribution of tens or hundreds of species. These data often are messy and statistical considerations need to be understood within the context of the available data.\nUp to 20 years ago, ecologists focused on populations (the dynamics of individuals of one species interacting among each other and with their environment) and communities (collections of multiple populations, how they interact with each other and their environment, and how this affects the structure and dynamics of ecosystems). This is a modern development of ecology. But ecologists have expanded their horizon regarding the questions they now seek answers for. Today, macroecology offers a broadened view of ecology. Macroecologists seek to find the geographical patterns and processes in biodiversity across all spatial scales, from local to global, across time scales from years to millennia, and across all taxonomic hierarchies (from genetic variability within species up to major higher-level taxa, such as families and orders). It attempts to arrive at a unifying theory for ecology across all of these scales—e.g. one that can explain all patterns in structure and functioning from microbes to blue whales. Perhaps most importantly, it attempts to offer mechanistic explanations for these patterns. At the heart of all ecological answers are also deep insights stemming from understanding evolution (facilitated by the growth of phylogenetic datasets—see below).\nOn a basic data analytical level, population ecology, community ecology, and macroecology all share the same approach regarding the underlying data. We start with data representing the species and the associated environmental conditions at a selection of sites (called species tables and environmental tables). The species tables are then converted to dissimilarity matrices and the environmental tables to distance matrices. From here, basic analyses can offer insights into how biodiversity is structured, e.g. species-abundance distributions, occupancy-abundance curves, species-area curves, distance decay curves, and gradient analyses (as seen in Shade et al. 2018). In the Labs, we will explore some of these properties."
  },
  {
    "objectID": "BDC334/01-introduction.html#ecological-data",
    "href": "BDC334/01-introduction.html#ecological-data",
    "title": "1. Ecological Data",
    "section": "\n2 Ecological data",
    "text": "2 Ecological data\n\n2.1 Properties of ecological datasets\nEcological data capture properties of the environment and properties of communities. They are typically stored as separate datasets, but they are analysed together.\nThese data sets are usually arranged in a matrix. In the case of community composition, a matrix has species (or higher level taxa whose resolution depends on the research question) arranged down columns and samples (typically the sites, stations, transects, time, plots, etc.) along rows. We call this a sites × species table. In the case of environmental data, a matrix is a site × environment table. The term ‘sample’ denotes the basic unit of observation. Samples on a map may be quadrats, transects, stations, locations, traps, seine net tows, trawls, grids cells, etc. It is essential to be unambiguous about the basic unit of the samples.\n\n2.2 The Doubs River data\nAn obvious example of environmental and species datasets is the Doubs River dataset. Please refer to David Zelený’s website for an explanation of these data. The primary publication outlining this study is Verneaux (1973), and an example analysis is provided by Borcard et al. (2011). These data demonstrate how one of the basic mechanisms of biodiversity patterning—gradients—can be seen operating in a real-world case study. It offers keen insight also into the properties of species and environmental tables and the dissimilarity and distance matrices derived from them.\n\n2.3 Looking at the files’ content\nThese data are available in CSV format, but we can open and view it in MS Excel. ‘CSV’ means comma separated value. It is a plain text file that can be edited in any text editor (such as Notepad on MS Windows, or VS Code, VIM, emacs, etc. on all platforms). Figure 1 shows what a CSV file looks like in a plain text editor, VS Code, on my computer. Once imported, it will look similar to the one seen in Figure 3.\n\n\nFigure 1: View of a CSV file inside VS Code.\n\n\n\n\n\n\n\nNote about CSV files and MS Excel\n\n\n\nCSV is a standard format used in the scientific disciplines as it is compatible with many software. Globally, scientists use a period ‘.’ as a decimal point separator. You can see this in the file above. Commas are used exclusively as field separators (you’ll see separate columns once opened in MS Excel).\nCSV files create a bit of a problem for South Africans, who are indoctrinated from a young age to use commas as a decimal point separators—this is to conform with the regional (South African) expectation that dictates commas be used as decimals. So, when you import a CSV file for the first time, you’ll likely see gibberish because your computer will probably be set up to honour the regional (locale) the expectation of commas as decimal points (and ‘R’ for currency, metric units of measurements, etc.). So, you need to know how to fix this to prevent upsetting me (it is a pet peeve and frustrates me endlessly) and yourselves.\nFixing this annoyance is not too tricky, as is demonstrated here. Follow the instruction under ‘Changing commas to decimals and vice versa by changing Excel Options’. Better still, change the global system settings, as the same article explains. Do this before importing the CSV file.\n\n\nAfter importing the Doubs River data, we see something that resembles the following two figures. First, in DoubsSpe.csv, we see the table (or spreadsheet) view of the species data. The species codes for 27 species of fish appear as column headers (not all species’ data are visible as the data are truncated to the right) and in rows 2 through 31 (30 rows) are each of the samples—in this case, there is one sample per site down the length of the river (Figure 2).\n\n\nFigure 2: The Doubs River species data seen in MS Excel.\n\nDoubsEnv.csv contains the environmental data, as seen in the following figure. The names of the 11 environmental variables appear as column headers, and there are 30 rows, one for each of the samples—the samples match that of the species data (Figure 3).\n\n\nFigure 3: The Doubs River environmental data in MS Excel.\n\nSpecies data may be recorded as various kinds of measurements, such as presence/absence data, biomass, frequency, or abundance. ‘Presence/absence’ of species simply tells us the species is there or is not there. It is binary. ‘Abundance’ generally refers to the the number of individuals per unit of area, volume. ‘Per cent cover’ refers to the proportion of a covered by a species. Per cent cover is used for vegetation, some encrusting species of animals (e.g. sponges), or organisms such as oysters or mussels that can be too numerous to count but whose abundance can be estimated as filling a portion of a sampling unit such as a quadrat. ‘Biomass’ refers to the species’ mass per unit of area or volume. The type of measure will depend on the taxa and the questions under consideration. The critical thing to note is that all species have to be homogeneous in terms of the metric used to quantify them (i.e. all of it as presence/absence, or abundance, or biomass, not mixtures of them). The matrix’s row vectors are the species composition for the corresponding sample. That is to say, a row runs across multiple columns, which tells us that the sample is comprised of all the species whose names are given by the column titles. Note that in the case of the data in the above figures, it is often the case that there are 0s, meaning that not all species are present at all sites. Species composition is frequently expressed in relative abundance, i.e. constrained to a constant total such as 1 or 100%, or biomass, where the upper limit might be arbitrary.\nThe environmental data may be heterogeneous, i.e. the units of measure may differ among the variables. For example, pH has no units, the concentration of some nutrients has a unit of (typically) μM, elevation may be in meters, etc. Because these units have different magnitudes and ranges, we may need to standardise them. To standardise data, we subtract the mean of each column from each data point in the column and then divide each of the resultant values by the standard deviation of the columns.\n\n\n\n\n\n\nLab 1\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nStandardise the Doubs River environmental data in MS Excel.\n\n\n\n\n2.4 Properties of species datasets\nMany community data matrices share some general characteristics:\n\nMost species occur only infrequently. The majority of species might typically be represented at only a few locations (where they might be pretty abundant). Or some species are simply rare in the sampled region (i.e. when they are present, they are present at a very low abundance). This results in sparse matrices where the bulk of the entries consists of zeros.\nEcologists tend to sample a multitude of factors that they think influence species composition, so the matching environmental data set will also have multiple (10s) columns that will be assessed in various hypotheses about the drivers of species patterning across the landscape. For example, fynbos biomass may be influenced by the fire regime, elevation, aspect, soil moisture, soil chemistry, edaphic features, etc. These datasets are called multi-dimensional matrices, with the ‘dimensions’ referring the the many species or environmental variables.\nEven though we may capture a multitude of information about many environmental factors, the number of important ones is generally relatively low—i.e. a few factors can explain the majority of the explainable variation, and it is our intention to find out which of them is most important.\nMuch of the signal may be spurious, i.e. the matrices have high noise. Variability is a general characteristic of the data, which may result in emerging false patterns. This is because sampling may capture a considerable amount of stochasticity that may mask the actual pattern of interest. Imaginative and creative sampling may reveal some of the ecological patterns we are after, but this requires long years of experience and is not something that can easily be taught as part of our module.\nThere is a significant amount of collinearity. This means that many correlated explanatory variables can explain patterning, but only a few act in a way that implies causation. Collinearity is something we will return to later on."
  },
  {
    "objectID": "BDC334/01-introduction.html#ecological-gradients",
    "href": "BDC334/01-introduction.html#ecological-gradients",
    "title": "1. Ecological Data",
    "section": "\n3 Ecological gradients",
    "text": "3 Ecological gradients\nAlthough there are many ways in which species can respond to their environment, one of the most striking responses can be seen along with environmental gradients. Next, we will explore this concept by discussing coenoclines and unimodal species distribution models.\n\n3.1 Coenoclines, coenoplanes, and coenospaces\nA coenocline is a graphical display of all species response curves (see definition below) simultaneously along one environmental gradient. It aids our understanding of the species response curve if we imagine the gradient operating in only one geographical direction. The coenoplane concept extends the coenocline to cover two gradients. Again, our visual representation can be facilitated if the two gradients are visualised orthogonal (in this case, at right angles) to each other (e.g. east-west and north-south) and do not interact. A coenospace complicates the model substantially, as it can allow for an unspecified number of gradients to operate simultaneously on multiple species simultaneously. It will probably also capture interactions of environmental drivers on the species.\n\nlibrary(coenocliner)\nset.seed(2)\nM &lt;- 20                                    # number of species\nming &lt;- 3.5                                # gradient minimum...\nmaxg &lt;- 7                                  # ...and maximum\nlocs &lt;- seq(ming, maxg, length = 100)      # gradient locations\nopt  &lt;- runif(M, min = ming, max = maxg)   # species optima\ntol  &lt;- rep(0.25, M)                       # species tolerances\nh    &lt;- ceiling(rlnorm(M, meanlog = 3))    # max abundances\npars &lt;- cbind(opt = opt, tol = tol, h = h) # put in a matrix\n\nmu &lt;- coenocline(locs, responseModel = \"gaussian\", params = pars,\n                 expectation = TRUE)\n\nmatplot(locs, mu, lty = \"solid\", type = \"l\", xlab = \"pH\", ylab = \"Abundance\")\n\n\n\nFigure 4: A coenocline.\n\n\n\nAbove is an example of a coenocline using simulated species data. It demonstrates an important idea: that of unimodal species distributions (Figure 4).\n\nset.seed(10)\nN &lt;- 30                                       # number of samples\nM &lt;- 20                                       # number of species\n## First gradient\nming1 &lt;- 3.5                                  # 1st gradient minimum...\nmaxg1 &lt;- 7                                    # ...and maximum\nloc1 &lt;- seq(ming1, maxg1, length = N)         # 1st gradient locations\nopt1 &lt;- runif(M, min = ming1, max = maxg1)    # species optima\ntol1 &lt;- rep(0.5, M)                           # species tolerances\nh    &lt;- ceiling(rlnorm(M, meanlog = 3))       # max abundances\npar1 &lt;- cbind(opt = opt1, tol = tol1, h = h)  # put in a matrix\n## Second gradient\nming2 &lt;- 1                                    # 2nd gradient minimum...\nmaxg2 &lt;- 100                                  # ...and maximum\nloc2 &lt;- seq(ming2, maxg2, length = N)         # 2nd gradient locations\nopt2 &lt;- runif(M, min = ming2, max = maxg2)    # species optima\ntol2 &lt;- ceiling(runif(M, min = 5, max = 50))  # species tolerances\npar2 &lt;- cbind(opt = opt2, tol = tol2)         # put in a matrix\n## Last steps...\npars &lt;- list(px = par1, py = par2)            # put parameters into a list\nlocs &lt;- expand.grid(x = loc1, y = loc2)       # put gradient locations together\n\nmu2d &lt;- coenocline(locs, responseModel = \"gaussian\",\n                   params = pars, extraParams = list(corr = 0.5),\n                   expectation = TRUE)\n\nlayout(matrix(1:4, ncol = 2))\nop &lt;- par(mar = rep(1, 4))\nfor (i in c(2,8,13,19)) {\n    persp(loc1, loc2, matrix(mu2d[, i], ncol = length(loc2)),\n          ticktype = \"detailed\", zlab = \"Abundance\",\n          theta = 45, phi = 30)\n}\n\n\n\nFigure 5: A smoothed coenoplane.\n\n\n\n\nsim2d &lt;- coenocline(locs, responseModel = \"gaussian\",\n                    params = pars, extraParams = list(corr = 0.5),\n                    countModel = \"negbin\", countParams = list(alpha = 1))\n\nlayout(matrix(1:4, ncol = 2))\nop &lt;- par(mar = rep(1, 4))\nfor (i in c(2,8,13,19)) {\n  persp(loc1, loc2, matrix(sim2d[, i], ncol = length(loc2)),\n        ticktype = \"detailed\", zlab = \"Abundance\",\n        theta = 45, phi = 30)\n}\n\n\n\nFigure 6: A ‘raw’ coenoplane.\n\n\n\nA coenoplane is demonstrated above (Figure 5). We see idealised surfaces (smooth models), and the ‘raw’ species counts are obscured. Plotting the actual count data looks messier (Figure 6).\n\n3.1.1 Species response curves\nPlotting the abundance of a species as a function of position along a the gradient is called a species response curve. If a long enough the gradient is sampled, a species typically has a unimodal response (one peak resembling a Gaussian distribution) to the gradient. Although the idealised Gaussian response is desired (for statistical purposes, largely), in nature, the curve might deviate quite noticeably from what’s considered ideal. It is probable that a perfectly normal species distribution along a gradient can only be expected when the gradient is perfectly linear in magnitude (seldom true in nature), operates along only one geographical direction (unlikely), and all other potentially additive environmental influences are constant across the ecological (coeno-) space (also not a realistic expectation).\n\n3.1.2 Unimodal species response\nThe unimodal model is an idealised species response curve (visualised as a coenocline) where a species has only one mode of abundance. In this species response curve, the species has one optimal environmental condition where it is most abundant (the fewest ecophysiological and ecological stressors). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance. The unimodal model offers a convenient heuristic tool for understanding how species can become structured along environmental gradients."
  },
  {
    "objectID": "BDC334/01-introduction.html#exploring-data",
    "href": "BDC334/01-introduction.html#exploring-data",
    "title": "1. Ecological Data",
    "section": "\n4 Exploring data",
    "text": "4 Exploring data\nAt the start of the analysis, before we go deeper into the patterns in the data, we need to explore the data and compute the various synthetic descriptors. This might involve calculating means and standard deviations for some of the variables we feel are most important. So, we say that we produce univariate summaries, and if there is a need we may also create some graphical summaries like line plots or frequency histograms. Be guided by the research questions as to what is required. Typically, I don’t like to produce too many detailed inferential statistics of the univariate data (there are special statistical techniques available that allow us to do so more efficiently and effectively, but we will get to it in the Honours Module Quantitative Ecology), choosing instead to see which relationships and patterns emerge from the exploratory summary plots before testing their statistical significance using multivariate approaches. But that is me. Sometimes, some hypotheses call for a few univariate inferential analyses (again, this is the topic of an Honours module on Biostatistics).\n\n\n\n\n\n\nLab 1 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nCreate an \\(x-y\\) plot of the geographical coordinates in DoubsSpa.csv.\nUsing some graphs that plot the trends of the Doubs River environmental variables along the length of the river, describe the patterns in some of the environmental variables and offer explanations for how they might be responsible for affecting species distributions down the length of the Doubs River. Which three variables do you think will be able to explain the trends in the species data?"
  },
  {
    "objectID": "BDC334/01-introduction.html#pairwise-matrices",
    "href": "BDC334/01-introduction.html#pairwise-matrices",
    "title": "1. Ecological Data",
    "section": "\n5 Pairwise matrices",
    "text": "5 Pairwise matrices\nAlthough we typically start our forays into data exploration using sites × species and sites × environment tables, the formal statistical analyses usually require ‘pairwise association matrices.’ Such matrices are symmetrical (sometimes only the lower or upper triangle is displayed) square matrices (i.e. \\(n \\times n\\)). These matrices tell us how related any sample is to any other sample in our pool of samples (i.e. relatedness among rows with respect to whatever populates the columns, be they species information of environmental information).\nLet us consider various kinds of distance matrices under the headings Distances, Correlations, Associations, Similarities, and Dissimilarities.\n\n5.1 Distances\nA frequently used distance metric is Euclidian distance. Euclidian distance is the ‘ordinary straight-line’ distance between two points in Euclidian space. Working with geographical coordinates over small areas of Earth’s surface, Euclidian distance is very similar (i.e. almost directly proportional) to the actual geographical distance, so the concept is intuitive to understand. In its simplest form, it is a planar Cartesian area, which you know of as a graph with \\(x\\)- and \\(y\\)-axes). So, in 2D and 3D space, it gives distances in Cartesian units between points on a plane \\(x\\), \\(y\\) or in volume \\(x\\), \\(y\\), \\(z\\); a linear relationship exists between the units in the physical realm and the units in Euclidian space. The implication is that on a map or graph, short distances between pairs of points indicate that there are also short geographic distances between these points on Earth.\nEuclidian distance is calculated using the Pythagorean theorem, and it is typically applied to standardised environmental data (not species data):\n\\[d(a,b) = \\sqrt{(a_x - b_x)^2 + (a_y - b_y)^2 + (a_z - b_z)^2} \\]\nIn the above equation, this ‘distance’ is calculated between a pair of sites, a and b, whose locations are marked by the coordinates \\(x\\), \\(y\\), and \\(z\\)—i.e. this is an example of 3-dimensional data (a space or volume, as opposed to 2D data situated on a \\(x\\), \\(y\\) place). We might also call each coordinate a ‘variable’ such as temperature, depth, or light intensity (sometimes also called ‘dimensions’ of environmental space).\n\\[d(a,b) = \\sqrt{(a_{temp} - b_{temp})^2 + (a_{depth} - b_{depth})^2 + (a_{light} - b_{light})^2} \\]\nIn the example dataset downloaded earlier (Euclidian_distance_demo_data_xyz.csv), we can calculate the distance between every pair of sites named a to g. The ‘raw’ data representing \\(x\\), \\(y\\) and \\(z\\) dimensions can be viewed in MS Excel, as we see in Figure 7.\n\n\nFigure 7: Data representing three dimensions, \\(x\\), \\(y\\), and \\(z\\).\n\nWe can substitute \\(x\\), \\(y\\) and \\(z\\) for environmental ‘dimensions,’ and we have a set of data that resembles what we see in Figure 8. Regardless of whether we have \\(x\\), \\(y\\) and \\(z\\) or environmental dimensions, the application of the Pythagorean Theorem is the same.\n\n\nFigure 8: Data representing three environmental ‘dimensions.’\n\nFigure 9 shows how we may calculate Euclidian distance in MS Excel uses some built-in functions. To produce the pairwise matrix, you’d have to do this for every pair of sites. As a minimum, calculate the bottom left triangle. For completeness, calculate the diagonal, which will be all zeros in this instance.\n\n\nFigure 9: Calculating Euclidian distance in MS Excel.\n\n\n5.2 Correlations\nWe use correlations to establish how environmental variables relate across the sample sites. Therefore, a correlation performed to a sites × variable table is done between columns (variables), not rows, as in the Euclidian distance calculation, which compares the rows (sites). We do not need to standardise as one would for calculating Euclidian distances (but it will do no harm if you do). Correlation coefficients (so-called \\(r\\)-values) vary in magnitude from -1 (a perfect inverse relationship) from 0 (no relationship) to 1 (a perfect positive linear relationship).\n\n\nFigure 10: Calculating pairwise correlations between environmental variables in MS Excel.\n\nThe resultant pairwise correlation matrix shows the names of the environmental variables as both column and row names. Contrast this with what is presented as row and column names in the distance matrix (Figure 10).\n\n5.3 Associations, similarities, and dissimilarities\nThus far, we have worked with environmental data. Associations, similarities, and dissimilarities extend the pairwise matrix to species data. We will discuss and calculate these matrices in Lab 3.\nThat’s it for this week, Folks! I’ll leave you with some lovely exercises to take you through the rest of the week.\n\n\n\n\n\n\nLab 1 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nUsing the Doubs River environmental data, calculate the lower left triangle (including the diagonal) distance matrix for every pair of sites in Sites 1, 3, 5, …, 29 (i.e. using only every second site). Explain any patterns or trends in this resultant distance matrix regarding how similar/different sites are relative to each other. Which of the graphs you came up with in Task 3 (if any) do you think are responsible for the patterns seen in the distance matrix?\nUsing the same sites as above (Question 4), calculate a pairwise correlation matrix (lower left and including the diagonal) for the Doubs River environmental data. Explain any patterns or trends in this resultant correlation matrix and offer mechanistic explanations for why these correlations might exist.\nDiscuss in detail the properties of distance and correlation matrices.\nIf you found this exercise annoying, explain why. Or if you loved it, state why. What could be done to ease your experience of the calculations?\n\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nThe Lab 1 assignment on Ecological Data was discussed on Monday 1 August and is due at 07:00 on Monday 8 August 2022.\nProvide a neat and thoroughly annotated MS Excel spreadsheet which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions. Written answers must be typed in an MS Word document. Please follow the formatting specifications precisely shown in the file BDC334 Example essay format.docx that was circulated at the beginning of the module. Feel free to use the file as a template.\nPlease label the MS Excel and MS Word files as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_1.xlsx, and\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_1.docx\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named spreadsheet and MS Word documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%)."
  },
  {
    "objectID": "BDC334/01-introduction.html#references",
    "href": "BDC334/01-introduction.html#references",
    "title": "1. Ecological Data",
    "section": "\n6 References",
    "text": "6 References\n\n\nBorcard D, Gillet F, Legendre P, others (2011) Numerical ecology with R. Springer\n\n\nShade A, Dunn RR, Blowes SA, Keil P, Bohannan BJ, Herrmann M, Küsel K, Lennon JT, Sanders NJ, Storch D, others (2018) Macroecology to unite all life, large and small. Trends in Ecology & Evolution 33:731–744.\n\n\nVerneaux J (1973) Cours d’eau de Franche-Comté (Massif du Jura). Recherches écologiques sur le réseau hydrographique du Doubs."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html",
    "href": "BDC334/04-biodiversity2.html",
    "title": "4. Describing Biodiversity Patterns",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html#species-abundance-distribution",
    "href": "BDC334/04-biodiversity2.html#species-abundance-distribution",
    "title": "4. Describing Biodiversity Patterns",
    "section": "\n1.1 Species-abundance distribution",
    "text": "1.1 Species-abundance distribution\nThe species abundance distribution (SAD) is a fundamental pattern in ecology. Typical communities have a few species that are very abundant, whereas most of them are quite rare; indeed—this is perhaps a universal law in ecology. SAD represents this relationship graphically by plotting the abundance rank on the \\(x\\)-axis and the number of species (or some other taxonomic level) along \\(y\\), as was first done by Fisher et al. (1943). He then fitted the data by log series that ideally capture situations where most of the species are quite rare with only a few very abundant ones—called Fisher’s log series distribution—and is implemented in vegan by the fisherfit() function (Figure 1). The curve in Fisher’s logarithmic series shows the expected number of species \\(f\\) with \\(n\\) observed individuals. In fact, the interpretation of the curve is the same for all species-abundance models shown below, and it is only the math and rationale that differ.\n\n# take one random sample of a row (site):\n# for this website's purpose, this function ensure the same random\n# sample is drawn each time the web page is recreated\nset.seed(13) \nk &lt;- sample(nrow(BCI), 1)\nfish &lt;- fisherfit(BCI[k,])\nfish\n\n\nFisher log series model\nNo. of species: 95 \nFisher alpha:   39.87658 \n\nplot(fish)\n\n\n\nFigure 1: Fisher’s log series distribution calculated for the Barro Colorado Island Tree Counts data.\n\n\n\nPreston (1948) showed that when data from a thoroughly sampled population are transformed into octaves along the \\(x\\)-axis (number of species binned into intervals of 1, 2, 4, 8, 16, 32 etc.), the SAD that results is approximated by a symmetric Gaussian distribution. This is because more thorough sampling makes species that occur with a high frequency more common and those that occur only once or are very rare become either less common will remain completely absent. This SAD is called Preston’s log-normal distribution. In the vegan package there is an updated version of Preston’s approach with a mathematical improvement to better handle ties. It is called prestondistr() (Figure 2):\n\npres &lt;- prestondistr(BCI[k,])\npres\n\n\nPreston lognormal model\nMethod: maximized likelihood to log2 abundances \nNo. of species: 95 \n\n      mode      width         S0 \n 0.9234918  1.6267630 26.4300640 \n\nFrequencies by Octave\n                0        1        2        3        4        5         6\nObserved 19.00000 27.00000 21.50000 17.00000 7.000000 2.500000 1.0000000\nFitted   22.49669 26.40085 21.23279 11.70269 4.420327 1.144228 0.2029835\n\nplot(pres)\n\n\n\nFigure 2: Preston’s log-normal distribution demonstrated for the BCI data.\n\n\n\nWhittaker (1965) introduced rank abundance distribution curves (RAD; sometimes called a dominance-diversity curve or Whittaker plots). Here the \\(x\\)-axis has species ranked according to their relative abundance, with the most abundant species at the left and rarest at the right. The \\(y\\)-axis represents relative species abundances (sometimes log-transformed). The shape of the profile as—influenced by the steepness and the length of the tail—indicates the relative proportion of abundant and scarce species in the community. In vegan we can accomplish fitting this type of SAD with the radfit() function. The default plot is somewhat more complicated as it shows broken-stick, preemption, log-Normal, Zipf and Zipf-Mandelbrot models fitted to the ranked species abundance data (Figure 3):\n\nrad &lt;- radfit(BCI[k,])\nrad\n\n\nRAD models, family poisson \nNo. of species 95, total abundance 392\n\n           par1      par2     par3    Deviance AIC      BIC     \nNull                                   56.3132 324.6477 324.6477\nPreemption  0.042685                   55.8621 326.1966 328.7504\nLognormal   0.84069   1.0912           16.1740 288.5085 293.6162\nZipf        0.12791  -0.80986          21.0817 293.4161 298.5239\nMandelbrot  0.66461  -1.2374   4.1886   6.6132 280.9476 288.6093\n\nplot(rad)\n\n\n\nFigure 3: Whittaker’s rank abundance distribution curves demonstrated for the BCI data.\n\n\n\nWe can also fit the rank abundance distribution curves to several sites at once (previously we have done so on only one site) (Figure 4):\n\nm &lt;- sample(nrow(BCI), 6)\nrad2 &lt;- radfit(BCI[m, ])\nrad2\n\n\nDeviance for RAD models:\n\n                  3       37       10       13        6       22\nNull        86.1127  93.5952  77.2737  52.6207  72.1627 114.1747\nPreemption  58.9295 104.0978  62.7210  57.7372  54.7709 110.5156\nLognormal   29.2719  19.0653  20.4770  15.8218  19.5788  26.2510\nZipf        50.1262  11.3048  39.7066  22.8006  32.4630  15.5222\nMandelbrot   5.7342   8.9107   9.8353  12.1701   5.5973   9.6047\n\nplot(rad2)\n\n\n\nFigure 4: Rank abundance distribution curves fitted to several sites.\n\n\n\nAbove, we see that the model selected for capturing the shape of the SAD is the Mandelbrot, and it is plotted individually for each of the randomly selected sites. Model selection works through Akaike’s or Schwartz’s Bayesian information criteria (AIC or BIC; AIC is the default—select the model with the lowest AIC).\nBiodiversityR (and here and here) also offers options for rank abundance distribution curves; see rankabundance() (Figure 5):\n\nlibrary(BiodiversityR)\nrankabund &lt;- rankabundance(BCI)\nrankabunplot(rankabund, cex = 0.8, pch = 0.8, col = \"indianred4\")\n\n\n\nFigure 5: Rank-abundance curves for the BCI data.\n\n\n\nRefer to the help files for the respective functions to see their differences."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html#occupancy-abundance-curves",
    "href": "BDC334/04-biodiversity2.html#occupancy-abundance-curves",
    "title": "4. Describing Biodiversity Patterns",
    "section": "\n1.2 Occupancy-abundance curves",
    "text": "1.2 Occupancy-abundance curves\nOccupancy refers to the number or proportion of sites in which a species is detected. Occupancy-abundance relationships are used to infer niche specialisation patterns in the sampling region. The hypothesis (almost a theory) is that species that tend to have high local abundance within one site also tend to occupy many other sites (Figure 6).\n\nlibrary(ggpubr)\n\n# A function for counts:\n# count number of non-zero elements per column\ncount_fun &lt;- function(x) {\n  length(x[x &gt; 0])\n}\n\nBCI_OA &lt;- data.frame(occ = apply(BCI, MARGIN = 2, count_fun),\n                     ab = apply(BCI, MARGIN = 2, mean))\n\nggplot(BCI_OA, aes(x = ab, y = occ/max(occ))) +\n  geom_point(colour = \"indianred3\") +\n  scale_x_log10() +\n  # scale_y_log10() +\n  labs(title = \"Barro Colorado Island Tree Counts\",\n     x = \"Log (abundance)\", y = \"Occupancy\") +\n  theme_linedraw()\n\n\n\nFigure 6: Occupancy-abundance relationships seen in the BCI data."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html#species-area-accumulation",
    "href": "BDC334/04-biodiversity2.html#species-area-accumulation",
    "title": "4. Describing Biodiversity Patterns",
    "section": "\n1.3 Species-area (accumulation)",
    "text": "1.3 Species-area (accumulation)\nSpecies accumulation curves (species area relationships, SAR) try and estimate the number of unseen species. These curves can be used to predict and compare changes in diversity over increasing spatial extent. Within an ecosystem type, one would expect that more and more species would be added (accumulates) as the number of sampled sites increases (i.e. extent increases). This continues to a point where no more new species are added as the number of sampled sites continues to increase (i.e. the curve plateaus). Species accumulation curves, as the name suggests, accomplishes this by adding (accumulation or collecting) more and more sites and counting the average number of species along \\(y\\) each time a new site is added. See Roeland Kindt’s description of how species accumulation curves work (on p. 41). In the community matrix (the sites × species table), we can do this by successively adding more rows to the curve (seen along the \\(x\\)-axis). The specaccum() function has many different ways of adding the new sites to the curve, but the default ‘exact’ seems to be a sensible choice. BiodiversityR has the accumresult() function that does nearly the same. Let’s demonstrate using vegan’s function (Figure 7, Figure 8, and Figure 9):\n\nsp1 &lt;- specaccum(BCI)\nsp2 &lt;- specaccum(BCI, \"random\")\n\n# par(mfrow = c(2,2), mar = c(4,2,2,1))\n# par(mfrow = c(1,2))\nplot(sp1, ci.type = \"polygon\", col = \"indianred4\", lwd = 2, ci.lty = 0,\n     ci.col = \"steelblue2\", main = \"Default: exact\",\n     ylab = \"No. of species\")\n\n\n\nFigure 7: Species-area accumulation curves seen in the BCI data.\n\n\n\n\nmods &lt;- fitspecaccum(sp2, \"arrh\")\nplot(mods, col = \"indianred\", ylab = \"No. of species\")\nboxplot(sp2, col = \"yellow\", border = \"steelblue2\", lty = 1, cex = 0.3, add = TRUE)\nsapply(mods$models, AIC)\n\n  [1] 311.4642 303.7835 346.3668 320.0786 338.7978 320.2538 325.6968 346.2671\n  [9] 320.3900 343.8570 318.2509 369.8303 335.9936 350.8711 327.9831 348.1287\n [17] 328.2393 347.8133 324.3837 314.8555 333.1390 340.5678 332.6836 360.5208\n [25] 335.3660 325.3150 347.4324 336.7498 336.6374 276.1878 349.9283 295.0268\n [33] 308.4656 315.8304 303.0776 329.8425 356.2393 368.4302 318.0514 359.5975\n [41] 327.4228 335.7604 259.8340 318.0063 335.7753 285.8790 323.5174 300.3546\n [49] 327.1448 355.2747 288.2583 366.5995 287.4120 327.5877 362.6487 323.5904\n [57] 339.5650 321.2264 336.6331 353.1295 317.9578 311.6528 336.3613 337.8327\n [65] 328.4787 311.6842 345.8035 367.5620 319.0269 305.6546 338.7805 321.8859\n [73] 330.6029 326.7097 345.8923 338.4755 352.8710 355.8038 307.7327 329.2355\n [81] 341.6628 340.1687 333.4771 348.3144 321.4417 317.4331 339.2211 313.1990\n [89] 305.3069 342.4581 318.0308 299.7067 294.7851 324.3237 333.5849 349.2749\n [97] 369.8287 323.0041 332.6820 329.3875\n\n\n\n\nFigure 8: Fit Arrhenius models to all random accumulations\n\n\n\n\naccum &lt;- accumresult(BCI, method = \"exact\", permutations = 100)\naccumplot(accum)\n\n\n\nFigure 9: A species accumulation curve.\n\n\n\nSpecies accumulation curves can also be calculated with the alpha.accum() function of the BAT package (Figure 10). In addition, the BAT package can also apply various diversity and species distribution assessments to phylogenetic and functional diversity. See the examples provided by Cardoso et al. (2015).\n\nlibrary(BAT)\nBCI.acc &lt;- alpha.accum(BCI, prog = FALSE)\n\npar(mfrow = c(1,2))\nplot(BCI.acc[,2], BCI.acc[,17], col = \"indianred\",\n     xlab = \"Individuals\", ylab = \"Chao1P\")\nplot(BCI.acc[,2], slope(BCI.acc)[,17], col = \"indianred\",\n     xlab = \"Individuals\", ylab = \"Slope\")\n\n\n\nFigure 10: A species accumulation curve made with the alpha.accum() function of BAT."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html#rarefaction-curves",
    "href": "BDC334/04-biodiversity2.html#rarefaction-curves",
    "title": "4. Describing Biodiversity Patterns",
    "section": "\n1.4 Rarefaction curves",
    "text": "1.4 Rarefaction curves\nLike species accumulation curves, rarefaction curves also try to estimate the number of unseen species. Rarefaction, meaning to scale down (Heck Jr et al. 1975), is a statistical technique used by ecologists to assess species richness (represented as S, or diversity indices such as Shannon diversity, \\(H'\\), or Simpson’s diversity, \\(\\lambda\\)) from data on species samples, such as that which we may find in site × species tables. Rarefaction can be used to determine whether a habitat, community, or ecosystem has been sufficiently sampled to fully capture the full complement of species present.\nRarefaction curves may seem similar to species accumulation curves, but there is a difference as I will note below. Species richness, S, accumulates with sample size or with the number of individuals sampled (across all species). The first way that rarefaction curves are presented is to show species richness as a function of number of individuals sampled. Here the principle demonstrated is that when only a few individuals are sampled, those individuals may belong to only a few species; however, when more individuals are present more species will be represented. The second approach to rarefaction is to plot the number of samples along \\(x\\) and the species richness along the \\(y\\)-axis (as in SADs too). So, rarefaction shows how richness accumulates with the number of individuals counted or with the number of samples taken. Rarefaction curves rise rapidly at the start when few species have been sampled and the most common species have been found; the slope then decreases and eventually plateaus suggesting that the rarest species remain to be sampled.\nBut what really distinguishes rarefaction curves from SADs is that rarefaction randomly re-samples the pool of \\(N\\) samples (that is equal or less than the total community size) a number of times, \\(n\\), and plots the average number of species found in each resample (1,2, …, \\(n\\)) as a function of individuals or samples. The rarecurve() function draws a rarefaction curve for each row of the species data table. All these plots are made with base R graphics Figure 11, but it will be a trivial exercise to reproduce them with ggplot2.\n\n# Example provided in ?vegan::rarefy\n# observed number of species per row (site)\nS &lt;- specnumber(BCI) \n\n# calculate total no. individuals sampled per row, and find the minimum\n(raremax &lt;- min(rowSums(BCI)))\n\n[1] 340\n\nSrare &lt;- rarefy(BCI, raremax, se = FALSE)\npar(mfrow = c(1,2))\nplot(S, Srare, col = \"indianred3\",\n     xlab = \"Sample size\\n(observed no. of individuals)\", ylab = \"No. species found\")\nrarecurve(BCI, step = 20, sample = raremax, col = \"indianred3\", cex = 0.6,\n          xlab = \"Sample size\\n(observed no. of individuals)\", ylab = \"No. species found\")\n\n\n\nFigure 11: Rarefaction curves for the BCI data.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\niNEXT\nWe can also use the iNEXT package for rarefaction curves. From the package’s Introduction Vignette:\niNEXT focuses on three measures of Hill numbers of order q: species richness (q = 0), Shannon diversity (q = 1, the exponential of Shannon entropy) and Simpson diversity (q = 2, the inverse of Simpson concentration). For each diversity measure, iNEXT uses the observed sample of abundance or incidence data (called the “reference sample”) to compute diversity estimates and the associated 95% confidence intervals for the following two types of rarefaction and extrapolation (R/E):\n\nSample‐size‐based R/E sampling curves: iNEXT computes diversity estimates for rarefied and extrapolated samples up to an appropriate size. This type of sampling curve plots the diversity estimates with respect to sample size.\nCoverage‐based R/E sampling curves: iNEXT computes diversity estimates for rarefied and extrapolated samples with sample completeness (as measured by sample coverage) up to an appropriate coverage. This type of sampling curve plots the diversity estimates with respect to sample coverage.\n\niNEXT also plots the above two types of sampling curves and a sample completeness curve. The sample completeness curve provides a bridge between these two types of curves.\nFor information about Hill numbers see David Zelený’s Analysis of community data in R and Jari Oksanen’s coverage of diversity measures in vegan.\nThere are four datasets distributed with iNEXT and numerous examples are provided in the Introduction Vignette. iNEXT has an ‘odd’ data format that might seem foreign to vegan users. To use iNEXT with dataset suitable for analysis in vegan, we first need to convert BCI data to a species × site matrix (Figure 12):\n\nlibrary(iNEXT)\n\n# transpose the BCI data: \nBCI_t &lt;- list(BCI = t(BCI))\nstr(BCI_t)\n\nList of 1\n $ BCI: int [1:225, 1:50] 0 0 0 0 0 0 2 0 0 0 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:225] \"Abarema.macradenia\" \"Vachellia.melanoceras\" \"Acalypha.diversifolia\" \"Acalypha.macrostachya\" ...\n  .. ..$ : chr [1:50] \"1\" \"2\" \"3\" \"4\" ...\n\nBCI_out &lt;- iNEXT(BCI_t, q = c(0, 1, 2), datatype = \"incidence_raw\")\nggiNEXT(BCI_out, type = 1, color.var = \"Order.q\")\n\n\n\nFigure 12: Demonstration of iNEXT capabilities.\n\n\n\nThe warning is produced because the function expects incidence data (presence-absence), but I’m feeding it abundance (count) data. Nothing serious, as the function converts the abundance data to incidences."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html#distance-decay-curves",
    "href": "BDC334/04-biodiversity2.html#distance-decay-curves",
    "title": "4. Describing Biodiversity Patterns",
    "section": "\n1.5 Distance-decay curves",
    "text": "1.5 Distance-decay curves\nThe principles of distance decay relationships are clearly captured in analyses of \\(\\beta\\)-diversity—see specifically turnover, \\(\\beta_\\text{sim}\\). Distance decay is the primary explanation for the spatial pattern of \\(\\beta\\)-diversity along the South African coast in Smit et al. (2017). A deeper dive into distance decay calculation can be seen in Deep Dive into Gradients."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html#elevation-and-other-gradients",
    "href": "BDC334/04-biodiversity2.html#elevation-and-other-gradients",
    "title": "4. Describing Biodiversity Patterns",
    "section": "\n1.6 Elevation and other gradients",
    "text": "1.6 Elevation and other gradients\nIn once sense, an elevation gradient can be seen as specific case of distance decay. The Doubs River dataset offer a nice example of data collected along an elevation gradient. Elevation gradients have many similarities with depth gradients (e.g. down the ocean depths) and latitudinal gradients.\n\n\n\n\n\n\nLab 4\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\n\nProduce the following figures for the species data indicated in [square brackets]:\n\nspecies-abundance distribution [mite];\noccupancy-abundance curves [mite];\nspecies-area curves [seaweed]—note, do not use the BAT package’s alpha.accum() function as your computer might fall over;\nrarefaction curves [mite].\n\nAnswer each under its own heading. For each, also explain briefly what the purpose of the analysis is (i.e. what ecological insights might be provided), and describe the findings of your own analysis as well as any ecological implications that you might be able to detect.\n\nUsing the biodiversityR package, find the most dominant species in the Doubs River dataset.\nDiscuss how elevation, depth, or latitudinal gradients are similar in many aspects to distance decay relationships.\n\n\n\n\n\n\n\n\n\nSubmission instructions\n\n\n\nThe Lab 4 assignment on Species Data was discussed on Thursday 25 August and is due at 07:00 on Thursday 1 September 2022.\nProvide a neat and thoroughly annotated R file which can recreate all the graphs and all calculations. Written answers must be typed in the same file as comments.\nPlease label the R file as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_4.R\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named R documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%)."
  },
  {
    "objectID": "BDC334/04-biodiversity2.html#references",
    "href": "BDC334/04-biodiversity2.html#references",
    "title": "4. Describing Biodiversity Patterns",
    "section": "\n1.7 References",
    "text": "1.7 References\n\n\nBorcard D, Legendre P (1994) Environmental control and spatial structure in ecological communities: An example using oribatid mites (acari, oribatei). Environmental and Ecological statistics 1:37–61.\n\n\nBorcard D, Legendre P, Drapeau P (1992) Partialling out the spatial component of ecological variation. Ecology 73:1045–1055.\n\n\nBorcard D, Gillet F, Legendre P, others (2011) Numerical ecology with R. Springer\n\n\nCardoso P, Rigal F, Carvalho JC (2015) BAT–biodiversity assessment tools, an r package for the measurement and estimation of alpha and beta taxon, phylogenetic and functional diversity. Methods in Ecology and Evolution 6:232–236.\n\n\nCondit R, Pitman N, Leigh Jr EG, Chave J, Terborgh J, Foster RB, Núnez P, Aguilar S, Valencia R, Villa G, others (2002) Beta-diversity in tropical forest trees. Science 295:666–669.\n\n\nFisher RA, Corbet AS, Williams CB (1943) The relation between the number of species and the number of individuals in a random sample of an animal population. The Journal of Animal Ecology 42–58.\n\n\nHeck Jr KL, Belle G van, Simberloff D (1975) Explicit calculation of the rarefaction diversity measurement and the determination of sufficient sample size. Ecology 56:1459–1461.\n\n\nPreston FW (1948) The commonness, and rarity, of species. Ecology 29:254–283.\n\n\nShade A, Dunn RR, Blowes SA, Keil P, Bohannan BJ, Herrmann M, Küsel K, Lennon JT, Sanders NJ, Storch D, others (2018) Macroecology to unite all life, large and small. Trends in Ecology & Evolution 33:731–744.\n\n\nSmit AJ, Bolton JJ, Anderson RJ (2017) Seaweeds in two oceans: Beta-diversity. Frontiers in Marine Science 4:404.\n\n\nVerneaux J (1973) Cours d’eau de Franche-Comté (Massif du Jura). Recherches écologiques sur le réseau hydrographique du Doubs.\n\n\nWhittaker RH (1965) Dominance and Diversity in Land Plant Communities: Numerical relations of species express the importance of competition in community function and evolution. Science 147:250–260."
  },
  {
    "objectID": "BDC334/BDC334_index.html",
    "href": "BDC334/BDC334_index.html",
    "title": "BDC334: Macroecology and Global Biogeography",
    "section": "",
    "text": "Welcome to Term 3 of BDC334 Biogeography and Global Ecology. This page provides the syllabus and teaching policies for the module, and it serves is a starting point for the Lab theory, instruction, and data."
  },
  {
    "objectID": "BDC334/BDC334_index.html#syllabus",
    "href": "BDC334/BDC334_index.html#syllabus",
    "title": "BDC334: Macroecology and Global Biogeography",
    "section": "1.1 Syllabus",
    "text": "1.1 Syllabus\nThese links point to online resources such as reading material in the form of publications, lecture slides, example workflows, datasets, and R scripts in support of the video and PDF lecture material. Note that the video material is housed on iKamva from where you may download it without incurring internet costs; various PDFs for reading can also be found there. It is essential that you work through these examples and workflows.\n\n\n\nWk\nType\nTopic\nPDFs etc.\nClass date\nExercise due\n\n\n\n\nW1\nL\nIntroduction\nslides\n24-28 Jul\n\n\n\n\nL\nKeith et al 2012\nreading\n\n\n\n\n\nL\nShade et al 2018\nreading\n\n\n\n\n\nL\nBDC334_Intro_Day_1_Module_Info_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Intro_Day_1_Pracs_Tests_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Intro_Day_1_Topic_1a_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Intro_Day_1_Topic_1b_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Intro_Day_1_Topic_1c_720p30.mp4\niKamva\n\n\n\n\n\nP1\nR and RStudio\n\n24 Jul\n31 Aug\n\n\nW2\nL\nGradients and community structure\n\n31 Jul–4 Aug\n\n\n\n\nL\nNekola and White 1999\nreading\n\n\n\n\n\nL\nSmit et al 2017\nreading\n\n\n\n\n\nL\nTittensor et al 2010.pdf\nreading\n\n\n\n\n\nL\nBDC334_Lecture_2a_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_2b_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_2c_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_2d_720p30.mp4\niKamva\n\n\n\n\n\nP2\nEcological data\nslides, slides\n31 Jul\n7 Aug\n\n\n\nP2\nIntroduction to Wiki Assignment\n\n31 Jul\n1 Sep\n\n\nW3\nL\nImpacts on biodiversity\n\n7-11 Aug\n\n\n\n\nL\nTilman et al 2017\nreading\n\n\n\n\n\nL\nMaxwell et al 2016\nreading\n\n\n\n\n\nL\nChapin III et al 2000\nreading\n\n\n\n\n\nL\nBDC334_Lecture_3a_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_3b_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_3c_1080p30.mp4\niKamva\n\n\n\n\n\nP3\nEnvironmental distance\n\n7 Jul\n14 Aug\n\n\nW4\nL\nNature’s contribution to people\n\n14-18 Aug\n\n\n\n\nL\nCostanza et al 1997\nreading\n\n\n\n\n\nL\nCostanza et al 2014\nreading\n\n\n\n\n\nL\nBurger et al 2012\nreading\n\n\n\n\n\nL\nBDC334_Topic_4a_Assignment_1_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Topic_4b_Assignment_1_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Topic_4c_Assignment_1_1080p30.mp4\niKamva\n\n\n\n\n\nP4\nQuantifying biodiversity\nslides\n14 Aug\n21 Aug\n\n\n\nP4\nIntroduction to Assignment 1\n\n14 Aug\n1 Sep\n\n\nW5\nL\nUnified accounting: patterns in diversity over space and time\nreading\n21-25 Aug\n\n\n\n\nL\nBDC334_Topic_5a_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Topic_5b_1080p30.mp4\niKamva\n\n\n\n\n\nP5\nBiodiversity structure\nslides\n21 Aug\n28 Aug\n\n\nW6\nL\nRevision\n\n28 Aug–1 Sep\n\n\n\nW7\nFIN\nWiki Essay due\n\n\n1 Sep\n\n\n\n\n1.1.1 Reading in support of the syllabus\nIn the table above, there are links to several key papers to read in preparation for each week’s theory. You must read these papers.\nI cite many other references in each Chapter. These serve several functions in that they:\n\nadd additional theory relevant to some ecological concepts;\nprovide background to some of the datasets used in my examples;\ndiscuss derivations of some equations used to calculate diversity concepts;\nprovide example walkthroughs of some of the computational aspects of the methods covered in the Labs;\ncollectively supplement the discussion about these concepts covered in the lectures.\n\nActively engaging with these reading materials will make the difference between a 60% average mark for the module and a mark in excess of 80%."
  },
  {
    "objectID": "BDC334/BDC334_index.html#term-3-theory-overview",
    "href": "BDC334/BDC334_index.html#term-3-theory-overview",
    "title": "BDC334: Macroecology and Global Biogeography",
    "section": "1.2 Term 3 theory overview",
    "text": "1.2 Term 3 theory overview\n\nEcology and macroecology\n\nrevisit concepts of ecology\nbiodiversity\nfrom ecology to macroecology\nquestions that macroecologists ask, incl., body size, determinants of geographical boundaries, diversity vs. latitude, etc.\ntheories biodiversity structuring, incl., neutral, niche, metabolic, etc. (?)\ngradients in diversity\nmacroecology: generalisations across marine and terrestrial realms\n\nExploration of seleced marine and terrestrial ecosystems\nOverview of anthropogenic and natural impacts on ecosystem integrity\nPlanetary boundaries (own revision)\nEcosystem goods and services\nValuation of biodiversity (mostly self-study) using reading provided and also your own research. We examine the TEEB and IPBES approaches."
  },
  {
    "objectID": "BDC334/BDC334_index.html#lectures",
    "href": "BDC334/BDC334_index.html#lectures",
    "title": "BDC334: Macroecology and Global Biogeography",
    "section": "4.1 Lectures",
    "text": "4.1 Lectures\n\nMonday 3rd period Lecture Halls C C6\nTuesday 2nd period Prefabs OM\nWednesday 1st period Lecture Halls C C6\n\nYou are provided with reading material (lecture slides, PDFs for reading) and pre-recorded video lectures that you are expected to consume before the Discussion classes on Wednesdays. The weekly face-to-face sessions are essential for discussing the work you covered the previous two days, and it also allows you to be like real students, attending actual lectures, for real, in person. The Discussion session is for free talk and bouncing of ideas. We can talk about anything related to the topic of biodiversity but will try and focus on the issues at hand.\nTypically, we will meet weekly, on Wednesdays, in person on campus. The rest of the time, we will proceed with pre-recorded lecture material from wherever in the world you choose to be.\nHowever, on the first Monday of Term 3, we will all meet in person on campus in the lecture venue (again on the first Wednesday of Term 3). You can then meet me for the first time (even if you saw me online last year), and I will give an outline of my portion of the course. Prof Boatwright will take over in Term 4."
  },
  {
    "objectID": "BDC334/BDC334_index.html#labs",
    "href": "BDC334/BDC334_index.html#labs",
    "title": "BDC334: Macroecology and Global Biogeography",
    "section": "4.2 Labs",
    "text": "4.2 Labs\nThe Labs take place on Mondays during Periods 6-8 (starting at 13:30) in the 5th floor computer lab in Biodiversity and Conservation Biology Department (starts 1 August).\nLab 1 starts in the second week of the module in Term 3. Labs are compulsory, and failing to attend will result in a penalty of 20% taken from your mark for the week.\nPlease ensure that you read through each Lab (accessible in the sidebar) before the start the Labs. You have until Monday at 07:00 to complete and submit all the material."
  },
  {
    "objectID": "BDC334/BDC334_index.html#labs-1",
    "href": "BDC334/BDC334_index.html#labs-1",
    "title": "BDC334: Macroecology and Global Biogeography",
    "section": "7.1 Labs",
    "text": "7.1 Labs\nThese Labs are hands-on. They can only deliver acceptable outcomes if you attend all Lab sessions. Sometimes an occasional absence cannot be avoided. Still, you need to provide evidence (affidavit, doctor’s note, or death certificate) for why you did not attend to avoid a non-attendance penalty. Please be courteous and notify the tutor or me before any absence. If you work with a partner in class, inform them too. Keep up with the reading assignments while you are away, and we will all work with you to get you back up to speed on what you miss. If you do miss a class, however, the assignments must still be submitted on time (also see Late submission of CA).\nSince you may decide to work in collaboration with a peer on tasks and assignments, please keep this person informed at all times in case some emergency makes you unavailable for some time. Someone might depend on your input and contributions—do not leave someone in the lurch so that they cannot complete a task in your absence."
  },
  {
    "objectID": "BDC334/BDC334_index.html#general-considerations",
    "href": "BDC334/BDC334_index.html#general-considerations",
    "title": "BDC334: Macroecology and Global Biogeography",
    "section": "7.2 General considerations",
    "text": "7.2 General considerations\nThe schedule is set and will not be changed. Sometimes an occasional absence cannot be avoided. Please be courteous and notify the tutor or me before any absence. If you work with a partner in class, inform them too. Keep up with the reading assignments while you are away, and we will all work with you to get you back up to speed on what you miss. However, if you miss a class, the assignments must still be submitted on time (also see ‘Late submissions’ below).\nSince you may decide to work in collaboration with a peer on tasks and assignments, please keep this person informed at all times in case some emergency makes you unavailable for some time. Someone might depend on your input and contributions—do not leave someone in the lurch so they cannot complete a task in your absence."
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html",
    "href": "BDC334/02a-r_rstudio.html",
    "title": "2a. R and RStudio",
    "section": "",
    "text": "“Ignorance more frequently begets confidence than does knowledge.”\n— Charles Darwin"
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html#download-and-install-r-and-rstudio",
    "href": "BDC334/02a-r_rstudio.html#download-and-install-r-and-rstudio",
    "title": "2a. R and RStudio",
    "section": "\n1 Download and install R and RStudio",
    "text": "1 Download and install R and RStudio\n\n1.1 Step 1: Install R\n“R is a free software environment for statistical computing and graphics” (R Core Team 2022). Download it from here and install it on your personal computers before going to the next step.\nR is the software that actually does the work we do from now on.\n\n1.2 Step 2: Install RStudio\nR runs within another piece of software called RStudio. RStudio is an Integrated Development Environment (IDE) and it can be downloaded here. You want the RStudio Desktop.\nRStudio can be seen as the vehicle body, seats, dashboard, and all other bells and whistles you might find in a car. R is the engine. RStudio does not work without R. The analyses, graphics, etc. are done with R (running inside RStudio) and not RStudio."
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html#setting-up-the-workspace",
    "href": "BDC334/02a-r_rstudio.html#setting-up-the-workspace",
    "title": "2a. R and RStudio",
    "section": "\n2 Setting up the workspace",
    "text": "2 Setting up the workspace\n\n2.1 General settings\nBefore we start using RStudio (which is a code editor and environment that runs R) let’s first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’) and select the tick boxes as shown in Figure 2 below.\n\n\nFigure 2: RStudio preferences.\n\n\n2.2 Customising appearance\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g. Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code. Refer to Figure 3.\n\n\nFigure 3: Appearance settings."
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html#the-rproject",
    "href": "BDC334/02a-r_rstudio.html#the-rproject",
    "title": "2a. R and RStudio",
    "section": "\n3 The Rproject",
    "text": "3 The Rproject\nA very nifty way of managing workflow in RStudio is through the built-in functionality of the Rproject. We do not need to install any packages or change any settings to use these. Creating a new project is a very simple task, as well. For this course we will be using the Intro_R_Workshop.Rproj file you downloaded with the course material so that we are all running identical projects. This will prevent a lot of issues by ensuring we are doing things by the same standard. Better yet, an Rproject integrates seamlessly into version control software (e.g. GitHub) and allows for instant world class collaboration on any research project. To initialise the ‘Intro_R_Workshop’ project on your machine please find where you saved Intro_R_Workshop.Rproj file and click on it. We will cover the concepts and benefits of an Rproject more as we move through the course."
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html#installing-packages",
    "href": "BDC334/02a-r_rstudio.html#installing-packages",
    "title": "2a. R and RStudio",
    "section": "\n4 Installing packages",
    "text": "4 Installing packages\nThe most common functions used in R are contained within the base package; this makes R useful ‘out of the box.’ However, there is extensive additional functionality that is being expanded all the time through the use of packages. Packages are simply collections of code called functions that automate complex mathematical or statistical tasks. One of the most useful features of R is that users are continuously developing new packages and making them available for free. You can find a comprehensive list of available packages on the CRAN website. There are currently (2022-04-29) 18907 packages available for R!\nIf the thought of searching for and finding R packages is daunting, a good place to start is the R Task View page. This page curates collections of packages for general tasks you might encounter, such as Experimental Design, Meta-Analysis, or Multivariate Analysis. Go and have a look for yourself, you might be surprised to find a good explanation of what you need.\nAfter clicking ‘Tools’/‘Install Packages’, type in the package name tidyverse in the ‘Packages’ text box (note that it is case sensitive) and select the Install button. The Console will run the code needed to install the package, and then provide some commentary on the installation of the package and any of its dependencies (i.e., other R packages needed to run the required package).\nThe installation process makes sure that the functions within the packages contained within the tidyverse are now available on your computer, but to avoid potential conflicts in the names of functions, it will not load these automatically. To make R ‘know’ about these functions in a particular session, you need either to load the package via ticking the checkbox for that package in the Packages tab, or execute:\n\nlibrary(tidyverse)\n\nTo prepare ourselves for the week ahead, let us also install the following packages. Here I demonstate the command line approach to achieve the same thing that can be done via the menu:\n\n# install.packages(\"rmarkdown\")\n# install.packages(\"tidyverse\")\n# install.packages(\"bindrcpp\")\n# install.packages(\"ggpubr\")\n# install.packages(\"magrittr\")\n# install.packages(\"boot\")\n# install.packages(\"ggsn\")\n# install.packages(\"scales\")\n# install.packages(\"maps\")\n# install.packages(\"ggmap\")\n# install.packages(\"lubridate\")\n# install.packages(\"bindrcpp\")\n\nSince we will develop the habit of doing all of our analyses from R scripts, it is best practice to simply list all of the libraries to be loaded right at the start of your script. Comments may be used to remind your future-self (to quote Hadley Wickham) what those packages are for.\n\n\n\n\n\n\nCopying code from RStudio\n\n\n\nHere you saw RStudio execute the R code needed to install (using install.packages()) and load (using library()) the package, so if you want to include these in one of your programs, just copy the text it executes. Note that you need only install the current version of a package once, but it needs to be loaded at the beginning of each R session.\n\n\n\n\n\n\n\n\nLab 2\n\n\n\n\nWhy is it best practice to include packages you use in your R program explicitly?"
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html#the-panes-of-rstudio",
    "href": "BDC334/02a-r_rstudio.html#the-panes-of-rstudio",
    "title": "2a. R and RStudio",
    "section": "\n5 The panes of RStudio",
    "text": "5 The panes of RStudio\nRStudio has four main panes each in a quadrant of your screen ((RStudio_panes?)): Source Editor 🅐, Console 🅑, Workspace Browser 🅒 (and History), and Plots 🅓 (and Files, Packages, Help). These can also be adjusted under the ‘Preferences’ menu. Note that there might be subtle differences between RStudio installations on different operating systems. We will discuss each of the panes in turn.\n\n\nFigure 4: RStudio window panes.\n\n\n5.1 Source Editor 🅐\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it isn’t already)\nNow double click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (for Windows machines; for Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon or Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18 and re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now… Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nThe #\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.\n\n\n\n5.2 Console 🅑\nThis is where you can type code that executes immediately. This is also known as the command line. Throughout the notes, we will represent code for you to execute in R as a different font.\n\n\n\n\n\n\nType it in!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really shouldn’t. The first reason is that you might unwittingly copy invisible PDF formatting errors into R, which will make the code fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your own errors. This is an invaluable way of learning and taking shortcuts now will only hurt you in the long run.\n\n\nEntering code in the command line is intuitive and easy. For example, we can use R as a calculator by typing into the Console (and pressing Enter after each line). Note that the output for every line of calculation (e.g. 6 * 3) is indicated by R&gt;, as we see here:\n\n6 * 3\n\n[1] 18\n\n\nR&gt; [1] 18\n\n5 + 4\n\n[1] 9\n\n\nR&gt; [1] 9\n\n2 ^ 3\n\n[1] 8\n\n\nR&gt; [1] 8\nNote that spaces are optional around simple calculations.\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\n[1] 9\n\n\nR&gt; [1] 9\nTo type the assignment operator (&lt;-) push the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\n[1] FALSE\n\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n\nWhat are the values after each hashed statement in the following? Use the RStudio Console to determine these values:\n\n\n\n\nmass &lt;- 48\nage &lt;- 78\nmass &lt;- mass * 2.0 # mass? \nage &lt;- age - 17 # age? m\nmass_index &lt;- mass / age # mass_index?\n\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n\nUse R to calculate some simple mathematical expressions entered. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y Display z in the console.\n\n\n\nWe can create a vector in R by using the combine c() function:\n\napples &lt;- c(5.3, 3.8, 4.5)\n\nA vector is a one-dimensional array (i.e., a list of numbers), and this is the simplest form of data used in R (you can think of a single value in R as just a very short vector). We’ll talk about more complex (and therefore more powerful) types of data structures as we go along.\nIf you want to display the value of apples type:\n\napples\n\n[1] 5.3 3.8 4.5\n\n\nR&gt; [1] 5.3 3.8 4.5\nFinally, there are default functions in R for nearly all basic statistical analyses, including mean() and sd() (standard deviation):\n\nmean(apples)\n\n[1] 4.533333\n\n\nR&gt; [1] 4.533333\n\nsd(apples)\n\n[1] 0.7505553\n\n\nR&gt; [1] 0.7505553\n\n\n\n\n\n\nVariable names\n\n\n\nIt is best not to use c as the name of a value or array. Why? What other words might not be good to use?\n\n\nOr try this:\n\nround(sd(apples), 2)\n\n[1] 0.75\n\n\nR&gt; [1] 0.75\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n\nWhat did we do above? What can you conclude from those functions?\n\n(Lab 2 continues in Lab 2b.)\n\n\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters app and then the Tab key. What happens?\nThe code completion feature also provides brief in-line help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl+Up to review the list (Cmd+Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane (see below).\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source pane using the buttons at the top-right or by double-clicking the title bar)\n\n5.3 Environment and History panes 🅒\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values and functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\n[1] \"a\"          \"age\"        \"apples\"     \"b\"          \"d\"         \n[6] \"mass\"       \"mass_index\"\n\n\nR&gt; [1] \"a\"        \"apples\"   \"b\"        \"pkgs_lst\" \"url\"\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?\n\n5.4 Files, Plots, Packages, Help, and Viewer panes 🅓\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where graphics you create will appear. The Packages tab shows you the packages that are installed and those that can be installed (more on this just now). The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console.\nMethods of getting help from the Console include…\n\n?mean\n\n…or:\n\nhelp(mean)\n\nWe will go into this in more detail in the next session.\nTo reproduced Figure 5 in the Plot tab, simply copy and paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.01)\ny &lt;- 2 * sin(2 * pi * (x - 1/4))\nggplot() +\n  geom_point(aes(x = x, y = y), shape = 21, col = \"indianred\", fill = \"steelblue\")+\n  theme_linedraw()\n\n\n\n\nFigure 5: An easy figure."
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html#references",
    "href": "BDC334/02a-r_rstudio.html#references",
    "title": "2a. R and RStudio",
    "section": "\n6 References",
    "text": "6 References\n\n\nR Core Team (2022) R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria"
  },
  {
    "objectID": "BDC334/02a-r_rstudio.html#session-info",
    "href": "BDC334/02a-r_rstudio.html#session-info",
    "title": "2a. R and RStudio",
    "section": "\n7 Session info",
    "text": "7 Session info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\n  forcats   stringr     dplyr     purrr     readr     tidyr    tibble   ggplot2 \n  \"1.0.0\"   \"1.5.0\"   \"1.1.0\"   \"1.0.1\"   \"2.1.4\"   \"1.3.0\"   \"3.1.8\"   \"3.4.1\" \ntidyverse \n  \"1.3.2\" \n\n\nR&gt;   forcats   stringr     purrr     readr    tibble   ggplot2 tidyverse     tidyr \nR&gt;   \"0.5.1\"   \"1.4.0\"   \"0.3.4\"   \"2.1.2\"   \"3.1.6\"   \"3.3.5\"   \"1.3.1\"   \"1.2.0\" \nR&gt;     dplyr     rvest \nR&gt;   \"1.0.8\"   \"1.0.2\""
  },
  {
    "objectID": "BDC334/Class_tests.html",
    "href": "BDC334/Class_tests.html",
    "title": "Class tests",
    "section": "",
    "text": "Discuss the unimodal species distribution model, and describe how this model can explain the structuring of communities along environmental gradients. In your discussion, also talk about β-diversity.\n\n\nThe unimodal model is an idealised species response curve where a species has only one mode of abundance—i.e. one locality on the landscape where conditions are optimal and it is most abundant (i.e. the fewest ecophysiological and ecological stressors are present there). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance (a lower fitness). The unimodal model in the most basic sense can be seen as a Gaussian curve, and it offers a convenient heuristic tool for understanding how species can become structured along environmental gradients. Multiple unimodal distributions are often visualised as a coenocline—a graphical display of all species response curves, which shows how a species’ fitness is affected by any one of a multitude of environmental variables e.g. pH in Figure 1.\n\n\n\nA coenocline.\n\n\n\\(\\beta\\)-diversity is a concept that describes how species assemblages (communities) measured within the ecosystem of interest vary from place to place, e.g. along the various transects or among the quadrats used to sample the ecosystem. \\(\\beta\\)-diversity can result from the gradual change in environmental characteristics along gradients. This can be clearly seen in a coenocline, where the modal centre of distribution of many species is arranged at different positions along an environmental gradient. See Figure 1. As Species A becomes less abundant when its physical distance away from the place on the landscape which is most conducive to its fitness increases, so it is replaced by Species B at a distant location where its environmental conditions are optimal. And so on with all the other species along the length of the gradient. This process is called environmental filtering, which results in a decrease in similarity as the distance between sites increases—sometimes this is called the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-1",
    "href": "BDC334/Class_tests.html#question-1",
    "title": "Class tests",
    "section": "",
    "text": "Discuss the unimodal species distribution model, and describe how this model can explain the structuring of communities along environmental gradients. In your discussion, also talk about β-diversity.\n\n\nThe unimodal model is an idealised species response curve where a species has only one mode of abundance—i.e. one locality on the landscape where conditions are optimal and it is most abundant (i.e. the fewest ecophysiological and ecological stressors are present there). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance (a lower fitness). The unimodal model in the most basic sense can be seen as a Gaussian curve, and it offers a convenient heuristic tool for understanding how species can become structured along environmental gradients. Multiple unimodal distributions are often visualised as a coenocline—a graphical display of all species response curves, which shows how a species’ fitness is affected by any one of a multitude of environmental variables e.g. pH in Figure 1.\n\n\n\nA coenocline.\n\n\n\\(\\beta\\)-diversity is a concept that describes how species assemblages (communities) measured within the ecosystem of interest vary from place to place, e.g. along the various transects or among the quadrats used to sample the ecosystem. \\(\\beta\\)-diversity can result from the gradual change in environmental characteristics along gradients. This can be clearly seen in a coenocline, where the modal centre of distribution of many species is arranged at different positions along an environmental gradient. See Figure 1. As Species A becomes less abundant when its physical distance away from the place on the landscape which is most conducive to its fitness increases, so it is replaced by Species B at a distant location where its environmental conditions are optimal. And so on with all the other species along the length of the gradient. This process is called environmental filtering, which results in a decrease in similarity as the distance between sites increases—sometimes this is called the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-2",
    "href": "BDC334/Class_tests.html#question-2",
    "title": "Class tests",
    "section": "2 Question 2",
    "text": "2 Question 2\nUsing a clear example that you can easily relate to, discuss the concept of ‘ecological infrastructure.’ In your explanation, mention other (i.e., in addition to the ‘infrastructural services’) ecological services this example ecosystem offers and any other benefits that people might derive from its existence and well-being. In your discussion, explain how the ecological infrastructure works (what it does and how) in a properly functioning ecosystem and, if people destroyed it, how we might replicate its service.\n\n2.1 Answer\n\n2.1.1 Wetlands as ecological infrastructure\nWhat is ecological infrastructure? Ecological infrastructure is natural ecosystems that provide services beneficial to people. These services would, in the absence of ecological infrastructure, have to be provided by engineering solutions.\nBenefits people derive from wetlands People tend to develop settlements, towns and cities in low-lying areas such as flood plains around estuaries. These areas are prone to periodic rising water levels, and recently they are also more and more being impacted by extreme floods (associated with climate change). Healthy flood plains often comprise wetlands, which are habitats occupied by dense emergent macrophytes along the edges of estuaries and flood plains. These systems can provide a buffer to rising water levels, and they may reduce the flow rate of water. People can benefit from intact wetlands as this buffer zone provides a level of protection to built structures in the vicinity of the estuaries. Wetlands also purify the water (water filtration removes excessive N, P and POM), which makes for an environment that is more supportive of good human health (fewer water borne diseases and pollutants which may be a public health concern).\nHowever, often wetlands are destroyed by dredging and then filled in to make area available for occupation by people. In such transformed systems, protection against floods and rising water levels can be provided by constructing engineered systems at great cost. Examples of such engineered systems include breakwaters and levees. These systems, however, do not provide the other services required for maintaining good water quality, and additional engineering solutions, costing yet more tax-payers money, need to be provided. Additionally, downstream natural areas on which people depend will also become increasingly impacted due to the deterioration or loss of wetlands, and engineering solutions cannot mitigate against such consequences.\nThus, ecological infrastructure provide services to people simply by virtue of being maintained in a healthy state. This economic cost of achieving this is virtually non-existent, provided people act responsibly to protect these systems.\nEcological services from wetlands Wetlands provide a complex 3D habitat that provides numerous ecological services to a host of associated fauna and flora. These biotic assemblages benefit from their association with wetlands from the feeding/foraging opportunities provided within the habitat structure, the breeding and nursery grounds wetlands provide, attachment surfaces on the wetland plants and the sediments trapped within, and shelter and hiding opportunities from predators. Wetlands also reduce the flow rate of the water passing through them, and as such the still water within is attractive to some species that are unable to tolerate faster flow rates. Typically, healthy wetlands are active in their cycling (uptake) of N and P, and as such the water quality may be better compared to surrounding areas. This is also true for decreasing the water turbidity due to their filtration services. This makes wetlands ideal environments to some species that are sensitive to pollution. &lt;Many more services are provided by the sediments in wetlands, which offer additional opportunities for enhancing biodiversity in these areas&gt;. Overall, the net effect it that it supports species diversity, i.e. higher biodiversity in landscapes with functional wetlands present compared to areas without wetlands. Higher species diversity also offer many bequest services, and offer a potential source of genetic diversity and materials to assay for important bio-active substances that might be useful to people."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-3",
    "href": "BDC334/Class_tests.html#question-3",
    "title": "Class tests",
    "section": "3 Question 3",
    "text": "3 Question 3\nDiscuss the general characteristics of species tables, environment tables, and dissimilarity and distance matrices we can derive from these tables.\n\n3.1 Answer\n\nEnvironmental tables Environmental tables have variables down the columns (headings are the names of the env vars) and the sites run across the columns (row names are the names of the sites), with one site in a row. Different kinds of environmental variables can be contained in the table, as many as the researcher thinks is necessary to explain the patterns in the species tables. In the cells are the quantities of the various environmental variables measured at the different sites. The measurement units may differ between columns, so later, before analysis, these data must be standardised.\nSpecies tables The species tables have as many rows as the number of rows in the environmental table—so, for each site where species are recorded, there will be corresponding measurements of the environmental conditions there. Rows in a species table have the same orientation and meaning as in the environmental table. The columns, however, contain the names of the species recorded at the sites. In the cells is some quantity that reflects something about the species at the sites—it might indicate whether a species is there or not (presence/absence), its relative abundance, or biomass. The way in which the species are quantified must be the same across all columns.\nDissimilarity matrix The dissimilarity matrix is derived from the species table by calculating one of the species dissimilarity indices (Bray-Curtis, Sørensen, Jaccard, etc.). It is square and symmetrical, and the diagonals are zero because they are essentially comparing sites with themselves in respect to the kinds of species and their abundance or presence/absence there. A value of 1 would mean that the sites are completely different from each other—this would be seen in a similarity matrix, which is the inverse of a dissimilarity matrix. Each of the other cells represent the community difference between a pair of sites whose names are present as column or row headers.\nDistance matrix A distance matrix is produced from a standardised environmental table. It is square and symmetrical, and there are as many rows and columns as there are variables in the environmental table. This matrix reflects how similar/dissimilar pairs of sites are with regards to the environmental conditions present there. The interpretation of the diagonal is the same as in dissimilarity matrices."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-4",
    "href": "BDC334/Class_tests.html#question-4",
    "title": "Class tests",
    "section": "4 Question 4",
    "text": "4 Question 4\nProvide a short explanation, with examples, for what is meant by this statement:\n“Communities often seem to display very strong structural graduation relative to ‘variables’ such as altitude, latitude, and depth; however, these variables are not the actual drivers of the processes that structure communities.”\n\n4.1 Answer\nAltitude, latitude, and depth serve to indicate the position sites on Earth’s surface. They do not have physical properties associated with them. Species cannot require altitude, latitude, or depth to sustain their physiological needs. They are merely proxy variables for other variables that can affect the physiology of the species occurring there. I am less interested in how beta-diversity (turnover, niche models, unimodal models) works, and more interested in how the proxy relationships might play out. For example, …"
  },
  {
    "objectID": "BDC334/Class_tests.html#question-5",
    "href": "BDC334/Class_tests.html#question-5",
    "title": "Class tests",
    "section": "5 Question 5",
    "text": "5 Question 5\nIt is the year 2034 and as a result of a decade of campaigning the South African Green Party has become a real contender to be the runner up behind the populist EFF, which has come into power in South Africa in 2029.\nAs the leader of the Green Party, write an Opinion Piece that outlines the ecological solutions your party has to offer for when (if) it becomes the official opposition to the current ruling party. Your party’s ecological solutions offer the promise of solving many of the socio-economic solutions that face South Africans in the 2030s.\n\n5.1 Answer\nThis is an opinion piece and an expected answer is not available.\nIn this answer I am looking for how ecosystems’ ecological services and goods may be used for the betterment of people, the environment, and the economy. I am not looking for a listing of SA’s problems. I am not looking for way in which budgets can be better spent, or how enforcement can be improved. I am also not really looking for the implementation of renewable energy sources as wind or solar (although that will definitely be part of the solution). We know that hunger needs to be alleviated; people must be educated; we need better farming techniques; developments must be sustainable; and people’s economic freedom ensured. But how? How can we use nature’s solutions to do so?\nWe need to build into the various initiatives a reliance on the country’s natural infrastructure. We can also develop novel, fit-for-purpose ‘ecological’ infrastructure that incorporate many of the principles of natural ecosystems with the same kinds of benefits to people (e.g. roof-top gardens, integrated forming and aquaculture, etc.).\nThe essay must consider these kinds of things."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-6",
    "href": "BDC334/Class_tests.html#question-6",
    "title": "Class tests",
    "section": "6 Question 6",
    "text": "6 Question 6\nExplain in a short (1/3 page paragraph) what is meant by ‘environmental distance.’\n\n6.1 Answer\nEnvironmental distance encompasses all the characteristics of a landscape, such as measurements of the variables temperature, water content, soil nutrient concentrations, pH, etc., in a manner that makes it possible to provide a single, integrative metric that informs the researcher how similar or different sites across the landscape are to each other. Environmental distances are typically calculated as Euclidian distances (using the Pythagorean Theorem), but others are available such as Gower’s or Manhattan Distances and can be used for specific needs. In R they can be calculated using the vegdist() function in the vegan package. The calculation results in a pairwise distance matrix, with each cell value containing the environmental distance between a pair of sites. All possible combinations of site pairs are represented in this square matrix. The larger the value between two sites—the distance—the more different sites are with respect to their environmental properties. These distances can be used as explanation for how species communities differ across the landscape, such that sites with large environmental distances between them typically develop very different ecological communities."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-7",
    "href": "BDC334/Class_tests.html#question-7",
    "title": "Class tests",
    "section": "7 Question 7",
    "text": "7 Question 7\nExplain how the data in the site-by-species matrix can be transformed into species-area curves. What are species area curves, and what explains their characteristic shape? What is the purpose of these curves?\n\n7.1 Answer\nTaken mostly directly from the online resource.\nSpecies accumulation curves (species area relationships, SAR) try and estimate the number of unseen species. These curves can be used to predict and compare changes in diversity over increasing spatial extent. Within an ecosystem type, one would expect that more and more species would be added (accumulates) as the number of sampled sites increases (i.e. extent increases). This continues to a point where no more new species are added as the number of sampled sites continues to increase (i.e. the curve plateaus). It plateaus because if a homogeneous landscape is comprehensively sampled, there will be a point beyond which no new species will be found as we sample even more sites.\nSpecies accumulation curves, as the name suggests, works by adding (accumulation or collecting) more and more sites along \\(x\\) and counting the number of species along \\(y\\) each time a new site is added. In the community matrix (the sites × species table), we can do this by successively adding more rows to the curve (seen along the \\(x\\)-axis). In other words, we plot on \\(y\\) the number of species associated with 1 site (the site on \\(x\\)), then we plot the number of species associated with 2 sites (the sum of the number of species in Site 1 and Site 2), then the number of species in Sites 1, 2, and 3. Etc. We do this until the cumulative sum of the species in all sites has been plotted in this manner. Typically some randomisation procedure is involved (the order in which sites are added up is randomised)."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-8",
    "href": "BDC334/Class_tests.html#question-8",
    "title": "Class tests",
    "section": "8 Question 8",
    "text": "8 Question 8\nUsing South African examples, discuss the principle of distance decay of similarity in biogeography and ecology.\n\n8.1 Answer\nTo follow tomorrow (I’m tired now)."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-9",
    "href": "BDC334/Class_tests.html#question-9",
    "title": "Class tests",
    "section": "9 Question 9",
    "text": "9 Question 9\nPlease refer to Figure 1: \n\n\n\nAn environmental distance matrix.\n\n\n\nTo graphically represent distance decay, we typically plot the data in the first column or first row (they are the same) of an environmental distance matrix. Why this row/column? What is unique about the first row/column? [3]\nHow does the information in the first row/column differ from that in the subdiagonal? [3]\nWhat information is contained in any other cell in the environmental distance matrix? [2]\nWhat values are in the blanks down the diagonal? Why are these values what they are? [2]\n\n\n9.1 Answer\n\nLooking down the first column, the environmental distance tends to increase the further a site is from Site 1. This is because sites further away from the origin (Site 1) tend to become increasingly dissimilar in terms of their environmental conditions as a host of drivers impact on (e.g. in the Doubs data) the water quality variables—–e.g. near the terminus of the river several pollutants will have perturbed the system (flatter slopes are more conducive to polluting human developments). Typically the increasing environmental distance that develops further away from the origin can directly be attributed to a few very influential variables; again, in the Doubs data, it is the variables nitrate, ammonium, flow rate, and biological oxygen demand that primarily affect the trend in environmental distance. At the source, there are pristine conditions (low DIN and low BOD) and near the terminus sites are polluted. Similar explanations to this one can be developed for a host of environmental gradients (e.g. along the coastline of SA where there is a temperature gradient; across the country along the rainfall gradient; with altitude; with depth; etc.). Any of these can be used as examples.\nWhereas the diagonal compares a site with itself, the subdiagonal (the diagonal row just one up or down from the diagonal filled with zeroes) captures the difference in environmental conditions (environmental distance) between adjoining sites (Site 1 vs Site 2, Site 2 vs Site 3, Site 3 vs Site 4, etc.). These changes are far more gradual than along the first row or down the first column. This is because the physical distance in geographical space is quite small for sites that are positioned next to one-another, and so too will be the environmental distance. Plotting these on a graph with environmental distance on \\(y\\) and the adjacent site pairs on \\(x\\) will generally yield a flat(-ish) line.\nAny other cell simply compares any arbitrary site with any other in terms of the difference in environmental conditions between them. This environmental distance will also be (generally) quite closely related to the physical geographical distance (or altitude, depth, etc.) between the sites.\nThe ‘blanks’ are actually zeroes, which you would get if one would compare a site with itself. There is no difference between a site and itself, so hence no environmental difference between them."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-10",
    "href": "BDC334/Class_tests.html#question-10",
    "title": "Class tests",
    "section": "10 Question 10",
    "text": "10 Question 10\n\nGiven a set of environmental data (e.g. pH, temperature, light, total N concentration, conductivity), what is the first step to follow prior to calculating environmental distance? Why is this necessary? [3]\nProvide an equation for how you would accomplish this first step. [2]\nWhat is the name of the equation / procedure to follow in the calculation of ‘environmental distance’? [1]\nDescribe the principle of ‘environmental distance’. [9]\n\n\n10.1 Answer\n\nThe first step would be to standardise the data. This is necessary because the different environmental variables are represented by different measurement scales (units). So, to prevent those with the largest magnitude (e.g. altitude, which is measured in 100s or 1000s of meters) to become the dominant ‘signal’ in the overall response when measured alongside something like temperature (10s of degrees Celsius), they have to be adjusted to comparables scales.\nStandardisation involves calculating the mean of a variable, \\(x\\), and then subtracting this mean from each observation, \\(x_{i}\\). This value is then divided by the standard deviation of \\(x\\). So, something like \\(x_{i} - \\bar{x} / \\sigma_x\\).\nTheorem of Pythagoras, or Euclidian distance.\nEnvironmental distance encompasses all the characteristics of a landscape, such as measurements of the variables temperature, water content, soil nutrient concentrations, pH, etc., in a manner that makes it possible to provide a single, integrative metric that informs the researcher how similar or different sites across the landscape are to each other. Environmental distances are typically calculated as Euclidian distances (using the Pythagorean Theorem), but others are available such as Gower’s or Manhattan Distances and can be used for specific needs. In R they can be calculated using the vegdist() function in the vegan package. The calculation results in a pairwise distance matrix, with each cell value containing the environmental distance between a pair of sites. All possible combinations of site pairs are represented in this square matrix. The larger the value between two sites—the distance—the more different sites are with respect to their environmental properties. These distances can be used as explanation for how species communities differ across the landscape, such that sites with large environmental distances between them typically develop very different ecological communities."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-11",
    "href": "BDC334/Class_tests.html#question-11",
    "title": "Class tests",
    "section": "11 Question 11",
    "text": "11 Question 11\nWhat makes macroecology different from the traditional view of ecology?\n\n11.1 Answer\nMacroecology is an all-encompassing view of ecology, which seeks to define the geographical patterns and processes in biodiversity across all spatial scales, from local to global, across time scales from years to millennia, and across all taxonomic hierarchies (from genetic variability within species, up to major higher level taxa, such as families and orders). It attempts to arrive a unifying theory for ecology across all of these scales—e.g. one that can explain all patterns in structure and functioning from microbes to blue whales. Most importantly, perhaps, is that it attempts to offer mechanistic explanations for these patterns. At the heart of all explanation is also deep insights stemming from understanding evolution (facilitated by the growth of phylogenetic datasets—see below).\nThis is a modern development of ecology, whereas up to 20 years ago the focus has been mostly on populations (the dynamics of individuals of one species interacting amongst each other and with their environment) and communities (collections of multiple populations, and how they interact with each other and their environment, and how this affects the structure and dynamics of ecosystems).\nOn a basic data analytical level, population ecology, community ecology, and macroecology all share the same approach as far as the underlying data are concerned. We start with tables of species and environmental conditions (along columns) at a selection of sites (along rows), and these are converted to distance and dissimilarity matrices. From here analyses can show insights into how biodiversity is structured, e.g. species-abundance distributions, occupancy-abundancy curves, species-area curves, distance decay curves, and gradient analyses. In the last decade, modern developments in statistical approaches have contributed towards the development of macroecology, because of the growth of hypotheses-driven multivariate statistical approaches geared to test for the presence of one or several ecological hypotheses—this was not seen in population and community ecology so much. Contributing towards the growth of macroecology and the underlying statistical approaches, the deluge of new data across vast scales has also necessitated deeper analytical development, i.e. leveraging statistical tools and also the power of modern computing infrastructure. These modern approaches are also bringing into the fold of combined computations based on species and environmental tables also data on the phylogenetic relationships amongst organisms (and hence this brings the context of evolution)."
  },
  {
    "objectID": "pages/case_for_promotion.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "href": "pages/case_for_promotion.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department",
    "text": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department\nCapitalising on an extensive history of curriculum development5, I have played a vital role in revitalising the core BSc (Hons) module, BCB744 Biostatistics, and in creating the innovative elective BSc (Hons) module, BCB743 Quantitative Ecology. My deep fascination with biological, ecological, and environmental data underpins these modules, fuelling my passion for data processing, analysis, interpretation, and the invaluable insights that emerge from such data-driven enquiries.5 I was instrumental in developing South Africa’s first undergraduate Marine Biology curriculum at the University of KwaZulu-Natal in 2007 (with Profs. Perissinotto and Schoeman)\nR, an open-source software ecosystem extensively adopted by ecologists, is the cornerstone of my core and elective BSc (Hons) modules. The increasing number of research papers and publications in biology and ecology utilising R and its packages attests to its importance. In academic settings such as UWC, Africa, and less developed countries, open-source software removes potential licensing obstacles presented by limited financial resources. This allows universal access to the software, enhancing scientific reporting, collaboration, and the principles of reproducible research, while fostering a culture of technological infusion6.6 See a discussion about how I allow modern technologies to influence and shape my teaching\nAnother new module, BDC334 Global Biogeography & Macroecology, for which I share 50% of the credit for its development, is less data-intensive. This module lays the groundwork for engaging with species and environmental data matrices from which functional ecological processes can be extracted. Recent feedback from students who completed this module in 2022 indicated that exposure to more data-intensive coursework and an introduction to basic coding skills significantly alleviated the anxiety many students feel about coding (scripting). They further suggested that this exposure smoothed their transition into BCB744, the core module they undertake at the start of their BSc (Hons) degrees.\nCollaborative learning is a cornerstone of my teaching approach7, the benefits of which I discuss in my online teaching materials. I use engaging teaching tools to instil interest in my subjects. For example, figures and maps8 serve as critical heuristic devices throughout the modules. The visually appealing and information-rich outcomes of their learning efforts provide an immediate measure of success. In this way, students develop programming skills by breaking down problems into computable parts, whilst also enhancing their visual literacy skills. This engaging and interactive approach is deeply integrated with an agile assessment policy that evaluates teaching and learning9 10. My modules demystify coding, making it more accessible and enjoyable for beginners.7 Views on collaborative learning8 Example exercises and bonus, designed to reward and incentivise continued learning towards advanced skills9 Assessment policy for BCB74410 Explanation of modes of assessment\nThe skills learnt and the graduate attributes11 developed are designed to produce competencies outside the narrow confines of Biodiversity and Conservation Biology. Transferable core skills include compartmentalising complex problems and finding analytical solutions to problems in diverse fields such as finance, market research, and data science. Many students who graduate with a BSc (Hons) course from the BCB Department will, without requiring further training, have the same skills as someone who has completed a data science course.12 Many of our graduates will not pursue a research-focused career, yet they would like to continue benefiting from the skills gained at the BCB Department.11 Module-specific graduate attributes12 The difference between science and data science\nStructured outlines of the syllabus, timetables, course content, learning outcomes, required and recommended reading, assessment policies, advice for success (e.g. how to learn to understand13), model answers to old tests and exam questions (e.g. for BDC33414), and much else, are made available for all modules. During 2023 I will continue to build upon existing content and expand my approach to the other module I teach, BDC223 Plant Ecophysiology.13 Thoughts about the learning process14 Access to old test and exam questions"
  },
  {
    "objectID": "pages/case_for_promotion.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "href": "pages/case_for_promotion.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules",
    "text": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules\nWhile I’m not particularly fond of PowerPoint slides, I recognise their utility in structuring lectures. My preference leans towards long-form, information-rich text for delivering in-depth content15. Ideally, I would base my teaching on textbooks, but these are not accessible to all our students. In our fast-paced world, information can quickly become outdated, posing a challenge to addressing students’ evolving learning and knowledge needs. The reality is that many students are averse to reading. To overcome this, I’ve developed and continue to enhance The Tangled Bank, a teaching-oriented website tailored to the needs of students enrolled in my Level-3 and BSc (Hons) modules. Leveraging the website format, I can ensure timely updates of knowledge and technologies in response to the swiftly changing scientific landscape and students’ learning requirements and feedback.15 For an example of information rich text, see the example page\nThe Tangled Bank serves as my main repository for lecture content and a continually expanding knowledge base for guiding research within my areas of focus. This website preserves invaluable behind-the-scenes insights16, contributes to the development of online textbooks, consolidates frequently asked questions about module content which ensures responsiveness to students17, and reinforces BCB Department modules by integrating relevant examples from my colleagues’ work18. The Tangled Bank aids peers in overcoming module-specific challenges, thereby enriching the learning experience.16 See the ‘vignettes’ menu at the top of The Tangled Bank.17 For example, the FAQ page for BDC22318 See feedback from colleagues about The Tangled Bank\nProviding students access to long-form written teaching materials and instilling an expectation to engage with this content are pivotal in preparing students for their undergraduate and graduate degree programs. Long-form content facilitates a thorough exploration of ideas, offering context, nuances, and essential background information that enable students to understand complex concepts. By immersing themselves in comprehensive texts, students can cultivate a profound understanding of intricate topics, empowering them to think critically and analytically.\nContrary to summarised bullet points, which can oversimplify and condense information, possibly omitting crucial details, long-form materials motivate students to delve deep into a subject and contemplate various perspectives. This approach fosters intellectual curiosity and instills a genuine interest in the subject, promoting a culture of lifelong learning. Engaging with long-form content allows the motivated student to build a robust knowledge base rooted in self-driven learning, forming a firm foundation for their future academic and professional pursuits. As an educator, this is my aspiration.\nFurthermore, interacting with long-form written materials enhances students’ reading comprehension skills. As they sift through dense texts, they learn to distinguish main ideas, supporting arguments, and potential counterarguments. This process refines their capacity to analyse and evaluate information—an essential skill in both academic and professional environments. Improving this skill is particularly crucial for the younger generation.\nBy supplying students with comprehensive content, I aim to foster a deeper appreciation for their chosen field, thus equipping them for success in their academic and professional journeys.\nLastly, The Tangled Bank strives to provide a detailed overview and breakdown of each module’s syllabus, including:\n\nan up-to-date timetable and links to each lecture’s material and assessments,\ninformation about the desired learning outcomes and graduate attributes,\nadditional supporting information,\nprerequisites,\nthe method of instruction,\nviews on the benefits of colaborative learning,\nattendance policies,\nassessment policies, and\nsupport.\n\nPlease refer to BDC33419, BCB744,20, and BCB74321 for the above-mentioned information.19 The BCB744 module syllabus and course outline20 The BCB743 syllabus and course outline21 The BDC334 syllabus and course outline"
  },
  {
    "objectID": "pages/case_for_promotion.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "href": "pages/case_for_promotion.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught",
    "text": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught\nThe following feedback was received from Prof. Sophie von der Heyden for BCB74322 following her assessment of the module in her capacity of External Module Evaluator for the BCB BSc (Hons) Programme: “This is an excellent course; I really appreciate that everything is online and very easy to follow. The course is appropriate and challenging at the Honours level, but there also seems excellent support for the students. Really a standout module.” Further, she says, ”There was a wide range of marks, from 45 – 88%, with only one student […] failing this module. Given that students can really struggle with R, it was good to see how well the class did overall. I think part of this is the breakdown into the multiple assignments, which allows students to build on their knowledge as the tasks get more difficult, rather than being overwhelmed with one large assignment.”22 Prof. Sophie von der Heyden’s feedback about BCB743 in 2022\nAbout BCB744,23 she says, “As with BCB743, I was very impressed by this course, particularly how easy it is to navigate around the online component. I am sure that the students will be able to access all the necessary components fairly easily. The course is very much at the level of Honours and I hope that for the final projects the students utilize their learning from this course.”23 Prof. Sophie von der Heyden’s feedback about BCB744 in 2022\nHowever, Prof von der Heydon’s comment on the question about whether the marks were assigned appropriately, she said, “This is a little difficult to comment on as I could not see how the marks were awarded, but given the consistency of marks for each student, I think that the marks are all appropriate.”\nSince the module content is continually being developed, expanded, and improved, I addressed Prof. von der Heydon’s concern about mark allocation by providing clear assessment policies for BCB74424, BCB74325, and BDC334.26 Further, the module content on The Tangled Bank has dramatically improved in all aspects since the modules were last evaluated at the end of 2022.24 BCB744 assessment policy25 BCB743 assessment policy26 BDC334 assessment policy\nFeedback from students about the modules is also available.27 Six students from a class of 14 responded to the module evaluation forms in 2022. Feedback about students’ experience with the module was positive for most of the questions, but 50% of the respondents felt that better feedback could be given to individual tasks. A third of the sample also indicated they felt uncertain about the module’s expectations.27 Student feedback about BDC223, BDC334, BCB744, and BCB743 are available at on Google Drive\nEighteen students took BCB744 in 2022, and eight provided feedback on the module. As with BCB743, the feedback was similar. Four students felt they could benefit from more comprehensive feedback, and three respondents felt somewhat uncertain about my expectations of them (including the quality of their work). Additionally, two students felt I could better explain concepts and give them more time to understand them. Another negative comment given by two students was that they could be better empowered to explore a variety of sources better to complete assessment tasks.\nThe BDC334 class comprised 41 students in 2022, and only five students tried to provide feedback. One person felt a mismatch between the assessment and the module’s content. Five students thought feedback on individual assessments could be better. There was also one instance of dissatisfaction with the following: sufficient time for communication, my effort to understand their challenges, and uncertainty about expectations. Feedback on BDC223 in 2022 was poor, with only nine responses. Their satisfaction with the module was mixed and polarised into two distinct groups. About 50% of respondents provided much of the same feedback as I received for BCB744, BCB743, and BDC334, and these people felt that feedback on individual assignments could be better. The other half had more negative experiences and I received negative feedback for several other questions. My experience with this class in 2022 was anomalous, as it is singular as the worst class I have ever taught at University. Ever."
  },
  {
    "objectID": "pages/case_for_promotion.html#tangled-bank-vignettes-and-reproducible-research",
    "href": "pages/case_for_promotion.html#tangled-bank-vignettes-and-reproducible-research",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.4. Tangled Bank vignettes and reproducible research",
    "text": "4.1.4. Tangled Bank vignettes and reproducible research\nInterdisciplinary research brings together a variety of expertise, resulting in challenges related to teamwork, data sharing, and coding. The importance of transparency in research methodologies, like reproducible research,28 is ever increasing. Conforming to FAIR principles, international standards, and discipline-specific norms is indispensable. Even though software provides solutions, numerous researchers require assistance to stay abreast and capitalise on new expectations and possibilities.28 See my essay on eResearch and reproducible research\nPhD students typically devote 3-4 months to active thesis writing, which often serves as the only tangible evidence of degree completion. However, the vast majority of the learning and methodological skills developed over the remaining 33-44 months often become lost and unshared, leading to duplicated research efforts and restricted knowledge transfer. This failure to share behind-the-scenes solutions often results in non-reproducible research and collaboration difficulties, sometimes even contributing to public mistrust in science. Furthermore, better scalability is needed as datasets and complexities grow, and inefficiencies due to inadequate documentation of data selection, filtering, metadata tracking, and processing changes need addressing.\nThe Tangled Bank is designed to encourage knowledge retention and transfer, both of which are crucial for success in the information economy. To tackle these issues, my research students craft lab notebooks using tools like RStudio or Jupyter Lab/Notebooks and monitor version changes with git (e.g., GitHub). These notebooks combine code and text, automatically updating results as new data become available, thereby ensuring reproducibility in their work.29 30 31 I emphasise these same principles in both undergraduate and postgraduate courses I teach. The website also includes a series of vignettes32 that capture some of the analytical data workflows that often raise questions. These vignettes will continually be updated, and more examples documenting my own and my colleagues’ data and statistical analysis challenges will be preserved here for posterity.29 Dr Robert Schlegel’s GitHub page30 Ms Amieroh Abrahams’s GitHub page31 Mr Ross Coppin’s GitHub page32 Examples of vignettes may be accessed at The Tangled Bank under the ‘vignettes’ menu at the top.\nOther vignettes are at the heatwaveR website.3333 The heatwaveR website—see the vignettes in the top menu."
  },
  {
    "objectID": "pages/case_for_promotion.html#successful-and-prolific-funding-attraction",
    "href": "pages/case_for_promotion.html#successful-and-prolific-funding-attraction",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.1. Successful and prolific funding attraction",
    "text": "4.2.1. Successful and prolific funding attraction\nMy H-index on Google Scholar is currently 2934, which ranks as the second highest in the BCB Department. As of 29 June 2023, the site has recorded a total of 4,167 citations, 2112 of which were garnered since 2018. Since joining UWC in 2014, my annual publication average stands at approximately five; however, this rate has somewhat dropped in light of the COVID-19 pandemic. With the induction of a new cohort of students into my postgraduate research group, I anticipate a resurgence in the publication rate.34 See my Google Scholar page\nMy leadership and management skills, cultivated over the past eight years, are demonstrated by my significant success in securing funding from national and international research programmes35. Moreover, I’ve successfully seen these programmes through to completion, aligning with well-defined goals and objectives. Since 2014, these research endeavours have cumulatively raised an estimated ZAR 28.74 million, bolstering the sustainability of research efforts for myself, my collaborators, and my students.35 List of national and international research funding received\nHistorically, I have primarily relied on the NRF for funding. However, in recent years, I have been diversifying my collaborations internationally. This strategy is facilitated by accessing global funding streams, such as those provided by the European Union, the Belmont Forum, and the SANOCEAN programme. These sources not only leverage funding from partnering countries, but they also foster a degree of collaboration that exceeds what is typically feasible with South Africa-centric funding.\nPreviously, I held a C2 rating, but chose to let it lapse after thoughtful consideration. I’ve expanded on my views regarding the rating system elsewhere36. Thus far, I’ve found that having an NRF rating does not necessarily enhance the likelihood of obtaining research funding.36 My thoughts about the NRF rating system and maintaining my own rating"
  },
  {
    "objectID": "pages/case_for_promotion.html#development-of-r-packages-in-marine-heatwave-analysis",
    "href": "pages/case_for_promotion.html#development-of-r-packages-in-marine-heatwave-analysis",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.2. Development of R packages in marine heatwave analysis",
    "text": "4.2.2. Development of R packages in marine heatwave analysis\nOne of my most distinctive and significant research contributions is the creation of two R packages: RmarineHeatWaves37 and heatwaveR38. These tools emerged as a response to the formal definition of marine heatwaves proposed by Alistair Hobday and his team in 2016. The algorithm to detect marine heatwaves based on standardised metrics was first published as an R package under the name RmarineHeatWaves, and later updated to heatwaveR in 2017. This software has since been downloaded more than 32k times39 by the international scientific community and has been cited in over 150 peer-reviewed papers since 201840. I, alongside Dr. Robert Schlegel, my former UWC PhD student, continue to maintain and enhance this package, introducing new functionalities in response to the needs of our user community.37 The RmarineHeatWaves documentation.38 heatwaveR. Also see the GitHub page39 The RmarineHeatWaves documentation.40 This number is hard to track, but a search in Google Scholar for the term “heatwaveR” (inverted commas included) yields at least 150 citations. A shorter list of the citations is provided on the heatwaveR website. Notable examples of high-impact publications are provided here\nThe influence of this R package on the global marine heatwave research community cannot be overstated. The standardisation of metrics it offers facilitates a more consistent global study of these events. Prior to its release, these tools were largely available only to physical oceanographers who primarily use Python; publishing it in R extended its reach to biologists and ecologists. This has sparked interdisciplinary collaboration across fields like oceanography, climatology, and ecology41. Interestingly, it is now being applied in areas beyond its initial intended marine scope, such as public health42, demonstrating its broad and unexpected utility.41 Examples of cross-discipline research in marine heatwaves promoted by the heatwaveR package are provided here42 Evidence of the application of the heatwaveR package outside of the initially intended field of application, marine science, here.\nGiven the consistency in reporting Marine Heat Wave (MHW) metrics, the quality of decision-making by policy-makers and resource managers has been significantly enhanced. For instance, gaining a more refined understanding of MHWs aids in devising strategies to mitigate the environmental repercussions of extreme thermal events, as well as adapting to their influences on fisheries and other marine resources43.43 For studies that have used metrics calculated by heatwaveR in support of policy development around the management of marine living resources, see this list\nFurther, heatwaveR also led to the development of novel research questions and hypotheses that better analyse and compare MHWs across different periods and regions and employ the metrics to design creative experiments that better link ecological impacts to precisely quantifiable properties of the temperature record.4444 Evidence of examples where such novel research questions and hypotheses have been addressed\nFinally, the heatwaveR package raises public awareness about MHWs and their impacts on marine ecosystems by making it easier for researchers to communicate their findings to a broader audience. For example, the marine heatwave tracker built by Dr Schlegel uses the heatwaveR package in the background.4545 Various online trackers of marine heatwaves use heatwaveR as the underlying processing engine"
  },
  {
    "objectID": "pages/case_for_promotion.html#student-supervision",
    "href": "pages/case_for_promotion.html#student-supervision",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.3. Student supervision",
    "text": "4.2.3. Student supervision\nMy UWC student supervision record is provided in my e-Portfolio.46 The record indicates 16 BSc (Hons) graduates, 11 MSc/MPhil graduates, and 7 PhD graduates. Appearing on the online NRF online system as active and continuing is Mr Phumlile Cotiyane, a PhD candidate registered with SAEON’s Elwandle Node whom I am co-supervising. Including postgraduate supervision prior to my tenure at the UWC in 2014 brings my career total to 57 graduates, across all levels.46 Extract from the NRFOnline system listing most of my post-graduate students\nI have five active MSc students (Ms Cayley Cammel, Mr McQuwaen Moonoosamy, Mr Jesse Philips, Mr Tom Spencer-Hicken, and Ms Carlin Landsberg) and four active BSc (Hons) candidates, Ms Aailyah Samsodien, Ms Zoë-Angelique Petersen, Mr Taine Trimmel, and Mr Isma-eel Jattiem. Since these students receive free-standing bursaries from the NRF, their names do not yet appear in my NRF database under the list of students associated with my research profile. This also applies to Ms Zara Prew, an active PhD student in my research group.\nRoughly 49% of all the individuals, above, are of previously disadvantaged backgrounds, and 12% were with my role as co-supervisor.\nI have had three post-docs in my lab: Dr Rob Williamson, Dr Christo Rautenbach, and Dr David Dyer, and the latter will be with me until December 2023."
  },
  {
    "objectID": "pages/case_for_promotion.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "href": "pages/case_for_promotion.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)",
    "text": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)\nRelated to my interest in marine heatwaves, I have also been instrumental in developing the South African Coastal Seawater Temperature Network (SACTN).47 This work brings together, for the first time, the disparate seawater temperature records measured over up to 4 decades by the KwaZulu-Natal Sharks Board (KZNSB), Ezemvelo KZN Wildlife (EKZNW), the South African Weather Service (SAWS), the Department of Forestry, Fisheries and Environment (DFFE), the South African Environmental Observation Network (SAEON), and the UWC. 48 This paper has been cited 166 times and instrumental in several other of my own frequently cited publications49 and stimulated further avenues of research regarding the variability of ocean temperature, including the research on marine heatwaves.47 The The South African Coastal Seawater Temperature Network (SACTN) GitHub page from where data can be downloaded48 Smit et al (2013)49 Schlegel et al (2017a) and Schlegel et al (2017b)"
  },
  {
    "objectID": "pages/case_for_promotion.html#editorial-contributions",
    "href": "pages/case_for_promotion.html#editorial-contributions",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.5. Editorial contributions",
    "text": "4.2.5. Editorial contributions\n2018–present Associate Editor, Aquatic Botany.5050 Associate Editor for Aquatic Botany\n2020–present Associate Editor Frontiers in Ecology & Evolution and Frontiers Topic Editor,51 Managing Deep-sea and Open Ocean Ecosystems at Ocean Basin Scale - Volume 251 My Reviewer’s profile on Loop for editorial contributions to Frontiers in Ecology & Evolution\n2023–present Guest Editor, Special Issue, Botanica Marina\nIn addition, reviewing done for Frontiers in Marine Science; Plos ONE; Proceedings of the National Academy of Sciences; Journal of Phycology; Estuarine Coastal & Shelf Science; African Journal of Marine Science; Hydrobiologia; Journal of Applied Phycology; Journal of Marine Systems; Marine Biology; Marine Ecology; Diversity & Distributions; Ecology & Evolution; Atmosfera; Big Earth Data; Botanica Marina; Environmental Pollution; Science of the Total Environment; Frontiers Ecology And Evolution; Meteorology and Atmospheric Physics; One Health; International Journal of Environmental Research and Public Health, Marine Pollution Bulletin."
  },
  {
    "objectID": "pages/case_for_promotion.html#future-research",
    "href": "pages/case_for_promotion.html#future-research",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.6. Future research",
    "text": "4.2.6. Future research\nMy future research endeavours will focus on investigating the interplay between coastal marine extreme events and the shifting climate. The objective is to ensure that this research is both relevant and beneficial to a broad spectrum of actors who gain from nature’s contributions. Building upon the foundation of my BlueConnect and EXEBUS programmes, the scope of my work will increasingly embody a transdisciplinary approach. This will be achieved through collaborations with experts in economics, sociology, and maritime law, rendering the research relevant to both society and industry. Within this field, my specific interests—the biogeochemical function of kelp and the detection and statistical analysis of extreme events in environmental time series—will be deployed to establish links between environmental drivers and their impacts on ecosystems and society."
  },
  {
    "objectID": "pages/case_for_promotion.html#academic-lead-kelp-scientific-collaboration-ppp",
    "href": "pages/case_for_promotion.html#academic-lead-kelp-scientific-collaboration-ppp",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP",
    "text": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP\nI have been the academic lead of the Kelp Scientific Collaboration52 consortium since September 2021 (ongoing). The consortium is a Public-Private-Partnership whose intention is to foster collaboration around kelp ecosystems for the betterment of sustainable practices that concern the industry and for scientific advancement on kelp ecological functioning.52 Kelp Scientific Collaboration mission statement"
  },
  {
    "objectID": "pages/case_for_promotion.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "href": "pages/case_for_promotion.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp",
    "text": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp\nThis project on the perceived value of kelp53 was heavily concerned with people’s relationship with kelp and produced several outputs:53 Perceived Value of Kelp\n\nJanuary 2022 Premier of Akshata Mehta’s movie, Kelp, South Africa’s Golden Forests (funded by myself through BlueConnect, and provided concept and oversight).54 The short film was first shown at the annual PSSA meeting in Arniston and subsequently entered into various nature documentary festivals. It is also on YouTube, where it has received 5.3k views.\nSeptember 2021 Supervise Akshata Mehta’s MPhil Thesis, “Golden Forests” of the Sea: Assessing Values and Perceptions of Kelp in the Western Cape Region of South Africa. This work continues to yield stakeholder engagements with community members and the seaweed industry of Southern Africa.55\n\n54 Kelp, South Africa’s Golden Forests on YouTube55 Akshata Mehta’s MPhil thesis"
  },
  {
    "objectID": "pages/case_for_promotion.html#blueconnect-engagements",
    "href": "pages/case_for_promotion.html#blueconnect-engagements",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.3. BlueConnect engagements",
    "text": "4.3.3. BlueConnect engagements\n\nContributing author to Chapter 3, UNEP report on global kelp forests.56\nGlobal Ecological Assessment of Kelp, June 15-17, 2022, in Arendal, Norway.57 This work stems directly from the SANOCEAN BlueConnect Programme, of which I am the South African PI. The work intended to bring together global kelp experts to evaluate kelp forests.\nBlueConnect Kelp Ecosystem 10-day Field Course, 16 – 26 March 2020, Cape Town and De Hoop Nature Reserve – this workshop was affected by COVID-19 and all field work was cancelled; it proceeded as an online course. Ten students from South Africa and Norway participated.58\nNovember 2019: Lead workshop with the kelp industry to gain perspectives about challenges they face about environmental and governance concerns they experience.\n\n56 United Nations Environment Programme, & Norwegian Blue Forests Network (2023). Into the Blue: Securing a Sustainable Future for Kelp Forests.57 Invitation letter to the GEAK workshop held in Norway58 BlueConnect March 2020 Field Course"
  },
  {
    "objectID": "pages/case_for_promotion.html#exebus-engagements",
    "href": "pages/case_for_promotion.html#exebus-engagements",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.4. EXEBUS engagements",
    "text": "4.3.4. EXEBUS engagements\nEXEBUS59 60 undertakes an Integrated Ecosystem Assessment (IEA) to establish the roles, trends, and range of variability and the extremities of natural and anthropogenic geophysical, biological, governance, socio-economic features and phenomena, and assess their impact on ecological, sociological, governance, and macroeconomic systems and processes in the Benguela Current Large Marine Ecosystem (BCLME) of South Africa (SA), Namibia, and Angola. The goal is to strengthen the rational basis for management on relevant spatial and temporal scales (up to 2070).59 Video on YouTube about EXEBUS60 EXEBUS website\nTo further these interests, my Team and I have had stakeholder engagements with (ongoing):\n\n2022 The Benguela Current Convention\n2022 The kelp industry in South Africa\n2022 An assortment of stakeholders (academia, the Ministry of Fisheries, University of Namibia academics)\n2023 Users and port operators of the Port of Cape Town"
  },
  {
    "objectID": "pages/case_for_promotion.html#other-community-engagements-and-capacity-building-contributions",
    "href": "pages/case_for_promotion.html#other-community-engagements-and-capacity-building-contributions",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.5. Other community engagements and capacity-building contributions",
    "text": "4.3.5. Other community engagements and capacity-building contributions\nI am currently involved with Cape Nature in initiatives aimed at building capacities among fishermen in the Helderberg region61. I am also an active participant in the Kogelberg Marine Working Group, which is dedicated to discussing and implementing conservation management initiatives in the Kogelberg region62.61 See most recent invitation to participate in a capacity building initiative62 Invitation quarterly Kogelberg Marine Working Group meeting\nSince 2017, I have been training students and budding scientists from previously disadvantaged Higher Education Institutions (HEIs) and NRF National Facilities. This includes teaching R courses at the University of Zululand, Walter Sisulu University, SAIAB, and SAEON. In the process of these collaborations, I regularly engage with young academics freshly appointed to their positions at these universities. The objective is to foster research proficiency and academic confidence, thereby amplifying their potential to positively influence subsequent generations of graduates.\nI have recently received and accepted an invitation from OceanHub Africa to spearhead a project at the Ocean Hackathon as a Challenge Owner. This platform allows me to interact with professional coders and jointly work towards data-driven solutions to address certain marine conservation and management challenges in the region63.63 See invitation letter"
  },
  {
    "objectID": "pages/case_for_promotion.html#covid-19-environmental-research-group",
    "href": "pages/case_for_promotion.html#covid-19-environmental-research-group",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.6. CoVID-19 Environmental Research Group",
    "text": "4.3.6. CoVID-19 Environmental Research Group\nDuring the first year of CoVID-19 I was part of the CoVID-19 Environmental reference Group (CERG) which aimed to establish the link between seasonality and the prevalence and spread of CoVID-19 in developing countries. An output of the work is the paper Smit et al. (2020).6464 Smit et al (2020) about CoVID-19"
  },
  {
    "objectID": "pages/BDC223_FAQ.html",
    "href": "pages/BDC223_FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "This question was asked by 3rd years, but it is relevant here too.\nI just need some advice from sir. I’m starting to study today and I just wanted to know what do you think is the best way to study for sir’s exam. I’m asking because I studied really hard for the midterm tests but I still got low marks lol. So I just wanted to know if there is maybe a different approach that I can take.\n\nI was trying to memorise things for the midterm tests but I know sir said we must focus on understanding but when I focus on understanding then the work does not stick in my head😂.\n\nSo far I’ve just read through the articles but is there any advice that sir can give me please?\n\n\nThank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2).\nStructure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the things most important things (attributes) which informs who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-how-to-study",
    "href": "pages/BDC223_FAQ.html#question-how-to-study",
    "title": "FAQ",
    "section": "",
    "text": "This question was asked by 3rd years, but it is relevant here too.\nI just need some advice from sir. I’m starting to study today and I just wanted to know what do you think is the best way to study for sir’s exam. I’m asking because I studied really hard for the midterm tests but I still got low marks lol. So I just wanted to know if there is maybe a different approach that I can take.\n\nI was trying to memorise things for the midterm tests but I know sir said we must focus on understanding but when I focus on understanding then the work does not stick in my head😂.\n\nSo far I’ve just read through the articles but is there any advice that sir can give me please?\n\n\nThank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2).\nStructure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the things most important things (attributes) which informs who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#what-are-climatic-envelopes",
    "href": "pages/BDC223_FAQ.html#what-are-climatic-envelopes",
    "title": "FAQ",
    "section": "What are climatic envelopes?",
    "text": "What are climatic envelopes?\nGood day sir, there isn’t a good definition of climatic envelopes on google. Sir spoke about it in consequences in climate change. Not really sure what it specifically is.\n\nAnswer\nClimatic envelopes are the suite of environmental conditions required for plant (or animal) growth that define the optimal niche area and hence the organism’s distribution.\nOne can model the future climatic envelopes using various statistical approaches, and hence so project the future distribution of the species (or ecosystems) whose distribution are linked to those envelopes. Such models are called bioclimatic models or niche models.\nThe process is called species distribution modelling. We will do this in Hons.\nEnough? The first little para I wrote is the definition and all you would put down if I asked."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-acclimatisation",
    "href": "pages/BDC223_FAQ.html#question-about-acclimatisation",
    "title": "FAQ",
    "section": "Question About Acclimatisation",
    "text": "Question About Acclimatisation\nI also wanted to ask. When plants avoid stress, is it not acclimatization as well?\n\nAnswer\nYes. But there’s only a certain range of env conditions plants can acclimatise to, and exceeding those limits will still cause stress.\nAcclimatisation can happen over minutes to hours to days. Or seasonally. But if env conditions exceed the normal range of variability they’ll become stressed."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-the-organic-foods-essay-topic",
    "href": "pages/BDC223_FAQ.html#question-about-the-organic-foods-essay-topic",
    "title": "FAQ",
    "section": "Question About the Organic Foods Essay Topic",
    "text": "Question About the Organic Foods Essay Topic\nI chose the organic food topic. My question is if I should find research papers for everything I state?\nE.g “Organic food has been a growing interest as people have become more concerned about their diet and what they chose to consume.”\nDo I need to search an article to support that or can I leave it as is since it’s something I’ve recently seen with friends, family and on social media platforms (how organic food is the “right food” to consume).\n\nAnswer\nI think it’s commonly knowledge based on lived experience that organic foods have become more widely consumed. So no need to ref that. But the claims that people make about why organic foods are ‘better’ often do not have factual support. So, if you state that it’s better for whatever reason, that needs factual support. If no support is available, your conclusion would have to be that the claim is dogma, i.e., untested, unsubstantiated, wishful thinking, etc.\nScientific studies need to be done in order to prove some hypothesis. Without it the claim remains unsubstantiated despite how many people buy into the claim. Simply because 10 million people think it is good does not actually provide any evidence that the claim is fact."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-pigments",
    "href": "pages/BDC223_FAQ.html#question-about-pigments",
    "title": "FAQ",
    "section": "Question About Pigments",
    "text": "Question About Pigments\nGood day sir, I have a question about accessory pigments. I know they help pass light onto chlorophyll-a for photosynthesis right? And different chlorophylls, especially chlorophyll-a bind to proteins in different ways. Is that in order to absorb more more that the chrolophyll pigment itself wouldn’t be able to absorb?\n\nAnswer\n“different chlorophylls, especially chlorophyll-a bind to proteins in different ways. Is that in order to absorb more more that the chlorophyll pigment itself wouldn’t be able to absorb” — No. If one would have to design something, then that would be the approach. But these molecules were not designed. They evolved. Evolution does not work by something functioning in a specific way in order for some other thing to do what it does. The specific protein binding between the pigments and proteins happened because, by chance, some configuration arose that happened to fill some need, that is, to fill the green gap. It happened by chance, not design."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-eutrophication",
    "href": "pages/BDC223_FAQ.html#question-about-eutrophication",
    "title": "FAQ",
    "section": "Question About Eutrophication",
    "text": "Question About Eutrophication\nGood day Professor, I was wondering if sir could clarify something. Is an anoxic water where there is no dissolve oxygen? And is that caused by oxygen-using bacteria that decompose dead organisms in eutrophic environments?\n\nAnswer\nNot no oxygen. But very little. Usually anoxia is reached at O2 concentrations below 2mg/L. Before that low level it’s called hypoxia.\nYes. It is caused by bacterial respiration. Hypoxia/anoxia causes even more species to die, and further reduces O2 concentrations.\nEutrophic conditions can cause biomass accumulation of photoautotrophs. During night extremely dense biomass of such accumulations don’t photosynthesise but continue to respire. This is when low O2 first starts, and it causes the initial die-off."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-biofouling",
    "href": "pages/BDC223_FAQ.html#question-about-biofouling",
    "title": "FAQ",
    "section": "Question About Biofouling",
    "text": "Question About Biofouling\nHi Professor is biofouling and epiphytes the same or different things?\n\nAnswer\nBiofouling is a process. It’s the process by which epiphytes colonise the surface of a basiphyte. The epiphytes in question might be macroalgae, but it’s most typically microalgae or bacteria (the latter two collectively called biofilm)."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-calculating-the-rate-of-uptake-v",
    "href": "pages/BDC223_FAQ.html#question-about-calculating-the-rate-of-uptake-v",
    "title": "FAQ",
    "section": "Question About Calculating the Rate of Uptake, V",
    "text": "Question About Calculating the Rate of Uptake, V\nGood day Professor, I am hoping sir could assist with my work. For the V column, does that represent the rate that N is being assimilated into the thallas? If so, then the values should be positive right? 😅.\nI’m asking because some students are getting negative values. Regards\nProfessors response.\n“Yes. Why do you think there’s a negative value? What does a negative rate mean—i.e. does it apply to the culture medium (where the concentration decreases) or to the seaweed (where it increases)?”\nI believe the values of the slope are negative because that shows the rate of N that leaves the solution. If I can put it like that\n\nAnswer\nYes! And thus the rate of appearance of N in the seaweed is of the opposite sign, so simply take the absolute value."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-calculating-s-in-the-nutrient-uptake-experiments",
    "href": "pages/BDC223_FAQ.html#question-about-calculating-s-in-the-nutrient-uptake-experiments",
    "title": "FAQ",
    "section": "Question About Calculating S in the Nutrient Uptake Experiments",
    "text": "Question About Calculating S in the Nutrient Uptake Experiments\nSir, do we consider the only culture volume when calculate our S (substrate conc)? and we use μmol N or μg N units or it doesn’t much matter\n\nAnswer\nIt is a function not so much of culture volume, but of the amount (micro moles or micrograms) of nutrients within a volume of seawater.\nVolume per se is not important: the concentration of a substance is the same in 1 ml or in 1 liter. The amount (moles or grams) of a substance is very different in that 1 ml or 1 liter, however. So, volume does not affect concentration, but it affect total amounts available in a volume."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-perturtbation-experiments",
    "href": "pages/BDC223_FAQ.html#question-about-perturtbation-experiments",
    "title": "FAQ",
    "section": "Question About Perturtbation Experiments",
    "text": "Question About Perturtbation Experiments\n(AJ?) Smit professor, with multiple flask experiment you said you can calculate update rate (so I’m assuming it’s a linear graph) and with perturbation you said it’s a depletion curve.\nWith the Michaelis- menten we measure substrate concentration against uptake rate but use perturbation methods (using the gradient for the uptake rate) Since multiple flask also shows uptake rate can you still use this methodology to generate a Michaelis-menten expression? Also wouldn’t it have been easier because then you don’t have the whole x-axis confusion\n\nAnswer\nWhatsApp Ptt 2022-10-12 at 10.25.49 PM.ogg"
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-ks-and-alpha",
    "href": "pages/BDC223_FAQ.html#question-about-ks-and-alpha",
    "title": "FAQ",
    "section": "Question About Ks and \\(\\alpha\\)",
    "text": "Question About Ks and \\(\\alpha\\)\nWith regards to Michaelis Menton expression and specifically the Ks and \\(\\alpha\\) does that specifically relate to diffusion ability?\nDoes a high Ks mean diffusion was rate limiting sooner whereas a low Ks meaning kinetics was rate limiting?\nOr am I completely misunderstanding the work?\n\nAnswer\nYes. Ks and \\(\\alpha\\) relate to the externally controlled phases of nutrient uptake, so they are controlled by diffusion (and thus also water motion and nutrient concentration)."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-the-nitrogen-cycle",
    "href": "pages/BDC223_FAQ.html#question-the-nitrogen-cycle",
    "title": "FAQ",
    "section": "Question – The Nitrogen Cycle",
    "text": "Question – The Nitrogen Cycle\nI just wanted some clarification, is it correct to say that the definition of the nitrogen cycle is a biogeochemical process through which nitrogen is converted into many chemical forms circulating in the marine, terrestrial and atmospheric ecosystems?\n\nAnswer\nN cycle. I’d say something like this:\nThe uptake, transformation, release, and transport of N-containing compounds through components of the Earth system, including the biosphere, geosphere, hydrosphere, cryosphere, and atmosphere. The underlying processes involve a series of biologically, physically, and chemically mediated processes which act on different compounds of inorganic and organic N.\nMore simply we can say the N cycle is N biogeochemistry, but less is explained by this short statement than by the longer one."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-whatsapp-ptt-2022-11-13-at-9.43.54-am.ogg",
    "href": "pages/BDC223_FAQ.html#question-whatsapp-ptt-2022-11-13-at-9.43.54-am.ogg",
    "title": "FAQ",
    "section": "Question – WhatsApp Ptt 2022-11-13 at 9.43.54 AM.ogg",
    "text": "Question – WhatsApp Ptt 2022-11-13 at 9.43.54 AM.ogg\n\nAnswer\nVery nice question! It is a pity I already set the exams.\nSo why does Ulva not show saturation at some point?\nWithin the range of N concentrations typically present in the ocean, say up to 20μM N in upwelling systems, uptake should (can) theoretically remain unsaturated, PROVIDED THAT ALL OTHER ENVIRONMENTAL CONDITIONS REMAIN OPTIMAL. There always has to be sufficient amounts of light; the temperature must be optimal, and so on. As soon as the GROWTH RATE slows down because the alga cannot capture enough light to drive photosynthesis (for cellular replication and biomass growth), there will be an upper limit to the amount of N taken up sequestered. So, the high uptake rates promised by a fully rate-unsaturated uptake mechanism supported by diffusion are only possible if the alga can produce enough biomass quickly so it can assimilate N into biomass (protein). Algae can only assimilate N if enough C comes in (through photosynthesis) for sufficient amounts of the C compounds containing N in an organic form.\nTherefore, all suboptimal environmental conditions influencing C uptake will affect N uptake.\nOnly some environmental conditions are optimal for long enough for algae to sustain high N uptake through rapid growth rates. Only because of fast growth rates will N be maintained at low enough concentrations in the vacuoles to prevent feedback inhibition. When feedback inhibition happens, the rate of N uptake is limited. Under most natural conditions, there is likely an upper limit to N uptake. However, we can create optimised conditions in the lab to maximise the algal growth rate; thus, N uptake could remain unsaturated.\nEven passive uptake (N uptake through diffusion) can be rate limited if the amount of N building up inside the cells is so high that it reduces the concentration gradient across the cell from outside (water) to inside (vacuole). In this situation, there would also be a Vmax, determined by the rate at which the alga can bind N into an inorganic form, typically as protein (including some phycobilins)."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-question-about-a-question",
    "href": "pages/BDC223_FAQ.html#question-question-about-a-question",
    "title": "FAQ",
    "section": "Question – Question About a Question",
    "text": "Question – Question About a Question\nSir with this, are we actually supposed to draw the graphs and do the calculations or simply state what needs to be done?\n\n\n\nThe question about which the question is asked.\n\n\n\nAnswer\nIt’s as the question says:\nDesign an experiment that will provide insight into both the optimum ratio of N and P and the optimum concentration of potassium nitrate and orthophosphoric acid to feed the U. lactuca mass culture (i.e. with the aim to maximise biomass production).\nIn your answer, please pay specific attention to the experimental conditions during the acclimation phase (i.e. a period lasting two weeks prior to the experiment), as well as during the experimental phase. Provide a rationale and justification for all your decisions that ultimately inform your experiment.\nCalculations can only done after the experiment is completed, and the question simply asks that you design the experiment."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-the-anthropocene",
    "href": "pages/BDC223_FAQ.html#question-the-anthropocene",
    "title": "FAQ",
    "section": "Question – The Anthropocene",
    "text": "Question – The Anthropocene\nJust a quick question to anyone who might know the answer: Based on Rockstrom’s paper, especially the intro, in which period are we currently? He first mentioned the Holocene, but then states that we have entered the Anthropocene. However, a few sentences later he talks about maintaining the the status and staying in the Holocene.\n\nAnswer\nThe American Geophysical Union does not recognise the Anthropocene as an actual geological epoch yet, so according to them we are still in the Holocene. But many people think that we have already deviated so far away from what was typical for Holocene into something very different, and that we should redefine the current era as the Anthropocene.\nWhat’s your personal view Prof?\nAnthropocene means ‘the age of humans’. So, humans have become so abundant that the signal of our activities have made an imprint on global biogeochemical systems such that in millennia from now when people no longer exist, ‘we’ (whatever replaces us or visits Earth) will be able to pick up signs of people’s existence in various geological strata on Earth.\nI think it makes sense to call where we are presently the Anthropocene, and I think Johan Rockström makes the same argument."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question---conclusions-drawn-from-the-keeling-curve",
    "href": "pages/BDC223_FAQ.html#question---conclusions-drawn-from-the-keeling-curve",
    "title": "FAQ",
    "section": "Question - Conclusions Drawn from the Keeling Curve",
    "text": "Question - Conclusions Drawn from the Keeling Curve\nCan we also use Keelings conclusion to justify that we are in anthropocene because I think it goes hand in with what John [Johan Rockström] is saying?\n\nAnswer\nRalph Keeling’s work is part of the justification. Much more has happened since, especially in the last decade. I don’t think a justification to use Anthropocene yet existed in the 1960s, but there’s plenty going on now to cause one to make that argument.\nSee The Keeling Curve for nice views into what constitutes the Keeling curve over various timescales."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-about-the-history-of-climate-change",
    "href": "pages/BDC223_FAQ.html#question-about-the-history-of-climate-change",
    "title": "FAQ",
    "section": "Question – About the History of Climate Change",
    "text": "Question – About the History of Climate Change\nGood day Prof for the key historical events with regards of climate change do we have to know the years or would the persons name and what they discovered be enough for an answer?\n\nAnswer\nI guess I’m not so much interested in exact dates, but do knowing which part of which century things happened is important. And the correct order of events. The fact is, we know about climate change far longer than people give credit to."
  },
  {
    "objectID": "pages/BDC223_FAQ.html#question-question-about-a-previous-exam-question",
    "href": "pages/BDC223_FAQ.html#question-question-about-a-previous-exam-question",
    "title": "FAQ",
    "section": "Question – Question About a Previous Exam Question",
    "text": "Question – Question About a Previous Exam Question\nSir, I’ve been struggling to contextualize the Guideline you gave us on Material and Methods.\nI saw a question on a previous question and I struggled to answer it for 25 marks.\n\n\n\nThe offending question.\n\n\n\nAnswer\nI gave you the answer on Friday [the one about N uptake, as seen above]. Something like that. Just adapt it for photosynthesis. You want to measure O2 production/consumption or CO2 production/consumption in stead of nutrients.\nJust pick your favourite plant or algal species. The experiment must be appropriate for plants or algae, of course. The difference is that plants live in air and algae in an aqueous medium, so the experiment must be set up appropriately.\nIn air we use an IRGA (infrared gas analyser) and in water we can use an O2 meter. Or we can use a C14-labelled source of CO2 and use scintillation counting to measure the appearance of a radioactive C for in the pool where CO2 accumulates.\nOtherwise, not too different from the N uptake answer, except we probably won’t use the perturbation method.\nAnd you probably want to measure net photosynthesis, so make sure you measure respiration too."
  },
  {
    "objectID": "pages/research_grants.html",
    "href": "pages/research_grants.html",
    "title": "National and international research grants",
    "section": "",
    "text": "2020 – 2022: Belmont Forum Collaborative Research Action on Transdisciplinary Research for Ocean Sustainability: Ecological and Economic impacts of the intensification of extreme events in the Benguela Upwelling System, Principal Investigator (EXEBUS) PDF\n2019 – 2021: SANOCEAN: Factors influencing the formation, fate and transport of microplastic in marine coastal ecosystems (FORTRAN) PDF\n2019 – 2021: SANOCEAN: Blue growth opportunities in changing kelp forests (BlueConnect) PDF\n2019 – 2023: Horizon 2020: iAtlantic, led by Prof. Murray Robert, own capacity as Regional Coordinator for the SE Atlantic PDF"
  },
  {
    "objectID": "pages/research_grants.html#international-grants",
    "href": "pages/research_grants.html#international-grants",
    "title": "National and international research grants",
    "section": "",
    "text": "2020 – 2022: Belmont Forum Collaborative Research Action on Transdisciplinary Research for Ocean Sustainability: Ecological and Economic impacts of the intensification of extreme events in the Benguela Upwelling System, Principal Investigator (EXEBUS) PDF\n2019 – 2021: SANOCEAN: Factors influencing the formation, fate and transport of microplastic in marine coastal ecosystems (FORTRAN) PDF\n2019 – 2021: SANOCEAN: Blue growth opportunities in changing kelp forests (BlueConnect) PDF\n2019 – 2023: Horizon 2020: iAtlantic, led by Prof. Murray Robert, own capacity as Regional Coordinator for the SE Atlantic PDF"
  },
  {
    "objectID": "pages/research_grants.html#national-grants",
    "href": "pages/research_grants.html#national-grants",
    "title": "National and international research grants",
    "section": "National grants",
    "text": "National grants\n\n2019 – 2021: NRF Global Change Grand Challenge: Earth System Science Research Programme — Extreme Climatic Events in the Coastal Zone, Principal Investigator (ESS180920360856) PDF\n2018 – 2020: NRF Competitive Programme for Rated Researchers —Upwelling dynamics in kelp beds: implications for trophic function PDF\n2017: CHEC/CCT Joint Research Programme 2017: What can kelp loss processes and beach cast patterns tell us about the sandy beach management? PDF\n2015 – 2017: NRF Competitive Programme for Rated Researchers (CPRR) — Thermal characteristics of the South African nearshore: implications for biodiversity PDF\n2014 – 2016: NRF Competitive Programme for Rated Researchers (CPRR) — Kelps and climate change: South Africa in a global context PDF\n2014 – 2016: NRF Incentive Funding for Rated Researchers (IPRR) Grant No. IFR14020764026 PDF"
  },
  {
    "objectID": "pages/technology_infusion.html",
    "href": "pages/technology_infusion.html",
    "title": "Technology infusion and reproducible research",
    "section": "",
    "text": "Coding skills supported by the intertwined technologies of R, RStudio IDE, and Quarto play a key role in my views on shaping modern-day learning and scientific processes. They equip students with the skills to become better collaborative learners and scientists. These technologies offer an extensive range of tools and libraries best known for data analysis, statistics, and visualisation. Coding skills and data analytical skills equip students to develop a deep understanding of complex data sets and derive meaningful insights from them, expanding their analytical thinking and problem-solving skills.\nRecently, Quarto has become tightly integrated into the R ‘ecosystem.’ The website states that Quarto is “an open-source scientific and technical publishing system.” At its heart, it is a dynamic document format based on R and Markdown. It enables students to create interactive, reproducible, well-documented reports, presentations, and websites that combine code, results, and narrative in a single document. The Tangled Bank was entirely developed within Quarto! This approach not only enhances students’ communication skills by encouraging clear and concise explanations of their findings but also promotes transparency and reproducibility in research. By integrating code and results seamlessly, Quarto reduces errors, simplifies the updating process, and ensures that results remain consistent with the underlying data and methods. Quarto is the de facto mode of reporting and communication that students must adopt in BCB744 and BCB743. I am exploring the feasibility of introducing it into BDC334, as feedback indicates that students are keen to develop their coding skills earlier in their undergraduate degrees.\nThe collaborative potential of R and Quarto further empowers students to work effectively in interdisciplinary teams. Students can easily share their code, data, and findings using version control systems, such as Git (as implemented in GitHub), alongside R and Quarto. This fosters a collaborative learning environment where students can collectively learn from each other’s expertise, troubleshoot problems, and develop innovative solutions. Moreover, creating and sharing well-documented Quarto reports improves communication among team members, ensuring everyone is on the same page and facilitating smoother project execution.\nIntegrating these collaborative, open, transparent coding technologies into the teaching, learning, and scientific processes cultivates essential skills in students, such as critical thinking, problem-solving, communication, and collaboration. By leveraging these technologies, students become better equipped to tackle the challenges of today’s data-driven research landscape, ultimately contributing to advancing science and developing innovative solutions to pressing global issues. These skills are also highly sought after in the workplace outside of science and academia and will significantly improve the employability of our graduates regardless of their future career paths.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2023,\n  author = {Smit, AJ},\n  title = {Technology Infusion and Reproducible Research},\n  date = {2023-04-24},\n  url = {https://tangledbank.netlify.app/pages/technology_infusion.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2023) Technology infusion and reproducible research. https://tangledbank.netlify.app/pages/technology_infusion.html."
  },
  {
    "objectID": "pages/Transboundary_systems.html",
    "href": "pages/Transboundary_systems.html",
    "title": "Transboundary systems",
    "section": "",
    "text": "Transboundary systems\nTransboundary systems refer to ecosystems that span the boundaries of more than one country or jurisdiction. These can include a variety of natural resources like water bodies (rivers, lakes, aquifers), marine ecosystems, forests, wildlife habitats, and mountain ranges, among others.\nTransboundary systems pose unique challenges and opportunities for management and conservation due to their shared nature. They require cooperative management strategies, often necessitating bilateral or multilateral agreements between the countries involved. This alliance ensures the sustainable use of the shared resource, while also managing any potential conflicts that may arise due to differing national interests.\nA transboundary river system, for example, may originate in one country, flow through another, and finally discharge into the ocean in a third country. Each country might have differing needs and priorities for the river’s use—for drinking water, irrigation, hydroelectric power, etc. Coordinated management is crucial to ensure the river’s health and equitable use.\n\n\nLarge marine ecosystems\nLarge Marine Ecosystems (LMEs) are regions of the world’s oceans, encompassing coastal areas from river basins and estuaries to the seaward boundaries of continental shelves and the outer margins of the major current systems. They are characterised by their vast size—typically they are over 200,000 square kilometers—and their distinctive bathymetry,1 hydrography,2 productivity,3 and trophically dependent populations.41 The underwater topography, including features like continental shelves, deep sea trenches, and seamounts.2 The physical and chemical characteristics of the water, including temperature, salinity, currents, and nutrient levels.3 The biological productivity of the area, including both primary producers like phytoplankton and the various levels of consumers in the food web.4 These are groups of species that are interconnected in the food web, including predators, prey, and competitors.\nThe concept of LMEs was developed in the 1980s by Dr. Ken Sherman of the US National Oceanic and Atmospheric Administration (NOAA) in response to the growing need for a comprehensive, ecosystem-based approach to manage and conserve coastal and marine resources. This approach recognises that marine resources are interconnected and that effective management must consider the entire ecosystem rather than individual species or issues in isolation. As such, the LME approach was intended to bridge the gap between single-species management and broader ecosystem-based management.\nThere are 66 recognised LMEs globally, seven of which are around the African continent (Sweijd and Smit 2020), including the Benguela Current LME off South Africa, Namibia, and Angola. Each LME is unique and requires a tailored management approach, but the overarching goal is the same: to ensure the long-term sustainability and health of the world’s coastal and marine ecosystems.\n\n\nThe Benguela Current Large Marine Ecosystem\nThe LME classification system, established in the 1980s, represents a giant stride in acknowledging and managing contiguous, transboundary marine ecosystems. Among the 66 LMEs identified worldwide, the Benguela Current LME (BCLME) stands out as a pivotal Eastern Boundary Upwelling System (EBUS), a category shared only by the Humboldt Current LME, the California LME, and the Canary Current.\nThe BCLME is comprised of the southern, central northern, and northern Benguela subsystems. This marine region extends from the shoreline at the high-water mark to the countries’ Exclusive Economic Zones (EEZs). From the Cape of Good Hope, its southern and eastern border seasonally stretches as far as 27°E longitude, near Gqeberha. Northward, the boundary reaches to 5°S near Nimibe in Angola, aligning with the southern edge of the Guinea Current Large Marine Ecosystem (GCLME). This boundary definition is fundamental to the sustainable management and conservation of the BCLME, thereby fortifying the environmental, economic, and social resilience of the region.\nThe BCLME is part of a mere 3% of the world’s sea surface occupied by the four EBUS but yields nearly 40% of the global annual marine fish catch. LMEs worldwide, though only accounting for a fraction of the ocean’s surface, contribute an impressive 80% to this vital food resource.\nYet the significance of BCLME transcends its remarkable productivity. It serves as a crucial climate regulator, with its abundant biomass acting as a significant carbon sink, mitigating the effects of climate change. This critical role underscores the BCLME’s global significance, as it helps maintain our planet’s delicate climatic balance.\nThe BCLME is also a reservoir of marine biodiversity that enriches our world ecologically and economically, and the upwelling of cool, nutrient-rich water is reasoned to act as a haven for species that might be prone to ocean warming. However, like many of Earth’s natural ecosystems, the BCLME is under severe stress. It faces challenges from overfishing, pollution, and climate change impacts, leading to biodiversity loss and habitat degradation. Consequences for the people making a living from the system are already emerging.\nIn light of these challenges, the conservation and sustainable management of the BCLME is not just a regional concern—it is a global imperative and a human right. The BCLME’s importance as a climate regulator, biodiversity reservoir, and primary productivity centre demands immediate attention and action. Investing in the health of this ecosystem is, in essence, investing in the future of our planet.\nThe commitment to ensuring a sustainable future of the BCLME is embodied in a tripartite alliance between Angola, Namibia, and South Africa, the parties to the Benguela Current Convention. This boundary demarcation facilitates the deployment of a practical ecosystem management framework for this transboundary ecosystem.\n\n\nManaging transboundary marine ecosystems\nManaging transboundary marine ecosystems is complex due to the multitude of stakeholders and jurisdictions involved, as well as the inherent dynamism and complexity of marine ecosystems. However, several strategies have been identified as effective:\n\nEcosystem-Based Management (EBM): This approach aims to balance ecological, social, and economic goals in managing marine resources. It takes into account the entire ecosystem, including human activities, rather than focusing on one species or resource at a time.\nMarine Spatial Planning (MSP): MSP is a practical way to create and establish a more rational use of marine space to benefit economic, social and environmental objectives. It involves allocating and managing parts of the ocean to specific uses or activities, in a way that minimises conflict and maximises compatibility among different activities.\nCooperative Management and Governance: Transboundary ecosystems require cooperation between all nations whose waters are part of the ecosystem. This can be achieved through international treaties, conventions, or other agreements. An example of this is the Benguela Current Convention between Angola, Namibia, and South Africa.\nScience-Based Decision Making: Regular monitoring and research are crucial to understand the state of the ecosystem and the impacts of human activities. This information should be used to inform management decisions and adaptive strategies.\nStakeholder Engagement: All relevant stakeholders, including governments, industry, indigenous communities, and the public, should be involved in decision-making processes. This ensures a diversity of perspectives and promotes equitable outcomes.\nAdaptive Management: Given the dynamic nature of marine ecosystems, management strategies need to be flexible and responsive to change. This involves regular monitoring, periodic evaluations, and adjustments to management plans as needed.\nIntegrated Coastal Management (ICM): This is a process for governance and management of coastal areas. ICM aims to balance the different objectives of society - economic development, coastal livelihoods, and environmental conservation.\nPrecautionary Approach: In situations of scientific uncertainty, the precautionary approach advocates for erring on the side of caution to prevent serious or irreversible damage to the ecosystem.\n\nThese strategies require significant resources and political will, but are crucial for the sustainable management of transboundary marine ecosystems.\n\n\nTreaties and Conventions\nTreaties and Conventions are fundamental to managing transboundary issues around LMEs. Given the inherently shared nature of marine resources that traverse political boundaries, international collaboration facilitated by such agreements is vital. They provide a legal framework that encourages cooperation and coordination among nations, ensuring sustainable management and conservation of marine resources, protection of marine biodiversity, and resolution of potential conflicts. Notably, they allow for integrated management strategies that consider the ecosystem as a whole, rather than fractured approaches divided by national boundaries. Such holistic approaches are crucial for preserving the health and resilience of LMEs in the face of pressing global challenges like overfishing, pollution, and climate change.\nIn the field of international law, the terms “treaty” and “convention” are often used interchangeably. Both are agreements under international law entered into by actors in international law, namely sovereign states and international organisations. They may also be known as international agreements, protocols, covenants, or exchanges of letters, among other terms.\nHowever, sometimes subtle distinctions are made between Treaties and Conventions:\n\nTreaty: This term is often used to describe an agreement of significant importance. Treaties generally require ratification by the national government of the signing parties and usually require approval by the executive or legislative branch, depending on a country’s laws. A treaty might address a specific issue, like a peace treaty or a treaty of alliance, or it might establish long-term relationships or conditions, like a free trade treaty.\nConvention: A convention is typically a broader agreement that deals with a wide area of concern or is used to codify and develop major areas of international law. Conventions are usually open for any relevant countries to join. An example would be the United Nations Framework Convention on Climate Change (UNFCCC), which establishes a framework for addressing the issue of climate change.\n\nDespite these subtle differences, the choice of term often depends more on tradition or the preference of the parties involved than any strict legal distinction. What matters most is the content of the agreement and how it is implemented and enforced, not the label given to it.\n\n\nThe Benguela Current Convention\nThe Benguela Current Convention and the Benguela Current Commission have their roots in a shared recognition by Angola, Namibia, and South Africa of the importance of the BCLME and the need for a cooperative approach to its management. Both stem from the earlier Benguela Environment Fisheries Interaction and Training (BENEFIT) program.\nBENEFIT was launched in 1997 as a bilateral initiative between Namibia and Angola, and South Africa joined later. It promoted the sustainable utilisation of marine resources in the Benguela Current region. The program placed an emphasis on capacity building, training, and scientific research, particularly focusing on the interactions between the environment and fisheries. Except for benefiting from the training component, people were not yet recognised as an important feature of the system. However, BENEFIT was instrumental in improving the understanding of the complex Benguela ecosystem and the impacts of various human activities on it.\nRecognising the ecological and economic significance of this region, the three nations initiated a cooperative venture in 1995, funded by the Global Environment Facility (GEF), to address shared marine and coastal management issues. This led to the creation of the BCLME Programme, which operated from 2002 to 2011. The work of BENEFIT was integrated into the new program and this ensured continuity in scientific research and capacity-building efforts, and allowed the BCLME Programme to advance BENEFIT’s achievements.\nBuilding on the early achievements and lessons of the BCLME Programme, the three countries formally established the Benguela Current Commission (‘the Commission’) in 2007 as an interim arrangement. The Commission’s objective was to promote a coordinated regional approach to the long-term conservation, protection, rehabilitation, enhancement, and sustainable use of the BCLME. This was intended to provide benefits to the countries through improving the conditions of the marine environment and promoting sustainable economic development.\nThe Benguela Current Convention (‘the BCC’), on the other hand, came into existence on 18 March 2013 when it was signed by the ministers responsible for fisheries and environment from Angola, Namibia, and South Africa. This legal agreement formalised the cooperative approach that had been initiated with the establishment of the Commission. The BCC committed the countries to work together through the Commission to promote a policy of ecosystem-based management, to share information and data, to harmonise policies and laws, and to seek funding for activities that support the BCC’s objectives.\nThus, the Commission5 was established first as an interim body to coordinate the management of the BCLME, and the Convention, i.e. the BCC,6 was subsequently signed to formalise and strengthen this regional cooperation, making the Commission the implementing body for the BCC.5 The Benguela Current Commission (the Commission) is the organisation or body that was established to implement the provisions of the Convention6 The Benguela Current Convention (BCC) is the actual legal agreement that was signed by the governments of the three countries.\nThe BCC reflects an ideology of shared responsibility, cooperation, and sustainable management of a transboundary marine ecosystem, the BCLME. It represents a commitment by the three coastal countries—Angola, Namibia, and South Africa—to the long-term conservation, protection, rehabilitation, enhancement, and sustainable use of this LME.\nThe BCC acknowledges the BCLME as a shared resource and emphasises the importance of regional cooperation to maintain its health and productivity. The ideology includes recognising the socio-economic and ecological importance of the region, the need to prevent and reduce environmental degradation, and the importance of basing management decisions on the best available scientific information.\nThe BCC also adopts the Ecosystem Approach to Fisheries (EAF) and Integrated Ocean Management (IOM), principles that emphasise holistic, precautionary, and adaptive management, considering ecological relationships among species and their habitats, and balancing diverse societal objectives.\nMoreover, the BCC recognises the importance of involving all stakeholders, including local communities, in the management process, reflecting an ideals of inclusivity and equitable benefit sharing. In essence, the BCC is underpinned by the principles of sustainability, shared responsibility, cooperative management, scientific knowledge, and inclusive stakeholder participation.\n\n\nOther Africa-focussed treaties and conventions\nYes, there are a number of transboundary conventions, agreements, and treaties active around Africa, including the following:\n\nNairobi Convention: Officially known as the Convention for the Protection, Management and Development of the Marine and Coastal Environment of the Western Indian Ocean, this convention involves ten countries: Comoros, France, Kenya, Madagascar, Mauritius, Mozambique, Seychelles, Somalia, Tanzania, and South Africa. Similar to the Benguela Current Convention, the Nairobi Convention provides a platform for governments, civil society and the private sector to work together for the sustainable management and use of the Western Indian Ocean.\nConvention for Cooperation in the Protection, Management and Development of the Marine and Coastal Environment of the Atlantic Coast of the West, Central and Southern Africa Region (Abidjan Convention): A comprehensive agreement among 22 African nations aimed at the protection and preservation of the marine environment and coastal areas. It is governed by the United Nations Environment Programme (UNEP) and provides a collaborative framework to address a wide range of environmental challenges, such as pollution from various sources, coastal erosion, and the conservation of biodiversity. It promotes cooperative research, monitoring, and the implementation of specific protocols, including those addressing oil spills and the establishment of protected areas, to ensure sustainable use and management of the region’s shared marine resources.\nAbuja Convention: This proposed convention is set to replace the Abidjan Convention, covering a larger geographical area and including more countries. Its main purpose is to promote regional cooperation for the protection and development of the marine and coastal environment of the Atlantic coast of West, Central and Southern Africa.\nBamako Convention: Although not specifically focused on marine environments, the Bamako Convention on the Ban of the Import into Africa and the Control of Transboundary Movement and Management of Hazardous Wastes within Africa has relevance in terms of preventing marine pollution. The convention prohibits the import of any hazardous (including radioactive) waste. The treaty also emphasizes reducing the production of hazardous waste and promoting environmentally sound management of such wastes.\nThe Joint Development Zone Treaty between Nigeria and Sao Tome and Principe: This is an agreement between the two nations to jointly develop petroleum and other resources in the maritime areas which both nations lay claim to.\nLake Chad Basin Commission (LCBC): While not marine-focused, the LCBC is a prime example of transboundary water management. It was established in 1964 by Cameroon, Chad, Niger, and Nigeria, with the Central African Republic joining later. The Commission aims to sustainably and equitably manage shared water resources and promote regional integration, peace, and security.\n\nEach of these agreements and conventions share similarities with the BCC in that they aim to foster cooperation and sustainable use of shared marine and environmental resources among the participating nations. However, they each have unique focuses and cover different geographical areas.\n\n\nComparing the BCC with the Abidjan Convention\nThe Abidjan Convention and the BCC are both concerned with the the west coast of the African continent. They share the common goal of protecting and managing marine and coastal environments, but they operate in different geographical regions and with some different focus areas. The Abidjan Convention covers the Atlantic coast of Africa, from Mauritania to South Africa, while the BCC covers the Benguela Current Large Marine Ecosystem (BCLME), which extends from South Africa to Angola. Both conventions adhere to an ecosystem-based approach to management. They acknowledge the interconnectedness of marine ecosystems and aim to manage these systems in a holistic manner. The importance of cooperation and collaboration among the member states in managing shared marine resources and addressing common environmental challenges is key to the success of both.\nThere are key differences between the two convention. The Abidjan Convention has a broader membership with 22 African countries, while the BCC only includes three countries—Angola, Namibia, and South Africa. The latter has a unique focus on the BCLME (i.e. it is designed on the idea of the LME), one of the world’s richest marine ecosystems with a high level of endemism and biodiversity. It is also particularly concerned with the effects of climate change and variability on this ecosystem. The Abidjan Convention, while also concerned with marine ecosystems and biodiversity, has a broader mandate that includes issues such as coastal erosion and marine pollution from various sources.\nThere are also differences in structure and governance. The BCC is led by a commission consisting of ministers from the three member states, while the Abidjan Convention is overseen by the United Nations Environment Programme (UNEP) and has a wider governance structure involving all member states. As such, the Abidjan Convention has established specific protocols to address issues like oil spills and protected areas. The BCC, while it does cover similar issues, does not have specific protocols but rather uses strategic action programs and other mechanisms to address these concerns. More recently, a Marine Spatial Plan has also been developed for the BCC.\n\n\nInternational examples of transboundary management of marine regions\nThere are several international treaties and conventions that aim to manage and protect transboundary marine ecosystems, similar to the Benguela Current Convention (BCC):\n\nConvention for the Protection of the Marine Environment of the North-East Atlantic (OSPAR Convention): This convention was established in 1992 and covers the north-east Atlantic. Like the BCC, it focuses on the protection and conservation of the marine environment. However, it differs in that it covers a broader geographic area and has more contracting parties, involving 15 Governments and the EU. The convention has five main strategies: Biodiversity and Ecosystems, Eutrophication, Hazardous Substances, Offshore Industry, and Radioactive Substances.\nConvention on the Protection of the Marine Environment of the Baltic Sea Area (Helsinki Convention): This convention was established in 1974 and revised in 1992. It covers the Baltic Sea area, which is bordered by nine countries. Similar to the BCC, it aims to prevent and eliminate pollution in order to promote the ecological restoration of the Baltic Sea. However, it covers a smaller geographic area and includes more specific commitments, such as banning dumping of waste from ships and aircraft.\nBarcelona Convention for the Protection of the Marine Environment and the Coastal Region of the Mediterranean: Established in 1976, this convention covers the Mediterranean Sea and its coastal areas. It involves 21 countries bordering the Mediterranean, and the European Union. Like the BCC, it focuses on the protection and sustainable development of the marine and coastal environment, but it has a greater emphasis on specific issues such as pollution from land-based sources, pollution by dumping, pollution from ships, and pollution resulting from exploration and exploitation of the continental shelf and the seabed and its subsoil.\n\nWhat makes the BCC unique is that it covers the Benguela Current Large Marine Ecosystem (BCLME), which is one of the richest marine ecosystems on earth and one of the four major eastern boundary upwelling systems. This system is of global importance for marine biodiversity and climate regulation. The BCC is the first to be based on the Large Marine Ecosystem (LME) concept of ocean governance, a concept that is endorsed by the United Nations. The BCC is also unique in its tri-national approach, involving Angola, Namibia, and South Africa, and in its comprehensive coverage of marine conservation, sustainable development, and the sharing of benefits and responsibilities among the contracting parties.\n\n\nOther notable treaties and conventions\nThe examples I provided earlier are some of the key international treaties and conventions that focus on the protection and management of transboundary marine ecosystems. However, there are other important marine conventions and agreements around the world. A few more include:\n\nRamsar Convention on Wetlands: Established in 1971, this convention provides a framework for the conservation and wise use of all wetlands, including marine systems in coastal zones, through local and national actions and international cooperation. It currently includes 171 contracting parties.\nConvention on Biological Diversity (CBD): Although this convention covers all ecosystems, its specific work on marine and coastal biodiversity is very significant. It recognises the ecological, economic, and cultural importance of marine and coastal ecosystems and aims to safeguard them through science-based management practices.\nWestern and Central Pacific Fisheries Convention (WCPFC): This convention specifically aims to conserve and manage highly migratory fish stocks across the western and central Pacific Ocean. It does this by cooperating with relevant countries and stakeholders to ensure long-term sustainability of these resources.\nAntarctic Treaty System: This includes the Antarctic Treaty and related agreements, such as the Convention for the Conservation of Antarctic Marine Living Resources. It’s unique in that it governs the entire Antarctic region, which is recognised as a natural reserve, devoted to peace and science.\nCartagena Convention: Formally known as the Convention for the Protection and Development of the Marine Environment of the Wider Caribbean Region, it aims to protect, develop and manage the Marine Environment of the Wider Caribbean Region in a sustainable way.\n\nThese, along with the ones already mentioned, are some of the many efforts globally to manage and conserve marine ecosystems. Each is unique in its focus, region, challenges, and approach to marine management. The BCC remains notable for its LME-based approach and its focus on one of the world’s most productive marine ecosystems.\n\n\n\n\n\n\n\n\nReferences\n\nSweijd N, Smit A (2020) Trends in sea surface temperature and chlorophyll-a in the seven african large marine ecosystems. Environmental Development 36:100585.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2023,\n  author = {Smit, AJ},\n  title = {Transboundary Systems},\n  date = {2023-05-15},\n  url = {https://tangledbank.netlify.app/pages/Transboundary_systems.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2023) Transboundary systems. https://tangledbank.netlify.app/pages/Transboundary_systems.html."
  },
  {
    "objectID": "pages/graduate_attributes.html",
    "href": "pages/graduate_attributes.html",
    "title": "Graduate attributes",
    "section": "",
    "text": "Key graduate attributes I emphasise in my BDC334, BCB744, and BCB743 syllabi are:\nBCB334, BCB744, and BCB743:\n\nAdvanced subject knowledge Deep understanding of the subject matter, its principles, and current research trends.\nCritical thinking Ability to evaluate scientific literature, identify gaps in knowledge, and propose novel research questions.\nCommunication skills Effective presentation of scientific concepts and research findings, both in written and oral formats, to diverse audiences.\nEthical awareness Understanding and adhering to ethical guidelines and principles in research, including responsible conduct of research, data management, and intellectual property rights.\n\nBCB744 and BCB743 additionally develop:\n\nProblem-solving Capacity to develop innovative solutions for complex scientific challenges.\nResearch skills Proficiency in experimental design, data collection, analysis, interpretation, and reporting of scientific findings.\nCollaboration Teamwork and interdisciplinary cooperation in research projects, fostering a productive scientific environment.\nAdaptability Flexibility and openness to new ideas, methods, and technologies, enabling continuous growth and development in the ever-evolving scientific landscape.\nProject management Planning, organising, and executing scientific projects while managing resources and time effectively.\nProfessional development Commitment to lifelong learning, networking, and career advancement through participation in conferences, workshops, and professional organisations.\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2023,\n  author = {Smit, AJ},\n  title = {Graduate Attributes},\n  date = {2023-04-24},\n  url = {https://tangledbank.netlify.app/pages/graduate_attributes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2023) Graduate attributes. https://tangledbank.netlify.app/pages/graduate_attributes.html."
  },
  {
    "objectID": "pages/promotion_index.html",
    "href": "pages/promotion_index.html",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "",
    "text": "About the square bracket `[]` notation\n\n\n\nA list of the links provided in my Case for Promotion document is provided here. The numbers in square brackets ‘[]’ refer to the footnote in the Case for Promotion document."
  },
  {
    "objectID": "pages/promotion_index.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "href": "pages/promotion_index.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department",
    "text": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department\n[5] I was instrumental in developing South Africa’s first undergraduate Marine Biology curriculum at the University of KwaZulu-Natal in 2007 (with Profs. Perissinotto and Schoeman)\n[6] See a discussion about how I allow modern technologies to influence and shape my teaching\n[7] Views on collaborative learning\n[8] Example exercises and bonus, designed to reward and incentivise continued learning towards advanced skills\n[9] Assessment policy for BCB744\n[10] Explanation of modes of assessment\n[11] Module-specific graduate attributes\n[12] The difference between science and data science\n[13] Thoughts about the learning process\n[14] Access to old test and exam questions"
  },
  {
    "objectID": "pages/promotion_index.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "href": "pages/promotion_index.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules",
    "text": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules\n[15] For an example of information rich text, see the example page\n[16] See the ‘vignettes’ menu at the top of The Tangled Bank.\n[17] For example, the FAQ page for BDC223\n[18] See feedback from colleagues about The Tangled Bank"
  },
  {
    "objectID": "pages/promotion_index.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "href": "pages/promotion_index.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught",
    "text": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught\n[19] Prof. Sophie von der Heyden’s feedback about BCB743 in 2022\n[20] Prof. Sophie von der Heyden’s feedback about BCB744 in 2022\n[21] BCB744 assessment policy\n[22] BCB743 assessment policy\n[23] BDC334 assessment policy\n[24] Student feedback about BDC223, BDC334, BCB744, and BCB743 are available at the links below:\n\nBDC223\nBDC334\nBCB744\nBCB743"
  },
  {
    "objectID": "pages/promotion_index.html#tangled-bank-vignettes-and-reproducible-research",
    "href": "pages/promotion_index.html#tangled-bank-vignettes-and-reproducible-research",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.4. Tangled Bank vignettes and reproducible research",
    "text": "4.1.4. Tangled Bank vignettes and reproducible research\n[25] See my essay on eResearch and reproducible research\n[26] Dr Robert Schlegel’s GitHub page\n[27] Ms Amieroh Abrahams’s GitHub page\n[28] Mr Ross Coppin’s GitHub page\n[29] Examples of vignettes may be access at The Tangled Bank under the ‘vignettes’ menu at the top. For example:\n\nRetrieving Chlorophyll-a Data from ERDDAP Servers\nWavelet analysis of diatom time series\nEvent horizon plots\n\nOther vignettes are at the heatwaveR website in the vignettes top menu."
  },
  {
    "objectID": "pages/promotion_index.html#successful-and-prolific-funding-attraction",
    "href": "pages/promotion_index.html#successful-and-prolific-funding-attraction",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.1. Successful and prolific funding attraction",
    "text": "4.2.1. Successful and prolific funding attraction\n[30] List of the more recent research funding received:\n\n2020 – 2022: Belmont Forum Collaborative Research Action on Transdisciplinary Research for Ocean Sustainability: Ecological and Economic impacts of the intensification of extreme events in the Benguela Upwelling System, Principal Investigator (EXEBUS) PDF\n2019 – 2021: SANOCEAN: Factors influencing the formation, fate and transport of microplastic in marine coastal ecosystems (FORTRAN) PDF\n2019 – 2021: SANOCEAN: Blue growth opportunities in changing kelp forests (BlueConnect) PDF\n2019 – 2023: Horizon 2020: iAtlantic, led by Prof. Murray Robert, own capacity as Regional Coordinator for the SE Atlantic PDF\n\n[31] List of older nationally funded research\n\n2019 – 2021: NRF Global Change Grand Challenge: Earth System Science Research Programme — Extreme Climatic Events in the Coastal Zone, Principal Investigator (ESS180920360856) PDF\nNRF GRANT for 2018 – 2020: Competitive Programme for Rated Researchers —Upwelling dynamics in kelp beds: implications for trophic function PDF\n2017: CHEC/CCT Joint Research Programme 2017: What can kelp loss processes and beach cast patterns tell us about the sandy beach management? PDF\n2015 – 2017: NRF COMPETITIVE PROGRAMME FOR RATED RESEARCHERS (CPRR) — Thermal characteristics of the South African nearshore: implications for biodiversity PDF\n2014 – 2016: NRF COMPETITIVE PROGRAMME FOR RATED RESEARCHERS (CPRR) — Kelps and climate change: South Africa in a global context PDF\n2014 – 2016: NRF GRANT FOR 2014: INCENTIVE FUNDING FOR RATED RESEARCHERS (IPRR) Grant No. IFR14020764026 PDF\n\n[32] My thoughts about the NRF rating system and maintaining my own rating"
  },
  {
    "objectID": "pages/promotion_index.html#development-of-r-packages-in-marine-heatwave-analysis",
    "href": "pages/promotion_index.html#development-of-r-packages-in-marine-heatwave-analysis",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.2. Development of R packages in marine heatwave analysis",
    "text": "4.2.2. Development of R packages in marine heatwave analysis\n[33] The RmarineHeatWaves documentation.\n[34] heatwaveR. Also see the GitHub page.\n[35] This number is hard to track, but a search in Google Scholar for the term “heatwaveR” (inverted commas included) yields at least 150 citations. A shorter list of the citations is provided at https://robwschlegel.github.io/heatwaveR/CITATIONS.html. Notable examples of high-impact publications are provided here:\n\nSmale, D. A., Wernberg, T., Oliver, E. C., Thomsen, M., Harvey, B. P., Straub, S. C., … & Moore, P. J. (2019). Marine heatwaves threaten global biodiversity and the provision of ecosystem services. Nature Climate Change, 9(4), 306-312.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Recent marine heatwaves in the North Pacific warming pool can be attributed to rising atmospheric levels of greenhouse gases. Communications Earth & Environment, 3(1), 131.\nThoral, F., Montie, S., Thomsen, M. S., Tait, L. W., Pinkerton, M. H., & Schiel, D. R. (2022). Unravelling seasonal trends in coastal marine heatwave metrics across global biogeographical realms. Scientific Reports, 12(1), 7740.\nBenedetti-Cecchi, L. (2021). Complex networks of marine heatwaves reveal abrupt transitions in the global ocean. Scientific Reports, 11(1), 1739.\nWoolway, R. I., Jennings, E., Shatwell, T., Golub, M., Pierson, D. C., & Maberly, S. C. (2021). Lake heatwaves under climate change. Nature, 589(7842), 402-407.\nGarcía Molinos, J., Hunt, H. L., Green, M. E., Champion, C., Hartog, J. R., & Pecl, G. T. (2022). Climate, currents and species traits contribute to early stages of marine species redistribution. Communications biology, 5(1), 1329.\nSmith, K. E., Burrows, M. T., Hobday, A. J., Sen Gupta, A., Moore, P. J., Thomsen, M., … & Smale, D. A. (2021). Socioeconomic impacts of marine heatwaves: Global issues and opportunities. Science, 374(6566), eabj3593.\n\n[36] Examples of cross-discipline research in marine heatwaves promoted by the heatwaveR package are provided here:\n\nSchlegel, R. W., Oliver, E. C., & Chen, K. (2021). Drivers of marine heatwaves in the Northwest Atlantic: The role of air–sea interaction during onset and decline. Frontiers in Marine Science, 8, 627970.\nHu, L. (2021). A global assessment of coastal marine heatwaves and their relation with coastal urban thermal changes. Geophysical Research Letters, 48(9), e2021GL093260.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Greenhouse Gas Forcing a Necessary Causation for Marine Heatwaves Over the Northeast Pacific Warming Pool.\n\n[37] Evidence of the application of the heatwaveR package outside of the initially intended field of application, marine science\n\nTassone, S. J., Besterman, A. F., Buelo, C. D., Ha, D. T., Walter, J. A., & Pace, M. L. (2023). Increasing heatwave frequency in streams and rivers of the United States. Limnology and Oceanography Letters, 8(2), 295-304.\nDiniz, F. R., Gonçalves, F. L. T., & Sheridan, S. (2020). Heat wave and elderly mortality: Historical analysis and future projection for metropolitan region of São Paulo, Brazil. Atmosphere, 11(9), 933.\nWoolway, R. I., Albergel, C., Frölicher, T. L., & Perroud, M. (2022). Severe Lake Heatwaves Attributable to Human‐Induced Global Warming. Geophysical Research Letters, 49(4), e2021GL097031.\nReynaert, S., De Boeck, H. J., Verbruggen, E., Verlinden, M., Flowers, N., & Nijs, I. (2021). Risk of short‐term biodiversity loss under more persistent precipitation regimes. Global Change Biology, 27(8), 1614-1626.\nWoolway, R. I., Anderson, E. J., & Albergel, C. (2021). Rapidly expanding lake heatwaves under climate change. Environmental Research Letters, 16(9), 094013.\nPaton, E. (2022). Intermittency analysis of dry spell magnitude and timing using different spell definitions. Journal of Hydrology, 608, 127645.\nMartinez-Baroja, L., Rey-Benayas, J. M., Perez-Camacho, L., & Villar-Salvador, P. (2022). Drivers of oak establishment in Mediterranean old fields from 25-year-old woodland islets planted to assist natural regeneration. European Journal of Forest Research, 141(1), 17-30.\nPappert, D., Barriendos, M., Brugnara, Y., Imfeld, N., Jourdain, S., Przybylak, R., … & Brönnimann, S. (2022). Statistical reconstruction of daily temperature and sea level pressure in Europe for the severe winter 1788/89. Climate of the Past, 18(12), 2545-2565.\nNgoungue Langue, C. G., Lavaysse, C., Vrac, M., & Flamant, C. (2023). Heat wave monitoring over West African cities: uncertainties, characterization and recent trends. Natural Hazards and Earth System Sciences, 23(4), 1313-1333.\n\n[38] For studies that have used metrics calculated by heatwaveR in support of policy development around the management of marine living resources, see this list\n\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2022). A climate-smart spatial planning framework.\nWegscheider, B., Linnansaari, T., Monk, W., Ndong, M., Haralampides, K., St-Hilaire, A., … & Allen, R. (2020). Quantitative modelling of fish habitat under future regulated and hydro-climatically driven flow regimes in the Saint John River (New Brunswick, Canada). Quantitative modelling of existing and future fish habitat in the Saint John River, NB, Canada, 184.\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2023). A metric-based framework for climate-smart conservation planning. Ecological Applications, e2852.\nMuñoz-Pizza, D. M., Sanchez-Rodriguez, R. A., & Manzano, E. G. Linking Climate Change to Urban Planning Through Vulnerability Assessment: The Case of Two Cities at the Mexico-Us Border. Available at SSRN 4348277.\n\n[39] Evidence of examples where such novel research questions and hypotheses have been addressed\n\nLeach, T. S., BuyanUrt, B., & Hofmann, G. E. (2021). Exploring impacts of marine heatwaves: paternal heat exposure diminishes fertilization success in the purple sea urchin (Strongylocentrotus purpuratus). Marine Biology, 168(7), 103.\nPegado, M. R., Santos, C. P., Raffoul, D., Konieczna, M., Sampaio, E., Maulvault, A. L., … & Rosa, R. (2020). Impact of a simulated marine heatwave in the hematological profile of a temperate shark (Scyliorhinus canicula). Ecological Indicators, 114, 106327.\nKraufvelin, L. (2021). Identification of marine heatwaves in the Archipelago Sea and experimental testing of their impacts on the non-indigenous Harris mud crab.\nOliveira, H., Maulvault, A. L., Santos, C. P., Silva, M., Bandarra, N. M., Valente, L. M., … & Anacleto, P. (2023). Can marine heatwaves affect the fatty acid composition and energy budget of the tropical fish Zebrasoma scopas?. Environmental Research, 224, 115504.\nLeach, T. S. (2022). The Role of Pre-and Post-Spawning Temperature Stress on Fertilization Dynamics Within Santa Barbara Channel Sea Urchin Species. University of California, Santa Barbara.\nMinuti, J. J., Byrne, M., Hemraj, D. A., & Russell, B. D. (2021). Capacity of an ecologically key urchin to recover from extreme events: Physiological impacts of heatwaves and the road to recovery. Science of the Total Environment, 785, 147281.\nClare, X. S., Kui, L., & Hofmann, G. E. (2022). Larval Thermal Tolerance of Kellet’s Whelk (Kelletia kelletii) as a Window into the Resilience of a Wild Shellfishery to Marine Heatwaves. Journal of Shellfish Research, 41(2), 283-290.\nMarochi, M. Z., De Grande, F. R., Pardo, J. C. F., Montenegro, Á., & Costa, T. M. (2022). Marine heatwave impacts on newly-hatched planktonic larvae of an estuarine crab. Estuarine, Coastal and Shelf Science, 278, 108122.\nVan Der Walt, K. A., Potts, W. M., Porri, F., Winkler, A. C., Duncan, M. I., Skeeles, M. R., & James, N. C. (2021). Marine Heatwaves Exceed Cardiac Thermal Limits of Adult Sparid Fish (Diplodus capensis, Smith 1884). Frontiers in Marine Science, 8, 702463.\n\n[40] Various online trackers of marine heatwaves use heatwaveR as the underlying processing engine, some of which are reported on my ePortfolio\n\nThe original marine heatwave tracker\nThe Physical Sciences Laboratory heatwave website\nWhaleMap"
  },
  {
    "objectID": "pages/promotion_index.html#student-supervision",
    "href": "pages/promotion_index.html#student-supervision",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.3. Student supervision",
    "text": "4.2.3. Student supervision\n[41] Extract from the NRFOnline system listing most of my post-graduate students"
  },
  {
    "objectID": "pages/promotion_index.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "href": "pages/promotion_index.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)",
    "text": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)\n[42] The The South African Coastal Seawater Temperature Network (SACTN) GitHub page from where data can be downloaded\n[43] Smit, A. J., Roberts, M., Anderson, R. J., Dufois, F., Dudley, S. F., Bornman, T. G., … & Bolton, J. J. (2013). A coastal seawater temperature dataset for biogeographical studies: large biases between in situ and remotely-sensed data sets around the coast of South Africa. PLoS One, 8(12), e81944.\n[44] A few personal well-cited publications that cite the SACTN:\n\nSchlegel, R. W., Oliver, E. C., Wernberg, T., & Smit, A. J. (2017). Nearshore and offshore co-occurrence of marine heatwaves and cold-spells. Progress in Oceanography, 151, 189-205.\nSchlegel, R. W., Oliver, E. C., Perkins-Kirkpatrick, S., Kruger, A., & Smit, A. J. (2017). Predominant atmospheric and oceanic patterns during coastal marine heatwaves. Frontiers in Marine Science, 4, 323."
  },
  {
    "objectID": "pages/promotion_index.html#editorial-contributions",
    "href": "pages/promotion_index.html#editorial-contributions",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.5. Editorial contributions",
    "text": "4.2.5. Editorial contributions\n[45] Associate Editor for Aquatic Botany\n[46] My Reviewer’s profile on Loop for editorial contributions to Frontiers in Ecology & Evolution"
  },
  {
    "objectID": "pages/promotion_index.html#committees-and-programmes",
    "href": "pages/promotion_index.html#committees-and-programmes",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.6. Committees and programmes",
    "text": "4.2.6. Committees and programmes"
  },
  {
    "objectID": "pages/promotion_index.html#academic-lead-kelp-scientific-collaboration-ppp",
    "href": "pages/promotion_index.html#academic-lead-kelp-scientific-collaboration-ppp",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP",
    "text": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP\n[47] Kelp Scientific Collaboration mission statement"
  },
  {
    "objectID": "pages/promotion_index.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "href": "pages/promotion_index.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp",
    "text": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp\n[48] Perceived Value of Kelp\n[49] Kelp, South Africa’s Golden Forests on YouTube\n[50] Akshata Mehta’s MPhil thesis"
  },
  {
    "objectID": "pages/promotion_index.html#blueconnect-engagements",
    "href": "pages/promotion_index.html#blueconnect-engagements",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.3. BlueConnect engagements",
    "text": "4.3.3. BlueConnect engagements\n[51] Invitation letter to the GEAK workshop held in Norway\n[52] BlueConnect March 2020 Field Course"
  },
  {
    "objectID": "pages/promotion_index.html#other-community-engagements-and-capacity-building-contributions",
    "href": "pages/promotion_index.html#other-community-engagements-and-capacity-building-contributions",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.4. Other community engagements and capacity-building contributions",
    "text": "4.3.4. Other community engagements and capacity-building contributions\n[53] See most recent invitation to participate in a capacity building initiative\n[54] Invitation quarterly Kogelberg Marine Working Group meeting"
  },
  {
    "objectID": "pages/promotion_index.html#covid-19-environmental-research-group",
    "href": "pages/promotion_index.html#covid-19-environmental-research-group",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.5. CoVID-19 Environmental Research Group",
    "text": "4.3.5. CoVID-19 Environmental Research Group\n[55] Smit et al (2020) about CoVID-19"
  },
  {
    "objectID": "pages/ABNJ.html",
    "href": "pages/ABNJ.html",
    "title": "Areas Beyond National Jurisdiction",
    "section": "",
    "text": "Areas Beyond National Jurisdiction (ABNJ) refer to the parts of the world’s oceans that fall outside of any country’s Exclusive Economic Zone (EEZ). These areas make up about 64% of the surface of the ocean, and include both the High Seas (the water column beyond the EEZ) and the Area (the seabed and subsoil beyond the limits of national jurisdiction).\nUnlike waters within national jurisdictions, where a country has the exclusive right to exploit resources and is responsible for managing and protecting the marine environment, ABNJ are governed by a complex framework of international laws and agreements.\nThe primary legal framework is the United Nations Convention on the Law of the Sea (UNCLOS), which came into force in 1994. UNCLOS sets out the legal framework for the conservation and sustainable use of oceans and their resources. It establishes the rights and obligations of states in relation to the use of the oceans, and provides mechanisms for dispute resolution.\nIn terms of ABNJ, UNCLOS recognises the concept of “the common heritage of mankind,” which asserts that the resources of the deep seabed beyond national jurisdiction are the common heritage of all humanity and should be managed for the benefit of all. However, the UNCLOS does not provide a comprehensive regime for the conservation and sustainable use of marine biodiversity in ABNJ, which is a gap that current negotiations at the UN are trying to fill.\nIn addition to UNCLOS, there are a number of other international agreements that pertain to ABNJ. These include the Convention on Biological Diversity (CBD), which has developed a set of criteria for identifying ecologically or biologically significant marine areas (EBSAs) in need of protection, including in ABNJ. There’s also the International Maritime Organization (IMO), which has the authority to designate Particularly Sensitive Sea Areas (PSSAs) in need of special protection, including in ABNJ.\nThe governance of ABNJ is a complex and evolving issue, with ongoing discussions at the international level about how to improve the management and conservation of these vast and largely unregulated areas of the ocean. This includes debates over issues such as the establishment of marine protected areas in ABNJ, the regulation of emerging industries like deep-sea mining, and how to share the benefits of marine genetic resources.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2023,\n  author = {Smit, AJ},\n  title = {Areas {Beyond} {National} {Jurisdiction}},\n  date = {2023-05-16},\n  url = {https://tangledbank.netlify.app/pages/ABNJ.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2023) Areas Beyond National Jurisdiction. https://tangledbank.netlify.app/pages/ABNJ.html."
  },
  {
    "objectID": "pages/kaggle_earthquakes.html",
    "href": "pages/kaggle_earthquakes.html",
    "title": "Kaggle Earthquake database",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggthemes)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthhires)\n\nHere’s a map of earthquake location and magnitude (&gt;=5.5) from 1965-2016. The data may be found on Kaggle.\n\nWGS84_proj &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\nNE_proj &lt;- \"+proj=natearth +lon_0=170\"\n\n\nquakes &lt;- read_csv(\"../data/kaggle_earthquakes_database.csv\",\n  skip = 3, col_types = cols(Date = col_date(format = \"%d/%m/%Y\")))\nquakes_sf &lt;- quakes |&gt; \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"),\n    crs = WGS84_proj)\nquakes_sf_trans &lt;- st_transform(quakes_sf, NE_proj)\nhead(quakes_sf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTime\nType\nDepth\nDepth Error\nDepth Seismic Stations\nMagnitude\nMagnitude Type\nMagnitude Error\nMagnitude Seismic Stations\nAzimuthal Gap\nHorizontal Distance\nHorizontal Error\nRoot Mean Square\nID\nSource\nLocation Source\nMagnitude Source\nStatus\ngeometry\n\n\n\n1965-02-01\n13:44:18\nEarthquake\n131.6\nNA\nNA\n6.0\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860706\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (145.616 19.246)\n\n\n1965-04-01\n11:29:49\nEarthquake\n80.0\nNA\nNA\n5.8\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860737\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (127.352 1.863)\n\n\n1965-05-01\n18:05:58\nEarthquake\n20.0\nNA\nNA\n6.2\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860762\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (-173.972 -20.579)\n\n\n1965-08-01\n18:49:43\nEarthquake\n15.0\nNA\nNA\n5.8\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860856\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (-23.557 -59.076)\n\n\n1965-09-01\n13:32:50\nEarthquake\n15.0\nNA\nNA\n5.8\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860890\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (126.427 11.938)\n\n\n1965-10-01\n13:36:32\nEarthquake\n35.0\nNA\nNA\n6.7\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860922\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (166.629 -13.405)\n\n\n\n\n\n\n\nplot(quakes_sf[,\"Magnitude\"])\n\n\n\n\n\nworld_1 &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3) |&gt; \n  st_break_antimeridian(lon_0 = 170) |&gt; \n  st_transform(NE_proj)\n\n\nggplot() +\n  geom_sf(data = world_1, colour = \"grey60\", fill = \"grey70\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = Magnitude, size = Magnitude),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.4) +\n  scale_colour_viridis_c(option = \"mako\", direction = 1) +\n  guides(size = \"none\",\n    colour = guide_colourbar(title = \"Magnitude\",\n      title.position = \"left\")) +\n  coord_sf(expand = FALSE) +\n  labs(x = NULL, y = NULL,\n    title = \"The Kaggle Earthquake Data\",\n    subtitle = \"Significant Earthquakes, 1965-2016\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_line(colour = \"grey90\"),\n    legend.background = element_blank(),\n    legend.title = element_text(angle = 90),\n    legend.title.align = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit,\n  author = {Smit, AJ and Smit, AJ},\n  title = {Kaggle {Earthquake} Database},\n  url = {https://tangledbank.netlify.app/pages/kaggle_earthquakes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A, Smit A Kaggle Earthquake database. https://tangledbank.netlify.app/pages/kaggle_earthquakes.html."
  },
  {
    "objectID": "pages/reproducible_research.html",
    "href": "pages/reproducible_research.html",
    "title": "Reproducible research and the information economy",
    "section": "",
    "text": "The information economy refers to the modern-day and continually evolving economic system where information, knowledge, and data are the primary drivers of productivity, growth, and innovation. In this economy, the creation, distribution, and consumption of information are more valuable than in traditional industries. The information economy relies on technological advancements, particularly in information and communication technologies (ICTs), to enable efficient processing, storage, and data sharing. Tech companies, digital service providers, and knowledge-intensive industries are typically seen as key players in the information economy. As biologists, however, we often overlook how our information pipelines and knowledge-sharing approaches might benefit from the principles that are now deeply ingrained in just about every aspect of our daily lives."
  },
  {
    "objectID": "pages/reproducible_research.html#eresearch-and-the-information-economy",
    "href": "pages/reproducible_research.html#eresearch-and-the-information-economy",
    "title": "Reproducible research and the information economy",
    "section": "",
    "text": "The information economy refers to the modern-day and continually evolving economic system where information, knowledge, and data are the primary drivers of productivity, growth, and innovation. In this economy, the creation, distribution, and consumption of information are more valuable than in traditional industries. The information economy relies on technological advancements, particularly in information and communication technologies (ICTs), to enable efficient processing, storage, and data sharing. Tech companies, digital service providers, and knowledge-intensive industries are typically seen as key players in the information economy. As biologists, however, we often overlook how our information pipelines and knowledge-sharing approaches might benefit from the principles that are now deeply ingrained in just about every aspect of our daily lives."
  },
  {
    "objectID": "pages/reproducible_research.html#embracing-technological-advancements-a-pathway-to-enhanced-research-and-collaboration",
    "href": "pages/reproducible_research.html#embracing-technological-advancements-a-pathway-to-enhanced-research-and-collaboration",
    "title": "Reproducible research and the information economy",
    "section": "Embracing Technological Advancements: A Pathway to Enhanced Research and Collaboration",
    "text": "Embracing Technological Advancements: A Pathway to Enhanced Research and Collaboration\nOver the years, I have enthusiastically adopted various technological advancements, recognising their potential to elevate my research impact both locally and globally and to keep pace with the evolving global landscape. However, I have observed that not all scientists share my enthusiasm for technology, leading to a sense of alienation among some colleagues who prefer traditional research methods where buckets and spades still rule.\nIt appears that, for some individuals, particularly in fields such as biology or ecology, there is a belief that focusing solely on their discipline-specific subject matter is sufficient and that insights from Computer Science Departments hold little relevance. This narrow perspective, in my view, is limiting and stifles creativity.\nBy embracing technology, we can not only broaden our horizons but also enhance our research capabilities and expand collaboration. We must remain open-minded, explore the potential of interdisciplinary learning, and leverage technology to maximise the possibilities in our respective fields."
  },
  {
    "objectID": "pages/reproducible_research.html#the-interconnected-nature-of-science-and-technology-an-ongoing-journey",
    "href": "pages/reproducible_research.html#the-interconnected-nature-of-science-and-technology-an-ongoing-journey",
    "title": "Reproducible research and the information economy",
    "section": "The Interconnected Nature of Science and Technology: An Ongoing Journey",
    "text": "The Interconnected Nature of Science and Technology: An Ongoing Journey\nAs the practice of science has undergone dramatic changes in recent years, driven in part by Moore’s Law, we are now tackling global issues across vast timescales. This transformation is largely attributed to the availability of vast amounts of data, which has necessitated the development of efficient algorithms to establish connections, access subsets, and distil complex information using supervised and unsupervised data-analytical techniques.\nConcurrently, this data explosion has spurred the advancement of hardware capable of handling the computational, memory, and data transfer demands of big data. While it is debatable whether hardware development has facilitated the collection of increasing amounts of data or vice versa, the ultimate takeaway remains the same: technological progress is relentless, and the practice of science must adapt swiftly to keep up. By acknowledging this interconnected nature of science and technology, we can work with agility, ensuring we remain at the forefront of scientific discovery and innovation."
  },
  {
    "objectID": "pages/reproducible_research.html#navigating-modern-science-interdisciplinary-collaboration-transparency-and-data-sharing",
    "href": "pages/reproducible_research.html#navigating-modern-science-interdisciplinary-collaboration-transparency-and-data-sharing",
    "title": "Reproducible research and the information economy",
    "section": "Navigating Modern Science: Interdisciplinary Collaboration, Transparency, and Data Sharing",
    "text": "Navigating Modern Science: Interdisciplinary Collaboration, Transparency, and Data Sharing\nContemporary science is characterised by the convergence of diverse skill sets to address complex problems through interdisciplinary and transdisciplinary research. This approach, however, presents challenges in team dynamics, data sharing, and code management. Additionally, there is an increasing demand for transparency in research methodologies, as exemplified by the International Panel for Climate Change, and the emergence of reproducible research.\nCompliance with data and information-sharing policies, such as the FAIR principles, global standards, national legislative acts, and discipline-specific norms, has become essential. Recognising the value of metadata alongside primary datasets is now the norm. While software offers solutions to these challenges, only a fraction of us, primarily the tech-savvy, possess the willingness to keep pace and fully embrace the opportunities.\nTo advance modern science, it is imperative that we adapt and cultivate the skills necessary to navigate interdisciplinary collaboration, ensure transparency, and adhere to evolving data-sharing standards."
  },
  {
    "objectID": "pages/reproducible_research.html#embracing-modern-technologies-across-disciplines-for-a-future-ready-workforce",
    "href": "pages/reproducible_research.html#embracing-modern-technologies-across-disciplines-for-a-future-ready-workforce",
    "title": "Reproducible research and the information economy",
    "section": "Embracing Modern Technologies Across Disciplines for a Future-Ready Workforce",
    "text": "Embracing Modern Technologies Across Disciplines for a Future-Ready Workforce\nModern technologies are indispensable for those of us working with extensive datasets, whether in climate change, computational linguistics, or small-scale studies. My disregard for traditional disciplinary boundaries has enabled me to stay informed about relevant advancements, driving my determination to develop this website, The Tangled Bank. My motivation is further fuelled by the concern that many colleagues are failing to maintain the necessary interest for continuous advancement.\nA reluctance to embrace change not only affects ourselves but also has a domino effect on postgraduate and undergraduate students. By not nurturing the required skills in students, academics hinder their ability to become well-rounded graduates equipped for the modern workplace and to develop transferable skills that transcend disciplinary boundaries. It is crucial to remember that many graduates, particularly those with Bachelor and Honours degrees, will pursue careers unrelated to their original fields of study. Yet, they want to have a degree that provides skills anywhere their future selves might find themselves.\nTo foster a future-ready workforce, it is necessary that we embrace technological advancements and cultivate adaptable, interdisciplinary skill sets in the next generation of graduates."
  },
  {
    "objectID": "pages/reproducible_research.html#exemplifying-the-importance-of-reproducible-research-and-eresearch-frameworks",
    "href": "pages/reproducible_research.html#exemplifying-the-importance-of-reproducible-research-and-eresearch-frameworks",
    "title": "Reproducible research and the information economy",
    "section": "Exemplifying the Importance of Reproducible Research and eResearch Frameworks",
    "text": "Exemplifying the Importance of Reproducible Research and eResearch Frameworks\nConsider the challenge of conducting reproducible research, which, when addressed, can resolve many eResearch framework issues. A typical PhD student spends a few months writing their thesis, which often serves as the sole evidence of degree completion. However, the majority of the learning and methodological expertise developed during the rest of the degree remains undocumented and eventually forgotten. This wealth of knowledge is rarely shared, leading to repeated dead-ends in knowledge transfer as new candidates embark on similar journeys.\nMost research neglects the full data lifecycle, focusing mainly on the initial steps. The failure to share behind-the-scenes solutions results in non-reproducible research, making the scientific process opaque and fostering public mistrust. This opacity hinders collaboration among supervisors and co-investigators, increases error-proneness, and scales poorly as datasets and complexities grow. Additionally, the research process becomes less efficient due to inadequate documentation of data selection, filtering justifications, metadata tracking, data versions, and processing changes.\nAddressing these challenges is essential to promote reproducible research, enhance collaboration, and build public trust in science, ultimately contributing to a more efficient and transparent research process. This makes the research process extremely wasteful in as far as preserving the full complexity of what a typical student learns."
  },
  {
    "objectID": "pages/reproducible_research.html#promoting-reproducible-research-through-lab-notebooks-and-proper-workflow-management",
    "href": "pages/reproducible_research.html#promoting-reproducible-research-through-lab-notebooks-and-proper-workflow-management",
    "title": "Reproducible research and the information economy",
    "section": "Promoting Reproducible Research through Lab Notebooks and Proper Workflow Management",
    "text": "Promoting Reproducible Research through Lab Notebooks and Proper Workflow Management\nMany solutions exist to address research reproducibility, but I find lab notebooks using RStudio’s markdown (for R users) or Jupyter Lab/Notebooks (for Python users) particularly effective. Version tracking can be achieved using git, such as in GitHub. These notebooks integrate code with text, allowing automatic updates of tables and figures with new data. My students are proficient in this approach, ensuring their work is reproducible.\nI advocate for the widespread adoption of lab notebooks at universities, making them a prerequisite for thesis submission in applicable disciplines. The thesis can be a reproducible document written in markdown, and typeset to various formats such as PDF, HTML, MS Word, or eBook. This method also incorporates proper bibliography management.\nThis reproducible workflow complies with funding instruments requiring data and code sharing, reproducibility, and open publication per FAIR principles. It is already prevalent in disciplines like ecology. While this example focuses on paper or thesis writing, technology impacts research practice across disciplines, commerce, arts, and law. A comprehensive overview is beyond our scope, but the examples provided illustrate the broader possibilities."
  },
  {
    "objectID": "pages/NRF_ratings.html",
    "href": "pages/NRF_ratings.html",
    "title": "NRF Rating: thoughts",
    "section": "",
    "text": "The South African National Research Foundation (NRF) rating system claims to evaluate and benchmark the research performance of individual researchers in the country. The system’s purpose is intended to:\n\nRecognise and reward research excellence The system acknowledges researchers who produce high-quality research and contribute significantly to their respective fields. A favourable rating is supposed to increase recognition, both nationally and internationally, as well as improve funding opportunities.\nEncourage research productivity By providing incentives and recognition for high-quality research, the NRF rating system aims to promote academic productivity and encourages continuous advancement.\nEnhance research capacity It supposedly identifies academics with solid potential and supports the development of research capacity in South Africa by providing funding and other resources to rated researchers.\nFacilitate collaboration The NRF rating system claims to facilitate scientific cooperation by enabling researchers, institutions, and funding agencies to identify potential partners based on their research expertise and performance.\nPromote international competitiveness The NRF suggests that a robust research evaluation system helps to ensure that South African researchers remain competitive on the global stage, which is essential for attracting international funding, partnerships, and talent.\nInform decision-making NRF ratings inform institutional, national, and international decision-making regarding research priorities, funding allocations, and strategic planning, ensuring that resources are directed towards high-impact research.\n\nThere are alternatives to the NRF rating system. The H-index is a globally recognised rapid assessment of research impact, of which Google offers one implementation on their Google Scholar system. This H-index is consistently applied to researchers from any country or any academic discipline. The metric is based on citation data and provides a more objective and quantitative measure of research impact. Since the H-index is easily accessible and hassle-free, it is calculated on the fly using various citation databases, such as Google Scholar. This last point contrasts starkly with the NRF rating system, which is lengthy, and requires significant effort and time from both the applicants and reviewers.\nFurther comparisons of the NRF rating system to a metric such as Google Scholar’s H-index reveal other possible advantages. The NRF’s approach is a more integrated and robust assessment of research ‘prowess’ as the system considers multiple aspects of academic contributions. This includes not only the quality, impact, and significance of research output (similar to the H-index, but differ in how these are assessed) but also a broader contribution to academics’ research fields using assessments that are not based on publications, such as participation in various international bodies, panels and working groups. This integrated assessment leads to a more nuanced evaluation of academic performance that citation metrics, such as the H-index, cannot capture. It also acknowledges academics for their role in developing research capacity, which in South Africa is a critical role that all academics must play.\nNRF ratings are determined through a rigorous peer-review process, which claims to ensure that the evaluations are fair and unbiased. However, despite the peer-review process, personal biases or conflicts of interest may still influence the ratings, and the system could be more objective. The system is also specific to South Africa, and the recognition that might stem from one’s NRF rating does not favour one as much as one would wish to think. This is true especially once international research funding becomes attractive and one is willing to enter more comprehensive international research consortia.\nIn the past, rated researchers were offered incentive funding. This system no longer exists, at least not in the format it was implemented in the early- to mid-2010s. Note, the ‘Competitive Support for Unrated Researchers (CSUR) - 2024 Funding Framework’ and ‘Competitive Programme for Rated Researchers (CPRR) – 2024 Funding Framework’ do take rating into account, but others, such as the ‘African Coelacanth Ecosystem Programme (ACEP) – South African Marine and Antarctic Research Strategy,’ do not. Similarly, international funders, where I will focus my attention in the future, also do not acknowledge NRF ratings.\nWhether or not one maintains an NRF rating depends on personal research values. This should be decided on personal conviction and not dictated by the institution within which one is employed. I have yet to experience the NRF rating system to offer me any tangible advantage regarding recognition of research excellence, encouragement of productivity, enhancement of capacity, the facilitation of collaboration, or enhanced international competitiveness. The only benefit resulting from NRF ratings goes to the employers to inform institutional decision-making.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2023,\n  author = {Smit, AJ},\n  title = {NRF {Rating:} Thoughts},\n  date = {2023-04-24},\n  url = {https://tangledbank.netlify.app/pages/NRF_ratings.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2023) NRF Rating: thoughts. https://tangledbank.netlify.app/pages/NRF_ratings.html."
  },
  {
    "objectID": "pages/How_to_learn.html",
    "href": "pages/How_to_learn.html",
    "title": "How to learn",
    "section": "",
    "text": "This question was asked by 3rd years, but it is relevant here too.\nI just need some advice from sir. I’m starting to study today and I just wanted to know what do you think is the best way to study for sir’s exam. I’m asking because I studied really hard for the midterm tests but I still got low marks lol. So I just wanted to know if there is maybe a different approach that I can take.\n\nI was trying to memorise things for the midterm tests but I know sir said we must focus on understanding but when I focus on understanding then the work does not stick in my head😂.\n\nSo far I’ve just read through the articles but is there any advice that sir can give me please?\n\n\nThank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2). At some point later on you will also know what you don’t know…\n\n“Reports that say that something hasn’t happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don’t know we don’t know. And if one looks throughout the history of our country and other free countries, it is the latter category that tends to be the difficult ones.”\n-- Donald Rumsfeld, February 12, 2002\n\nYou already know things. Structure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles in a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the most important attributes which inform who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful."
  },
  {
    "objectID": "pages/How_to_learn.html#how-does-one-study-to-understand",
    "href": "pages/How_to_learn.html#how-does-one-study-to-understand",
    "title": "How to learn",
    "section": "",
    "text": "This question was asked by 3rd years, but it is relevant here too.\nI just need some advice from sir. I’m starting to study today and I just wanted to know what do you think is the best way to study for sir’s exam. I’m asking because I studied really hard for the midterm tests but I still got low marks lol. So I just wanted to know if there is maybe a different approach that I can take.\n\nI was trying to memorise things for the midterm tests but I know sir said we must focus on understanding but when I focus on understanding then the work does not stick in my head😂.\n\nSo far I’ve just read through the articles but is there any advice that sir can give me please?\n\n\nThank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2). At some point later on you will also know what you don’t know…\n\n“Reports that say that something hasn’t happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don’t know we don’t know. And if one looks throughout the history of our country and other free countries, it is the latter category that tends to be the difficult ones.”\n-- Donald Rumsfeld, February 12, 2002\n\nYou already know things. Structure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles in a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the most important attributes which inform who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful."
  },
  {
    "objectID": "pages/heatwaveR_publ.html",
    "href": "pages/heatwaveR_publ.html",
    "title": "Notable heatwaveR citations",
    "section": "",
    "text": "Smale, D. A., Wernberg, T., Oliver, E. C., Thomsen, M., Harvey, B. P., Straub, S. C., … & Moore, P. J. (2019). Marine heatwaves threaten global biodiversity and the provision of ecosystem services. Nature Climate Change, 9(4), 306-312.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Recent marine heatwaves in the North Pacific warming pool can be attributed to rising atmospheric levels of greenhouse gases. Communications Earth & Environment, 3(1), 131.\nThoral, F., Montie, S., Thomsen, M. S., Tait, L. W., Pinkerton, M. H., & Schiel, D. R. (2022). Unravelling seasonal trends in coastal marine heatwave metrics across global biogeographical realms. Scientific Reports, 12(1), 7740.\nBenedetti-Cecchi, L. (2021). Complex networks of marine heatwaves reveal abrupt transitions in the global ocean. Scientific Reports, 11(1), 1739.\nWoolway, R. I., Jennings, E., Shatwell, T., Golub, M., Pierson, D. C., & Maberly, S. C. (2021). Lake heatwaves under climate change. Nature, 589(7842), 402-407.\nGarcía Molinos, J., Hunt, H. L., Green, M. E., Champion, C., Hartog, J. R., & Pecl, G. T. (2022). Climate, currents and species traits contribute to early stages of marine species redistribution. Communications biology, 5(1), 1329.\nSmith, K. E., Burrows, M. T., Hobday, A. J., Sen Gupta, A., Moore, P. J., Thomsen, M., … & Smale, D. A. (2021). Socioeconomic impacts of marine heatwaves: Global issues and opportunities. Science, 374(6566), eabj3593."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-impact",
    "href": "pages/heatwaveR_publ.html#sec-impact",
    "title": "Notable heatwaveR citations",
    "section": "",
    "text": "Smale, D. A., Wernberg, T., Oliver, E. C., Thomsen, M., Harvey, B. P., Straub, S. C., … & Moore, P. J. (2019). Marine heatwaves threaten global biodiversity and the provision of ecosystem services. Nature Climate Change, 9(4), 306-312.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Recent marine heatwaves in the North Pacific warming pool can be attributed to rising atmospheric levels of greenhouse gases. Communications Earth & Environment, 3(1), 131.\nThoral, F., Montie, S., Thomsen, M. S., Tait, L. W., Pinkerton, M. H., & Schiel, D. R. (2022). Unravelling seasonal trends in coastal marine heatwave metrics across global biogeographical realms. Scientific Reports, 12(1), 7740.\nBenedetti-Cecchi, L. (2021). Complex networks of marine heatwaves reveal abrupt transitions in the global ocean. Scientific Reports, 11(1), 1739.\nWoolway, R. I., Jennings, E., Shatwell, T., Golub, M., Pierson, D. C., & Maberly, S. C. (2021). Lake heatwaves under climate change. Nature, 589(7842), 402-407.\nGarcía Molinos, J., Hunt, H. L., Green, M. E., Champion, C., Hartog, J. R., & Pecl, G. T. (2022). Climate, currents and species traits contribute to early stages of marine species redistribution. Communications biology, 5(1), 1329.\nSmith, K. E., Burrows, M. T., Hobday, A. J., Sen Gupta, A., Moore, P. J., Thomsen, M., … & Smale, D. A. (2021). Socioeconomic impacts of marine heatwaves: Global issues and opportunities. Science, 374(6566), eabj3593."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-cross",
    "href": "pages/heatwaveR_publ.html#sec-cross",
    "title": "Notable heatwaveR citations",
    "section": "Examples of cross-discipline research in marine heatwaves",
    "text": "Examples of cross-discipline research in marine heatwaves\n\nSchlegel, R. W., Oliver, E. C., & Chen, K. (2021). Drivers of marine heatwaves in the Northwest Atlantic: The role of air–sea interaction during onset and decline. Frontiers in Marine Science, 8, 627970.\nHu, L. (2021). A global assessment of coastal marine heatwaves and their relation with coastal urban thermal changes. Geophysical Research Letters, 48(9), e2021GL093260.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Greenhouse Gas Forcing a Necessary Causation for Marine Heatwaves Over the Northeast Pacific Warming Pool."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-outside",
    "href": "pages/heatwaveR_publ.html#sec-outside",
    "title": "Notable heatwaveR citations",
    "section": "Use outside of the initially intended field of application",
    "text": "Use outside of the initially intended field of application\n\nTassone, S. J., Besterman, A. F., Buelo, C. D., Ha, D. T., Walter, J. A., & Pace, M. L. (2023). Increasing heatwave frequency in streams and rivers of the United States. Limnology and Oceanography Letters, 8(2), 295-304.\nDiniz, F. R., Gonçalves, F. L. T., & Sheridan, S. (2020). Heat wave and elderly mortality: Historical analysis and future projection for metropolitan region of São Paulo, Brazil. Atmosphere, 11(9), 933.\nWoolway, R. I., Albergel, C., Frölicher, T. L., & Perroud, M. (2022). Severe Lake Heatwaves Attributable to Human‐Induced Global Warming. Geophysical Research Letters, 49(4), e2021GL097031.\nReynaert, S., De Boeck, H. J., Verbruggen, E., Verlinden, M., Flowers, N., & Nijs, I. (2021). Risk of short‐term biodiversity loss under more persistent precipitation regimes. Global Change Biology, 27(8), 1614-1626.\nWoolway, R. I., Anderson, E. J., & Albergel, C. (2021). Rapidly expanding lake heatwaves under climate change. Environmental Research Letters, 16(9), 094013.\nPaton, E. (2022). Intermittency analysis of dry spell magnitude and timing using different spell definitions. Journal of Hydrology, 608, 127645.\nMartinez-Baroja, L., Rey-Benayas, J. M., Perez-Camacho, L., & Villar-Salvador, P. (2022). Drivers of oak establishment in Mediterranean old fields from 25-year-old woodland islets planted to assist natural regeneration. European Journal of Forest Research, 141(1), 17-30.\nPappert, D., Barriendos, M., Brugnara, Y., Imfeld, N., Jourdain, S., Przybylak, R., … & Brönnimann, S. (2022). Statistical reconstruction of daily temperature and sea level pressure in Europe for the severe winter 1788/89. Climate of the Past, 18(12), 2545-2565.\nNgoungue Langue, C. G., Lavaysse, C., Vrac, M., & Flamant, C. (2023). Heat wave monitoring over West African cities: uncertainties, characterization and recent trends. Natural Hazards and Earth System Sciences, 23(4), 1313-1333."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-policy",
    "href": "pages/heatwaveR_publ.html#sec-policy",
    "title": "Notable heatwaveR citations",
    "section": "Support of policy development around the management of marine living resources",
    "text": "Support of policy development around the management of marine living resources\n\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2022). A climate-smart spatial planning framework.\nWegscheider, B., Linnansaari, T., Monk, W., Ndong, M., Haralampides, K., St-Hilaire, A., … & Allen, R. (2020). Quantitative modelling of fish habitat under future regulated and hydro-climatically driven flow regimes in the Saint John River (New Brunswick, Canada). Quantitative modelling of existing and future fish habitat in the Saint John River, NB, Canada, 184.\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2023). A metric-based framework for climate-smart conservation planning. Ecological Applications, e2852.\nMuñoz-Pizza, D. M., Sanchez-Rodriguez, R. A., & Manzano, E. G. Linking Climate Change to Urban Planning Through Vulnerability Assessment: The Case of Two Cities at the Mexico-Us Border. Available at SSRN 4348277."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-novel",
    "href": "pages/heatwaveR_publ.html#sec-novel",
    "title": "Notable heatwaveR citations",
    "section": "Novel research questions and hypotheses",
    "text": "Novel research questions and hypotheses\n\nLeach, T. S., BuyanUrt, B., & Hofmann, G. E. (2021). Exploring impacts of marine heatwaves: paternal heat exposure diminishes fertilization success in the purple sea urchin (Strongylocentrotus purpuratus). Marine Biology, 168(7), 103.\nPegado, M. R., Santos, C. P., Raffoul, D., Konieczna, M., Sampaio, E., Maulvault, A. L., … & Rosa, R. (2020). Impact of a simulated marine heatwave in the hematological profile of a temperate shark (Scyliorhinus canicula). Ecological Indicators, 114, 106327.\nKraufvelin, L. (2021). Identification of marine heatwaves in the Archipelago Sea and experimental testing of their impacts on the non-indigenous Harris mud crab.\nOliveira, H., Maulvault, A. L., Santos, C. P., Silva, M., Bandarra, N. M., Valente, L. M., … & Anacleto, P. (2023). Can marine heatwaves affect the fatty acid composition and energy budget of the tropical fish Zebrasoma scopas?. Environmental Research, 224, 115504.\nLeach, T. S. (2022). The Role of Pre-and Post-Spawning Temperature Stress on Fertilization Dynamics Within Santa Barbara Channel Sea Urchin Species. University of California, Santa Barbara.\nMinuti, J. J., Byrne, M., Hemraj, D. A., & Russell, B. D. (2021). Capacity of an ecologically key urchin to recover from extreme events: Physiological impacts of heatwaves and the road to recovery. Science of the Total Environment, 785, 147281.\nClare, X. S., Kui, L., & Hofmann, G. E. (2022). Larval Thermal Tolerance of Kellet’s Whelk (Kelletia kelletii) as a Window into the Resilience of a Wild Shellfishery to Marine Heatwaves. Journal of Shellfish Research, 41(2), 283-290.\nMarochi, M. Z., De Grande, F. R., Pardo, J. C. F., Montenegro, Á., & Costa, T. M. (2022). Marine heatwave impacts on newly-hatched planktonic larvae of an estuarine crab. Estuarine, Coastal and Shelf Science, 278, 108122.\nVan Der Walt, K. A., Potts, W. M., Porri, F., Winkler, A. C., Duncan, M. I., Skeeles, M. R., & James, N. C. (2021). Marine Heatwaves Exceed Cardiac Thermal Limits of Adult Sparid Fish (Diplodus capensis, Smith 1884). Frontiers in Marine Science, 8, 702463."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-trackers",
    "href": "pages/heatwaveR_publ.html#sec-trackers",
    "title": "Notable heatwaveR citations",
    "section": "Online trackers of marine heatwaves",
    "text": "Online trackers of marine heatwaves\n\nThe original marine heatwave tracker\nThe Physical Sciences Laboratory heatwave website\nWhaleMap"
  },
  {
    "objectID": "pages/assessment_theory.html",
    "href": "pages/assessment_theory.html",
    "title": "Assessment theory",
    "section": "",
    "text": "BCB744 and BCB743 thoroughly implement formative and summative assessments.\nFormative assessment is ‘academic speak’ for continuous assessment. It provides you with ongoing feedback that you can use to track your performance and to self-evaluate your understanding. Formative assessment also lets me see your development as we progress from simple to more complex topics. Since this is done daily with feedback the next day, I can identify and address any hurdles before they become problematic and impede progression. Formative assessments may include quizzes, discussions, observations, group activities, or small focussed tasks. They are designed to gauge your progress and identify areas of strength or weakness. Continual monitoring and feedback allow me to modify and adapt my instructional strategies in real-time to meet your needs as students better. I intend for this dynamic approach to assessment to create a more engaging, interactive, collaborative, and supportive learning environment, ultimately promoting deeper understanding and long-term retention of knowledge.\nSummative assessment is the second and final mode of assessment. It is designed to evaluate your understanding and mastery of subjects in their full complexity at the end of the learning period. These assessments are in the form of standardised tests or exams and may also comprise comprehensive integrative projects. This mode of assessment provides us (you, me, the BCB Department, and the UWC) with a view of attaining the desired teaching outcomes as stated in the modules’ preambles. It is also a yardstick we use to rate and rank the effectiveness of my instructional methods and the extent to which you have acquired knowledge.\nFormative and summative assessments must inform decisions regarding student advancement and future instructional needs. They contribute to the continuous improvement of the integrated educational program, the curriculum, and teaching practices.\nHere’s a summary of the two modes of assessment:\n\nPurpose Formative assessment mainly monitors your progress and provides feedback during the learning process. In contrast, summative assessment evaluates your performance and understanding at the end of a learning period.\nTiming Formative assessments frequently occur throughout a course or unit, allowing continuous feedback and adjustment. Summative assessments typically occur at the end of a course, unit, or semester.\nFeedback Formative assessment offers real-time, actionable feedback that enables you and me to adjust learning and teaching strategies. Summative assessment provides a more comprehensive evaluation of your knowledge and skills, which can inform future instruction or determine advancement.\nImpact on grades Formative assessments are often weighted less regarding how much tasks contribute to the final mark; it focuses instead on learning and improvement. Summative assessments typically count more towards the final grade and allow us to establish whether you have attained specific learning objectives.\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{smit2023,\n  author = {Smit, AJ},\n  title = {Assessment Theory},\n  date = {2023-04-24},\n  url = {https://tangledbank.netlify.app/pages/assessment_theory.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit A (2023) Assessment theory. https://tangledbank.netlify.app/pages/assessment_theory.html."
  },
  {
    "objectID": "vignettes/README_Lengau.html",
    "href": "vignettes/README_Lengau.html",
    "title": "Using Lengau",
    "section": "",
    "text": "The Lengau High Performance Computing (HPC) system is a supercomputer located at the Centre for High Performance Computing (CHPC) in Cape Town, South Africa. “Lengau” is a Setswana word meaning “cheetah,” reflecting the system’s speed and power.\nThe Lengau system is one of the most powerful supercomputers in Africa. Lengau is used for a wide range of computationally intensive tasks, including climate modelling, bioinformatics, materials science simulations, computational fluid dynamics, and other computations that require large-scale data processing and complex calculations."
  },
  {
    "objectID": "vignettes/README_Lengau.html#lengau",
    "href": "vignettes/README_Lengau.html#lengau",
    "title": "Using Lengau",
    "section": "",
    "text": "The Lengau High Performance Computing (HPC) system is a supercomputer located at the Centre for High Performance Computing (CHPC) in Cape Town, South Africa. “Lengau” is a Setswana word meaning “cheetah,” reflecting the system’s speed and power.\nThe Lengau system is one of the most powerful supercomputers in Africa. Lengau is used for a wide range of computationally intensive tasks, including climate modelling, bioinformatics, materials science simulations, computational fluid dynamics, and other computations that require large-scale data processing and complex calculations."
  },
  {
    "objectID": "vignettes/README_Lengau.html#chpc-quick-start-guide",
    "href": "vignettes/README_Lengau.html#chpc-quick-start-guide",
    "title": "Using Lengau",
    "section": "\n2 CHPC Quick Start Guide",
    "text": "2 CHPC Quick Start Guide\nThe CHPC maintains a quick start guide.\nThe CHPC has approved the registration of my Research Programme entitled “Extreme climatic events in the coastal zone.” The shortname ‘ERTH1192’ has to be used in all associated computations."
  },
  {
    "objectID": "vignettes/README_Lengau.html#login",
    "href": "vignettes/README_Lengau.html#login",
    "title": "Using Lengau",
    "section": "\n3 Login",
    "text": "3 Login\n\n$ ssh asmit@lengau.chpc.ac.za\n$ &lt;password&gt;\n\nOnce ssh’d into login node, make sure to always work within a tmux session.\nThere are two visualisation nodes, which may be useful for access FTP sites via wget (take a look here). One of them, chpclic1, has access to the internet in such that wget can be used to retrieve data from a FTP site. To do this, log into Lengau as usual, then ssh into chpclic1; simply give the command ssh chpclic1 (there is no need for user-ID or password) and the server has exactly the same file systems mounted as the rest of the cluster:\n\n$ ssh chpclic1\n\nSo it is quite convenient to download files from it. One can also switch between the various other nodes by sshing:\n\n$ ssh login1\n$ ssh login2 # this is not for computing as processes will be killed\n$ ssh dtn # the data transfer node"
  },
  {
    "objectID": "vignettes/README_Lengau.html#interactive-nodes",
    "href": "vignettes/README_Lengau.html#interactive-nodes",
    "title": "Using Lengau",
    "section": "\n4 Interactive nodes",
    "text": "4 Interactive nodes\nFor testing code and small jobs, use an interactive node. To request an interactive session on 6x cores, the full command for qsub is:\n\n$ qsub -I -P ERTH1192 -q serial -l select=1:ncpus=6:mpiprocs=6:nodetype=haswell_reg\n\nAbove, the -q serial indicates that we want less than 1 full compute node. Else, for the command for a full core is:\n\n$ qsub -I -P ERTH1192 -q smp -l select=1:ncpus=24:mpiprocs=24:nodetype=haswell_reg\n\nNote:\n\nplease think carefully about whether you really need a full node (-q smp), or if 1, 2 or 3 cores might be sufficient (-q serial)\n\nI selects an interactive job\n\nl informs about the node, cpu, and mpiprocs, etc. specs.\nthe queue must be smp, serial or test\n\ninteractive jobs only get one node: select=1\n\nfor the smp queue you can request several cores: ncpus=24\n\nyou can add -X to get X-forwarding for graphics applications\nyou still must specify your project\nyou can run MPI code: indicate how many ranks you want with mpiprocs=\n\n\nIf you find your interactive session timing out too soon then add -l walltime=4:0:0 to the above command line to request the maximum 4 hours."
  },
  {
    "objectID": "vignettes/README_Lengau.html#the-lustre-file-system",
    "href": "vignettes/README_Lengau.html#the-lustre-file-system",
    "title": "Using Lengau",
    "section": "\n5 The lustre file system",
    "text": "5 The lustre file system\nUse the lustre filesystem for all jobs:\n\n$ /mnt/lustre/users/asmit/\n# or\n$ cd lustre\n\nCopy files to lustre filesystem:\ncd into local directory that has the files, then…\n\n$ scp &lt;file(s)&gt; asmit@scp.chpc.ac.za:/mnt/lustre/users/asmit/&lt;directory&gt;/"
  },
  {
    "objectID": "vignettes/README_Lengau.html#modules",
    "href": "vignettes/README_Lengau.html#modules",
    "title": "Using Lengau",
    "section": "\n6 Modules",
    "text": "6 Modules\nCheck which are available:\n\n$ module avail\n\nLoad one, e.g.:\n\n$ module load chpc/R/3.5.1-gcc7.2.0\n\n# or\n\n$ module load chpc/earth/R/4.3.1"
  },
  {
    "objectID": "vignettes/README_Lengau.html#r-on-lengau",
    "href": "vignettes/README_Lengau.html#r-on-lengau",
    "title": "Using Lengau",
    "section": "\n7 R on Lengau",
    "text": "7 R on Lengau\n\n7.1 Set-up and install packages\nAlthough R is available in the chpc/earth/R/4.3.1 module, I have installed Miniconda and run R within it. This is to avoid some challenges with required libraries that cannot always be located when some R packages (devtools and tidyverse) with the R in the above module.\nTo install Miniconda and R, I used the latest Linux installer and then follow these instructions:\n\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-py311_23.5.2-0-Linux-x86_64.sh\n$ bash Miniconda3-py311_23.5.2-0-Linux-x86_64.sh\n$ conda deactivate # deactivate the base env\n$ conda config --set auto_activate_base false # don't automagically load the base env\n$ conda create --name r_env r-base r-essentials # create a new env with R\n$ pip3 install -U radian # because I like radian instead of the default R console\n\nRunning R then does not require that you load R with one of the modules on Lengau, but you need to activate the conda environment first:\n\n$ conda activate r_env\n\nInstall R packages following the instructions here. Basically, when using conda to install R packages, add r- before the regular package name. For instance, if you want to install ncdf4, use:\n\n$ conda install r-ncdf4\n\nPackages can also be installed within the R (or radian) console with install.packages() but some dependency issues might arise. Compiling and installing R packages must be done in login1 as it has internet access:\n\n$ ssh login1\n# create a new tmux session or load an existing one\n$ radian\n\nTo get heatwaveR running requires a workaround as it is not available via conda. Install RcppArmadillo first with conda install r-RcppArmadillo and then enter the R console and do install.packages(\"heatwaveR\").\n\n7.2 Using R\nIt is recommended to do all light R-related tasks within an interactive node within tmux:\n\n# create a new tmux session or load an existing one\n$ qsub -I -P ERTH1192 -q smp -l select=1:ncpus=24:mpiprocs=24:nodetype=haswell_reg\n$ conda activate r_env\n$ radian\n\nTo find the number of physical CPUs and the total number of cores on the Linux command line:\n\n$ sysctl hw.ncpu hw.physicalcpu\nhw.ncpu: 8\nhw.physicalcpu: 4\n\nOn Lengau from the login node on scp.chpc.ac.za:\n\n$ lscpu | egrep 'CPU\\(s\\)|per core|per socket'\nCPU(s):                24\nOn-line CPU(s) list:   0-23\nThread(s) per core:    1\nCore(s) per socket:    12\nNUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22\nNUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23\n\nAbove is the configuration of a single node on Lengau: i.e. 24 CPUs spread across 2 x 12 core physical units. To undertake calculations, one could either:\n\nuse only one (or half) node and use a multi-core approach to parallelisation to use all of the CPUs on the node; or,\nuse parallelisation tools to spread our computations across multiple host nodes in the cluster.\n\n7.3 Batch processing big jobs\nSome text to follow here…"
  },
  {
    "objectID": "vignettes/README_Lengau.html#hardware-terminology",
    "href": "vignettes/README_Lengau.html#hardware-terminology",
    "title": "Using Lengau",
    "section": "\n8 Hardware terminology",
    "text": "8 Hardware terminology\n\n\nNode: a single motherboard, with possibly multiple sockets for multiple processors\n\nProcessor/Socket: the silicon-containing likely multiple cores (the physical CPU, maybe with multiple cores)\n\nCore: the unit of computation; often has hardware support for multiple…\n\nPseudo-cores: (aka ‘threads’) can appear to the OS as multiple cores but share much functionality with other pseudo-cores on the same core\n\nWall-time: the amount of time your code runs on the cluster\n\nMemory: the amount of memory your job will require to run"
  },
  {
    "objectID": "vignettes/README_Lengau.html#software-terminology-processes-and-threads",
    "href": "vignettes/README_Lengau.html#software-terminology-processes-and-threads",
    "title": "Using Lengau",
    "section": "\n9 Software terminology (processes and threads)",
    "text": "9 Software terminology (processes and threads)\nProcess: data and code in memory:\n\nthere are one or more threads of execution within a process\nthreads in the same process can see most of the same memory\nprocesses generally cannot peer into another processes memory\ninterpreted languages: generally, you can only directly work with processes\ncan call libraries that invoke threads (e.g. BLAS/LAPACK, which has been enabled in R)\n\nThe basic idea of parallel computing:\n\nsplit the problem (or data) into pieces\napply a computation to each piece in parallel (e.g. across multiple cores within a processor and maybe also across multiple nodes in a cluster)\ncombine the results back together\n\nHow does R do this?\n\nfor single node (no inter-node communication): doMC\n\nfor multi-node (with internode communication): foreach, parallel, doMC, doSNOW\n\n\nparallel and foreach functions distribute for loop to resident cores\n\nmulticore, batch & condor serve multicore computers\n\nmclapply applies any function to each element of a vector in parallel (note:… mcparallel works very well for task parallelism; mclapply for data parallelism)\n\nmultidplyr is a backend for dplyr that partitions a data frame across multiple cores\n\nfuture, future.apply, and furrr"
  },
  {
    "objectID": "vignettes/README_tmux.html",
    "href": "vignettes/README_tmux.html",
    "title": "Using tmux",
    "section": "",
    "text": "Always work within tmux\ntmux is a terminal multiplexer, an important tool for scientific computing. It offers a solution for managing multiple terminal sessions simultaneously on local machines and remote servers, such as High-Performance Computing (HPC) systems. Its flexibility allows concurrently operating several console-based applications within a single window, spanning multiple sessions as needed.\nOn a local machine, tmux proves invaluable for maintaining long-running processes or those operating in the background, eliminating the need for a constant active terminal window. This practicality ensures that your work environment remains uncluttered and allows for greater focus on complex tasks at hand.\nIt is within the context of remote systems where tmux demonstrates its convenience. In scientific computing, computations on HPC systems can often extend over long periods, sometimes lasting several hours or even days. During such extensive operations, there may arise a need to disconnect, perhaps due to network instability or simply the wish to transition between different work environments.\ntmux addresses this issue by offering persistent sessions. This means your computations continue uninterrupted even when network connectivity is lost or intentionally disconnected. You can shut down your laptop, move between the office, home, or your favourite coffee shop, and reconnect to your session without disrupting your ongoing processes.\nSo, tmux optimises the potential of remote computing, ensuring maximum productivity and minimum interruption in your scientific work. The adoption of tmux into your workflow not only enhances your computing capabilities but also revolutionises the way you manage your scientific computing."
  },
  {
    "objectID": "vignettes/README_tmux.html#installation",
    "href": "vignettes/README_tmux.html#installation",
    "title": "Using tmux",
    "section": "\n1 Installation",
    "text": "1 Installation\nPrerequisites:\n\n\ntmux &gt;= v2.4\nMac OS X, Linux (tested on Ubuntu 14 and CentOS7), FreeBSD (tested on 11.1)\n\nOn Mac OS X, install the latest 2.6 version with brew install tmux, assuming, of course, Homebrew is installed.\nI rely on a tmux config by samoshkin that makes working with simultaneous local and remote sessions easier. Much of the inspiration for this tutorial comes from him.\nTo install the modified tmux config, execute on both the local machine and the remote host:\n\n$ git clone https://github.com/samoshkin/tmux-config.git\n$ ./tmux-config/install.sh\n\nThe install.sh script does following:\n\ncopies files to ~/.tmux directory\nsymlink tmux config file at ~/.tmux.conf; if an existing ~/.tmux.conf is found it will be backed up\n\nTmux Plugin Manager will be installed at the default location ~/.tmux/plugins/tpm, unless it is already presemt\nrequired tmux plugins will be installed"
  },
  {
    "objectID": "vignettes/README_tmux.html#tmux-on-local-and-remote-machines",
    "href": "vignettes/README_tmux.html#tmux-on-local-and-remote-machines",
    "title": "Using tmux",
    "section": "\n2 tmux on local and remote machines",
    "text": "2 tmux on local and remote machines\n\n2.1 Basic tmux usage on the local machine\nBelow are the most basic steps for getting started with tmux:\n\nOn the command prompt, type tmux new -s &lt;session_name&gt;\n\nRun the desired program\nUse the key sequence Ctrl-a d to detach from the session\nReattach to the tmux session by typing tmux attach-session -t &lt;session_name&gt;\n\nSplit panes with a vertical division (left-right): Ctrl-a |\n\nSplit panes with a horizontal division (top-bottom): Ctrl-a -\n\nNavigation between panes: Ctrl-a →, Ctrl-a ←, Ctrl-a ↑ and Ctrl-a ↓\n\n\n2.2 tmux on remote HPC clusters (persistent ssh sessions)\nIf you spend most of your time ssh-ing to remote hosts, make use of the persistent sessions made possible by tmux.\nUsing tmux on the cluster allows you to create interactive allocations you can detach from. Usually, if you get an interactive allocation and then disconnect from the cluster, for example, by putting your laptop to sleep, your allocation will be terminated and your job killed. By using tmux, you can detach gracefully and tmux will maintain your allocation in an active session running on the remote cluster.\nFirst, ssh into a server (e.g. Lengau) and then establish a tmux session there. Only then initiate the work processes (e.g. ftp, wget, R or other processing, etc.). If you need to shut your local machine down, simply detach from the remote session using Ctrl-a d (or just leave it running). If necessary, log out from the server (or simply leave the ssh connection active) and come back later into the tmux session to resume the work there.\nHere are the steps to follow to do this correctly:\n\n\nssh to the cluster of choice\nStart tmux in the login node (not the compute node)\nInside your tmux session, submit an interactive or batch job\nInside your job allocation (on a compute node), start your application (e.g. R)\nYou can split the session into a duplicate pane and view the process there with htop or bpytop or equivalent\nDetach from tmux by typing Ctrl + a then d and carry on with your life\nLater, on the same login node, reattach by running **tmux** attach\n\n\nMake sure to:\n\nrun tmux on the login node, NOT on compute nodes\nrun salloc inside tmux, not the reverse\n\nWith the snooty new tmux config file, local and remote tmux instances are mapped Ctrl-a. To allow the prefix to be unambiguously assigned to either the local (outer tmux session) machine or the remote (inner tmux session) host, one presses F12 when operating in the local session. This action disables all key bindings and prefix handling in the local session, providing unhindered interaction with the inner remote session as if it were the local one. Consequently, the usual Ctrl-a prefix can be used in the remote session without any interference. This technique ensures that the outer session remains passive, eliminating the chance of any keystroke disruption being transmitted to the inner session.\nSo, when a tmux pane is active on the local machine and I’m connected via ssh to Lengau and another tmux session activated there, one can supply tmux commands to the remote session by first pressing F12 and then Ctrl-a followed by a specific command one wants to run on the remote host. For example, to detach the remote tmux session, type this: Ctrl-a d. Allowing the prefix to again focus on the local machine involves simply pressing F12 again and then one can continue to work there as usual with Ctrl-a &lt;command&gt;.\n\n2.3 Create a new named session\n\n$ tmux new -s &lt;session_name&gt;\n\nFor the GEOMAR FTP download I called it:\n\n&gt; tmux new -s GEOMAR_FTP\n# this is just one window for the wget session\n\nAny other processes are here:\n\n&gt; tmux new -s WORK_SESSION\n# here are two short panes and a tall one\n\n\n2.4 Commands on panes\nHere is the table with the third column removed:\n\n\n\n\n\n\ntmux key\nDescription\n\n\n\nC-a\nDefault prefix, used instead of “C-b”.\n\n\n&lt;prefix&gt; C-e\nOpen ~/.tmux.conf file in your $EDITOR\n\n\n&lt;prefix&gt; C-r\nReload tmux configuration from ~/.tmux.conf file\n\n\n&lt;prefix&gt; r\nRename current window\n\n\n&lt;prefix&gt; R\nRename current session\n\n\n&lt;prefix&gt; -\nSplit new pane horizontally\n\n\n&lt;prefix&gt; |\nSplit new pane vertically\n\n\n&lt;prefix&gt; &lt;\nSelect next pane\n\n\n&lt;prefix&gt; &gt;\nSelect previous pane\n\n\n&lt;prefix&gt; ←\nSelect pane on the left\n\n\n&lt;prefix&gt; →\nSelect pane on the right\n\n\n&lt;prefix&gt; ↑\nSelect pane on the top\n\n\n&lt;prefix&gt; ↓\nSelect pane on the bottom\n\n\n&lt;prefix&gt; C-←\nResize pane to the left\n\n\n&lt;prefix&gt; C-→\nResize pane to the right\n\n\n&lt;prefix&gt; C-↑\nResize pane to the top\n\n\n&lt;prefix&gt; C-↓\nResize pane to the bottom\n\n\n&lt;prefix&gt; &gt;\nMove to next window\n\n\n&lt;prefix&gt; &lt;\nMove to previous window\n\n\n&lt;prefix&gt; Tab\nSwitch to most recently used window\n\n\n&lt;prefix&gt; L\nLink window from another session by entering target session and window reference\n\n\n&lt;prefix&gt; \\\nSwap panes back and forth with 1st pane. When in main-horizontal or main-vertical layout, the main panel is always at index 1. This key binding let you swap secondary pane with main one, and do the opposite.\n\n\n&lt;prefix&gt; C-o\nSwap current active pane with next one\n\n\n&lt;prefix&gt; +\nToggle zoom for current pane\n\n\n&lt;prefix&gt; x\nKill current pane\n\n\n&lt;prefix&gt; X\nKill current window\n\n\n&lt;prefix&gt; C-x\nKill other windows but current one (with confirmation)\n\n\n&lt;prefix&gt; Q\nKill current session (with confirmation)\n\n\n&lt;prefix&gt; C-u\nMerge current session with another. Essentially, this moves all windows from current session to another one\n\n\n&lt;prefix&gt; d\nDetach from session\n\n\n&lt;prefix&gt; D\nDetach other clients except current one from session\n\n\n&lt;prefix&gt; C-s\nToggle status bar visibility\n\n\n&lt;prefix&gt; m\nMonitor current window for activity\n\n\n&lt;prefix&gt; M\nMonitor current window for silence by entering silence period\n\n\n&lt;prefix&gt; F12\nSwitch off all key binding and prefix handling in current window. See “Nested sessions” paragraph for more info\n\n\nexit\nCloses a pane (all processes killed)\n\n\n\nNote that I’ve remapped Ctrl-b (default) to Ctrl-a for easier access (on the local terminal).\n\n\nCtrl-a ; Toggles between the current and previous pane\n\nCtrl-a o Goes to the next pane\n\n2.5 Navigation commands on windows\n\n\nCtrl-a c Creates a new window (with shell)\n\nCtrl-a w Chooses a window from a list\n\nCtrl-a 0 Switches to window 0 (by number )\n\nCtrl-a , Renames the current window\n\nCtrl-a x Closes the current pane\n\nCtrl-a c Creates a new window\n\nCtrl-a p Previous window\n\nCtrl-a n Next window\n\nCtrl-a &lt;number&gt; Navigates amongst windows by number"
  },
  {
    "objectID": "vignettes/README_tmux.html#detaching-and-re-attaching",
    "href": "vignettes/README_tmux.html#detaching-and-re-attaching",
    "title": "Using tmux",
    "section": "\n3 Detaching and re-attaching",
    "text": "3 Detaching and re-attaching\nDetach from a session and return to your normal shell by $ Ctrl-a d. All active processes continue to run. To attach to a session first, you need to find the name of the session. To get a list of the currently running sessions type **tmux** lsand then re-attach by **tmux** attach-session -t &lt;session_name&gt; or **tmux** attach-session -t &lt;number&gt;."
  },
  {
    "objectID": "vignettes/README_tmux.html#clipboard-integration",
    "href": "vignettes/README_tmux.html#clipboard-integration",
    "title": "Using tmux",
    "section": "\n4 Clipboard integration",
    "text": "4 Clipboard integration\nIn the default setting, when you copy text within tmux, it is retained in the private tmux buffer and does not interface with the system clipboard. This also applies when you establish a ssh connection to a remote machine and attach to a tmux session there. The copied text remains confined to the buffer of the remote session, not transferred or synchronised with your local system clipboard. Naturally, if you initiate a local tmux session and subsequently engage in a nested remote session, any copied text will also be exclusive to that session’s buffer and will not reach your system clipboard.\nThis is one of the major limitations of tmux, that you might just decide to give up using it. Let’s explore possible solutions. The overcome this problem samoshkin has implemented some magic resulting in the improved copy/paste functionality documented next.\nThere are some tweaks to copy mode and scrolling behaviour that you should be aware of. There is a root keybinding to enter Copy mode: M-Up. Once in copy mode, you have several scroll controls:\n\n\nM-Up, M-down scroll by line\n\nM-PageUp, M-PageDown scroll by half screen\n\nPageUp, PageDown scroll by whole screen\nscroll by mouse wheel, scroll step is changed from 5 lines to 2\n\n\nSpace starts selection\n\nEnter copies selection and exits copy mode (equivalent to y)\n\nY copies the whole line\n\nD copies to the end of line\n\nprefix C-p lists all items in copy buffer\n\nprexix p pastes the most recent item from the buffer\n\nNote that any trailing newline characters are removed when text is copied. Consequently, when you paste the buffer into a command prompt, it will not execute immediately.\nFurthermore, the mouse can be employed to select text. By default, the action of copying text triggers an immediate exit from the copy mode upon a MouseDragEnd event. This can be quite inconvenient because occasionally, you might just want to highlight the text, but tmux abruptly terminates the copy mode and resets the scroll at the end. To alleviate this issue, a modified behaviour causes the MouseDragEnd event to not prompt the copy-selection-and-cancel action. Consequently, the text is copied but the copy mode is not cancelled, and the selection is not cleared. You can reset the selection simply by clicking the mouse."
  },
  {
    "objectID": "vignettes/README_PBSPro.html",
    "href": "vignettes/README_PBSPro.html",
    "title": "PBS Pro workload manager on Lengau",
    "section": "",
    "text": "PBS Pro is a workload management system that is designed to manage and optimise the scheduling of computational tasks on supercomputers and clusters. In PBS Pro, tasks are represented by jobs, which are units of work that the system schedules and manages. PBS Pro can handle two types of jobs: interactive and batch.\n\nInteractive jobs are those in which the user wants to interact with the job while it’s running. This can be useful for debugging or for running applications that require user input. When you submit an interactive job, PBS Pro allocates resources for it and then provides a shell prompt on one of the allocated nodes where you can run your commands. For example, you could submit an interactive job using the ‘qsub’ command with the ‘-I’ flag:\n\n$ qsub -I -P ERTH1192 -l select=1:ncpus=4:mem=4gb,walltime=01:00:00\n\nThis command submits an interactive job requesting one node, with 4 CPUs, 4 GB of memory, and a maximum runtime of 1 hour.\nBelow is a list of the most commonly used PBS Pro arguments given to qsub used for initiating and specifying an interactive job, together with a brief explanation for each argument:\n\n\n\n\n\n\nArgument\nExplanation\n\n\n\n-I\nInitiates an interactive job.\n\n\n-P projectname\nAssociates the job with a specific project name (ERTH1192). Replace “projectname” with the desired project name. This is usually used in multi-project environments where resources are divided among multiple projects.\n\n\n-N name\nSpecifies the name of the job. Replace name with the desired name.\n\n\n-l select=value:ncpus=value:mem=value\nThis argument specifies the resources required for the job. select is the number of nodes, ncpus is the number of CPUs per node, and mem is the memory required. Replace value with the desired amount.\n\n\n-l walltime=HH:MM:SS\nSpecifies the maximum running time for the job in hours (HH), minutes (MM), and seconds (SS).\n\n\n-q queue\nSpecifies the queue to which the job is submitted. Replace queue with the desired queue name.\n\n\n-j oe\nMerges the standard output and error streams into a single file.\n\n\n-o path_to_file\nSpecifies the path to the file where the standard output stream of the job is saved.\n\n\n-e path_to_file\nSpecifies the path to the file where the standard error stream of the job is saved.\n\n\n-V\nExports all environment variables to the job.\n\n\n-m be\nSends an email at the beginning and the end of the job.\n\n\n\nThe available queues with their nominal parameters are given in the following table. Please take note that these limits may be adjusted dynamically to manage the load on the system.\n\n\n\n\n\n\n\n\n\n\n\n\nQueue Name\nMax. cores per job\nMin. cores per job\nMax. jobs in queue\nMax. jobs running\nMax. time (hrs)\nNotes\nAccess\n\n\n\nserial\n23\n1\n24\n10\n48\nFor single-node non-parallel jobs.\n\n\n\nseriallong\n12\n1\n24\n10\n144\nFor very long sub 1-node jobs.\n\n\n\nsmp\n24\n24\n20\n10\n96\nFor single-node parallel jobs.\n\n\n\nnormal\n240\n25\n20\n10\n48\nThe standard queue for parallel jobs\n\n\n\nlarge\n2400\n264\n10\n5\n96\nFor large parallel runs\nRestricted\n\n\nxlarge\n6000\n2424\n2\n1\n96\nFor extra-large parallel runs\nRestricted\n\n\nexpress\n2400\n25\nN/A\n100 total nodes\n96\nFor paid commercial use only\nRestricted\n\n\nbigmem\n280\n28\n4\n1\n48\nFor the large memory (1TiB RAM) nodes.\nRestricted\n\n\nvis\n12\n1\n1\n1\n3\nVisualisation node\n\n\n\ntest\n24\n1\n1\n1\n3\nNormal nodes, for testing only\n\n\n\ngpu_1\n10\n1\n\n2\n12\nUp to 10 cpus, 1 GPU\n\n\n\ngpu_2\n20\n1\n\n2\n12\nUp to 20 cpus, 2 GPUs\n\n\n\ngpu_3\n36\n1\n\n2\n12\nUp to 36 cpus, 3 GPUs\n\n\n\ngpu_4\n40\n1\n\n2\n12\nUp to 40 cpus, 4 GPUs\n\n\n\ngpu_long\n20\n1\n\n1\n24\nUp to 20 cpus, 1 or 2 GPUs\nRestricted\n\n\n\nBatch jobs, on the other hand, are jobs that can run without user interaction. These are typically used for long-running tasks or for running scripts. When you submit a batch job, you need to provide a script that contains the commands you want to run. For example:\n\n#!/bin/bash\n#PBS -N MyBatchJob\n#PBS -l select=1:ncpus=4:mem=4gb,walltime=01:00:00\n\n$ cd $PBS_O_WORKDIR\n$ ./my_program\n\nThis script, when submitted as a batch job using qsub, will run my_program on a single node with 4 CPUs and 4 GB of memory. The PBS_O_WORKDIR variable is automatically set by PBS Pro to the directory from which the qsub command was run.\nBatch jobs are typically used when you have a set of commands or a script that you want to run without needing to manually intervene or interact with the job while it’s running."
  },
  {
    "objectID": "vignettes/README_PBSPro.html#pbs-pro",
    "href": "vignettes/README_PBSPro.html#pbs-pro",
    "title": "PBS Pro workload manager on Lengau",
    "section": "",
    "text": "PBS Pro is a workload management system that is designed to manage and optimise the scheduling of computational tasks on supercomputers and clusters. In PBS Pro, tasks are represented by jobs, which are units of work that the system schedules and manages. PBS Pro can handle two types of jobs: interactive and batch.\n\nInteractive jobs are those in which the user wants to interact with the job while it’s running. This can be useful for debugging or for running applications that require user input. When you submit an interactive job, PBS Pro allocates resources for it and then provides a shell prompt on one of the allocated nodes where you can run your commands. For example, you could submit an interactive job using the ‘qsub’ command with the ‘-I’ flag:\n\n$ qsub -I -P ERTH1192 -l select=1:ncpus=4:mem=4gb,walltime=01:00:00\n\nThis command submits an interactive job requesting one node, with 4 CPUs, 4 GB of memory, and a maximum runtime of 1 hour.\nBelow is a list of the most commonly used PBS Pro arguments given to qsub used for initiating and specifying an interactive job, together with a brief explanation for each argument:\n\n\n\n\n\n\nArgument\nExplanation\n\n\n\n-I\nInitiates an interactive job.\n\n\n-P projectname\nAssociates the job with a specific project name (ERTH1192). Replace “projectname” with the desired project name. This is usually used in multi-project environments where resources are divided among multiple projects.\n\n\n-N name\nSpecifies the name of the job. Replace name with the desired name.\n\n\n-l select=value:ncpus=value:mem=value\nThis argument specifies the resources required for the job. select is the number of nodes, ncpus is the number of CPUs per node, and mem is the memory required. Replace value with the desired amount.\n\n\n-l walltime=HH:MM:SS\nSpecifies the maximum running time for the job in hours (HH), minutes (MM), and seconds (SS).\n\n\n-q queue\nSpecifies the queue to which the job is submitted. Replace queue with the desired queue name.\n\n\n-j oe\nMerges the standard output and error streams into a single file.\n\n\n-o path_to_file\nSpecifies the path to the file where the standard output stream of the job is saved.\n\n\n-e path_to_file\nSpecifies the path to the file where the standard error stream of the job is saved.\n\n\n-V\nExports all environment variables to the job.\n\n\n-m be\nSends an email at the beginning and the end of the job.\n\n\n\nThe available queues with their nominal parameters are given in the following table. Please take note that these limits may be adjusted dynamically to manage the load on the system.\n\n\n\n\n\n\n\n\n\n\n\n\nQueue Name\nMax. cores per job\nMin. cores per job\nMax. jobs in queue\nMax. jobs running\nMax. time (hrs)\nNotes\nAccess\n\n\n\nserial\n23\n1\n24\n10\n48\nFor single-node non-parallel jobs.\n\n\n\nseriallong\n12\n1\n24\n10\n144\nFor very long sub 1-node jobs.\n\n\n\nsmp\n24\n24\n20\n10\n96\nFor single-node parallel jobs.\n\n\n\nnormal\n240\n25\n20\n10\n48\nThe standard queue for parallel jobs\n\n\n\nlarge\n2400\n264\n10\n5\n96\nFor large parallel runs\nRestricted\n\n\nxlarge\n6000\n2424\n2\n1\n96\nFor extra-large parallel runs\nRestricted\n\n\nexpress\n2400\n25\nN/A\n100 total nodes\n96\nFor paid commercial use only\nRestricted\n\n\nbigmem\n280\n28\n4\n1\n48\nFor the large memory (1TiB RAM) nodes.\nRestricted\n\n\nvis\n12\n1\n1\n1\n3\nVisualisation node\n\n\n\ntest\n24\n1\n1\n1\n3\nNormal nodes, for testing only\n\n\n\ngpu_1\n10\n1\n\n2\n12\nUp to 10 cpus, 1 GPU\n\n\n\ngpu_2\n20\n1\n\n2\n12\nUp to 20 cpus, 2 GPUs\n\n\n\ngpu_3\n36\n1\n\n2\n12\nUp to 36 cpus, 3 GPUs\n\n\n\ngpu_4\n40\n1\n\n2\n12\nUp to 40 cpus, 4 GPUs\n\n\n\ngpu_long\n20\n1\n\n1\n24\nUp to 20 cpus, 1 or 2 GPUs\nRestricted\n\n\n\nBatch jobs, on the other hand, are jobs that can run without user interaction. These are typically used for long-running tasks or for running scripts. When you submit a batch job, you need to provide a script that contains the commands you want to run. For example:\n\n#!/bin/bash\n#PBS -N MyBatchJob\n#PBS -l select=1:ncpus=4:mem=4gb,walltime=01:00:00\n\n$ cd $PBS_O_WORKDIR\n$ ./my_program\n\nThis script, when submitted as a batch job using qsub, will run my_program on a single node with 4 CPUs and 4 GB of memory. The PBS_O_WORKDIR variable is automatically set by PBS Pro to the directory from which the qsub command was run.\nBatch jobs are typically used when you have a set of commands or a script that you want to run without needing to manually intervene or interact with the job while it’s running."
  }
]