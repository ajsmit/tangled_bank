[
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html",
    "href": "BDC334/BDC334-Lecture-Transcripts.html",
    "title": "",
    "section": "",
    "text": "BDC334: Biogeography & Global EcologyBCB334 Lecture Transcript Code",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-core-material",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-core-material",
    "title": "",
    "section": "The Core Material",
    "text": "The Core Material\nPlease consider the lecture transcripts in this book as the core material for your BDC334 course — an online version of this same material is provided on my Tangled Bank website, specifically the web pages about BDC334. Have a look there for further information. Those materials provide supplementary information and should be read alongside the content contained within this book. Everything there is examinable.\nOn the Tangled Bank, you will also find links to the various practical sessions (the Labs). In addition, some data necessary for completing the various laboratory exercises can be downloaded from there. A range of other information is available as well.\nOn the About page, on the left-hand side under the BDC334 website link on Tangled Bank, you will find details about the lecture schedule, when the practical sessions occur, and a collection of other necessary information you will need throughout this module. I will also list the dates and times of the two class tests that you are expected to complete during the course of this module.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-importance-of-reading-in-scientific-training",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-importance-of-reading-in-scientific-training",
    "title": "",
    "section": "The Importance of Reading in Scientific Training",
    "text": "The Importance of Reading in Scientific Training\nA point I want to emphasise — perhaps at the risk of sounding old-fashioned — is the necessity of reading detailed, long-form scientific literature. Many students, nowadays, prefer consuming information in bite-sized chunks that fit onto a mobile phone screen; however, scientific knowledge and argumentation require sustained engagement with complete texts. This is the mode of teaching that informed my own education 20 or 30 years ago, and it remains indispensable for your intellectual development. The technical expertise in this module is largely around matrix interpretation, not computation; your focus should be on synthesising qualitative knowledge across sources.\nYour assessments will involve long-form, essay-type questions, not superficial answers drawn from isolated papers. You will be expected to draw upon and integrate your understanding from several readings with the lecture material into a cohesive response. This kind of applied knowledge is necessary, not just for in-person or remote assessments, but also in professional scientific communication.\nTherefore, online, alongside the lecture material and accompanying slides, I’ll be providing various papers for you to read. Direct links to the papers are provided — follow those links to download them. I expect you to read and understand all these papers. If there’s anything you don’t grasp, please discuss it with your classmates or arrange an appointment with me — either in a group of three, four, or more — on Monday afternoons, Wednesday mornings, or Thursday afternoons during the practical sessions. You can schedule meetings with me then to discuss such matters.\nThe following papers are expected to be read as part of many of the upcoming lectures. Please keep an eye out in the lectures for specific mention of these papers and make sure that you read them well in advance of attending my lectures. Everything in these papers is examinable and you are expected to read all of it and know all of it. The expected additional reading includes the following (their full titles are in the References section at the end):\nWeek 1:\n\nKeith et al. (2012)\nShade et al. (2018)\nMcGill (2019)\n\nWeek 2:\n\nNekola and White (1999)\nSmit et al. (2017)\nTittensor et al. (2010)\n\nWeek 3:\n\nShade et al. (2018)\n\nWeek 4:\n\nChapin III et al. (2000)\nGotelli and Chao (2013)\nMaxwell et al. (2016)\nTilman et al. (2017)\n\nWeek 5:\n\nBurger et al. (2012)\nCostanza et al. (1997)\nCostanza et al. (2014)\n\nThere are also several other papers that I mention throughout all the lectures. These are intended to provide background information that will assist you in understanding the lecture content a bit better. Unlike the papers mentioned above, it is not expected that you read, know, and fully understand them; however, they are important for providing additional context that will facilitate your understanding of specific issues raised in some of the labs and certain lectures.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#developing-your-own-study-framework-and-academic-integrity",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#developing-your-own-study-framework-and-academic-integrity",
    "title": "",
    "section": "Developing Your Own Study Framework and Academic Integrity",
    "text": "Developing Your Own Study Framework and Academic Integrity\nIf you feel, after reading these assigned papers, that your comprehension is inadequate, take the initiative to seek further information in the primary scientific literature. It is not sufficient to rely solely on the specific documents I have distributed; critical engagement with broader literature is a core expectation at university level.\nIn terms of referencing, please note: websites are generally not accepted as valid sources. Peer-reviewed publications, indicated in the reference lists of your readings, are the standard. This is the academic protocol you should follow—building up your knowledge from scientifically credible sources.\nSome of you have enquired about the need for textbooks. While textbooks can be helpful, they are ultimately compilations of information available in the primary literature. The expectation is that, as mature university students, you will be able to navigate primary sources as needed.\nAs always, if you have questions or are struggling with particular concepts, you are welcome to reach out to me—on WhatsApp, by email, or during Wednesday morning lectures.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#labs",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#labs",
    "title": "",
    "section": "Labs",
    "text": "Labs\nDuring the course of this module, there will be four labs, or practicals. These labs will take place on the fifth floor computer lab of the Biodiversity and Conservation Biology Department building. It is expected that you attend all of these practicals.\nFor those of you who have your own personal computers or laptops, please bring them with you. It will probably help you a great deal if you get the necessary software set up on your own computers. The demonstrator and I will guide you through the installation processes and ensure that you have the required software, R and RStudio, installed on your laptops.\nIf you do not have a laptop, you are welcome to use the facilities in the computer lab. All of the workstations there have the necessary software installed.\nDuring the first week, we will do some exercises in Excel. Thereafter, in the following week, we will provide a brief introduction to the software R, running within RStudio.\nFor those of you who are apprehensive about using scripting or coding languages, please note it is a necessary component of modern ecological research. So, the intention of these next few weeks is to give you a brief, introductory background into scripting languages, with the aim of solving some ecologically relevant problems.\nThroughout all of these exercises, both myself and the demonstrator will be available, walking around the floor to assist you with any questions that you might have. Thank you.\n\nLab 1\nThis Lab accompanies the following lectures:\n\nChapter 5 on Multivariate Data and the rest of this page.\n\nThe data for this Lab pertains to the Doubs River (Verneaux 1973; Borcard et al. 2011) study and some toy data, which may be found at the links below:\n\nThe environmental data – DoubsEnv.csv\nThe species data – DoubsSpe.csv\nThe spatial data – DoubsSpa.csv\nExample xyz data – Euclidean_distance_demo_data_xyz.csv\n\n\n\nLab 2\nLabs 2a and 2b accompany the following lecture:\n\nChapter 5 (in this book) on Multivariate Data and the rest of this page.\n\nLab 2b uses these data:\n\nExample xyz data – Euclidean_distance_demo_data_xyz.csv\nExample env data – Euclidean_distance_demo_data_env.csv\nThe seaweed environmental data (Smit et al. 2017) – SeaweedEnv.RData\nThe seaweed coastal sections (sites) – SeaweedSites.csv\nThe Doubs River environmental data – DoubsEnv.csv\n\n\n\nLab 3\nThis Lab accompanies the following lecture:\n\nLecture 4: Biodiversity Concepts\n\nThe data for this Lab are the seaweed (Smit et al. 2017) as well as some toy data at the links below:\n\nThe seaweed species data – SeaweedSpp.csv\nThe seaweed environmental data – SeaweedEnv.csv\nThe seaweed coastal sections – SeaweedSites.csv\nThe fictitious light data light_levels.csv\n\n\n\nLab 4\nFinally, this Lab accompanies:\n\nLecture 6: Unified Ecology\n\nThe data for this Lab include:\n\nThe Barro Colorado Island Tree Counts data (Condit et al. 2002) – load vegan and load the data with data(BCI)\nThe Oribatid mite data (Borcard et al. 1992; Borcard and Legendre 1994) – load vegan and load the data with data(mite)\nThe seaweed species data (Smit et al. 2017) – SeaweedSpp.csv\nThe Doubs River species data (Verneaux 1973; Borcard et al. 2011) – DoubsSpe.csv",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#questions-answers",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#questions-answers",
    "title": "",
    "section": "Questions & Answers",
    "text": "Questions & Answers\nBefore you approach me with questions about the coursework, I’d like you to do one thing: explain your thought processes up to the point where you get stuck. So, for example, if you have a question about some aspect of, say for argument’s sake, beta diversity, then before I answer your question, I want you to explain what you’ve thought about beta diversity thus far and where you become stuck — where your thinking could not proceed. Once you can demonstrate your reasoning process up until that point, I’ll be happy to take over from there. I do need some evidence from you that you’ve honestly tried — either individually or in collaboration with others in the class — to develop an explanation for the area you’re finding difficult.\nOkay. Let’s start with the lectures.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#papers-to-read",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#papers-to-read",
    "title": "",
    "section": "Papers to Read",
    "text": "Papers to Read\nThe first very important paper that you need to read discusses the whole concept of macroecology. The paper was written by Sally Keith and her colleagues, and it was published in \\(2012\\). What is macroecology? It deals with the development of the field over the past \\(30\\text{--}35\\) years or so, since the coining of the term by Brown and Maurer (1989). The paper highlights the necessity of understanding the processes that are linked to the development of structure in biodiversity across the face of Earth. Additionally, it addresses the breaking down of barriers that existed between “old-fashioned” population and community ecology, moving instead towards a far more comprehensive and integrated view of ecology. This integrated perspective brings together knowledge acquired from a wide variety of new applications.\nPlease read that paper, as it is quite foundational to the development of your understanding of the concept of macroecology.\nThe second foundational paper for this week is by Ashley Shade and colleagues published in 2018. It is going to emerge several times during the course of this module, such as in Lecture 3 and Lecture 5. This is essentially a review detailing the core principles of macroecology. It aims to unite ecological understanding across various scales, ranging from the very small to the very large, and spanning local spatial interactions through to global patterns.\nParticularly notable within this paper are what can almost be called ‘laws’ of biodiversity. These are general patterns and principles that recur consistently, whether in microbes like bacteria and archaea, or in much larger organisms such as blue whales. The population dynamics that apply to small organisms also scale to larger ones, reinforcing the universality of certain ecological dynamics.\nHowever, you must not study the material from this paper in isolation from its broader context. The definitions and key ideas—some of which I have highlighted for you—are crucial, but they must be considered within the flow of the entire document.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#introduction-to-ecosystems",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#introduction-to-ecosystems",
    "title": "",
    "section": "Introduction to Ecosystems",
    "text": "Introduction to Ecosystems\n\n\n\nSlide 2\n\n\n\n\n\nSo, we’re going to look at a conceptual overview of what ecosystems are, their characteristics, and what makes ecosystems work (Slide 2). An ecosystem is easy to observe when you go out into nature; what you see is, indeed, an ecosystem. However, they’re present because something explains their existence at a particular place and time. These are the environmental factors that drive them, support their operation, and allow them to function.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients-and-biodiversity",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients-and-biodiversity",
    "title": "",
    "section": "Environmental Gradients and Biodiversity",
    "text": "Environmental Gradients and Biodiversity\nWe’ll discuss the broad concept of gradients in biodiversity, which is important for you to consider. You need to think about all the gradients in abiotic variables that exist across the surface of the planet. An obvious example of a gradient is the one that exists from the tropical regions at low latitudes to the high latitudes, the polar regions.\nAs we move from the tropical regions towards the polar regions, it becomes progressively colder. The day length, or the ratio between day and night, changes significantly, and the seasonal effect becomes more pronounced. The amount of light decreases, and so on. There are many different factors that vary along these large gradients from tropical to polar regions.\nThere are also similarly strong gradients that exist on local scales. For example, looking at Cape Point, there’s a very strong gradient in temperature as we move from the western side of Cape Point, around Cape Point, and into False Bay; as you move, the temperatures become increasingly warmer. That’s a gradient that exists on a small spatial scale, but you also have global scale gradients.\nThe intention of macroecology is to understand how ecosystems are structured along these gradients.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#ecosystem-structure-and-human-influences",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#ecosystem-structure-and-human-influences",
    "title": "",
    "section": "Ecosystem Structure and Human Influences",
    "text": "Ecosystem Structure and Human Influences\nWe’ll also discuss what it means for an ecosystem to have structure. As we’ve just spoken about gradients, most of these are natural gradients. However, there are also anthropogenic gradients — human impacts or factors. These are things that people do which cause ecosystems to change, affecting how they function and how they’re structured.\nTo demonstrate these various principles, we’ll explore a selection of the more interesting and important ecosystems on the planet, and I’ll leave it to you to decide which ones you find most interesting. You’ll have the opportunity to explore some of your own ecosystems, looking at them in terms of both anthropogenic impacts and the natural influences that make them different from other ecosystems. We’ll investigate some of the more important gradients responsible for structuring ecosystems and examine their characteristics in terms of biodiversity, structure, form, and so on.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#course-structure-professor-boatwright",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#course-structure-professor-boatwright",
    "title": "",
    "section": "Course Structure: Professor Boatwright",
    "text": "Course Structure: Professor Boatwright\n\n\n\nSlide 3\n\n\n\n\n\nProfessor Boatwright will take over in the fourth term. He will cover other aspects of macroecology and global ecology, including subjects like continental drift and glaciation (Slide 3). This will involve looking back into the palaeohistories of Earth, so his emphasis will be more historical, whereas my emphasis will be on contemporary processes and those we anticipate in the future. In fact, we can state with a great deal of confidence — up to perhaps about \\(100\\) years, possibly \\(150\\) years — what the future climate, temperature, and other variables on Earth will likely be. Because ecosystems respond to changes in these factors over such time scales, we can also infer the future biogeography and macroecology of systems.\nProfessor Boatwright will also delve into phylogeography, which deals with the genetic lineages of different forms of life across Earth’s surface, and how these are structured as a consequence of continental drift and glaciation. He will further explore current patterns in body size and population size as related to biodiversity and distribution. Lastly, he will cover the theory of island biogeography.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#gradients-beyond-earth",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#gradients-beyond-earth",
    "title": "",
    "section": "Gradients Beyond Earth",
    "text": "Gradients Beyond Earth\n\n\n\nSlide 4\n\n\n\n\n\nThose gradients I mentioned also exist on much larger scales — outside of Earth itself (Slide 4). For instance, consider the arrangement of all the various planets from Mercury, Venus, Earth, Mars, and so on. As you move farther away from the Sun, it’s not necessarily that it becomes colder immediately, but the amount of heat available becomes less and less. At a certain distance from the Sun, we find Earth, where the conditions are just right for water to exist as a liquid, as ice, and as vapour in clouds.\nGo closer to the Sun and you come to Venus, which is the second planet from the Sun. There, it’s too warm and no water is available at all. Move a little further away and you reach Mars, the fourth planet from the Sun, where it’s a bit too cold, so most of the available water occurs as ice. Progress even further and, on the distant planets, even some elements typically gaseous on Earth exist as ice. This gradient in the solar system — a function of distance from the Sun — is what creates Earth’s unique set of conditions that permit life, as it depends on the presence of liquid water, ice, and vapour.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#outline-of-topics-for-this-module",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#outline-of-topics-for-this-module",
    "title": "",
    "section": "Outline of Topics for This Module",
    "text": "Outline of Topics for This Module\n\n\n\nSlide 5\n\n\n\n\n\nIn my section of the module, we will start by explaining what macroecology is, contrasting it with more traditional approaches to ecology (Slide 5). We will explore various concepts related to diversity. Then, we will discuss how to do macroecology, which will require us to examine some data and look at the properties of datasets from which we can extract knowledge about how ecosystems are structured in space and time, and how they function. To do this, we’ll need to understand some slightly mathematical concepts, including similarity and dissimilarity matrices.\nLater, we’ll consider some unifying theories of macroecology. In recent years, there has been a movement toward finding unifying explanations for ecological patterns and processes on Earth. In the past, there were collections of hypotheses for different situations, varying according to organism size, the nature of the ecosystem, and so forth — separate theories for marine, aquatic, soil, terrestrial environments, etc. But today, there is an interest in looking at all these aspects in an integrated way.\nThen, we will examine what biodiversity is, why it’s important, and what differentiates ecosystems with high biodiversity from those with reduced diversity. We’ll also look at the principles of biodiversity’s value — the “so what” question — by considering ecological goods and services. What benefits do people derive from nature? Why does biodiversity matter for us? Even if you do not live in a natural ecological system — because it’s been transformed into, say, a residential area — you are still dependent on the wellbeing of natural portions of Earth. If those landscapes lack biodiversity, people would be far worse off.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#looking-into-the-future-and-broader-applications",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#looking-into-the-future-and-broader-applications",
    "title": "",
    "section": "Looking into the Future and Broader Applications",
    "text": "Looking into the Future and Broader Applications\nWe will then look to the future by considering global change and sustainability. We will also see if we can find some parallels between macroecology and infectious diseases, perhaps even try to understand whether the COVID epidemic makes more sense given our knowledge of macroecology.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#definition-of-macroecology",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#definition-of-macroecology",
    "title": "",
    "section": "Definition of Macroecology",
    "text": "Definition of Macroecology\n\n\n\nSlide 6\n\n\n\n\n\n\n\n\nSlide 7\n\n\n\n\n\nI have already spoken a bit about this, but let me say a bit more. What is macroecology (Slides 6-7)? If you were to summarise it in a sentence or two, it is the study of the mechanisms underlying general patterns in ecology, across scales. There are words there worth unpacking — ‘patterns’ is probably one, and patterns in ecology across scales. The two important ideas — patterns and scales — we’ll be unpacking further, if not today then in due course. I’ll show you what “patterns” in ecological space can look like. But that’s the essence of macroecology.\nLet’s examine the definition in a little more detail.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#traditional-approaches-in-ecology",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#traditional-approaches-in-ecology",
    "title": "",
    "section": "Traditional Approaches in Ecology",
    "text": "Traditional Approaches in Ecology\n\n\n\nSlide 8\n\n\n\n\n\nTo understand macroecology, you first need to understand how ecology has traditionally been practised (Slide 8). Going back perhaps a hundred years or more, even to Darwin’s era, ecology was about observing and investigating individual species. That is, the study of populations — a collection of individuals of the same species, occupying a specific space and time. The focus was to examine the dynamics of a species within a population: how it is affected by the environment, by other species sharing the same space, and so on. Traditional ecology, then, was very local in scale — limited to what you could see, for instance, standing at Cape Point and surveying the kelp forest before you. The boundaries of that study would be as far as your eye could discern the kelp — very much a local scale.\nBut this ignores that kelp occurs not only at Cape Point, but also in Norway, Iceland, and elsewhere worldwide. Macroecology would look at kelp not only in South Africa, but also Norway, Iceland, the United States, Canada, and everywhere kelp occurs. The aim is an integrated understanding of the processes that make kelp forests work, regardless of whether they are found in South Africa or New Zealand. Traditional ecology, by contrast, kept its focus strictly local.\nNow, due to advances in technology, data processing, and the sorts of questions we’re able to ask, the scope — the scale — of our enquiry has greatly expanded. Today, macroecology can examine patterns at the global level. Darwin embarked on a voyage round the world in the Beagle, observing numerous locales — it took him two, perhaps three years. Today, in just \\(24\\) hours, we can obtain a ‘snapshot’ of the entire Earth and collect sufficient ecological data world-wide, something previously unimaginable [attention: Darwin’s ability to analyse global ecology in a single synthesis was much more limited than described here]. Thus, new technologies have altered our perspective.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#biodiversity-definitions-and-scales",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#biodiversity-definitions-and-scales",
    "title": "",
    "section": "Biodiversity: Definitions and Scales",
    "text": "Biodiversity: Definitions and Scales\n\n\n\nSlide 9\n\n\n\n\n\nBiodiversity is another key concept — it appears throughout this module, including in its very name. The traditional definition, as described by the International Union for Conservation of Nature (IUCN; Slide 9), defines biodiversity as “the variability among living organisms from all sources, including terrestrial, marine and other aquatic ecosystems and the ecological complexes of which they are a part. This includes diversity within species, between species, and of ecosystems.”\nAgain, the question of scale becomes evident — diversity within species, for example, means taking humans: within Homo sapiens there is great diversity, all the way down to genetic differences. That’s a scale we can go down to — though Prof Boatwright will cover genetics; I won’t get into that aspect here.\nDiversity also exists between species — species occupying the same ecosystem. In a kelp forest you might have Ecklonia maxima, Laminaria pallida, Macrocystis pyrifera, various red baits, fish, sharks, and so on — all interacting within the kelp forest. Then, there is diversity at the ecosystem level — kelp forests interact with pelagic ecosystems nearby, with the rocky shore, with coastal dunes on the land, and so forth. Globally, a diversity of ecosystems exists, each with its own species assemblages and modes of environmental interaction.\nSo, biodiversity is essentially all life on Earth, at all the various scales in which we observe it, and in all the different configurations, forming habitats or ecosystems regardless of location — from \\(11,000\\,\\mathrm{m}\\) below the ocean surface to the summit of Mount Everest.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#populations-communities-and-the-move-to-macroecology",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#populations-communities-and-the-move-to-macroecology",
    "title": "",
    "section": "Populations, Communities, and the Move to Macroecology",
    "text": "Populations, Communities, and the Move to Macroecology\nSo, we’ve mentioned populations (collections of one species) and communities (collections of multiple species). Ecology studies the processes by which species relate to their environment, to each other, and how the environment influences both populations and communities.\nMacroecology naturally starts from population and community ecology: it makes sense to move from the local scale, to groups of communities, and then ultimately to encompass the whole Earth, which is the domain of global ecology and macroecology.\nA proper understanding of the effects of scale, and of various scaling processes and gradients — as they occur from local to global levels — is absolutely crucial. This knowledge helps explain why certain species exist in particular locales but not in others. For example, why do kelp forests thrive in Cape Town, but not off Durban in KwaZulu-Natal? It’s because the environmental conditions differ: Cape Town’s seawater is much colder throughout the year, making it suitable for kelp, while Durban’s warmer temperatures exclude kelp from surviving there.\nSome organisms actually require kelp forests to survive or to reach their full productivity — certain species can only occur within kelp forests. So, the presence of kelp creates an environment that supports many other species. Thus, if kelp is absent (as off Durban), these organisms are also absent.\nIn short, global ecology seeks to understand how variations in temperature, light, soil characteristics, air quality, snow, rainfall, drought, humidity — all these environmental variables — combine to create a patchwork of suitable conditions for some species, but not others. Ecology, and especially macroecology, attempts to find global explanations for these broad patterns.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#questions-answers-1",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#questions-answers-1",
    "title": "",
    "section": "Questions & Answers",
    "text": "Questions & Answers\n\n\n\nSlide 10\n\n\n\n\n\nA question arose: when referring to patterns and processes in traditional ecology, is there such a thing as modern ecology? Yes, this module is very much about modern ecology. Traditional ecological approaches would focus on surveys at a local scale, such as conducting a transect survey in a nearby nature reserve, limited by what can be physically accessed.\nToday, with computers and satellite remote sensing, we can examine large-scale patterns — across countries, continents, or even globally — often using satellite data (Slide 10). Not only can we synthesise many small-scale surveys collected by different people over time, but we can also employ advanced numerical analyses to make sense of very large data sets — ones so substantial, they can no longer fit within Excel.\nModern ecologists now collaborate across the globe, pool significant data sets, and use advanced methods to reveal broad-scale patterns in biodiversity, species composition, and ecological functioning. Whereas traditional studies looked at the local, modern ecology can rigorously address processes at global, continental, or deep historical time scales.\n\nExample from South Africa\nThe earliest botanical research conducted in South Africa dates back to 1772 – 1774, when a Swedish botanist named Carl Peter Thunberg (1743 – 1828), who had trained under Carl von Linné (Linnaeus) (1707 – 1778), travelled to South Africa to explore the Western Cape and parts of the Southern Cape region as far east as Addo. Most of his journeys were made on horseback, and each of his three expeditions lasted several months at a time.\nDuring this period, his primary focus was the collection of plant and insect specimens. These he subsequently sent back to Europe, with the intention of classifying them according to the taxonomical system developed by Carl Linnaeus.\nFast-forward almost two centuries to the 1940s. This time the botanist John Acocks (7 April 1911 – 20 May 1979), again undertook a major botanical survey, but with the intention to classify all of South Africa’s vegetation. He travelled by train, classifying the habitats he saw through the window, and his classification became known as the Veld Types of South Africa. Even this method was constrained compared to the view we now have through remote sensing satellites.\nNow, we can “stand” \\(80\\,\\mathrm{km}\\) above Earth and map entire landscapes from above, unconstrained by natural or political boundaries. This is in fact what the recent BioScape programme has accomplished. BioScape is a programme that was implemented, run, and funded by the United States government. A significant portion of the funding was allocated to NASA, and in 2023, NASA, in collaboration with a group of South African scientists—of which I was a small part—brought a series of high-tech instruments all mounted on aeroplanes to South Africa. The intention was to survey the Cape Floristic Region during that period.\nThe instruments used included a range of ultraviolet, visible, and shortwave infrared imaging spectroscopy instruments; laser altimetry instruments known as LiDAR; as well as various other sensors, some of which were also mounted on satellites and other platforms. They surveyed much of the Fynbos region in the Cape, as well as some of the kelp forests around South Africa.\nThe examples above, spanning almost 3 centuries, offer an illustration of how the field of ecology has evolved over time. Initially, surveys were conducted on horseback, with the prime interest being the naming of various species. Two centuries later the focus shifted more towards classifying different vegetation types. In the present day, around two years ago, this endeavour culminated in the deployment of a comprehensive suite of extremely high-tech instruments, all functioning in concert to elucidate both the patterns observable at the landscape scale and the processes that structure these systems across space.\n\n\nBroader Shifts in Approach\nTraditional ecological studies focused on what happens in places within easy reach — a single nature reserve, for example. Modern studies look for patterns across nations or hemispheres, and also explore new levels of taxonomic detail, such as genetic variation and subspecies.\n‘Scale’ can refer both to spatial scale — local to global — as well as temporal scale: considering recent changes versus millennia or longer time spans. Modern approaches allow us to examine ecological phenomena and biogeographic patterns at both these broader spatial and longer temporal dimensions.\nCollaboration is increasingly important. Where once ecological studies might have one or two authors focused on a single location, it’s now common to find large teams of co-authors bringing together expertise and data from multiple sites or even continents in pursuit of broader ecological generalities.\n\n\nThe Value of Global Approaches\nThe aim of global ecology is to derive general ecological ‘laws’ or repeatable principles that apply across the full diversity of ecosystems — from Russian tundra to Amazonian rainforest to the Australian outback. Though these systems may look entirely different, we seek to identify commonalities in their fundamental processes.\nThirty years ago, when I was a student, almost all work was at a very local scale and typically on one’s nearest nature reserve. Today, with advances in technology and computational power, questions can be more complex and less parochial. The questions themselves have evolved and broadened: “What can South Africa’s biodiversity teach Patagonian ecologists?” Global-scale studies provide answers of relevance far beyond one region or ecosystem.\n\n\nClosing and Summary\nIf you have further questions — about the module structure, assessments, or the content of the introductory material — please ask, either now or later via the chat or WhatsApp group.\nIf there are no more questions, I’ll post this video online for you to access within the next half an hour or so. Thank you.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#revisiting-definitions-and-scales",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#revisiting-definitions-and-scales",
    "title": "",
    "section": "Revisiting Definitions and Scales",
    "text": "Revisiting Definitions and Scales\nRegional to global scales — I’ve spoken about all of this already, so I don’t need to go into regional to global scales again. You’ll understand this in a little bit more detail once you read that paper that I’ve given you. There are going to be two other papers now which you’re expected to read as well.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#patterns-and-processes",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#patterns-and-processes",
    "title": "",
    "section": "Patterns and Processes",
    "text": "Patterns and Processes\n\n\n\nSlide 11\n\n\n\n\n\nOkay, patterns and processes (Slide 11). Traditional ecology essentially focused on patterns. It looked at the world and observed that there is a patchwork of different kinds of ecosystems, even on local scales and then regional scales. It noted that this ecosystem often appears different from the one next door, and described how it is different in terms of species present there, and in terms of the structure of the community. However, it didn’t really attempt to explain the mechanism that created those differences in the first place.\nIn contrast, macroecology tries to add a mechanistic explanation for why and how things differ across the surface of Earth. To do this, we need to start treating ecology as a proper science, not merely as a form of natural history as it had been approached in the past. We need to ask questions about nature, to form hypotheses about nature that can be tested statistically, so that we can have a cause–effect explanation for why things are the way they are, or how things came to be as we observe them now.\nThis is, in fact, a very critical feature of modern-day ecology, particularly in macroecology, but also in contemporary population and community ecology at the local scale. We must ask testable hypotheses about nature — questions that we can actually go and test experimentally. Experimental assessment, experimental science, is the true test for whether something is so, or is not so. The necessity to measure things, and the necessity to have a statistical model or hypothesis, requires that we have data — that we go out into the world and measure things in specific ways, in order to have data that can be tested in a hypothesis setting via statistical models.\nThis is a very key part of modern ecology, and it’s only something that has become feasible since about the 1970s. Before that, people did not look at ecosystems with the intention of asking hypotheses of them. They mostly described how things are, rather than why things became the way we observe them to be now.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#local-interactions-to-global-theories",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#local-interactions-to-global-theories",
    "title": "",
    "section": "Local Interactions to Global Theories",
    "text": "Local Interactions to Global Theories\nWe’re going to examine local species interactions all the way up to global species distributions. Hence the necessity, once we have the entire earth in view, to develop unified theories. There are, of course, various applications.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#so-what",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#so-what",
    "title": "",
    "section": "So What?",
    "text": "So What?\n\n\n\nSlide 12\n\n\n\n\n\nWhy do we want to do macroecology (Slide 12)? Because we want to create something for policymakers to help them understand the world better; to identify that certain regions of the world are of great importance, both strategically and ecologically, and for the benefit of people. It may be better not to have developments in such areas, or instead to conserve portions of biodiversity, to plan land use accordingly, and to understand what the future world is likely to be like as biodiversity is lost to an ever-greater extent.\nSo, understanding macroecological processes influences the way that policies unfold. One of the major visible policies in the world today is the tendency for nations to move away from fossil fuels towards renewable energy, because we know that fossil fuels cause climate change, and we know that climate change is having an effect on species globally. We want to minimise this effect, because if we do, the consequences for people will also be reduced, since humans are so strongly linked to the environment.\nAdditionally, explanations of epidemiology also become possible: understanding the ways in which diseases spread and operate around the world, their origins, and so forth. There are many reasons why macroecology is interesting and important. For me, it is important because people are making a living from the world around us, and we want to ensure that the way people are making a living from the world today will still be viable a century from now — for your children, perhaps, to make a similar kind of living from the world, if you indeed directly rely on natural systems. Even if you don’t directly depend on ecosystems, you are indirectly supported by ecological goods and services.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#self-study-and-assignments",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#self-study-and-assignments",
    "title": "",
    "section": "Self-Study and Assignments",
    "text": "Self-Study and Assignments\n\n\n\nSlide 13\n\n\n\n\n\n\n\n\nSlide 14\n\n\n\n\n\nAnyway, that brings me to the end of what I needed to say today. There are two papers — or rather, one paper and one additional paper (Slides 13-14). There’s the one you saw before, and another one, which is also available to download from Tangled Bank. I would like you to read them both by the end of this week, so that by Friday afternoon, if you have questions about them, you can ask me. I’ll be available on Google Meet if you make an appointment to see me in groups of more than three.\nSo that’s your self-study. Your assignments will also require that you understand these topics in quite a bit of detail.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#looking-ahead",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#looking-ahead",
    "title": "",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nDuring the next lecture, we shall move on to topic number two, and we’re going to look at some of the questions that we can ask within the framework of macroecology.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#papers-to-read-1",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#papers-to-read-1",
    "title": "",
    "section": "Papers to Read",
    "text": "Papers to Read\nThe next paper you need to be familiar with addresses the ‘distance decay’ phenomenon. It is by Nekola and White (1999). This paper links directly to concepts introduced in the previous Ashley Shade paper, but here it is explored in greater detail.\nThe distance decay relationship fundamentally explains how ecological similarity decreases with geographical or environmental distance. In my view, gradients—particularly environmental gradients—are the major structuring agents of life on Earth, and distance decay is the pattern that emerges when we observe biodiversity at broad, often global, scales.\nZooming in to finer spatial scales, randomness or stochasticity becomes more influential, and the structure of beta diversity becomes more complex. Specifically, the paper introduces concepts such as turnover beta diversity and nestedness resultant beta diversity. If you find these terms challenging or require deeper understanding, you should consult foundational works by Whittaker and Baselga, who have written extensively on these topics. Understanding both nestedness resultant and turnover beta diversity is essential for your overall grasp of the subject.\nThis paper by David Tilman (2017) explores global-scale environmental gradients and their explanatory power in patterns of biodiversity. It specifically delves into the mechanisms underlying global patterns, focusing on the marine environment.\nYou are expected to understand the unimodal species distribution model, as it underpins the interpretation of distance decay across environmental gradients. While one group will further explore terrestrial patterns of biodiversity as part of the wiki assignment, this particular paper gives a clear overview for marine systems. Pay special attention to the graphs presented, as these summarise the major explanatory patterns in a digestible format.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#macroecology-and-environmental-gradients",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#macroecology-and-environmental-gradients",
    "title": "",
    "section": "Macroecology and Environmental Gradients",
    "text": "Macroecology and Environmental Gradients\n\n\n\nSlide 17\n\n\n\n\n\nWe are starting with topic number two in biogeography and global ecology. Today, our discussion focuses on the effect that gradients — specifically, environmental gradients — have on the distribution of life across the planet (Slide 17).\nTo remind you what macroecology is concerned with, we can use it to ask almost any question about the biodiversity of life on Earth. More specifically, we explore how biodiversity is arranged according to geographical location. This pertains to differences between continents, across continents, and indeed, across the entire Earth. Our scope is broad: we consider patterns found on very small, local scales right here around us, scaling up to global patterns that encompass the whole planet.\nMoreover, macroecology allows us to look deep into the past, using palaeorecords to explore the distribution of plants, animals, and also organisms that are neither plant nor animal. Equally, it grants us tools to study what is happening right now, in the present day. Looking to the future is also now possible due to technological advancements, such as computational modelling and remote sensing.\nFor my particular section of the module, as I mentioned yesterday, we are focusing mainly on contemporary processes. We will also look, albeit briefly, at methodologies for measuring these distributions and at how we establish the patterns of distribution for both plants, animals, and other organisms globally.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#drivers-of-biogeographical-patterns",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#drivers-of-biogeographical-patterns",
    "title": "",
    "section": "Drivers of Biogeographical Patterns",
    "text": "Drivers of Biogeographical Patterns\nLet’s firstly examine the processes present around us that structure the global distribution of life. The way we currently observe life arranged at the global scale is termed ‘biogeography’.\nGenerally speaking, biogeography and the biodiversity patterns associated with different continents and regions depend largely upon the underlying geographical character of those regions. Climate is an important factor here — it has a substantial, direct influence on these patterns.\nHowever, it is crucial to appreciate that the deeper history, or palaeohistory, of Earth also matters. The original evolution of life, and, long ago, the manner in which today’s continents were previously joined into supercontinents — initially Pangaea and, subsequently, Gondwana — are instrumental in explaining our current patterns. The break-up of these supercontinents, driven by plate tectonics, has critically shaped the biological structures we observe across the planet’s surface today.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#remote-sensing-and-modern-observation",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#remote-sensing-and-modern-observation",
    "title": "",
    "section": "Remote Sensing and Modern Observation",
    "text": "Remote Sensing and Modern Observation\nThese processes are not just theoretical; we can observe and quantify them. We have access to high-resolution spatial data, much of it obtained from satellites that orbit Earth daily. Since roughly \\(1981\\) — the beginning of what we call the satellite era — we have been able to compile global images of Earth’s surface. This has enabled an unprecedented understanding of patterns and processes relating to terrestrial life.\nEnvironmental differences across Earth’s surface produce varying ecological structures and outcomes. These outcomes, meaning both the structure and function of ecosystems, depend on — and can be measured across — different places and environments on our planet.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#classical-and-modern-ecological-methods",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#classical-and-modern-ecological-methods",
    "title": "",
    "section": "Classical and Modern Ecological Methods",
    "text": "Classical and Modern Ecological Methods\nClassical ecological approaches — such as population and community ecology — have, for the last hundred years or so, helped elucidate how such ecological patterns develop and persist. These approaches include basic methods such as sampling using quadrats or transects, with researchers counting the number of different species co-existing in defined areas, and then tracking how these assemblages vary both spatially and temporally.\nYou should recall from your earlier studies the relationship between plants, animals, and their environment, particularly regarding how the environment acts upon the physiology of specific organisms.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#linking-environment-physiology-and-ecology",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#linking-environment-physiology-and-ecology",
    "title": "",
    "section": "Linking Environment, Physiology, and Ecology",
    "text": "Linking Environment, Physiology, and Ecology\nFurthermore, macroecological questions encompass the many rate processes that move major nutrients — such as nitrogen, phosphorus, and carbon — as well as both micronutrients and macronutrients, into and away from plants and animals. These environmental influences on living organisms can be measured in a field known as ecophysiology. This discipline examines the rate processes affecting both plants and animals: for plants, things like nutrient uptake, and for animals, factors such as prey capture or their movement capabilities, as discussed previously by Prof Maritz All these variables are studied within ecophysiology.\nImportantly, outcomes from ecophysiological processes can have broad ecological consequences. That is, changes at the level of organismal physiology often scale up to influence community structure and even biogeographical patterns.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#global-change-past-present-and-future",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#global-change-past-present-and-future",
    "title": "",
    "section": "Global Change: Past, Present, and Future",
    "text": "Global Change: Past, Present, and Future\nFinally, we must recognise that the world, at all levels, is being transformed by global changes, including shifts in climate, and in nutrient cycles — such as those for nitrogen and phosphorus. This revisits topics from your Planetary Boundaries lectures in second year. Global change will influence — and in many cases, is already influencing — the outcomes of ecophysiological processes, which translate upstream to affect ecological patterns and, eventually, broad-scale biogeographical distributions.\nTo summarise, all these various processes — ranging from global change, through ecophysiology and ecological outcomes, to biogeography — occur across a huge variety of scales, both spatially (from the entire Earth down to highly local settings) and temporally (from the deep past, through the present, and projecting into the far future).\nThese are the foundational perspectives you should keep in mind as we proceed.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients",
    "title": "",
    "section": "Environmental Gradients",
    "text": "Environmental Gradients\nWe have previously discussed gradients, particularly environmental gradients. When I refer to gradients, I mean the changes in an environmental variable, such as temperature or rainfall, as you move from one place to another. For example, consider the temperature difference between Johannesburg and Cape Town, or the rainfall difference as you move from Durban to Cape Town. As you travel across the land surface, you experience a gradient.\nA prominent example is the rainfall gradient as one moves from east to west across South Africa. KwaZulu-Natal, on the eastern side, is very wet, with high rainfall and high humidity. However, as you move westwards, into the Western Cape, the Northern Cape, and even further towards Namibia, the environment becomes increasingly dry and desert-like.\nOn the eastern side of the country, the climate is very wet, and thus we find plants and animals that are adapted to, and require, very wet and moist conditions — examples include tropical or subtropical forests and coastal forests. However, if you think back to the last time you drove from Durban into the Northern Cape, you would have noticed how the landscape became increasingly dry. As you continue across the landscape, the vegetation also changes. It shifts towards types of vegetation that are able to persist and thrive under quite dry conditions.\nIn the Northern Cape and further west towards the South African coast, vegetation becomes increasingly sparse. There are fewer plants present — not necessarily fewer species, but rather, the individuals are far more separated from each other in space. They are less dense, in other words. This provides an example of a gradient related to rainfall, or water availability.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients-1",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients-1",
    "title": "",
    "section": "Environmental Gradients",
    "text": "Environmental Gradients\nEach different environmental variable can constitute a gradient. Gradients occur for temperature, humidity, soil nutrients, soil characteristics, cloud cover — essentially, anything you can think of regarding the environment. All these gradients operate across Earth’s surface.\nLet us focus, for instance, on plant species. An individual species of plant will often be well-adapted to a particular, relatively narrow, range of environmental conditions — such as temperature. Most individuals of a given species tend to occur around a ‘sweet spot’ where conditions, such as temperature, are most comfortable for them.\nTo put this in more relatable terms, if you are in Cape Town on a sunny summer’s day, you will naturally gravitate towards the spot that is most comfortable, perhaps choosing to sit in the shade rather than the direct sun. Plants, of course, lack the ability to move from place to place as we do. They are fixed in position, but over evolutionary timescales, both plants and animals become most abundant where environmental conditions are the most suitable for them.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-unimodal-response",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-unimodal-response",
    "title": "",
    "section": "The Unimodal Response",
    "text": "The Unimodal Response\n\n\n\nSlide 18\n\n\n\n\n\nConsider the example of a graph displaying the abundance of a particular species in relation to temperature (Slide 18). For instance, the majority of a species’ individuals may be found where the temperature is around \\(12.5~^\\circ\\text{C}\\), as that is the most suitable value for them. As you move away from that optimal temperature, the abundance of individuals decreases. This general pattern of abundance along an environmental gradient is known as a ‘unimodal’ species distribution.\nYou may read more about the origins of this concept in the work of Roger Wittig [attention: likely incorrect, please verify author and publication details] from 1967 or 1969, where this idea of the unimodal species distribution was first discussed.\nOf course, this applies only to one particular species. A different species may have an optimal temperature around \\(20~^\\circ\\text{C}\\), others at \\(5~^\\circ\\text{C}\\); some will prefer lower, some higher, and so on. These preference curves exist for every single environmental gradient and for all species present.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#gradients-beyond-temperature",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#gradients-beyond-temperature",
    "title": "",
    "section": "Gradients Beyond Temperature",
    "text": "Gradients Beyond Temperature\nIt is important to recognise that this pattern is not restricted to temperature. The same kind of unimodal distribution occurs for gradients in humidity, water availability, soil type, nutrient concentration, and other factors that have ecological or physiological consequences for species.\nWhen those factors operate simultaneously, they result in complex patterns known as coenoclines.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#coenoclines-coenoplanes-and-coenospaces",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#coenoclines-coenoplanes-and-coenospaces",
    "title": "",
    "section": "Coenoclines, Coenoplanes, and Coenospaces",
    "text": "Coenoclines, Coenoplanes, and Coenospaces\n\n\n\nSlide 19\n\n\n\n\n\n\n\n\nSlide 20\n\n\n\n\n\nA coenocline is essentially a more complex representation of species distributions, where the response to every environmental variable and every species on earth is superimposed to obtain a composite visualisation (Slides 19-20). This is, as you can imagine, extremely difficult to visualise directly, as it essentially combines all these different gradients into one highly complex picture. A coenocline represents the ‘sweet spot’ or the shift in landscape associated with changing environmental conditions and the location where particular types of populations will peak in abundance.\nInstead of just thinking of a gradient and a unimodal distribution for one species, imagine a unimodal distribution for every species, across every environmental condition that influences growth and fitness. When you superimpose the outcomes, you produce what is called a coenocline.\nIf you examine two environmental dimensions together — for example, temperature and humidity — this produces a two-dimensional plane called a coenoplane. If you add additional variables, such as soil characteristics, it becomes a multi-dimensional space called a coenospace. A coenospace is, therefore, a multi-dimensional representation of the best locations for collections of species given all relevant environmental gradients.\nThis move from thinking about a single gradient to a complex coenocline reflects a major step in understanding the ecology of species distributions.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#statistical-approaches",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#statistical-approaches",
    "title": "",
    "section": "Statistical Approaches",
    "text": "Statistical Approaches\nWe have fairly specialised statistical methodologies for studying coenoclines and related phenomena. We will touch briefly on some of these in this module, though there may be challenges due to the need for suitable computer lab access. Those of you progressing to honours will take an entire module in Quantitative Ecology, which lasts six or seven weeks and covers these statistical methods in greater depth — specifically targeting coenoclines, coenospaces, coenoplanes, and associated analytical approaches.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-earth-system-and-global-change",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-earth-system-and-global-change",
    "title": "",
    "section": "The Earth System and Global Change",
    "text": "The Earth System and Global Change\n\n\n\nSlide 21\n\n\n\n\n\nLet us examine all the processes currently impacting Earth. In the age we live in today, there is a particular need to be concerned with global change, which encompasses a variety of components. The most obvious, and certainly the most widely discussed in the popular media, is climate change (Slide 21).\nClimate change fundamentally arises due to the release of carbon dioxide (\\(\\mathrm{CO}_2\\)) into the atmosphere by human activity, particularly through the burning of fossil fuels. This \\(\\mathrm{CO}_2\\) does not originate from the sun, but rather acts to trap the sun’s energy within our atmosphere, preventing it from escaping back into space. This process leads to an accumulation of heat on Earth, which we observe as an increase in the general heat content — measured as a higher temperature — across the globe.\nAs more heat builds up, it causes changes in atmospheric pressure systems. Regions warming up more than others develop areas of low pressure, where air rises and circulates. As air rises, it contributes to the formation of winds, and these changes in heat content are not limited to the atmosphere alone. A significant proportion of this heat is absorbed by the surface of the oceans, referred to as the sea surface temperature (SST). As the ocean’s surface absorbs this additional heat, we see a rise in sea surface temperature.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#atmospheric-and-oceanic-responses",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#atmospheric-and-oceanic-responses",
    "title": "",
    "section": "Atmospheric and Oceanic Responses",
    "text": "Atmospheric and Oceanic Responses\nOne of the most measurable atmospheric responses to increased heat content is an increase in global wind activity. Of course, the real-world system is much more complex than this simple description, but it provides a useful starting point. In the case of the ocean, the most noticeable change is the rise in sea surface temperature. Both the atmosphere and the oceans experience this rise in temperature, which manifests as what we term anthropogenic climate change.\nThe implications of these temperature changes are profound. Many species have evolved to thrive in relatively narrow environmental conditions — what we might refer to as “sweet spots” (not a technical term, so don’t use it when you communicate professionally). A particular plant, for example, may be optimally adapted to the current temperature of Cape Town. If Cape Town warms by \\(2\\,^\\circ\\mathrm{C}\\), this plant finds itself outside of its optimal range. At that point, it faces a choice: it must either die out or, if its biological processes enable a sufficiently rapid response, it can shift geographically to remain within its preferred temperature range. This would require the plant to “move” towards the area where the climatic conditions mirror what used to be present in Cape Town — possibly to the west — as the climate envelope shifts.\nSo, climate change is already influencing the distribution of biota on Earth. We must therefore be aware of climate change as a new, critical process, and work to understand how it is likely to affect all aspects of the environment — particularly from both an ecophysiological and ecological perspective. For marine systems, this includes not only changes to swells and waves but also to the biogeochemistry of key nutrients such as nitrogen, phosphorus, and carbon. Biological interactions will change as well, exerting a profound influence on population ecology, among other fields.\nThis means that all modern biologists must grapple with climate change as an additional source of variation layered atop the myriad other processes already operating within Earth’s systems. Fully understanding climate change — and projecting its effects into the next \\(100\\) to \\(150\\) years — is critically important for anticipating how the biogeography of the future world will differ from that of today.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#regional-gradients-focus-on-the-ocean",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#regional-gradients-focus-on-the-ocean",
    "title": "",
    "section": "Regional Gradients: Focus on the Ocean",
    "text": "Regional Gradients: Focus on the Ocean\n\nThe Role of the Agulhas Current\n\n\n\nSlide 22\n\n\n\n\n\nLet me now focus more specifically on the ocean, as this is where much of my research is conducted. One of the most influential systems impacting South Africa — as well as many other coastal regions worldwide — is the large ocean current running along our coast. Although it appears snake-like on maps and diagrams, this is in fact the Agulhas Current (Slide 22).\nThe Agulhas Current flows from the north, past South Africa’s east coast, moving southwards before looping back into the South Indian Ocean. The water it transports from the north is warm, as it originates close to the equator. Regions nearer the equator experience greater day length and are closer to the sun, leading to higher heat absorption. Therefore, both the ocean and the overlying atmosphere are warmer in tropical regions.\nThis warm tropical water is carried southwards along the east coast of South Africa, bringing it into regions that would otherwise be significantly cooler. The presence of this warm water not only raises the temperature of the overlying atmosphere, but also drives greater rates of evaporation. As warm water evaporates, it injects moisture into the atmosphere, which then becomes available for rainfall.\nWithin this system, the rising warm air over the ocean creates a low-pressure area, while the relatively cooler land retains higher pressure. This pressure differential drives winds from the ocean towards the land, carrying with them moisture-laden air — and, as a consequence, there is considerable rainfall along South Africa’s eastern coastline.\nIf you recall the east-to-west rainfall gradient in South Africa — with KwaZulu-Natal in the east being particularly wet and moving towards increasing aridity as you travel westward — the Agulhas Current is largely responsible. The abundance of moisture and rainfall along the east coast owes much to the warmth of this current, which brings water from the tropics and sustains the region’s lush vegetation.\nHowever, as you move away from the direct influence of the Agulhas Current, further west towards central South Africa, the oceanic influence diminishes. The water becomes colder, less moisture evaporates from the surface, and significantly less rainfall occurs. This renders the central and western regions of South Africa considerably drier and more arid, with less vegetation and runoff.\n\n\nWestern Boundary Currents around the World\nThis pattern is not unique to South Africa. Similar warm ocean currents flow along the eastern margins of major continents and are collectively known as western boundary currents. Examples include:\n\nThe Brazil Current along the east coast of South America\nThe Gulf Stream along the east coast of North America\nThe Kuroshio Current off the east coast of Japan\nThe East Australian Current alongside eastern Australia\n\nThese currents, known as western boundary currents because they flow along the western edge of their respective ocean basins, carry warm water from the tropics into the mid-latitudes, depositing moisture-rich air and promoting rainfall across large coastal regions.\nAs a general rule, continents influenced by these warm currents display a moisture gradient from east to west. For example, in Brazil, the region affected by the Brazil Current is warm and moist, but as one travels westwards into the interior — and especially into Chile and Peru [attention: Chile and Peru are west of Brazil, but separated by the Andes and not on the same cross-sectional gradient; this is an oversimplification] — the climate becomes progressively more arid. Similar principles applies to North America and Australia.\n\n\nThe Importance of Ocean Currents for Regional Climatic Gradients\nOcean currents play an absolutely critical role in establishing these large-scale regional gradients, which then determine how vegetation and associated biota are distributed. The moisture content of the environment is the primary driver shaping these patterns, though other factors become increasingly important as one moves further from the influence of warm currents.\nIt is important to appreciate the significance of the Agulhas Current in shaping South African climate and ecology. If you were to “switch off” the Agulhas Current and replace it with a cold current [attention: not physically possible, but a useful thought experiment], the entire east of South Africa would resemble the arid, desert-like conditions currently found along the west coast. Therefore, the ocean — specifically, these powerful currents — is fundamental to the regional climate patterns that support life as we know it on land.\nIf you wish to deepen your understanding, I suggest reading further about the Agulhas Current and its effects. Its presence is precisely what makes South Africa’s eastern seaboard lush and habitable, in stark contrast to the much drier west.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-role-of-the-agulhas-current-in-setting-gradients",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-role-of-the-agulhas-current-in-setting-gradients",
    "title": "",
    "section": "The Role of the Agulhas Current in Setting Gradients",
    "text": "The Role of the Agulhas Current in Setting Gradients\nAnother aspect that occurs due to the Agulhas Current is that, as the current moves — recall, as we travel from north to south, moving progressively away from the tropical regions into the subtropics and then into temperate regions — evaporation happens along this journey. The residual water in the ocean becomes increasingly cooler and cooler. This cooling occurs because the heat that was originally in the ocean is now being transferred into the atmosphere, warming the land adjacent to it. Thus, as we head further south, the seawater temperature drops as the heat from further north has dissipated and now resides in the atmosphere and over the land.\n\n\n\nSlide 23\n\n\n\n\n\nSeawater in the southern regions is substantially colder compared to somewhere like Durban (Slide 23). You can actually feel the difference. By the time you reach Cape Town, the seawater is even colder, owing to the presence of a different ocean current, which brings about a process called upwelling rather than the warming effect of the Agulhas. So, in addition to setting up a gradient over the land in terms of various factors such as moisture, temperature, and erosion — all processes linked to rainfall — the Agulhas Current also sets up a strong temperature gradient along the coastline. At the northern border, north of Sodwana Bay with Mozambique, sea temperatures are at their highest, and as you progress down the coast, the temperature decreases consistently, becoming coldest at Cape Town. Therefore, there is a clear, almost linear, gradient in decreasing temperature from north to south along the coast of South Africa. Again, this gradient is a direct consequence of the Agulhas Current.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#examples-of-environmental-gradients-in-false-bay",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#examples-of-environmental-gradients-in-false-bay",
    "title": "",
    "section": "Examples of Environmental Gradients in False Bay",
    "text": "Examples of Environmental Gradients in False Bay\n\n\n\nSlide 24\n\n\n\n\n\nHere is a figure illustrating waves — this is False Bay (Slide 24). This bay is where many of you find yourselves; Cape Town is in this region. In the Southern Ocean, far south of South Africa, there are strong prevailing winds that generate large swells, sometimes originating \\(1{,}000\\)–\\(2{,}000\\) kilometres away. These waves eventually propagate and arrive at the shores of False Bay as swells.\nThis is just one more example of a regionally important environmental gradient. The spatial scale here is more restricted — we are now considering False Bay, which is about \\(50\\)–\\(60\\) kilometres across. Even across such a small distance, you can observe a gradient: from the sheltered western sides of False Bay, such as Muizenberg, which experiences very low winds and small waves, moving south and east into more exposed sections, the wave height increases substantially. Within False Bay, there is a gradient in wave energy: lower in the west, higher in the east, and peaking further south. On the other side of the Cape Peninsula, exposed to the Atlantic, waves are higher still, as they directly intercept swells from the South Atlantic Ocean.\nWave gradients, such as those found in False Bay, influence the distribution of kelp and other marine organisms. Simultaneously, there is a recognised temperature gradient across False Bay, as well as a depth gradient: moving from the coastline towards central False Bay, the water depth transitions from only \\(1\\)–\\(3\\) metres near the shore to around \\(70\\) metres in the centre.\nRemember from your BDC223 module: as we go deeper into the ocean, there is a vertical light gradient — the deeper you go, the less light is available. Thus, environmental gradients exist at multiple dimensions: horizontal gradients such as temperature, waves or salinity, and vertical ones like light with depth.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#gradients-across-scales-from-regional-to-global",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#gradients-across-scales-from-regional-to-global",
    "title": "",
    "section": "Gradients Across Scales: From Regional to Global",
    "text": "Gradients Across Scales: From Regional to Global\nThese environmental gradients operate at multiple spatial scales — from gradients at the southern hemisphere or continental scale, to those across a bay only a few dozen kilometres wide, right down to vertical gradients in the ocean. On a planetary scale, gradients extend from the tropics to the poles. All of these gradients, at every scale, are responsible for allowing certain organisms to persist in particular environments, while excluding others.\nThe work of ecologists, especially macroecologists, is to investigate how these gradients structure the organisation of life across Earth’s surface.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#remote-sensing-and-observing-patterns",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#remote-sensing-and-observing-patterns",
    "title": "",
    "section": "Remote Sensing and Observing Patterns",
    "text": "Remote Sensing and Observing Patterns\n\n\n\nSlide 26\n\n\n\n\n\nLet us now look at an image of Earth’s surface. Ecology, in essence, is the study of patterns. Here, you can observe a patchwork of different colours — dark green, brown, grey — each representing distinct surface properties or vegetation cover (Slide 26).\nFor instance, the regions with dark green typically indicate dense, healthy vegetation — vast patches of green associated with the Western Cape. In other areas, browner patches mean the vegetation is more scrubby, sparse, or replaced with barren sand.\nMacroecologists would ask: why is this patch green and that patch brown, sometimes only a few kilometres apart? Looking closely, greenness is often associated with coastal zones, particularly along the Garden Route and Western Cape. This is a function of atmospheric and oceanic patterns, especially the influence of the Agulhas Current. However, in some regions, especially inland, apparent greenness in satellite images may be attributable to intensive farming and land transformation, rather than natural processes. [attention: Not every green patch is natural vegetation; some are vineyards, canola, or other agricultural fields.]\nIf you zoom in, you can see a clear patchwork reflective of agricultural practices such as viticulture and other crops. Remaining tracts of natural fynbos are also visible, structured according to elevation: lush and green in valleys, but sparse and grey at higher altitudes — demonstrating how temperature and exposure control plant community composition even at relatively small spatial scales.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#using-temporal-data-to-track-environmental-change",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#using-temporal-data-to-track-environmental-change",
    "title": "",
    "section": "Using Temporal Data to Track Environmental Change",
    "text": "Using Temporal Data to Track Environmental Change\nSatellite data have been available daily since 1981. Comparing present-day maps to those from one decade ago, or two decades ago, reveals changes in landscape patterns. These shifts are mostly consequences of anthropogenic environmental modification: farming, deforestation, urbanisation, and fire. In some places, you can also observe temporary or seasonal phenomena like snow cover.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#integrating-multiple-types-of-environmental-information",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#integrating-multiple-types-of-environmental-information",
    "title": "",
    "section": "Integrating Multiple Types of Environmental Information",
    "text": "Integrating Multiple Types of Environmental Information\nFrom a single remote sensing image, you can extract vast quantities of information — vegetation type, land use, altitude and topography, river catchments, coastal processes, and more. For example, wave action stirs up sand in the water, which appears milky blue or white from space, especially where long sandy beaches are present. Rocky areas have less suspended sediment, and thus appear darker in satellite imagery. Visible drainage lines indicate the position of rivers and the amount of water they transport.\nAt even finer scales, satellite imagery can be used to monitor fire scars and the impact of wildfire, as fires appear starkly in the imagery.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#biological-productivity-and-the-agulhas-bank",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#biological-productivity-and-the-agulhas-bank",
    "title": "",
    "section": "Biological Productivity and the Agulhas Bank",
    "text": "Biological Productivity and the Agulhas Bank\n\n\n\nSlide 27\n\n\n\n\n\nHere’s another satellite image of South Africa. Again, there’s False Bay, and some white regions here are clouds, but look at these pale blue swirls in the ocean — these are areas of phytoplankton bloom. Interestingly, these blooms are restricted in location due to the dynamics of the Agulhas Current. Phytoplankton that drift into the Agulhas Current quickly get swept away, so their retention above the Agulhas Bank (Slide 27) — a region extending up to \\(200\\) kilometres offshore but with a maximum depth of about \\(150\\) metres — is especially significant for local productivity.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#infrared-imagery-and-vegetation-detection",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#infrared-imagery-and-vegetation-detection",
    "title": "",
    "section": "Infrared Imagery and Vegetation Detection",
    "text": "Infrared Imagery and Vegetation Detection\n\n\n\nSlide 28\n\n\n\n\n\nHere, in an infrared image of the tip of the Cape Peninsula (Slide 28), you can clearly distinguish natural vegetation, which appears in red, from exposed bedrock and sand, which appear white. Off the coast, red patches indicate the presence of kelp beds and kelp forests, which are so large and dense they can be detected from space.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-macroecologists-challenge",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-macroecologists-challenge",
    "title": "",
    "section": "The Macroecologist’s Challenge",
    "text": "The Macroecologist’s Challenge\nAll of this information — vegetation types, land use, altitudinal patterns, wave exposure, kelp forests, riverine systems, and even the presence of fire — can now be accessed and analysed by macroecologists. Our task in this module is to understand how to use such data, and thereby to interpret how the physical environment structures patterns of life at a range of scales.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#assignment-instructions",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#assignment-instructions",
    "title": "",
    "section": "Assignment Instructions",
    "text": "Assignment Instructions\n\n\n\nSlide 29\n\n\n\n\n\nTo conclude, as a preparation for an upcoming assignment, I would like you to select two or three examples of environmental gradients you can identify — some operating at local, others at regional, and others at global scales (Slide 29). Prepare an essay, according to the specific guidelines I’ll provide shortly, in which you explain in detail how these gradients are capable of structuring biodiversity.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#introduction-to-biodiversity",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#introduction-to-biodiversity",
    "title": "",
    "section": "Introduction to Biodiversity",
    "text": "Introduction to Biodiversity\nToday, we’ll be discussing the various concepts of biodiversity. This concerns how we quantify diversity, both in terms of which species are present and the proportions of those species existing within a particular habitat, environment, or ecosystem. The key concepts to focus on include alpha, beta, and gamma diversity — those are the three Greek-lettered types.\nAt its most basic, we use what are called univariate measures. That is, all the variety of plants, animals, and things that are neither plant nor animal can be condensed into a single measurement — one variable. That’s essentially what “univariate” means: one variable.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#univariate-indices-and-overview",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#univariate-indices-and-overview",
    "title": "",
    "section": "Univariate Indices and Overview",
    "text": "Univariate Indices and Overview\nTo make this clearer, let’s consider the UWC Nature Reserve — you know where it is. It contains a wide array of plants and animals, but all of that complexity can be reduced to a single measurement for alpha, beta, or gamma diversity.\nFocusing specifically on alpha and gamma diversity, the univariate measurements commonly used are the Shannon and Simpson indices. These are the two most typical ways you’ll see alpha and gamma diversity quantified, and I’ll give more detail shortly on what those indices are and how they’re applied.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#alpha-diversity",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#alpha-diversity",
    "title": "",
    "section": "Alpha Diversity",
    "text": "Alpha Diversity\n\nWhat is Alpha Diversity?\n\n\n\nSlide 31\n\n\n\n\n\nLet’s look first in more detail at alpha diversity (Slide 31). Alpha diversity is the diversity of a community, plot, habitat, or ecosystem at the smallest scale at which we measure. Returning to the UWC Nature Reserve example, if we wish to know what plants and animals are present, the standard approach in ecology is to lay down various transects or plots — also called quadrats — across the area.\nQuadrats are simply small subsections or representations, essentially samples, of a much larger environment. We use a sufficiently large number of quadrats to try to capture the full range of biodiversity in a given place. Alpha diversity, therefore, accounts for diversity at this very local, smallest scale.\n\n\nHow Do We Measure Alpha Diversity?\nFor example, if you place a single quadrat within the entire UWC Nature Reserve, that quadrat forms the basis for measuring or representing alpha diversity. Alpha diversity is essentially biodiversity at the local scale, and there are three principal ways to express it:\n\nSpecies Richness:\nThe simplest measure is just counting the number of species present. For example, “There are \\(15\\) species of plants and \\(12\\) species of vertebrates” within the UWC Nature Reserve. At the smallest scale, this involves counting the number of plant and animal species within a single quadrat.\nIndices (Shannon and Simpson’s):\nYou can also use indices, such as the Shannon or Simpson index. These take into account not only the number of species (species richness) but also the abundance or “how much” of each species is present in your quadrat.\nDissimilarity Indices:\nA more complex way involves looking at all the quadrats placed within an area at once, quantifying differences between them. While species richness or the univariate indices often focus on the individual quadrat, you can compare every quadrat pairwise with every other to create a dissimilarity index. Common dissimilarity indices include Bray–Curtis similarity, Sørensen dissimilarity, and Jaccard dissimilarity.\n\nBear in mind, I’ll touch more on dissimilarity indices in another lecture. But for now, recognise that the synthetic diversity indices mean comparing every quadrat with every other, using a variety of metrics. Besides Bray–Curtis, Sørensen, or Jaccard, there are at least another \\(21\\) such metrics or more.\n\n\nInterpreting Diversity Metrics\nIt’s rather like measuring distance with a ruler. The ruler might be marked in centimetres, and in the same way, indices such as Bray–Curtis, Sørensen, Simpson, or Shannon are the “rulers” you use for biodiversity. The actual value you get is measured in units of that respective index, indicating biodiversity in quantifiable terms.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#beta-diversity",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#beta-diversity",
    "title": "",
    "section": "Beta Diversity",
    "text": "Beta Diversity\n\n\n\nSlide 32\n\n\n\n\n\n\nWhat is Beta Diversity?\nBeta diversity, by contrast, is sometimes referred to as “species turnover.” It measures how different each quadrat placed within a habitat is from every other quadrat — essentially, the variation from place to place across the landscape (Slide 32). In this way, it quantifies heterogeneity — how communities differ from spot to spot.\n\n\nBeta Diversity Along Gradients\nTo make this real, recall the example from last week: the temperature gradient along the east coast of South Africa. As you move from Sodwana Bay southwards, the temperature changes gradually. The further you go, the more the temperature differs from your starting point. As this physical variable changes, so too does the potential for different types of plants and animals to exist. Thus, species composition shifts along the gradient.\nBeta diversity works particularly well in these scenarios, where we measure community structure along environmental gradients. There is a paper I’ve uploaded to Econva (and another associated one), which provides visual explanations for how environmental gradients influence beta diversity. Please make sure to look at those.\n\n\nSummary on Beta Diversity\nBeta diversity is the second major measurement of biodiversity, highly useful for examining how quickly communities change along gradients. As the environment changes — temperature, rainfall, soil type, etc. — so too does the composition of plants and animals, and beta diversity allows us to quantify that change across the landscape.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#gamma-diversity-the-largest-scale",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#gamma-diversity-the-largest-scale",
    "title": "",
    "section": "Gamma Diversity: The Largest Scale",
    "text": "Gamma Diversity: The Largest Scale\n\n\n\nSlide 33\n\n\n\n\n\nAt the very largest scale, the total amount of biodiversity is generally called gamma diversity (Slide 33). If we go back to our example of the UWC Nature Reserve, let us say we place one quadrat, and within that single quadrat, we find seven species of plants and two species of vertebrates. The total diversity for that quadrat would then be seven.\nHowever, if we place multiple quadrats throughout the UWC Nature Reserve, with each new quadrat, we are likely to encounter new species. The more quadrats we place, the more species we will count, because species are distributed across the landscape. Thus, gamma diversity examines the diversity of the entire UWC Nature Reserve, and states that there are, for example, 23 species of plants and seven species of vertebrates across the whole reserve.\nGamma diversity can also be considered at even greater scales. It can scale up to the entire planet, to all of Earth, at which point we might say that Earth has \\(X\\) million species of organisms. So, the entire Earth represents the largest possible scale at which we can account for the total number of living organisms, or species of living organisms, present on the planet.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#local-and-regional-scales",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#local-and-regional-scales",
    "title": "",
    "section": "Local and Regional Scales",
    "text": "Local and Regional Scales\nAt smaller scales, a continent could be considered a sampling unit. As an example, Africa might have \\(X\\) hundred thousand species of organisms, and South America another \\(X\\) hundred thousand, depending on definitions and available data. In this context, the “local” scale could be a country, so if we look at species within South Africa, for example, that could be defined as alpha diversity.\nAlpha diversity and gamma diversity are both measures that can, in principle, apply to a very localised area. The largest possible extent of that localised environment, such as the outer boundaries of the UWC Nature Reserve, would count as gamma diversity for that smaller study. If the study is instead concerned with the whole planet, then the entire Earth is gamma diversity, and the continent, country, or region becomes the scale for measuring alpha diversity.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#defining-the-scales-researchers-perspective",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#defining-the-scales-researchers-perspective",
    "title": "",
    "section": "Defining the Scales: Researcher’s Perspective",
    "text": "Defining the Scales: Researcher’s Perspective\nWhether we use alpha or gamma diversity depends very much on the research question. These terms are not fixed; as an investigator, it is up to you to define the minimum and maximum extents of your study. For example, if you are interested in the flora of the Western Cape, you would draw a boundary around the Western Cape and define your gamma diversity as all the species observed within those boundaries.\nFor alpha diversity in this context, you might look at the number of species present in Belleville, in Rondebosch, in Worcester, and so forth — each a different locality within your region of study. Hence, the use of alpha and gamma diversity depends entirely upon your definition and the scale at which your research is taking place. The concept is flexible, and is relative to the extent of your particular study — what is “gamma diversity” for one study may be “alpha diversity” for a larger study, and so forth.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#species-richness",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#species-richness",
    "title": "",
    "section": "Species Richness",
    "text": "Species Richness\n\n\n\nSlide 34\n\n\n\n\n\nSpecies richness is a term that brings us back to both alpha and gamma diversity (Slide 34). As I have mentioned before, species richness is simply the number of different species present. Measured at a small, highly localised scale, species richness gives alpha diversity. Measured across the full extent of your study region, species richness provides gamma diversity. Both are ultimately just a count of how many species are present.\n\n\n\nSlide 35\n\n\n\n\n\nImagine a pink area, representing your total study habitat — this is your study area (Slide 35). You cannot count every single organism within this space, so you sample by placing quadrats (the grey squares), each representing a subset of the biodiversity present. If you deploy enough quadrats, your sampling will hopefully capture every species present in your study area.\nFor each quadrat, you tally up the number of different species present. For example, one quadrat might contain five species. Another might contain three. Some quadrats share species, others have unique species. Suppose you have six quadrats, and their species richness values are \\(5\\), \\(5\\), \\(6\\), \\(5\\), \\(4\\), and \\(3\\). To calculate the average species richness for the landscape, you simply find the mean:\n\\[\n\\frac{5 + 5 + 6 + 5 + 4 + 3}{6} = 4.667\n\\]\n\n\n\nSlide 36\n\n\n\n\n\nThis is your average species richness. At the largest scale, you simply count the unique species present across all quadrats. If, collectively, across all quadrats, there are nine unique species, then the gamma diversity for that region is \\(9\\) (Slide 36).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#beta-diversity-measuring-variation",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#beta-diversity-measuring-variation",
    "title": "",
    "section": "Beta Diversity: Measuring Variation",
    "text": "Beta Diversity: Measuring Variation\n\n\n\nSlide 37\n\n\n\n\n\nNow let us move to beta diversity (Slide 37). Beta diversity focuses on how different each quadrat is from every other quadrat; it measures turnover in species composition. For instance, if one quadrat contains species A, D, B, C, and E, and the quadrat next to it contains A, D, F, G, and E, you see that they share two species (A and D), but differ by three species each. Similarly, quadrats below or adjacent to one another can be compared in the same way.\nTo calculate beta diversity, you must compare every quadrat to every other quadrat — that is, for every possible pair of quadrats, you calculate the number of shared and unique species. This results in a table of dissimilarity values (a dissimilarity index), where each value shows how different one quadrat is from another.\nWe will discuss dissimilarity indices and how to interpret them in detail in a later section of the course, where I shall provide some pre-calculated examples for you to practise with.\nThe important point is that the landscape is almost never perfectly homogeneous. For example, perhaps most quadrats have species A (present in five quadrats) but not all. Species B might be present in three quadrats, not everywhere else, and so on. In general, almost every quadrat will be at least slightly different from the next. Beta diversity captures the amount of this variation, or heterogeneity, in your study landscape.\n\n\n\nSlide 38\n\n\n\n\n\nOkay, continuing with our example of beta diversity, there are two different ways in which we can approach beta diversity (Slide 38). One is, as shown in the top panel — Slide 38 (a) — we assume that there is no spatial relationship between one sampling unit and the next. So, they are unordered across the landscape. This is the typical inference we can make about biodiversity: we compare every unit to every other unit. This is similar to the previous illustration in the earlier slide we saw.\nHowever, if we take a more structured approach to how we measure beta diversity across the landscape — looking at the bottom panel, panels (b) and (c) — we see that the sampling units are arranged in a logical order. In this example, they are spatially arranged in increasing distance from the west of the country.\nThis is the example of the seaweed data in Smit et al. (2017): site number one (sampling unit one) is in the west, and we move all the way to sampling unit number 58, which is situated far to the east.\nNow, if we take one sampling unit — for instance, sampling unit number one in the west — and use that as the reference unit (it remains constant, fixed in the west), we can then compare it to sampling unit number two, next to it. We’ll see that the difference in biodiversity between one and two is going to be quite slight, because the spatial distance between those two units is only about \\(50\\mathrm{km}\\) or so.\nIf we compare sampling unit number three to sampling unit number one, the spatial distance increases to about \\(100;\\mathrm{km}\\), so there’s a slight increase in the beta diversity between those two pairs of sites. Next, we compare site number five, which is about \\(200;\\mathrm{km}\\) further to the east, to site number one. In this case, the change in dissimilarity between one and five is a bit greater.\nSo, the larger the distance becomes between a pair of sites, the greater the change in the underlying environmental variables due to the environmental gradient along the coast. Consequently, the species dissimilarity also increases. The greater the distance between a pair of sites, the more dissimilar they become.\nBy the time we reach section number 58, far to the east, the distance between sites one and 58 is about \\(2{,}700\\) to \\(2{,}800 \\mathrm{km}\\). The environmental conditions in the subtropical northeastern part of South Africa are very different from the cold temperate conditions in the western part of the country. Consequently, the species diversity is also vastly different. Virtually no species are in common between sites one and 58. In contrast, when comparing sections one and two, or sections one and three, because they are closer together, the environments are more similar, and more species will be in common.\nThis approach, which I’ve just explained, is called distance-decay beta diversity.\nSerial beta diversity takes another approach — shown in portion (c) of the figure. In this approach, we compare section one with section two: the difference in beta diversity is slight. Then section two to section three — again, a slight difference. Section three to section four — still very small.\nIf we take the cumulative dissimilarities between every consecutive pair of sites in the sequence from west to east, we find that the overall beta diversity is the same as the difference between site one and site 58. So, the sum of consecutive pairwise comparisons adds up (more-or-less) to the total beta diversity measured across the entire distance between sites one and 58.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#heterogeneity-and-homogeneity",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#heterogeneity-and-homogeneity",
    "title": "",
    "section": "Heterogeneity and Homogeneity",
    "text": "Heterogeneity and Homogeneity\nHeterogeneity refers to variability or difference — if a landscape is highly heterogeneous, it features a high amount of variation from place to place. The opposite is homogeneity, where conditions or communities are uniform throughout the study area. Very few natural landscapes are perfectly homogeneous; most exhibit moderate heterogeneity, which can be measured and interpreted via beta diversity.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#summary-distinguishing-alpha-beta-and-gamma-diversity",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#summary-distinguishing-alpha-beta-and-gamma-diversity",
    "title": "",
    "section": "Summary: Distinguishing Alpha, Beta, and Gamma Diversity",
    "text": "Summary: Distinguishing Alpha, Beta, and Gamma Diversity\nIn summary, you should remember the distinctions among alpha, beta, and gamma diversity:\n\nAlpha diversity is typically measured at the smallest sampling unit within your study area.\nGamma diversity is the total number of unique species present within your whole study area or landscape.\nBeta diversity is the amount of variation or difference in species composition among the various sampling units (quadrats) within the landscape.\n\nDefining these scales and diversity measures is essential for meaningful biodiversity studies, and the way you decide to structure them will depend upon your research aims and the boundaries you set for your particular study.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#introduction-to-selecting-diversity-measurements",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#introduction-to-selecting-diversity-measurements",
    "title": "",
    "section": "Introduction to Selecting Diversity Measurements",
    "text": "Introduction to Selecting Diversity Measurements\n\n\n\nSlide 39\n\n\n\n\n\nWhich of these various different measurements of diversity we use, is going to depend on your specific question (Slide 39). As you are ecologists, or training to become ecologists, it is up to you to decide what the question is that you wish to ask about the landscape you want to study. One day, when you are professional ecologists, you will decide which landscape to study, for what reason, what the total extent will be, and whether a small \\(2\\;\\mathrm{m} \\times 2\\;\\mathrm{m}\\) quadrat, or a smaller \\(30\\;\\mathrm{cm} \\times 30\\;\\mathrm{cm}\\) quadrat, would be more appropriate for your sampling.\nYou will define the scales at which you apply the terms alpha, beta, and gamma diversity, as well as the amount of variation within the total landscape — the area for which you calculate gamma diversity. The variation within that landscape is beta diversity, but again, the choice of spatial scale and focus is dependent on you, as researchers.\nSo, in designing any particular research project, there are many questions around spatial scales that you must consider as part of the experimental design process. This will result in the structure within which you sample the environment — a structured way to obtain the data you need to make a proper assessment of ecological diversity.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#overview-of-diversity-indices",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#overview-of-diversity-indices",
    "title": "",
    "section": "Overview of Diversity Indices",
    "text": "Overview of Diversity Indices\n\n\n\nSlide 40\n\n\n\n\n\nThe diversity indices, as I have mentioned before, are simply ways of representing species diversity (Slide 40). Let us return to alpha diversity as an example. The simplest way to measure alpha diversity is to calculate species richness — that is, to record how many species are present within a small sampling unit.\nHowever, landscapes are not only defined by a simple list of species. Of course, it is important whether a species is present or absent, but another essential consideration is how much of each species is present in the sample. For example, consider two communities, two different habitats. Both have 10 species present, so in terms of species richness, both community A and community B are equal: \\(10\\) and \\(10\\).\nBut in community A, there are \\(10\\) individuals of every species — an even distribution. In community B, there is only \\(1\\) individual of each species from \\(1\\) to \\(9\\), but species \\(10\\) has \\(91\\) individuals present. So, although both communities have identical lists of species, they differ substantially in terms of the abundances of those species.\nThis is where diversity indices for alpha diversity become important. These indices take into account both the number of species (species richness) and the relative abundance, or number of individuals, of each species. The two most common ways we represent or express diversity as a function of species richness and abundance are through diversity indices: the Shannon diversity index and the Simpson’s diversity index. Each of these has a particular equation to calculate their values, but the software we use typically performs the computation for you.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#calculating-diversity-indices",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#calculating-diversity-indices",
    "title": "",
    "section": "Calculating Diversity Indices",
    "text": "Calculating Diversity Indices\nSome of the exercises that you will tackle later will require you to calculate these indices by hand. Unfortunately, this year, due to the lockdown and not being able to use the university’s computer labs, you will need to perform these calculations manually. In every other year, you would have used standard software for these.\nI shall give you, as an exercise later in the week, some sets of diversity data and ask you to calculate these indices yourselves, by hand.\n\n\n\nSlide 41\n\n\n\n\n\nNow, the two indices — Shannon’s and Simpson’s — differ slightly, although there is ongoing debate about precisely how much they differ and in what aspect (Slide 41). Many people say that Shannon’s diversity index favours species richness. That is, it puts more emphasis on place-to-place differences that result from the number of species present. Simpson’s index, by contrast, is said to be more important in contexts where the number of individuals per species varies greatly across the landscape.\nBut as I noted, there is much debate as to which index is preferable. There is even a slide, or paragraph from the software we use, which states: “Better stories can be told about Simpson’s index than about Shannon’s index, and still grander narratives about rarefaction.” (Rarefaction is yet another way of considering species diversity.) However, all these indices are closely related, and there is no reason to prefer or despise one over the others. The same paragraph, however, gives a word of advice: “If you are a graduate student, do not drag me in, but obey your professor’s order.” So, at the end of the day — whether you use Simpson’s or Shannon — much will depend on the preferences of your future supervisors. Everyone has their own opinions. In my personal view, it makes little difference; they are, in fact, linearly related to one another. Still, please do listen to what your supervisor says.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#structure-of-diversity-data",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#structure-of-diversity-data",
    "title": "",
    "section": "Structure of Diversity Data",
    "text": "Structure of Diversity Data\n\n\n\nSlide 42\n\n\n\n\n\nLet us return to the matter of how these indices are calculated. The essential thing to learn now is how your data should be structured when entering it into the computer for analysis.\nTypically, we enter all the various places (the sites or quadrats) along the rows, and the species — by name — along the columns (Slide 42). The numbers in the table represent the abundance: for example, species A at site A, there is one; species B at site C, there are four; species B at site D, there are eleven; and so on.\nSpecies richness is easy to calculate using this structure: for any site, simply count how many columns (species) contain a positive number. For instance, site A might have six species present (species richness = \\(6\\)), as might site B. Importantly, the abundance — that is, the actual number of individuals per species — is not considered when calculating species richness.\nBut the Shannon-Wiener index does take these abundances into account, as does the Simpson’s index. For example, the Simpson’s index emphasises sites with higher evenness of abundance between species: if, for an area, only two species are present and the others have zero, you will get a low Simpson’s diversity value. Conversely, if the abundances are more evenly distributed among species, the Simpson index value is higher.\nEvenness refers to how similar the abundances of the different species are: a site where all species are represented by roughly equal numbers of individuals has high evenness; a site where one species dominates and the rest are rare has low evenness.\nPlease familiarise yourselves with the process of working out these indices using sample tables. I will provide some examples for you to work through. Normally, we would use software — “R” in our case — to calculate these indices, but for now, manual calculation will suffice. If you go on to Honours next year, you will have the opportunity to catch up with the R software then.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#application-to-south-africa-example-using-simpsons-index",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#application-to-south-africa-example-using-simpsons-index",
    "title": "",
    "section": "Application to South Africa: Example Using Simpson’s Index",
    "text": "Application to South Africa: Example Using Simpson’s Index\n\n\n\nSlide 43\n\n\n\n\n\nHere is an example where I have applied Simpson’s diversity index to various places in South Africa (Slide 43). If South Africa represents the area for which we define gamma diversity, then each square or quadrant on the map represents an area where we calculate alpha diversity.\nThe number of crickets present per area has been plotted across South Africa, and you can see that dartker colours indicate areas with higher cricket abundance. These numbers — or rather, the diversity indices calculated from them — show substantial variation across the country. The most diverse areas appear in Limpopo and along the coast, particularly in northern KwaZulu-Natal, where evenness is also highest.\nWhereas in other areas, especially inland, there are many more locations with low diversity and a few with significantly more individuals of particular species. Along the coast, most species are fairly evenly represented.\nWith this sort of information, we can begin to classify South Africa into regions that share similar levels or patterns of diversity, in terms of both the type and presence/abundance of species. This is the value of using these diversity indices — a starting point for further analyses. The kind of calculation I have described here is known as clustering analysis. This will not be covered this year, but this is just to show you an example of potential future applications.\nSuch analyses can be useful on their own, as they visually reveal the different diversity patterns present across a region.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#reading-and-administrative-notes",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#reading-and-administrative-notes",
    "title": "",
    "section": "Reading and Administrative Notes",
    "text": "Reading and Administrative Notes\n\n\n\nSlide 44\n\n\n\n\n\nA reminder: I have given you two papers to read — “What is Macroecology?” and “Macroecology to Unite All Life, Large and Small” (Slide 43). You should have read and understood both, as last week was allocated for that reading. The assumption is that you now understand everything covered in those papers, otherwise you would have asked by now. That opportunity has passed.\nFor this week, you have additional reading around ecological gradients: (1) “Distance, Decay of Similarity in Biogeography and Ecology” by Jeffrey Nicola, and (2) “Seaweeds in Two Oceans: Beta Diversity” by myself. Please read these two papers this afternoon.\nOn Friday, or Thursday, you are welcome to make an appointment with me, in groups of three or four or more, if you have any questions about these two papers. Failing to ask me questions by Thursday will imply that you understand everything, and I am then free to ask you anything from these papers in future tests and exams.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#introduction",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nWe’re going to look at the multivariate nature of ecological data. Last week, I spoke about how to go about collecting samples of species from a particular landscape or habitat, using the UWC Nature Reserve as an example.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#types-of-ecological-data",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#types-of-ecological-data",
    "title": "",
    "section": "Types of Ecological Data",
    "text": "Types of Ecological Data\n\n\n\nSlide 46\n\n\n\n\n\nThe kinds of data we can obtain from a place like the UWC Nature Reserve include a collection of quadrats, which I’ve labelled here as ‘site A’ to ‘site H’ — so there are eight of them (Slide 46). At each site, for every one of the quadrats we place on the landscape, we count the number of species present.\nFor instance, in this example, site A has six species present, while site E has only two species present. The zeros indicate that none of those particular species were present.\nNow, these two sets of data tables — the one on the left and the one on the right — are more or less identical in that they represent the same samples. The difference is that the table on the left, in addition to indicating whether a species is present (a ‘1’) or absent (a ‘0’), also shows, if a species is present at a particular site, how much of the species is present — for example, its abundance, biomass, percentage cover, and so on. If it is not present, there will be a zero. Wherever there is a ‘1’ on this particular table, on the left, the ones could be any number greater than zero, indicating how much of that species is present.\nThe table on the right simply shows a ‘1’ to indicate presence and a ‘0’ for absence. This is what we call presence-absence data.\nSo, the left-hand data type is called abundance data, and the right-hand side is presence-absence data. On the right, we only know whether a species is there or not. On the left, if a species is present, we also know how much of it is present. This is a critical distinction you need to keep in mind.\nYou’ll encounter both of these data types as we progress through the module.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#determining-similarity-between-sites",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#determining-similarity-between-sites",
    "title": "",
    "section": "Determining Similarity Between Sites",
    "text": "Determining Similarity Between Sites\nToday, the goal is to determine how similar various places are to each other, especially regarding their species composition.\nLet’s refer back to the earlier slide. We can see that certain sites are more similar to others but in different ways. In the first instance, two sites could be similar because they share the same species. For example, both site A and site B each have species A, B, C, D, E, and F. The difference between site A and site B is primarily due to species F, where site A has much more of species F compared to site B.\nSo, overall, there are two main reasons why locations can be similar or different. The first is that they share the same species, and the second is that, even if they share species, the abundance of each species is unequal; one place may have more individuals of a species, while another has fewer, and so on.\nAnother kind of difference comes when, say, comparing site D and site E: site E may have only two species that are also present in site D, whereas site D has four other species present not found in site E. Sites might therefore share some species, differ in others, or have uneven abundances of shared species.\nSo, as an ecologist, it’s your job to determine why particular places are different from one another in terms of community structure.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#reasons-for-differences-in-communities",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#reasons-for-differences-in-communities",
    "title": "",
    "section": "Reasons for Differences in Communities",
    "text": "Reasons for Differences in Communities\nCommunities differ from place to place for at least three reasons:\n\nEnvironmental differences: The environment may be different at the two places. For example, one environment may be too warm, excluding species that prefer colder temperatures. Environmental differences may explain why community compositions vary.\nUnmeasured influences: If it’s not due to the environment (or not the part we measured), there might be other unaccounted or unknown influences. These are unmeasured factors for which we can pose hypotheses for further research and data collection.\nRandom noise: Alternatively, differences may simply be due to random noise, stochastic events, or measurement inaccuracies that obscure genuine patterns.\n\nSo, understanding community differences involves asking whether differences are due to measurable environmental factors, unknown influences, or just random variation. Analysing the data helps narrow down which of these is most likely.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#data-representation-distance-similarity-and-dissimilarity-matrices",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#data-representation-distance-similarity-and-dissimilarity-matrices",
    "title": "",
    "section": "Data Representation: Distance, Similarity, and Dissimilarity Matrices",
    "text": "Data Representation: Distance, Similarity, and Dissimilarity Matrices\n\n\n\nSlide 47\n\n\n\n\n\n\n\n\nSlide 48\n\n\n\n\n\n\n\n\nSlide 49\n\n\n\n\n\n\n\n\nSlide 50\n\n\n\n\n\n\n\n\nSlide 51\n\n\n\n\n\nThe different kinds of data for comparing places — be that similarities in species presence or differences in environmental variables — can be represented as distance matrices, more specifically, similarity or dissimilarity matrices (Slides 47-51).\nWhen discussing environmental or species differences, we use distinct types of matrices. Broadly, each matrix is a distance matrix, but the way we calculate distance depends on the type of data.\n\nFor environmental data (e.g., temperature, humidity, soil type), we use an actual distance measure, typically the Euclidean distance.\nFor species composition data (abundance or presence-absence), instead of Euclidean distance, we use similarity/dissimilarity indices such as Bray-Curtis, Sørensen, or Jaccard indices.\n\nSo, just to recap: environmental data includes things like temperature, humidity, depth, light intensity, soil and nutrient composition, and so on; all the things we measure about the environment which might explain community differences. Species data is what species are present or absent, and potentially, how much of each species is present.\nFrom both, we can calculate distance matrices:\n\nEnvironmental data \\(\\rightarrow\\) Euclidean distance\nSpecies data \\(\\rightarrow\\) Bray-Curtis, Sørensen, Jaccard, etc.\n\nThese matrices represent the pairwise differences between each pair of sites.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#pairwise-comparisons",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#pairwise-comparisons",
    "title": "",
    "section": "Pairwise Comparisons",
    "text": "Pairwise Comparisons\nBy ‘pairwise’, I simply mean comparing every site to every other site.\nSo, for example, site A is compared to site B, site A is compared to site C, site A is compared to site D, site E to site C, site G to site F, and so forth. For each pair, we calculate how similar or dissimilar they are, for both environmental and species data, using the appropriate metric.\nThere are many dissimilarity indices you can use for species data, and you’ll see some examples in class. But the principle is always the same: you calculate the similarity or difference for every possible combination of pairs.\nThe result is a matrix where every entry shows the similarity or difference between one site and another.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#calculating-euclidean-distance",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#calculating-euclidean-distance",
    "title": "",
    "section": "Calculating Euclidean Distance",
    "text": "Calculating Euclidean Distance\n\n\n\nSlide 53\n\n\n\n\n\nSo, how do we actually calculate Euclidean distance, which is the main way we measure how different our environmental samples are from each other (Slide 53)?\nEuclidean distance is the direct, straight-line measure between two points. Imagine plotting two points on a coordinate plane with x- and y-axes; each point has an x-coordinate and a y-coordinate. The straight-line, or shortest, distance between the two points is the Euclidean distance. The unit of this distance is the same as the unit used for the axes.\nIf both the x- and y-axes are measured in centimetres, then the diagonal (shortest) distance between your two points will also be measured in centimetres. This is sometimes called Cartesian distance.\nYou can extend this idea to three dimensions — for example, x, y, and z — with the Euclidean distance representing the straight line between two points in three-dimensional space.\nBut you are not limited to two or three dimensions. Ecological data is often ‘multi-dimensional’ because for each site we may have ten, twenty, or even a hundred environmental variables (dimensions) measured. Humans can’t visualise more than three dimensions, but mathematically, calculating the Euclidean distance between points with many dimensions works just the same.\nEuclidean distance aligns with our intuitive sense of “distance” when we’re talking about physical or geographic space, but in the context of ecological data, it reflects how different or similar environmental samples are, based on the variables we’ve measured.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#applying-the-pythagorean-theorem",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#applying-the-pythagorean-theorem",
    "title": "",
    "section": "Applying the Pythagorean Theorem",
    "text": "Applying the Pythagorean Theorem\nTo calculate Euclidean distance, we use the Pythagorean theorem, which you should remember from secondary school mathematics.\nSuppose you have a two-dimensional graph (your y-axis is vertical, x-axis is horizontal) and two points, P and Q.\n\nPoint P is at coordinates \\((P_1, P_2)\\).\nPoint Q is at coordinates \\((Q_1, Q_2)\\).\n\nTo calculate the straight-line (Euclidean) distance between P and Q:\n\nFind the difference in \\(x\\) between \\(Q\\) and \\(P\\): \\(Q_1 - P_1\\).\nFind the difference in \\(y\\) between \\(Q\\) and \\(P\\): \\(Q_2 - P_2\\).\nSquare both differences: \\((Q_1 - P_1)^2 + (Q_2 - P_2)^2\\).\nTake the square root: \\(\\sqrt{(Q_1 - P_1)^2 + (Q_2 - P_2)^2}\\).\n\nThat’s your Euclidean distance.\nIf you have three dimensions, say \\(x, y,\\) and \\(z\\), you simply extend the equation:\n\\[\n\\sqrt{(Q_1 - P_1)^2 + (Q_2 - P_2)^2 + (Q_3 - P_3)^2}\n\\]\nAnd for \\(n\\) dimensions, you generalise:\n\\[\n\\sqrt{\\sum_{i=1}^{n} (Q_i - P_i)^2}\n\\]\nSo, it’s straightforward. For as many variables as you have, just extend the formula, square the differences for each corresponding variable, sum them, and take the root.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#worked-example",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#worked-example",
    "title": "",
    "section": "Worked Example",
    "text": "Worked Example\n\n\n\nSlide 54\n\n\n\n\n\nImagine our sites and their coordinates. Each site has an \\(x\\) and \\(y\\) coordinate (Slide 54). For instance:\n\nSite A: \\((2, 1)\\)\nSite B: \\((3, 5)\\)\n\nThe difference in the \\(x\\) dimension between A and B is \\(3 - 2 = 1\\), and in the \\(y\\) dimension is \\(5 - 1 = 4\\).\nSo the Euclidean distance between A and B is:\n\\[\n\\sqrt{1^2 + 4^2} = \\sqrt{1 + 16} = \\sqrt{17} \\approx 4.123\n\\]\nYou would repeat this process for every pair of sites, filling in the matrix of pairwise distances.\n\n\n\nSlide 55\n\n\n\n\n\nIf you had more dimensions, you’d follow the same logic, adding more squared differences and including them under the square root (Slide 55).\nYou can see that pairs of points which are close in (two-dimensional) space have small values in the distance matrix, while those farther apart have larger values.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#multidimensional-ecological-distance",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#multidimensional-ecological-distance",
    "title": "",
    "section": "Multidimensional Ecological Distance",
    "text": "Multidimensional Ecological Distance\n\n\n\nSlide 56\n\n\n\n\n\n\n\n\nSlide 57\n\n\n\n\n\nHow does this apply to ecological data, which might not be spatial at all? Well, each environmental variable — temperature, depth, light intensity, pH, CO\\(_2\\) content, soil condition, whatever you’re measuring — can be treated as one dimension (Slides 56-57).\nSo each site becomes a point in this multi-dimensional space, and the environmental distance between two sites is simply calculated using the Euclidean formula: for example, with environmental variables temperature, depth, and light, the ecological distance between site A and site B would be:\n\\[\n\\sqrt{(T_A - T_B)^2 + (D_A - D_B)^2 + (L_A - L_B)^2}\n\\]\nWhere \\(T\\) is temperature, \\(D\\) is depth, \\(L\\) is light.\nIf you add more variables, simply keep extending the formula.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#take-home-message",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#take-home-message",
    "title": "",
    "section": "Take-Home Message",
    "text": "Take-Home Message\nThis is why we say ecological data has a multivariate or multidimensional nature. Whether we’re using environmental variables or species abundances or presences, we’re working in a space with as many dimensions as we have types of data measured.\nFor environmental data, we use Euclidean distance to build these matrices.\nRemember: Euclidean distance is appropriate for environmental (quantitative) data. Do not apply it to species data — you shouldn’t. For species data (particularly presence/absence or abundance), use Bray-Curtis, Sørensen, Jaccard, or other appropriate indices.\nSo, in summary, the multivariate nature of ecological data comes from the multiple dimensions contained in our data — each dimension being an environmental characteristic or a species measure. We express the similarity or difference between sites through pairwise comparison, using the appropriate formula to build our distance (or similarity/dissimilarity) matrices. These matrices are the foundation for further analyses you’ll be doing throughout this module.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#applying-euclidean-distances-to-environmental-variables",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#applying-euclidean-distances-to-environmental-variables",
    "title": "",
    "section": "Applying Euclidean Distances to Environmental Variables",
    "text": "Applying Euclidean Distances to Environmental Variables\nRight, so you understand now how to use Euclidean distances to calculate for us how different places are in terms of ecological conditions, or more specifically, the environmental conditions present there. We apply the theorem of Pythagoras to environmental data, where each one of the environmental variables becomes a dimension in our analysis. In this instance, think of temperature, depth, and light. Temperature would be dimension one, depth would be dimension two, and light would be dimension three. So, we have three dimensions in our equation.\nIt does not matter what order they are arranged in; it is completely arbitrary. But because all of these feature together, simultaneously, in some kind of combined measurement of how different the environment is from place to place, the specific units actually fall away. In this calculation, the values in environmental units become meaningless — it becomes just ‘ecological distance’. That is simply how it is.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#standardising-environmental-data",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#standardising-environmental-data",
    "title": "",
    "section": "Standardising Environmental Data",
    "text": "Standardising Environmental Data\n\n\n\nSlide 59\n\n\n\n\n\nNow, here is another example. It’s a similar kind of example to what we looked at before, but you will notice that there is a new table inside here (Slide 59). The reason we have this new table is because we have standardised the data. It is actually the same data, but the values are now very different. For example, the values for pH are recognisable pH numbers, more or less close to neutral. We have moderately aerated water, fairly moderate temperatures as well, and a very shallow kind of freshwater environment. All of these values look familiar because these are things you have probably come across before, and intuitively, you can understand them.\nHowever, when you look at the standardised data, you’ll see that these numbers almost look — well, not random, they’re definitely not random — but to the untrained eye, if you do not know why the numbers look the way they do, it might as well look random to you. What is important here is that we have standardised the data. We have transformed the raw data into standardised data.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#why-standardise",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#why-standardise",
    "title": "",
    "section": "Why Standardise?",
    "text": "Why Standardise?\nThe reason why we standardise things is this: if we do not, then variables like temperature are going to become far more important in influencing the environmental distances. This is because the units and the values of temperature are much larger than, say, the values for depth.\nSo, in our previous example, where we had variables like \\(x\\) and \\(y\\), because \\(x\\), \\(y\\), and \\(z\\) were all measured in, say, centimetres, there was no need to transform the data, since the values are comparable in magnitude. But here, the magnitude of values is very different between the variables. Temperature is measured in degrees Celsius, depth is measured in metres. The units cannot be compared to each other because they are entirely different measures.\nTherefore, in order to rescale them — so that temperature does not become more important in the calculation than depth or any other variable — we standardise them.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#how-standardisation-works",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#how-standardisation-works",
    "title": "",
    "section": "How Standardisation Works",
    "text": "How Standardisation Works\n\n\n\nSlide 60\n\n\n\n\n\n\n\n\nSlide 61\n\n\n\n\n\nStandardising the data essentially scales the mean and the standard deviation. If you transform your raw data into standardised data, the property of this data is such that:\n\nThe mean of the standardised data is \\(0\\).\nThe standard deviation is \\(1\\).\n\nSo, you rescale the data from the raw measurement units into this standardised format. This means that, in your standardised data, the average value of temperature or depth, for example, will be exactly \\(0\\), and all variables are now comparable in magnitude. Temperature will no longer have values with an average of about \\(12\\), and depth will no longer have an average of around \\(1.6\\) or \\(1.7\\), but all means will be \\(0\\) (Slides 60-61).\nAs a result, temperature does not become the overriding factor in our Euclidean distance calculation. When we calculate the Euclidean distance between sites, all the environmental variables have exactly the same weight in the calculation.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#calculating-euclidean-distances-after-standardisation",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#calculating-euclidean-distances-after-standardisation",
    "title": "",
    "section": "Calculating Euclidean Distances after Standardisation",
    "text": "Calculating Euclidean Distances after Standardisation\nSo, we always standardise raw environmental data so that the units of measurement become comparable, and one variable does not become far more influential in the calculation compared to another. Once standardised, we can then apply the Euclidean distance calculation properly.\nI’ll show you the calculation or the equation you will use to standardise your data. It is not very tricky at all; it’s straightforward to do in Excel, or even just with a calculator — nothing complicated there.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#species-data-a-different-kind-of-distance",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#species-data-a-different-kind-of-distance",
    "title": "",
    "section": "Species Data: A Different Kind of Distance",
    "text": "Species Data: A Different Kind of Distance\n\n\n\nSlide 62\n\n\n\n\n\nSo far, we have spoken about environmental data. But we can also know what the difference is in species composition from place to place. To do that, we no longer use environmental data but data on whether species are present or not, so-called presence–absence data, or abundance data if available (Slide 62).\nIn this case, we do not use the Euclidean distance calculation. The Euclidean distance relies on the Pythagorean theorem. However, when calculating the distance between sites in terms of which species are present, or their abundance, we must use a different measure.\nHere, we use indices such as the Bray–Curtis, Jaccard, or Sørensen index. These are used instead of Euclidean distance to calculate how different the species assemblages are from one another.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#applying-the-indices-to-species-data",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#applying-the-indices-to-species-data",
    "title": "",
    "section": "Applying the Indices to Species Data",
    "text": "Applying the Indices to Species Data\nHere you would have species data. So, suppose we have sites, let’s say sites \\(1\\) to \\(10\\). These are the same sites as before. As I said in an earlier lecture, the rows always tell you which places you have sampled (Slides 63-66). So, for example, ten replicates within the UWC Nature Reserve, each one identified by an integer — quadrat \\(1\\), \\(2\\), \\(3\\), and so on, up to \\(10\\).\n\n\n\nSlide 63\n\n\n\n\n\n\n\n\nSlide 64\n\n\n\n\n\n\n\n\nSlide 65\n\n\n\n\n\n\n\n\nSlide 66\n\n\n\n\n\nWithin that first quadrat, you would measure all the different environmental conditions, and, in the same place, you would also record which species are present and, if they are present, how many of them there are.\nYou would use environmental data, after standardising (for example, to bring water hardness to a comparable range with altitude), to calculate the Euclidean distance between every pair of sites. In this table, there are \\(11\\) dimensions — that is, \\(11\\) environmental variables.\nFor every one of the sites, you would compare every possible pair of sites within the collection. For the species data, you then apply the Bray–Curtis, Jaccard, or Sørensen index. I have uploaded onto iKamva a paper which you should read. That will explain how to calculate pairwise differences, and what the relevant index is that you should use to compare (for example) site \\(1\\) to site \\(2\\), or site \\(1\\) to site \\(3\\), and so on. You will need to figure that out by reading the paper.\nIt is a very simple procedure, which you can do by hand with a calculator if you wish, or in Excel. That is your task.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#preview-properties-of-your-data",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#preview-properties-of-your-data",
    "title": "",
    "section": "Preview: Properties of Your Data",
    "text": "Preview: Properties of Your Data\nSo, I will show you what some of the data you produce will look like. First, let’s look at some of the properties of the data that are generated.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#introduction-1",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#introduction-1",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nThis is a proper set of data taken from South Africa. This relates to that paper you read by me, which I wrote in 2017 or so. These are the temperature and various other data collected at different places along our shoreline — specifically, at \\(58\\) locations.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#constructing-distance-matrices",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#constructing-distance-matrices",
    "title": "",
    "section": "Constructing Distance Matrices",
    "text": "Constructing Distance Matrices\nSo, if you consider that there are \\(58\\) sites, you can imagine just how many different pairs of sites there would be if you paired every one with every other one. If you apply that Euclidean distance calculation to this, you end up with a big thing that looks like that. Let me put it up in full screen and make it a bit bigger for you. This is what it is going to look like. If you apply the calculation to the environmental data from the \\(58\\) places — applying Euclidean distance to every possible pair — this is the outcome: a large, dense matrix, which is, obviously, not something you can do by hand. It would take you weeks.\nAn important aspect to note is that, when you do this for a species or environment table — a table with all the sites along the rows, and all the environmental variables along the columns — when you calculate the Euclidean distance for every site, what you create is a square distance matrix.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#understanding-the-distance-matrix",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#understanding-the-distance-matrix",
    "title": "",
    "section": "Understanding the Distance Matrix",
    "text": "Understanding the Distance Matrix\n\n\n\nSlide 61\n\n\n\n\n\nThis is a distance matrix, and it is square (Slide 61). Why do I say it is square? There are \\(58\\) rows and \\(58\\) columns, running from \\(1\\) up to \\(58\\). That is why it is a square matrix.\nThere is also another interesting feature — a diagonal line running from top left to bottom right, filled with zeros. The reason for this is, if you compare site \\(1\\) with site \\(1\\), in terms of how different they are, the ecological distance is zero — because it’s the same site. Site \\(1\\) is site \\(1\\); thus, the difference in ecological space between them is zero.\nThe bigger the number in the matrix entry, the more different those two sites will be. So, if we compare site \\(1\\) on the \\(1\\)st column and \\(1\\)st row, that is the diagonal. If you then compare, for instance, the entry at column \\(2\\), row \\(1\\) — that is the pair site \\(1\\) and site \\(2\\). That value is the same as the value at column \\(1\\), row \\(2\\). The matrix is symmetrical.\nSo, the upper right triangle (above the diagonal of zeros) will contain exactly the same numbers as the lower left triangle (below the diagonal). Typically, when we display these calculations, it is only really interesting to display the lower left triangle.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#key-properties-of-distance-matrices",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#key-properties-of-distance-matrices",
    "title": "",
    "section": "Key Properties of Distance Matrices",
    "text": "Key Properties of Distance Matrices\nThere are three interesting things about a distance or dissimilarity matrix, as used for species data:\n\nIt is square. There are as many columns as rows — \\(58\\) in this example.\nIt is symmetrical. The upper triangle is a mirror image of the lower triangle.\nThere is a zero diagonal. Each site compared to itself contains a zero, because there is no difference.\n\nAdditionally, as you move further from site \\(1\\) along your environmental gradient, these numbers increase, reflecting how different the sites are. For example, site \\(1\\) compared to site \\(2\\) (adjacent sites) will have a small ecological distance. Site \\(1\\) compared to site \\(4\\) is a little bit bigger; site \\(1\\) compared to site \\(18\\) is even bigger, and so forth, until you reach site \\(1\\) compared to site \\(58\\), which would be at the opposite end of the gradient and will provide the largest difference.\nAll these numbers tell you, for every possible pair of sites, how different they are in ecological space.\n\n\n\nSlide 67\n\n\n\n\n\nYour assignment is to calculate these matrices for yourselves (Slide 67).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#introduction-2",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#introduction-2",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nToday we are just going to quickly review those few papers that I handed out to you over the previous weeks, particularly with the aim of arriving at a unified accounting of what macroecology truly is. The drive to achieve such a unified view arises because, in recent years — especially since the 2000s — technologies have come on stream that allow us to apply ecological thinking to microbial communities. Lessons that had, for decades, been learned through studying large, visible multicellular organisms are now actively being adapted and tested on microbes.\nMoreover, it is now possible to pose the question: do the same ecological patterns and explanations that have been identified in multicellular organisms, also hold true for microbial life? Historically, microbes and multicellular organisms have been investigated by largely separate groups of people, with their respective fields developing quite independently. Macroecology, however, seeks to bridge these divides and, as that notable study by Shade et al. (2018) puts it, to examine life across all scales — from mammoths and mules to marmots and microbes.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-scope-and-aim-of-unified-macroecology",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-scope-and-aim-of-unified-macroecology",
    "title": "",
    "section": "The Scope and Aim of Unified Macroecology",
    "text": "The Scope and Aim of Unified Macroecology\n\n\n\nSlide 69\n\n\n\n\n\nLet us situate our discussion with the opening of Shade et al. (2018), which frames the intention to unify our understanding of ecological patterns that exist across the full spectrum of living organisms, big and small (Slide 69). As I have stressed in earlier lectures, the most direct way to describe community patterns is by examining which species are present (their identity), whether they are present or absent, and the relative abundance of each. These metrics, quite naturally, fluctuate both spatially and temporally.\n\n\n\nSlide 70\n\n\n\n\n\nMany of you will already have encountered examples of such spatial patterns in the work by Nicola and White, and also in my own paper that dealt with seaweed distribution along the South African coastline. If you recall, as environmental gradients shift, so does community composition — altering which species are present and in what abundance. Our current task is to review how these ideas scale — whether similar patterns unite community structure for all life forms, from microbes right through to the largest multicellular animals (Slide 70).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#new-technologies-and-sampling-in-microbial-communities",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#new-technologies-and-sampling-in-microbial-communities",
    "title": "",
    "section": "New Technologies and Sampling in Microbial Communities",
    "text": "New Technologies and Sampling in Microbial Communities\n\n\n\nSlide 71\n\n\n\n\n\nThis inquiry into unifying macroecology is possible, particularly for microbes, because of advances in genetic tools. Before the 2000s, most microbial studies focused only on individual species using traditional methods. Now, however, with the development of high-throughput sequencing, one can take a single soil sample or a drop of water, sequence all the genetic material therein, and generate a list of all the taxonomic units (species, or operational taxonomic units) present. Effectively, you can treat each sample as analogous to a quadrat or transect used in large-scale ecological studies of plants and animals, allowing similar methodologies to be applied across kingdoms (Slide 71).\nAdditionally, increases in computing power have made it feasible to analyse the vast datasets produced by these sequencing methods. As a result, we are now able to compare the structure and dynamics of microbial communities directly to those of macroorganisms.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#metabolic-scaling-across-organisms",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#metabolic-scaling-across-organisms",
    "title": "",
    "section": "Metabolic Scaling Across Organisms",
    "text": "Metabolic Scaling Across Organisms\n\n\n\nSlide 72\n\n\n\n\n\nOne of the first major insights gained by comparing across these scales is in the realm of metabolic scaling (Slide 72). A classic graph, which some of you may have seen, plots the logarithm of metabolic rate against the logarithm of body mass for different groups of organisms: bacteria, protists, and multicellular forms (the latter shown in blue in the referenced figure).\nFor multicellular organisms, the relationship follows what is known as the three-quarters power law: for every four-fold (\\(4\\)-unit) increase in body mass, metabolic rate increases by three-fold (\\(3\\) units; this is often shown as \\(R \\propto M^{3/4}\\)). This scaling relationship appears to hold across the diversity of multicellular life.\nWhen we examine protists, the relationship shifts: for every unit increase in body mass, there is a proportional (\\(1:1\\)) increase in metabolic rate, or \\(R \\propto M^1\\). For bacteria and archaea, the scaling becomes even steeper, with a one-unit increase in body mass corresponding to a doubling of metabolic rate — indicating a different underlying relationship.\nThe underlying reasons for these disparate scaling laws are rooted in physiology. For bacteria, metabolic rate predominantly scales as a function of the genes and proteins present. Protists’ metabolic rates are influenced primarily by the number of mitochondria per cell. For multicellular organisms, scaling emerges from the surface area to volume ratio — a topic familiar from Prof Maritz’s lectures and our own discussions last year in BDC 223. The efficiency of metabolic processes in large organisms depends fundamentally on their ability to supply nutrients and gases to their tissues, which relates directly to surface area and volume relationships.\nThis difference in scaling points to significant physiological divergence across life forms, and strongly suggests that, at the ecosystem level, both differences and possible similarities might persist as we scale from bacteria to elephants and blue whales.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#key-concepts-in-shade-et-al.-2018",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#key-concepts-in-shade-et-al.-2018",
    "title": "",
    "section": "Key Concepts in Shade et al. (2018)",
    "text": "Key Concepts in Shade et al. (2018)\nWhen you read the Shade et al. (2018) paper, there are critical sections and concepts that I would like you to focus on. First, under the heading ‘Unified Currency, Individuals and Species,’ you’ll find discussion about the challenges in defining what exactly constitutes an ‘individual’ or a ‘species,’ particularly in microbes. Unlike animals and many plants, where individuals are generally discrete entities, microbial individuals and even many plants (such as grasses or fungi) pose considerable identification challenges. For instance, in a patch of lawn, each visible tuft of grass may appear physically separate above ground, but may, in fact, be interconnected below the surface via rhizomes, making it very difficult to delineate individual organisms.\nThere is also an extended glossary within the paper — please ensure you understand these terms, as they are foundational for your comprehension of the subject.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#unified-accounting-patterns-and-relationships",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#unified-accounting-patterns-and-relationships",
    "title": "",
    "section": "Unified Accounting: Patterns and Relationships",
    "text": "Unified Accounting: Patterns and Relationships\nIn the latter sections of Shade et al., attention turns to the notion of unified accounting — how we might quantitatively relate the number of species (richness), or their abundance, to space, sample size, and similar factors. These relationships are central to macroecology and form the theoretical backbone of the field.\nWe will discuss a selection of these relationships, as described in the paper, in detail during the remainder of today’s session and in future lectures. For now, I want you to note how new technological and analytical advances are truly allowing us, for the first time, to weigh microbial and macroorganismal communities on the same theoretical and empirical scales.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#recap-the-basis-of-species-by-site-matrices",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#recap-the-basis-of-species-by-site-matrices",
    "title": "",
    "section": "Recap: The Basis of Species by Site Matrices",
    "text": "Recap: The Basis of Species by Site Matrices\nIn last week’s lectures, we delved into the concept of species by site matrices. Most of you spent time working with these matrices throughout the week, and I noticed there was a particularly lively discussion — mainly driven by two or three individuals — over the weekend regarding certain calculations. That engagement was valuable, and I trust those who did not participate still gained insight from following the discussions. The reason I have emphasised these matrices is that they form the foundation for understanding relationships between community structure and space. We begin with these samples.\n\n\n\nSlide 73\n\n\n\n\n\nAs an example, if you look at the data set from Shade et al. (2018) — as shown on slide A at the top — you will see exactly the same table repeated below, except that I have transposed it (Slide 73). Species 1 through 6 run along the columns, while sites are arranged in rows. There are six species and six sites. This is how I recommend you work with the data, and it mirrors the requirements of some quantitative ecology software you may use next year, if you choose to take that course. I find it much more intuitive to work with a species-by-environment or species-by-site table where species occupy columns, and sites fill the rows.\nSo, what I have done here is merely transpose the data set — swapping rows for columns. The underlying data remains unchanged. This is precisely the type of data structure you worked with in the Doubs River data exercise last week. From this structure, you can then calculate either presences and absences or work with abundance data directly.\nIf you wish to convert abundance data to a presence–absence matrix for site A, for example, you simply recode the abundances as presences (\\(1\\)) or absences (\\(0\\)). So for site A, it would read \\(011100\\); for site B, \\(011110\\) — and so on. This generates a new matrix next to your abundance data.\n\n\n\nSlide 74\n\n\n\n\n\nIt is important to understand that whether you are using presence–absence data or abundance data, these are the starting points from which we calculate a range of diversity indices — whether alpha, gamma, or beta diversity. Often, these lead to measures known as dissimilarity matrices. From there, we can begin to unravel the relationship between community composition and spatial patterns (Slide 74).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#species-abundance-distributions-and-the-rank-abundance-curve",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#species-abundance-distributions-and-the-rank-abundance-curve",
    "title": "",
    "section": "Species Abundance Distributions and the Rank Abundance Curve",
    "text": "Species Abundance Distributions and the Rank Abundance Curve\n\n\n\nSlide 75\n\n\n\n\n\nThe first key concept is the species abundance distribution, often visualised with a rank abundance curve (Slide 75). The basic idea is this: when you plot the logarithm of the number of species against the rank order of their abundance, you see an interesting pattern. Whether with microbes or macro-organisms, most species tend to have only a few individuals, with just a handful of species being extremely abundant.\nA simple example comes from the UWC Nature Reserve. If you look around, you will notice that the vast majority of the vegetation is comprised of perhaps one or two highly abundant species. There may also be only a single individual of a rare species, or a predator present, but the dominant species will each be represented by many individuals.\nThis fundamental pattern exists regardless of whether we discuss microbes or mammals. There tend to be many rare species, each with few individuals, and a very small number of dominant species containing many individuals. Thus, your rank abundance curve will always reflect this structure: least abundant species on the left, most abundant species on the right.\nSo, for a particular example — say with moths — the least abundant species is plotted furthest left; as abundance increases, we move to the right along the x-axis. The paper we read explains this process clearly, so please review that section as needed.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#occupancy-and-abundance-distributions",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#occupancy-and-abundance-distributions",
    "title": "",
    "section": "Occupancy and Abundance Distributions",
    "text": "Occupancy and Abundance Distributions\n\n\n\nSlide 76\n\n\n\n\n\nNext, there is the occupancy–abundance relationship (Slide 76). Occupancy is defined as the number or proportion of sites at which a species is present. Generally, if a species is very abundant, it will likely be found at most, if not all, sampled sites. To visualise this: on a graph with abundance on the x-axis and occupancy (number of quadrats or sites occupied) on the y-axis, species with high abundance tend to have high occupancy.\nConversely, rare species — those found as just a single individual in just one quadrat — will have a much lower occupancy. Thus, a positive relationship exists between abundance and occupancy across sites.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-speciesarea-curve",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-speciesarea-curve",
    "title": "",
    "section": "The Species–Area Curve",
    "text": "The Species–Area Curve\n\n\n\nSlide 77\n\n\n\n\n\nThe species–area curve is straightforward and highly practical (Slide 77). It underlies the logic of sampling effort. If you place down a single quadrat and count the number of species (species richness, or alpha diversity), you might find \\(10\\) species present. With a second quadrat, you may record \\(15\\) species in total (cumulatively). With each additional quadrat, the cumulative tally of species discovered will rise — up to a certain point. Eventually, with the addition of further quadrats — say \\(20\\) or \\(30\\) — the number of new species detected plateaus.\nIn real-life fieldwork, for example in the UWC Nature Reserve, you might tally the species in one quadrat, then a second, and so forth. Initially, you will add new species with each quadrat, but at some point, adding further quadrats introduces no new species — the curve plateaus. When you reach this point, your sample size is likely sufficient; further sampling does not increase the observed richness.\nThis approach is commonly used to validate that sampling intensity within a habitat is adequate to capture essentially all the species there. In homogeneous landscapes, this plateau appears quickly; in heterogeneous areas or along environmental gradients, additional sampling continually reveals new species, and the curve flattens more slowly.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#distance-decay-relationships",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#distance-decay-relationships",
    "title": "",
    "section": "Distance Decay Relationships",
    "text": "Distance Decay Relationships\n\n\n\nSlide 78\n\n\n\n\n\nTurning to distance decay relationships — these concepts appeared in the Doubs River data exercise (Slide 78). Here, you calculated the Jaccard similarity (or dissimilarity) between pairs of sites, sometimes confusing the two, but I believe this was clarified over the weekend. These measures capture how similar or different two sites are in terms of species composition.\nIn a homogeneous landscape, this similarity remains high and quite stable regardless of the spatial distance between sampling points. Beta diversity (the measure of how much communities change from one site to the next) is therefore low. This pattern applies whether examining microbes or larger organisms.\n\n\n\nSlide 79\n\n\n\n\n\nHowever, in heterogeneous landscapes — such as across the South African coastline — if you compare sites with large spatial separation (e.g., from Cape Vidal to Port Shepstone, a distance approaching \\(770\\)–\\(800\\;\\mathrm{km}\\)), you would expect low similarity and high beta diversity (Slide 79). This distance-decay relationship arises because sites that are further apart tend to experience larger differences in environmental conditions, such as temperature, especially when a physical gradient (like the difference in sea temperature along the coast) exists.\nTherefore, in landscapes with pronounced environmental gradients, greater spatial separation results in greater community turnover (beta diversity), while in homogeneous regions, this pattern is far weaker.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients-2",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#environmental-gradients-2",
    "title": "",
    "section": "Environmental Gradients",
    "text": "Environmental Gradients\n\n\n\nSlide 80\n\n\n\n\n\n\n\n\nSlide 81\n\n\n\n\n\n\n\n\nSlide 82\n\n\n\n\n\n\n\n\nSlide 83\n\n\n\n\n\n\n\n\nSlide 84\n\n\n\n\n\nClosely related are diversity gradients associated with environmental distances rather than merely spatial ones (Slides 80-84). Environmental distance, which some of you calculated using Euclidean distances, quantifies how different two sites are in terms of their physical environment. The greater this distance, the more dissimilar the species composition typically is.\nA classic example can be found when looking at elevation gradients. Ascending a mountain, one observes substantial shifts in vegetation and community structure. For instance, ant or microbial diversity decreases as elevation increases. In such cases, plotting alpha diversity (species richness) against elevation produces a declining trend. Alternatively, one might plot species dissimilarity, Shannon diversity, or any other diversity metric.\nThe data you produced calculating pairwise dissimilarities and environmental distances can be used to generate these plots: environmental distance along the x-axis, species dissimilarity on the y-axis. Where a strong environmental gradient is present, this yields an inclined (increasing) line. Without an environmental gradient, the relationship is flat.\nIt is useful to practice producing and interpreting these kinds of plots, as they readily test your comprehension of ecological relationships.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#application-and-broader-patterns",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#application-and-broader-patterns",
    "title": "",
    "section": "Application and Broader Patterns",
    "text": "Application and Broader Patterns\nThis kind of thinking should not be limited just to the examples discussed in class, or within South Africa. These diversity patterns are present — whether in deep oceans, soils, among microbes, or elephants — across a wide range of environments. The Shade et al. (2018) paper and other references, such as Nekula and White, discuss these processes in further detail. Do review those papers for expanded explanations, particularly around concepts like distance decay.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#key-concepts-review",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#key-concepts-review",
    "title": "",
    "section": "Key Concepts Review",
    "text": "Key Concepts Review\n\n\n\nSlide 85\n\n\n\n\n\n\n\n\nSlide 86\n\n\n\n\n\n\n\n\nSlide 87\n\n\n\n\n\nThere are some concepts to review (Slides 85-87):\n\nAlpha, beta, and gamma diversity: You must clearly understand these.\nBeta diversity can be decomposed into turnover and nestedness components, as discussed in the seaweed paper.\nThe relationship between beta diversity and environmental gradients should be understood.\nNeutral processes (see the seaweed paper) and dispersal limitation often explain observed beta diversity patterns. While these are not synonymous, dispersal limitation is frequently invoked to explain neutral processes.\nScale dependence, as discussed by Nekula and White, is linked to species–area relationships, occupancy–abundance distributions, and underpins many ecological patterns.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#summary",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\nTo consolidate: all the knowledge you have built up this week and last — on how to derive and interpret diversity metrics from species by site or environment by site tables, how to read species–area relationships, occupancy–abundance distributions, distance decay, and environmental gradients — are essential tools in the ecologist’s analytical toolkit.\nThese concepts allow you to interrogate and explain the vast array of biodiversity patterns seen across the world. Explore and understand the readings, and ensure you master the analytical approaches to diversity that we have discussed, as they underpin all further professional work in ecology and biogeography.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#threats-to-biodiversity-paper-by-david-tilman",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#threats-to-biodiversity-paper-by-david-tilman",
    "title": "",
    "section": "Threats to Biodiversity (Paper by David Tilman)",
    "text": "Threats to Biodiversity (Paper by David Tilman)\nThis paper, which you should have read last week, discusses the variety of global threats to biodiversity. It reviews differences between countries and the driving factors behind declining biodiversity. The paper is accessible, and if you engage carefully with the entire text, you should have no difficulty comprehending its arguments. There is nothing in this reading that requires supplementary technical detail beyond what is provided.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#theme-of-this-week-the-value-and-valuation-of-ecosystems",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#theme-of-this-week-the-value-and-valuation-of-ecosystems",
    "title": "",
    "section": "Theme of This Week: The Value and Valuation of Ecosystems",
    "text": "Theme of This Week: The Value and Valuation of Ecosystems\nToday’s lecture—and associated readings—centres on what humans derive from biodiversity, focusing on the ecosystem services concept and the valuation of these services.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#consequences-of-biodiversity-loss",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#consequences-of-biodiversity-loss",
    "title": "",
    "section": "Consequences of Biodiversity Loss",
    "text": "Consequences of Biodiversity Loss\nYou are now to read work concerning the consequences of declining biodiversity. The key question is: What happens, both to people and to ecosystems, when biodiversity diminishes over time? One of your readings today addresses the ensuing changes and articulates precisely how biodiversity is useful to people—what services it provides, and how we conceptualise this utility.\n\nThe Costanza et al. (1997) Paper: Monetary Valuation of Ecosystem Services\nThe classic 1997 paper by Bob Costanza is foundational. It quantifies the value of the world’s major ecosystem services. The central idea here is ‘value’, most commonly communicated in terms of financial or monetary value—how much a hectare of natural habitat is worth in rands, dollars, pounds, and so on.\nThe paper discusses both market and non-market frameworks for valuation:\n\nMarket Valuation: This converts ecosystem services directly into monetary units. For example, how much it would cost to replace the gas regulation function of the ocean if we had to engineer it ourselves.\nNon-market & Intrinsic Value: The value derived here is not directly monetary—it often depends on culture, personal attachment, or aesthetic appreciation. This can vary dramatically from place to place, or person to person. For example, some value the ocean primarily for surfing, others for fisheries.\n\nThe paper, however, focuses on market-based approaches, such as how much it would cost to engineer breakwaters to replace kelp forests, which naturally protect shorelines from erosion and wave damage. To value such services, one method is to estimate the cost of constructing a substitute infrastructure—for example, if removing kelp would demand construction projects with a price tag of \\(\\text{R}\\,84\\,\\text{million}\\) or more. The essential idea is: ecosystems provide free services, and it is only when we lose them that their economic value becomes undeniable.\nSimilarly, for recreational value, you might ask users directly—how much would they pay to guarantee continued access? This contingent valuation approach translates their willingness to pay into monetary value.\nMarket-based methods work for many ecosystem services, but intrinsic, spiritual, and cultural values remain difficult to quantify. These will come up again in wiki assignments, but are not a main focus in this module.\nThe upshot of this paper is a comprehensive estimate of what ecosystem services are worth globally: Table 2 quotes a total value of \\(\\$33\\) trillion for global ecosystem services (in 1997), a staggering sum.\n\n\nUpdated Valuation: Costanza et al. (2014)\nA subsequent paper, authored roughly \\(15\\) years after the original study, updates these figures. It evidences the decline in ecosystem value as biodiversity is lost or natural land is converted to agriculture or built environments.\nBetween the initial study and the later one, estimated global ecosystem service value has fallen by between \\(\\$4\\) trillion and \\(\\$20\\) trillion per year. This is a direct consequence of resource extraction, habitat loss, and other threats. The loss is not abstract; it has clear, quantifiable costs.\nAn important figure in the 2014 paper illustrates the relationships among various forms of capital:\n\nHuman capital (people and communities)\nBuilt capital (infrastructure and human-made environments)\nSocial capital (institutions, cultural systems)\nNatural capital (ecosystem services and natural resources)\n\nHuman, built, and social capital all ultimately depend on natural capital. If the latter is eroded too severely, the rest lose their foundation and value.\nThese readings compel you to consider deeply: Why are ecosystem services valuable? What sorts of value are there, and which are measurable in monetary terms and which are not?\nPay attention to how all these values—market and nonmarket—play out in real life. For instance, when you see litter accumulating in public spaces or natural landscapes, consider how it devalues these environments, both psychologically and economically.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#this-weeks-assigned-reading-and-assessment",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#this-weeks-assigned-reading-and-assessment",
    "title": "",
    "section": "This Week’s Assigned Reading and Assessment",
    "text": "This Week’s Assigned Reading and Assessment\nTo summarise the required reading for this week, you are to focus on three specific papers—labelled as numbers \\(7\\), \\(8\\), and \\(9\\) in your list.\nTheir content will be directly relevant to both your wiki assessment and your overall personal knowledge. Mastery of these readings will be assessed for the first time in the upcoming Monday test, and then again in the final exam.\nContinue to integrate, synthesise, and critically evaluate what you read. Do not confine yourself to rote memorisation of facts; rather, strive for an understanding of how broad themes relate and interconnect. This will best prepare you for your current and future coursework, and also for real-world scientific reasoning.\n\nSlide references would be integrated here if available; please match discussion points to specific slides in your notes as we proceed in class.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tangled Bank Blog",
    "section": "",
    "text": "The Blog of the Tangled Bank R Teaching Website at the University of te Western Cape dedicated to the teaching of R and RStudio to students in the Biological Sciences. The Blog deals specifically with applications of R in the Ocean and Biological Sciences, with each post dedicated to solving a computational problem of interest to students and researchers in these fields.\nThe Blog is written by AJ Smit, a Professor in the Department of Biodiversity and Conservation Biology at the University of the Western Cape. AJ is a marine ecologist with a keen interest in the use of R for data analysis and visualisation. AJ is also the author of the Tangled Bank R Teaching Website.\n\n\n\n\n\nBasic Detection and Visualisation of Marine Heatwaves\n\n\n\nR\n\nanalysis\n\nMHW\n\n\n\nThis vignette demonstrates the basic use of the heatwaveR package for the detection and visualisation of marine heatwaves.\n\n\n\n\n\nNov 11, 2023\n\n\nSmit, A. J., Robert Schlegel\n\n15 min\n\n\n\n\n\n\nheatwaveR\n\n\n\nR\n\nanalysis\n\nMHW\n\n\n\nIntroducing heatwaveR to a non-marine science audience.\n\n\n\n\n\nNov 22, 2023\n\n\nSmit, A. J., Robert Schlegel\n\n3 min\n\n\n\n\n\n\nDetect event streaks based on specified thresholds\n\n\n\nR\n\nanalysis\n\nMHW\n\n\n\nThis vignette demonstrates how to use a heatwaveR function to the analysis of experimental data for finding the run lengths of events that meet certain criteria.\n\n\n\n\n\nNov 22, 2023\n\n\nSmit, A. J.\n\n5 min\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.,\n  author = {Smit, A. J.,},\n  title = {Tangled {Bank} {Blog}},\n  url = {http://tangledbank.netlify.app/blog.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. Tangled Bank Blog. http://tangledbank.netlify.app/blog.html."
  },
  {
    "objectID": "BDC223/BDC223_index.html",
    "href": "BDC223/BDC223_index.html",
    "title": "BDC223: Plant Ecophysiology",
    "section": "",
    "text": "Plants and other photo-oxygenic organisms are foundational to most life on Earth. They form part of complex interactions with the non-living and living world, and are severely being impacted by many of the components of global change. In this module, BDC223, we will explore the fundamental concepts, characteristics, and driving forces that shape and maintain plant-based productivity across Earth.\nThe pages for BDC223 are still being developed. Please check back later for more content.\n\n1 Practicals\nPlease find here links to the practical sessions, which are sceduled for Monday afternoons:\n\n\n\nDate\nTopic\nLink\nDue Date\n\n\n\n\n2024-09-16\nLab 1\nSurface Area to Volume (S/V) Ratios in Biology\n2024-09-23\n\n\n2024-09-23\nLab 2\nMiscellaneous Calculations\n2024-09-30\n\n\n2024-09-30\nLab 3\nPI Curves – Jassby and Platt\n2024-10-07\n\n\n2024-10-07\nLab 4\nUptake Kinetics – Michaelis-Menten\n2024-10-14\n\n\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.,\n  author = {Smit, A. J.,},\n  title = {BDC223: {Plant} {Ecophysiology}},\n  url = {http://tangledbank.netlify.app/BDC223/BDC223_index.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. BDC223: Plant Ecophysiology. http://tangledbank.netlify.app/BDC223/BDC223_index.html.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC223/Lab3_PI_curves.html",
    "href": "BDC223/Lab3_PI_curves.html",
    "title": "Lab 3: PI Curves – Jassby and Platt",
    "section": "",
    "text": "This Lab Accompanies the Following Lecture\n\n\n\n\nSlides: Pigments and Photosynthesis\nReading: Lecture 6: PI Curves – Jassby and Platt\nStudents will work as individuals; assignments are per individual. This lab is due on Monday 7 October 2024 at 7:00 on iKamva.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lab 3: PI Curves -- Jassby and Platt"
    ]
  },
  {
    "objectID": "BDC223/Lab3_PI_curves.html#experimental-procedure-photosynthesis-irradiance-p-i-curve",
    "href": "BDC223/Lab3_PI_curves.html#experimental-procedure-photosynthesis-irradiance-p-i-curve",
    "title": "Lab 3: PI Curves – Jassby and Platt",
    "section": "1 Experimental Procedure: Photosynthesis-Irradiance (P-I) Curve",
    "text": "1 Experimental Procedure: Photosynthesis-Irradiance (P-I) Curve\nIn this experiment, you will measure the photosynthetic response of Elodea sp. plants at varying light intensities. You will quantify the amount of oxygen produced at each photon flux density and use these data to calculate the photosynthetic rate and create a Photosynthesis-Irradiance (P-I) curve. By plotting the P-I curve, you will visually estimate the maximum photosynthetic rate (\\(P_{\\text{max}}\\)), the initial slope of the curve (\\(\\alpha\\)), the light compensation point (LCP), and the respiration rate (\\(R\\)) based on the modified Jassby and Platt model.\n\n1.1 Materials\n\nElodea sp. plants (approximately 4.5 g per replicate)\nAquatic medium for submerging plants\nLight source with adjustable intensities (0 to 550 μmol photons·m⁻²·s⁻¹)\nOxygen probe or dissolved oxygen meter\nIncubation chamber (to control environmental conditions)\nTimer\nData recording sheet\n\n\n\n1.2 Experimental Procedure\n\nSet up the experiment:\n\nPlace the plant material (Elodea sp., weighed to between 4.42 and 4.69 g) in an aquatic medium within a closed incubation chamber.\nEnsure that the oxygen probe is calibrated and submerged properly to continuously measure the oxygen concentration.\nAdjust the light source to create different light intensity levels, starting from 0 μmol photons·m⁻²·s⁻¹ (dark conditions) and increasing incrementally up to around 550 μmol photons·m⁻²·s⁻¹.\n\nMeasure oxygen evolution:\n\nFor each light intensity, incubate the plants for approximately 600 seconds (10 minutes). Record the exact incubation time, as small variations in time may occur due to experimental conditions.\nMeasure the total amount of oxygen produced (or consumed) during each incubation period. Oxygen production indicates net photosynthesis, while oxygen consumption in dark conditions reflects respiration.\nRepeat the measurements for five different replicates of plant mass to account for variability and obtain a robust data set.\n\nRecord light intensities:\n\nFor each replicate, ensure that you document the light intensity (μmol photons·m⁻²·s⁻¹) for each corresponding oxygen measurement. The intensity should vary from 0 μmol photons·m⁻²·s⁻¹ to about 550 μmol photons·m⁻²·s⁻¹ in a regular stepwise fashion, allowing you to cover a broad range of photosynthetically active radiation.\n\n\n\n\n1.3 Calculating the Photosynthetic Rate\nTo calculate the photosynthetic rate for each light intensity, follow these steps:\n\nDetermine the total oxygen evolved: Using the data recorded during the experiment, identify the total amount of oxygen evolved or consumed (in mg O₂) for each light intensity and for each replicate.\nCalculate the time in hours: Convert the incubation time (in seconds) to hours. Use the formula: \\[\n\\text{Time (h)} = \\frac{\\text{Time (s)}}{3600}\n\\]\nDetermine the oxygen production rate per plant mass: Calculate the oxygen production rate per gram of plant material per hour, using the formula: \\[\nP(I) = \\frac{\\text{Total O}_2 \\text{ evolved (mg)}}{\\text{Time (h)} \\times \\text{Plant mass (g)}}\n\\] This will give you the net photosynthetic rate \\(P(I)\\) at each light intensity \\(I\\), expressed in mg O₂ produced per gram per hour.\n\n\n\n1.4 Plotting the P-I Curve\n\nCreate a plot:\n\nOn graph paper or using plotting software, plot the net photosynthetic rate \\(P(I)\\) (mg O₂·g⁻¹·h⁻¹) on the y-axis against the light intensity \\(I\\) (μmol photons·m⁻²·s⁻¹) on the x-axis.\n\nDraw the fitted line:\n\nUsing a smooth curve, fit the data points to represent the trend of photosynthesis at increasing light levels. The curve will initially show a steep increase as light intensity rises (due to light-limited photosynthesis), followed by a gradual plateau as the rate of photosynthesis approaches the maximum capacity of the plant (\\(P_{\\text{max}}\\)).\n\nIdentify the key parameters:\n\nFrom the curve, estimate:\n\n\\(P_{\\text{max}}\\): The maximum photosynthetic rate, where the curve flattens.\n\\(\\alpha\\): The initial slope of the curve, representing the efficiency of photosynthesis at low light levels.\nLight compensation point: The point where the curve crosses the x-axis, indicating the light intensity at which net photosynthesis is zero.\nRespiration rate (\\(R\\)): The rate of oxygen consumption in the absence of light (when \\(I = 0\\)).\n\n\n\n\n\n1.5 Estimating the Jassby and Platt Model Parameters\nThe modified Jassby and Platt model is used to describe the relationship between light intensity and photosynthetic rate. The model equation is:\n\\[\nP(I) = P_{\\text{max}} \\times \\tanh\\left(\\frac{\\alpha I}{P_{\\text{max}}}\\right) - R\n\\]\n\n\\(P_{\\text{max}}\\) is the maximum rate of photosynthesis.\n\\(\\alpha\\) is the initial slope of the P-I curve, representing the photosynthetic efficiency at low light levels.\n\\(R\\) is the dark respiration rate, calculated from the negative O₂ evolution in the absence of light.\n\nFit this equation to your data and estimate \\(P_{\\text{max}}\\), \\(\\alpha\\), and \\(R\\). The light compensation point can also be derived from the model, as it is the light intensity where the net photosynthesis rate equals zero.\nThis experiment has already been done for you and the data are provided below for your analysis.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lab 3: PI Curves -- Jassby and Platt"
    ]
  },
  {
    "objectID": "BDC223/Lab3_PI_curves.html#pi-data",
    "href": "BDC223/Lab3_PI_curves.html#pi-data",
    "title": "Lab 3: PI Curves – Jassby and Platt",
    "section": "2 PI Data",
    "text": "2 PI Data\nBelow are the data tables for each replicate. Each table includes:\n\nLight Intensity (I): in μmol photons·m⁻²·s⁻¹\nIncubation Time (T): in seconds\nTotal O₂ Evolved: in mg O₂ per incubation period\n\n\n2.1 Replicate 1 (Plant Mass: 4.50 g)\n\n\n\nLight Intensity (μmol·m⁻²·s⁻¹)\nIncubation Time (s)\nTotal O₂ Evolved (mg)\n\n\n\n\n0\n605\n-1.495\n\n\n50\n595\n0.335\n\n\n100\n610\n2.089\n\n\n150\n600\n3.567\n\n\n200\n590\n4.800\n\n\n250\n615\n5.941\n\n\n300\n605\n6.590\n\n\n400\n600\n7.489\n\n\n500\n610\n8.078\n\n\n550\n605\n8.130\n\n\n\n\n\n2.2 Replicate 2 (Plant Mass: 4.42 g)\n\n\n\nLight Intensity (μmol·m⁻²·s⁻¹)\nIncubation Time (s)\nTotal O₂ Evolved (mg)\n\n\n\n\n0\n590\n-1.483\n\n\n50\n600\n0.321\n\n\n100\n610\n2.044\n\n\n150\n595\n3.523\n\n\n200\n605\n4.741\n\n\n250\n600\n5.896\n\n\n300\n610\n6.504\n\n\n400\n595\n7.378\n\n\n500\n605\n7.967\n\n\n550\n600\n8.025\n\n\n\n\n\n2.3 Replicate 3 (Plant Mass: 4.61 g)\n\n\n\nLight Intensity (μmol·m⁻²·s⁻¹)\nIncubation Time (s)\nTotal O₂ Evolved (mg)\n\n\n\n\n0\n610\n-1.558\n\n\n50\n600\n0.350\n\n\n100\n590\n2.128\n\n\n150\n605\n3.609\n\n\n200\n600\n4.836\n\n\n250\n610\n5.998\n\n\n300\n595\n6.635\n\n\n400\n605\n7.542\n\n\n500\n600\n8.142\n\n\n550\n590\n8.185\n\n\n\n\n\n2.4 Replicate 4 (Plant Mass: 4.43 g)\n\n\n\nLight Intensity (μmol·m⁻²·s⁻¹)\nIncubation Time (s)\nTotal O₂ Evolved (mg)\n\n\n\n\n0\n600\n-1.501\n\n\n50\n610\n0.327\n\n\n100\n595\n2.065\n\n\n150\n605\n3.545\n\n\n200\n600\n4.765\n\n\n250\n590\n5.905\n\n\n300\n615\n6.543\n\n\n400\n605\n7.454\n\n\n500\n600\n8.046\n\n\n550\n610\n8.098\n\n\n\n\n\n2.5 Replicate 5 (Plant Mass: 4.69 g)\n\n\n\nLight Intensity (μmol·m⁻²·s⁻¹)\nIncubation Time (s)\nTotal O₂ Evolved (mg)\n\n\n\n\n0\n595\n-1.575\n\n\n50\n605\n0.361\n\n\n100\n600\n2.152\n\n\n150\n590\n3.637\n\n\n200\n610\n4.870\n\n\n250\n600\n6.025\n\n\n300\n590\n6.675\n\n\n400\n615\n7.596\n\n\n500\n605\n8.189\n\n\n550\n600\n8.240\n\n\n\n\n\n2.6 Notes:\n\nNegative Values: Negative total O₂ evolved indicates net respiration (O₂ consumption) at low light intensities.\nVariability: Incubation times and O₂ measurements include random variability to simulate real experimental conditions.\nData Usage: You can calculate the photosynthetic rate \\(P(I)\\) using: \\[\nP(I) = \\frac{\\text{Total O}_2 \\text{ evolved}}{\\left(\\frac{T}{3600}\\right) \\times \\text{Plant Mass}}\n\\]\n\nThis will yield \\(P(I)\\) in mg O₂·g⁻¹·h⁻¹.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lab 3: PI Curves -- Jassby and Platt"
    ]
  },
  {
    "objectID": "BDC223/Lab3_PI_curves.html#for-submission",
    "href": "BDC223/Lab3_PI_curves.html#for-submission",
    "title": "Lab 3: PI Curves – Jassby and Platt",
    "section": "3 For Submission",
    "text": "3 For Submission\n\nCalculate the photosynthetic rate \\(P(I)\\) for each replicate.\nCalculate the mean and standard deviation of \\(P(I)\\) at each light intensity.\nProvide the following answers:\n\nExhibit 1: Plot the mean \\(P(I)\\) values with error bars (±1 SD) as a function of light intensity.\nExhibit 2: Fit the data to the model to estimate all the parameters of the modified Jassby and Platt model (including the saturating irradiance, \\(I_{\\text{k}}\\)). You can ‘fit’ the model by hand or, for bonus marks, use a curve-fitting tool in a spreadsheet or programming language. Neatly present these data as a table.\nExhibit 3: Discuss the results in the context of the model and the experimental data. What do the parameters of the model tell you about the photosynthetic performance of the plant? What are the limitations of the model? How does all of this relate to the theory of photosynthesis (i.e. the relationship between light intensity and photosynthetic rate)?\nExhibit 4: Why is it necessary to control the environmental conditions during the experiment? Which conditions, and why? What are any other potential sources of error in this experiment?\nExhibit 5: In this experiment we measured oxygen evolution. Name and discuss a few other approaches we can use to measure photosynthetic rate.\n\nSubmit your work as a MS Word file on iKamva.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lab 3: PI Curves -- Jassby and Platt"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html",
    "href": "BDC223/L08a-nutrient_uptake.html",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "Today’s lecture is centred on the topic of nutrient uptake. We are using nitrogen as our principal example, given its status as a ubiquitous nutrient, essential to all plants for successful growth. Additionally, we’ll be considering the environmental consequences of there being excessive nitrogen in the environment. This is tied directly to the planetary boundaries concept expounded by Johan Rockström, specifically the quadrant concerning the nitrogen and phosphorus cycles—two key global biogeochemical cycles involving the transportation and transformation of these elements between the biosphere, geosphere, atmosphere, and oceans.\nNitrogen is a particular concern, as it is one of the major thresholds humanity has already exceeded globally. This excess results in numerous environmental problems, especially where processes involve plants—primarily aquatic and marine plants, although to a lesser extent, it does impact certain terrestrial plants as well.\n\n\nNitrogen’s importance is clear when you consider its abundant presence. Approximately \\(79\\%\\) of the air we breathe is composed of nitrogen, and it’s found in soils, sand, and oceans, where it is accessed by plant roots or available in dissolved form for algae and marine plants. The most productive patches of green—on land and visible as blooms in the ocean—are areas of high nitrogen availability, where lush plant and algal growth is possible.\nFor this module, whilst terrestrial plants will feature in our discussions, our focus throughout the examples will be on aquatic environments, with particular attention to nitrogen dissolved in seawater and its role in triggering phytoplankton blooms. These can be so prolific that they’re visible from space—swirls and green patches near the UK, Ireland, or east of the Falkland Islands, all testify to high concentrations of phytoplankton supported by nitrogen availability.\nThe twirling and swirling patterns you observe in satellite imagery arise from physical ocean mixing processes—currents, eddies—distributing dissolved nitrogen, which in turn supports phytoplankton blooms.\n\n\n\nAs I’ve mentioned, excess nitrogen in the environment, from pollution, sewage, or runoff from fertilisers, contributes to unsightly and sometimes malodorous nuisance algal blooms. These blooms are ecologically damaging, reduce water quality, and negatively affect ecosystems and human livelihoods. They’re typically accompanied by visible pond scum, floating litter, and other environmental degradation.\nFor instance, in China, the large population density and the dispersal of untreated sewage directly into water bodies has led to massive blooms of phytoplankton. Later in the lecture, we’ll discuss the process of eutrophication, which explains in detail how these blooms develop.\nAdditionally, certain bacteria, such as photosynthetic cyanobacteria (“blue-green algae”), are part of the problem. As blooms expand, they block light, darkening the water and, through their respiration (especially at night), use up oxygen. Upon death, bacteria decompose the overwhelming biomass, a process which consumes even more oxygen and releases large amounts of \\(\\mathrm{CO}_2\\). The result—known as a dystrophic or anoxic event—is hypoxia or complete anoxia, which leads to further die-offs, especially of aquatic animals requiring oxygen. The largest consumer of oxygen here is the decomposition of dead organic material by bacteria through respiratory processes.\n\n\n\n\n\n\nNutrients—alongside light, oxygen, and carbon dioxide—are indispensable for plants and algae to grow, reproduce, and persist. In aquatic environments, algae are fully immersed in nutrient-rich water, allowing them to absorb dissolved nutrients directly. In contrast, terrestrial plants can only access nutrients through roots that penetrate soil, extracting dissolved nutrients from soil water.\nAlgae, because of their immersion, do not require roots. Their entire body (the “thallus”) is bathed in nutrients. By contrast, plants depend on root systems both for nutrient uptake and for transport to other parts of the organism. You should recall from previous modules how the surface area to volume ratio becomes decisive for nutrient uptake efficiency, particularly in aquatic environments where mixing is driven by environmental processes.\n\n\nTerrestrial plants often benefit from symbiotic relationships with fungi and bacteria—mycorrhizae and root nodules—helping them acquire and process nutrients from the soil. Aquatic algae generally do not require such associations, although bacteria in the marine environment do help make nitrogen available for algal uptake.\nBacteria, in terms of both biomass and individual numbers, are among the planet’s most abundant organisms; without them, no form of life would exist.\n\n\n\n\nOur current understanding of plant nutrient uptake is largely indebted to studies conducted between the 1930s and 1970s. Algae, because of their direct exposure to dissolved nutrients, provided a simple and convenient model to study the principles of nutrient uptake, eventually informing our understanding of the entire plant kingdom.\n\n\nThrough these studies, nutrients have been sorted in multiple ways, principal among which are:\n\nEssential versus beneficial nutrients: Essential nutrients are those without which a plant cannot survive or complete its life cycle. Even the absence of a single essential nutrient will halt growth, productivity, or reproduction. Beneficial nutrients enhance or facilitate physiological processes, but are not strictly required for survival or completion of the life cycle.\n\nBy Epstein’s (1972) definition, a nutrient is essential if the plant cannot complete a normal life cycle without it, and the element forms part of an essential plant constituent (e.g., magnesium in chlorophyll a).\nEssential nutrients cannot be substituted by another element and must have a direct effect, not just act as a cofactor.\n\nMacronutrients versus micronutrients: This classification reflects the relative quantity needed by the plant. Macronutrients are present and required in much higher concentrations; their roles are often structural, contributing to the biomass of the plant (e.g., carbon, nitrogen, phosphorus, oxygen, potassium). Micronutrients, though required in far smaller amounts, function mainly as catalysts or regulators (e.g., iron in nitrate reductase).\n\n\n\n\nTables commonly show, per \\(\\mathrm{kg}\\) of dry plant material, that macronutrient concentrations are several orders of magnitude greater than those of micronutrients.\n\n\n\n\nTo reiterate: macronutrients contribute to the structure and mass of the plant; micronutrients act as catalytic or regulatory substances. For example, if you removed all the iron from a large tree, you’d be left with only a handful, but that tiny amount is indispensable for the plant’s metabolic processes.\n\n\n\nAlgae (seaweeds) require around \\(20\\) varieties of nutrients, including: - Nitrogen - Phosphorus - Potassium - Calcium, among others.\nNitrogen is crucial—found in amino acids, nucleic acids, proteins—because proteins require nitrogen for their formation. Phosphorus is critical for structural and metabolic functions such as nucleic acids, phospholipids in membranes, and ATP transfer. Potassium and others play similar roles.\nWhile the precise list of essential nutrients varies modestly between algae and higher plants (the latter require \\(17\\) essential elements), the principle remains the same: the majority of biomass is composed of macronutrients.\n\n\nI will not set examination questions that require simple regurgitation of lists (such as “List five essential elements in seaweeds”). Focus, rather, is on understanding the processes and underlying principles.\n\n\n\n\n\nA key physiological contradiction prompts interesting questions: inside the plant, the concentration of key nutrients is typically much higher than outside—in seawater or soil. Passive uptake via diffusion or osmosis cannot account for this, as both processes follow concentration gradients (from areas of high to low concentration).\nTherefore, nutrient uptake often requires active transport—energy-dependent mechanisms that move nutrients against their concentration gradient into the plant, where they are assimilated into new biomass.\n\n\nIf we compare concentrations (for example, micrograms per gram of seawater versus of seaweed dry mass), plants can have much higher internal nutrient concentrations. For iron, particularly scarce in seawater, seaweeds maintain relatively high tissue concentrations, which demands an energetic uptake strategy.\n\n\n\n\nBesides inorganic nutrients (the “bare elements,” not bound in organic molecules), plants can, through mixotrophy, also absorb dissolved organic compounds, though this is much less common and less of a focus for today’s discussion.\n\n\nBeginning in the 1960s-70s, researchers recognised that at any time, a particular nutrient could be ‘limiting’. That is, if it is removed or absent, growth ceases. Professor Dugdale and colleagues demonstrated that in most seawaters, nitrogen is the major limiting nutrient. Experiments adding nitrogen to seawater samples resulted in rapid phytoplankton growth; adding other nutrients like phosphorus or potassium generally produced no such effect unless these were limiting.\nTherefore, a nutrient is ‘limiting’ in a given context if its addition results in increased growth; if not, it isn’t currently limiting. In most marine environments, nitrogen is limiting; phosphorus sometimes is, but this is more common in freshwater environments.\n\n\n\n\nThe “Redfield ratio” is a critical empirical observation: for every \\(106\\) atoms of carbon in microalgae, there must be \\(16\\) atoms of nitrogen and \\(1\\) atom of phosphorus for optimal growth. That is, the ideal \\(\\mathrm{C}:\\mathrm{N}:\\mathrm{P}\\) ratio is \\(106:16:1\\).\nFor macroalgae, a similar ratio exists: \\(550:30:1\\) (C:N:P), reflecting the greater carbon requirement for structural integrity in larger, multicellular algae.\nThis optimal ratio is essential. Any deviation means one of the nutrients becomes limiting, restricting growth. Microalgae, being unicellular and minute, need less structural carbon than macroalgae.\n\n\n\nLieben’s law, or “the law of the minimum,” was articulated in the 19th century. It states that the yield of a plant is determined by the single most limiting nutrient, regardless of the abundance of others. Thus, if any one nutrient is below its critical threshold, it will restrict growth, no matter how abundantly everything else is supplied.\nThis principle is vital for optimising fertilisation strategies in both agriculture and aquaculture: knowing which nutrient is limiting allows for targeted supplementation for maximal growth.\n\n\nFor a red macroalga with an optimal N:P ratio of \\(30:1\\), suppose more nitrogen is present than phosphorus as required. Phosphorus becomes the limiting nutrient, constraining growth despite surplus nitrogen. Conversely, if nitrogen is below the required ratio, then it is the limiting nutrient.\nThis concept extends to all primary producers—seaweeds, microalgae, and terrestrial plants.\n\n\n\n\nLuxury consumption describes the phenomenon where some plants — particularly certain seaweeds — can take up more of a nutrient than is immediately required for growth, storing the excess for future use. When environmental levels of nitrogen or phosphorus later dip below optimal, the plant draws on these internal reserves to maintain growth.\nThis adaptation is especially valuable in environments with fluctuating nutrient availability and features prominently in aquaculture. Here, seaweeds can be provided with nitrogen and phosphorus at optimal ratios, and their ability to undertake luxury consumption helps buffer against subsequent scarcity.\nLuxury consumption is a survival strategy most evident among K-selected, climax species in the ocean, conferring resilience in the face of unpredictable nutrient supply, and ensuring continued survival and growth despite external variability.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#introduction-the-centrality-of-nitrogen",
    "href": "BDC223/L08a-nutrient_uptake.html#introduction-the-centrality-of-nitrogen",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "Today’s lecture is centred on the topic of nutrient uptake. We are using nitrogen as our principal example, given its status as a ubiquitous nutrient, essential to all plants for successful growth. Additionally, we’ll be considering the environmental consequences of there being excessive nitrogen in the environment. This is tied directly to the planetary boundaries concept expounded by Johan Rockström, specifically the quadrant concerning the nitrogen and phosphorus cycles—two key global biogeochemical cycles involving the transportation and transformation of these elements between the biosphere, geosphere, atmosphere, and oceans.\nNitrogen is a particular concern, as it is one of the major thresholds humanity has already exceeded globally. This excess results in numerous environmental problems, especially where processes involve plants—primarily aquatic and marine plants, although to a lesser extent, it does impact certain terrestrial plants as well.\n\n\nNitrogen’s importance is clear when you consider its abundant presence. Approximately \\(79\\%\\) of the air we breathe is composed of nitrogen, and it’s found in soils, sand, and oceans, where it is accessed by plant roots or available in dissolved form for algae and marine plants. The most productive patches of green—on land and visible as blooms in the ocean—are areas of high nitrogen availability, where lush plant and algal growth is possible.\nFor this module, whilst terrestrial plants will feature in our discussions, our focus throughout the examples will be on aquatic environments, with particular attention to nitrogen dissolved in seawater and its role in triggering phytoplankton blooms. These can be so prolific that they’re visible from space—swirls and green patches near the UK, Ireland, or east of the Falkland Islands, all testify to high concentrations of phytoplankton supported by nitrogen availability.\nThe twirling and swirling patterns you observe in satellite imagery arise from physical ocean mixing processes—currents, eddies—distributing dissolved nitrogen, which in turn supports phytoplankton blooms.\n\n\n\nAs I’ve mentioned, excess nitrogen in the environment, from pollution, sewage, or runoff from fertilisers, contributes to unsightly and sometimes malodorous nuisance algal blooms. These blooms are ecologically damaging, reduce water quality, and negatively affect ecosystems and human livelihoods. They’re typically accompanied by visible pond scum, floating litter, and other environmental degradation.\nFor instance, in China, the large population density and the dispersal of untreated sewage directly into water bodies has led to massive blooms of phytoplankton. Later in the lecture, we’ll discuss the process of eutrophication, which explains in detail how these blooms develop.\nAdditionally, certain bacteria, such as photosynthetic cyanobacteria (“blue-green algae”), are part of the problem. As blooms expand, they block light, darkening the water and, through their respiration (especially at night), use up oxygen. Upon death, bacteria decompose the overwhelming biomass, a process which consumes even more oxygen and releases large amounts of \\(\\mathrm{CO}_2\\). The result—known as a dystrophic or anoxic event—is hypoxia or complete anoxia, which leads to further die-offs, especially of aquatic animals requiring oxygen. The largest consumer of oxygen here is the decomposition of dead organic material by bacteria through respiratory processes.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#understanding-nutrients-and-their-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#understanding-nutrients-and-their-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "Nutrients—alongside light, oxygen, and carbon dioxide—are indispensable for plants and algae to grow, reproduce, and persist. In aquatic environments, algae are fully immersed in nutrient-rich water, allowing them to absorb dissolved nutrients directly. In contrast, terrestrial plants can only access nutrients through roots that penetrate soil, extracting dissolved nutrients from soil water.\nAlgae, because of their immersion, do not require roots. Their entire body (the “thallus”) is bathed in nutrients. By contrast, plants depend on root systems both for nutrient uptake and for transport to other parts of the organism. You should recall from previous modules how the surface area to volume ratio becomes decisive for nutrient uptake efficiency, particularly in aquatic environments where mixing is driven by environmental processes.\n\n\nTerrestrial plants often benefit from symbiotic relationships with fungi and bacteria—mycorrhizae and root nodules—helping them acquire and process nutrients from the soil. Aquatic algae generally do not require such associations, although bacteria in the marine environment do help make nitrogen available for algal uptake.\nBacteria, in terms of both biomass and individual numbers, are among the planet’s most abundant organisms; without them, no form of life would exist.\n\n\n\n\nOur current understanding of plant nutrient uptake is largely indebted to studies conducted between the 1930s and 1970s. Algae, because of their direct exposure to dissolved nutrients, provided a simple and convenient model to study the principles of nutrient uptake, eventually informing our understanding of the entire plant kingdom.\n\n\nThrough these studies, nutrients have been sorted in multiple ways, principal among which are:\n\nEssential versus beneficial nutrients: Essential nutrients are those without which a plant cannot survive or complete its life cycle. Even the absence of a single essential nutrient will halt growth, productivity, or reproduction. Beneficial nutrients enhance or facilitate physiological processes, but are not strictly required for survival or completion of the life cycle.\n\nBy Epstein’s (1972) definition, a nutrient is essential if the plant cannot complete a normal life cycle without it, and the element forms part of an essential plant constituent (e.g., magnesium in chlorophyll a).\nEssential nutrients cannot be substituted by another element and must have a direct effect, not just act as a cofactor.\n\nMacronutrients versus micronutrients: This classification reflects the relative quantity needed by the plant. Macronutrients are present and required in much higher concentrations; their roles are often structural, contributing to the biomass of the plant (e.g., carbon, nitrogen, phosphorus, oxygen, potassium). Micronutrients, though required in far smaller amounts, function mainly as catalysts or regulators (e.g., iron in nitrate reductase).\n\n\n\n\nTables commonly show, per \\(\\mathrm{kg}\\) of dry plant material, that macronutrient concentrations are several orders of magnitude greater than those of micronutrients.\n\n\n\n\nTo reiterate: macronutrients contribute to the structure and mass of the plant; micronutrients act as catalytic or regulatory substances. For example, if you removed all the iron from a large tree, you’d be left with only a handful, but that tiny amount is indispensable for the plant’s metabolic processes.\n\n\n\nAlgae (seaweeds) require around \\(20\\) varieties of nutrients, including: - Nitrogen - Phosphorus - Potassium - Calcium, among others.\nNitrogen is crucial—found in amino acids, nucleic acids, proteins—because proteins require nitrogen for their formation. Phosphorus is critical for structural and metabolic functions such as nucleic acids, phospholipids in membranes, and ATP transfer. Potassium and others play similar roles.\nWhile the precise list of essential nutrients varies modestly between algae and higher plants (the latter require \\(17\\) essential elements), the principle remains the same: the majority of biomass is composed of macronutrients.\n\n\nI will not set examination questions that require simple regurgitation of lists (such as “List five essential elements in seaweeds”). Focus, rather, is on understanding the processes and underlying principles.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#concentration-gradients-and-uptake-mechanisms",
    "href": "BDC223/L08a-nutrient_uptake.html#concentration-gradients-and-uptake-mechanisms",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "A key physiological contradiction prompts interesting questions: inside the plant, the concentration of key nutrients is typically much higher than outside—in seawater or soil. Passive uptake via diffusion or osmosis cannot account for this, as both processes follow concentration gradients (from areas of high to low concentration).\nTherefore, nutrient uptake often requires active transport—energy-dependent mechanisms that move nutrients against their concentration gradient into the plant, where they are assimilated into new biomass.\n\n\nIf we compare concentrations (for example, micrograms per gram of seawater versus of seaweed dry mass), plants can have much higher internal nutrient concentrations. For iron, particularly scarce in seawater, seaweeds maintain relatively high tissue concentrations, which demands an energetic uptake strategy.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#nutrient-classes-and-special-cases",
    "href": "BDC223/L08a-nutrient_uptake.html#nutrient-classes-and-special-cases",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "Besides inorganic nutrients (the “bare elements,” not bound in organic molecules), plants can, through mixotrophy, also absorb dissolved organic compounds, though this is much less common and less of a focus for today’s discussion.\n\n\nBeginning in the 1960s-70s, researchers recognised that at any time, a particular nutrient could be ‘limiting’. That is, if it is removed or absent, growth ceases. Professor Dugdale and colleagues demonstrated that in most seawaters, nitrogen is the major limiting nutrient. Experiments adding nitrogen to seawater samples resulted in rapid phytoplankton growth; adding other nutrients like phosphorus or potassium generally produced no such effect unless these were limiting.\nTherefore, a nutrient is ‘limiting’ in a given context if its addition results in increased growth; if not, it isn’t currently limiting. In most marine environments, nitrogen is limiting; phosphorus sometimes is, but this is more common in freshwater environments.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#the-redfield-ratio-and-nutrient-limitation",
    "href": "BDC223/L08a-nutrient_uptake.html#the-redfield-ratio-and-nutrient-limitation",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "The “Redfield ratio” is a critical empirical observation: for every \\(106\\) atoms of carbon in microalgae, there must be \\(16\\) atoms of nitrogen and \\(1\\) atom of phosphorus for optimal growth. That is, the ideal \\(\\mathrm{C}:\\mathrm{N}:\\mathrm{P}\\) ratio is \\(106:16:1\\).\nFor macroalgae, a similar ratio exists: \\(550:30:1\\) (C:N:P), reflecting the greater carbon requirement for structural integrity in larger, multicellular algae.\nThis optimal ratio is essential. Any deviation means one of the nutrients becomes limiting, restricting growth. Microalgae, being unicellular and minute, need less structural carbon than macroalgae.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#liebigs-law-of-the-minimum",
    "href": "BDC223/L08a-nutrient_uptake.html#liebigs-law-of-the-minimum",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "Lieben’s law, or “the law of the minimum,” was articulated in the 19th century. It states that the yield of a plant is determined by the single most limiting nutrient, regardless of the abundance of others. Thus, if any one nutrient is below its critical threshold, it will restrict growth, no matter how abundantly everything else is supplied.\nThis principle is vital for optimising fertilisation strategies in both agriculture and aquaculture: knowing which nutrient is limiting allows for targeted supplementation for maximal growth.\n\n\nFor a red macroalga with an optimal N:P ratio of \\(30:1\\), suppose more nitrogen is present than phosphorus as required. Phosphorus becomes the limiting nutrient, constraining growth despite surplus nitrogen. Conversely, if nitrogen is below the required ratio, then it is the limiting nutrient.\nThis concept extends to all primary producers—seaweeds, microalgae, and terrestrial plants.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#luxury-consumption",
    "href": "BDC223/L08a-nutrient_uptake.html#luxury-consumption",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "",
    "text": "Luxury consumption describes the phenomenon where some plants — particularly certain seaweeds — can take up more of a nutrient than is immediately required for growth, storing the excess for future use. When environmental levels of nitrogen or phosphorus later dip below optimal, the plant draws on these internal reserves to maintain growth.\nThis adaptation is especially valuable in environments with fluctuating nutrient availability and features prominently in aquaculture. Here, seaweeds can be provided with nitrogen and phosphorus at optimal ratios, and their ability to undertake luxury consumption helps buffer against subsequent scarcity.\nLuxury consumption is a survival strategy most evident among K-selected, climax species in the ocean, conferring resilience in the face of unpredictable nutrient supply, and ensuring continued survival and growth despite external variability.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#introduction",
    "href": "BDC223/L08a-nutrient_uptake.html#introduction",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Introduction",
    "text": "Introduction\nYesterday we spoke about nutrients. I gave you a brief introduction to what nutrients are, and explained that they can be classified into macronutrients and micronutrients, as well as essential and beneficial nutrients. Today, we need to talk about the consequences—the environmental consequences—of nutrients. We’ll also begin to explore the field of measuring the uptake of nutrients by seaweeds. That’s our plan for today.\nOne of the things we’re going to do is to use nitrogen as our example. Nitrogen is convenient and easy to work with. The uptake mechanisms seen in many other nutrients are similar to those for nitrogen, so we can use it as a nice case study. But, of course, nitrogen is also one of the most important nutrients, both in the ocean and on land. It’s often a limiting nutrient in the ocean and is important in many environmental problems we face today, such as eutrophication.\nIn today’s lecture, I’ll provide some of the ecophysiological background for why some seaweeds respond particularly well under eutrophic conditions. You will understand the physiological basis for why some seaweeds become what we refer to as nuisance algae.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#case-study-the-beijing-olympics-and-eutrophication",
    "href": "BDC223/L08a-nutrient_uptake.html#case-study-the-beijing-olympics-and-eutrophication",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Case Study: The Beijing Olympics and Eutrophication",
    "text": "Case Study: The Beijing Olympics and Eutrophication\nAs an example, I’ll refer to what happened during the Beijing Olympics in around 2008. Just prior to the Olympics, vast parts of the Chinese shoreline were covered with nuisance green seaweeds. Authorities had to employ a whole group of people to clean up the shoreline. All those green bits—the seaweed blooms in China—were a direct result of nitrogen entering the ocean and polluting the waterways. It’s unsightly, it’s smelly, and it’s dangerous, so it’s a huge problem around the world.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#sources-and-forms-of-nitrogen",
    "href": "BDC223/L08a-nutrient_uptake.html#sources-and-forms-of-nitrogen",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Sources and Forms of Nitrogen",
    "text": "Sources and Forms of Nitrogen\nSo, where does nitrogen come from? Nitrogen is quite abundant in the atmosphere—actually, the bulk of the atmosphere is comprised of nitrogen, about 79% of it is gaseous nitrogen, \\(N_2\\). Gaseous nitrogen itself cannot be used by plants, so certain processes are required to convert that nitrogen into a bioavailable form that plants can take up.\nNitrogen is brought into the oceans via rivers, mostly in the form of nitrate and ammonium. It can also be present in the atmosphere as nitrous oxides and, in water, as nitrous oxides, entering the ocean via river runoff or various atmospheric processes. Other sources include processes in the Earth’s crust, like volcanic eruptions, as well as fossil fuel burning from industrial operations, which both put nitrous oxides into the atmosphere. In certain cases, that nitrogen becomes available as a very acidic form—nitric acid—which is a source of some acid rain.\nBut once the nitrogen enters the ocean, many interesting processes take place. It gets recycled, taken up by algae, released by algae, released by animals that feed upon the algae—so the whole big global biogeochemical process is seen in the ocean, as well as elsewhere on the planet.\nWe’ll delve a bit more into the detail of the global biogeochemical cycles shortly.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#nitrogen-fixation-and-cycling",
    "href": "BDC223/L08a-nutrient_uptake.html#nitrogen-fixation-and-cycling",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Nitrogen Fixation and Cycling",
    "text": "Nitrogen Fixation and Cycling\nAs I said before, gaseous nitrogen in the atmosphere, which is the bulk of it, cannot be used directly by plants. It must somehow become available, and this is accomplished by the process of nitrogen fixation, carried out by organisms such as cyanobacteria. Cyanobacteria can take up atmospheric dinitrogen (\\(N_2\\)), lock it internally in organic forms or as ammonia. When the cyanobacteria die and decompose or are eaten, that nitrogen is recycled in the form of ammonium or nitrate back into the ocean. Thus, cyanobacteria fix atmospheric nitrogen, making it available to the rest of the ecosystem to be taken up as ammonium or nitrate. This supports much of the planet’s photoautotrophs, on both land and in the ocean.\nIf we look at the various sources of nitrogen: gaseous nitrogen is very abundant in the ocean and atmosphere. In the ocean, once it’s dissolved, the total amount of all forms of nitrogen in the ocean is about \\(95\\%\\) or so gaseous nitrogen—meaning dissolved \\(N_2\\) from the atmosphere. A much smaller fraction is available as nitrate—\\(NO_3^-\\)—which comprises about \\(5\\%\\) of the nitrogen in the ocean. An even smaller amount is present as nitrite \\(NO_2^-\\); it’s almost immeasurable in many instances, because nitrite is only present in seawater for very short periods as an intermediary between ammonium and nitrate. Concentrations of nitrite are, therefore, very low.\nNitrogen is also present as ammonium (\\(NH_4^+\\)); close to about \\(0.1\\%\\) of the total nitrogen in the ocean is present as ammonium. So those three compounds—\\(NO_3^-\\), \\(NO_2^-\\), and \\(NH_4^+\\) (nitrate, nitrite, and ammonium)—are together known as DIN: dissolved inorganic nitrogen. This is the amount of nitrogen available for uptake by plants in the ocean.\n“Dissolved” because it’s in ionic form, within the water; “inorganic” because, while it’s not bonded within an organic molecule, it does not contain carbon-hydrogen structures; and “nitrogen” because the major macronutrient atom in all these is nitrogen.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#the-marine-nitrogen-cycle",
    "href": "BDC223/L08a-nutrient_uptake.html#the-marine-nitrogen-cycle",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "The Marine Nitrogen Cycle",
    "text": "The Marine Nitrogen Cycle\nOnce nitrogen enters the ocean, it is cycled in various different ways. It’s a complex set of reactions and processes, involving uptake by phytoplankton, their death, their consumption by zooplankton and fish, as well as decomposition—a whole host of processes.\nThe science that studies the transformation of various forms of nutrients between abiotic and biotic pools within the Earth system is called biogeochemistry. Biogeochemistry is concerned with the movement and the rates of transformation of nutrients between, for example, phytoplankton and zooplankton (organic or biotic components), the ocean (the hydrosphere), the atmosphere, and the geosphere.\nHere is a basic, simplified representation of the nitrogen cycle in the ocean: At the top, you have the atmosphere, at the bottom the ocean floor, and in between is the water column. In and out of the atmosphere, gases such as \\(O_2\\), \\(CO_2\\), and \\(N_2\\) move into the ocean, so we end up with DIN (ammonium, nitrate, nitrite) dissolved in the water. Algae then take up this DIN to produce algal biomass via photosynthesis. Animals consume phytoplankton, relying on them for biomass production, and in turn carry out respiration, taking up oxygen produced by the algae.\nAs animals eat algae, they excrete waste products, releasing \\(CO_2\\), more DIN, and dissolved organic forms of nitrogen into the water. In feeding on algae, animals might only consume parts, allowing algal cell contents to leak out, making dissolved organic nitrogen (DON) available to the environment. Algae can then take up this DON.\nHere, you see a cycling: nitrogen comes from the atmosphere, dissolves in seawater, is taken up by algae, consumed by animals, and released again. But not all nitrogen is continually cycled—some is lost. Particulate forms of nitrogen, called POM (particulate organic matter), settle down through the water column as “marine snow.” As it falls, marine snow is decomposed by bacteria, which release more DIN and \\(CO_2\\) in the process. Thus, concentrations of DIN generally increase deeper into the ocean, as marine snow decomposes and releases more nutrients.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#remineralisation-and-upwelling",
    "href": "BDC223/L08a-nutrient_uptake.html#remineralisation-and-upwelling",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Remineralisation and Upwelling",
    "text": "Remineralisation and Upwelling\nAnimals on the seafloor can consume marine snow, releasing DIN, DON, and \\(CO_2\\) into the water column, with bacteria contributing to remineralisation. Remineralisation is the process that converts organic forms of nitrogen back into inorganic forms like ammonium and nitrate.\nOver time, DIN accumulates in the deeper ocean, but physical ocean processes, such as ocean currents (upwelling), transport some of this deep, nutrient-rich water back to the surface, injecting remineralised nitrogen into sunlit upper layers, where algae can again take it up.\nSo, these cycles are coupled by biological processes—linking algae to animals through heterotrophy (predation, grazing), decomposition, excretion, faecal pellet production, as well as by physical processes like upwelling. Photosynthesis is a surface process, so nitrogen uptake by algae occurs mainly in surface waters, not in the deep ocean where there is no light.\nAlso, don’t forget the role of nitrogen-fixing bacteria like cyanobacteria, which fix atmospheric nitrogen and make it bioavailable for oceanic and other organisms. This is how atmospheric nitrogen becomes available in the surface ocean.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#definitions-some-key-terms",
    "href": "BDC223/L08a-nutrient_uptake.html#definitions-some-key-terms",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Definitions: Some Key Terms",
    "text": "Definitions: Some Key Terms\nSome important definitions:\n\nDIN: Dissolved Inorganic Nitrogen; includes ammonium, nitrate, nitrite.\nDIP: Dissolved Inorganic Phosphorus; phosphorus equivalents to DIN.\nDON: Dissolved Organic Nitrogen.\nPOM: Particulate Organic Matter; also includes particulate forms of both nitrogen and phosphorus.\n\nThe biogeochemical cycle operates similarly on land, but most of the transformations happen within the soil, particularly around plant roots, as well as via aboveground decomposition processes—for example, as leaves fall, decompose, and transfer nitrogen back into the soil.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#units-concentrations-and-oceanographic-patterns",
    "href": "BDC223/L08a-nutrient_uptake.html#units-concentrations-and-oceanographic-patterns",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Units, Concentrations, and Oceanographic Patterns",
    "text": "Units, Concentrations, and Oceanographic Patterns\nWhen reading literature about nitrogen as a macronutrient, you’ll encounter various units: micromolar (\\(\\mu\\)M), microgram atom per litre, and so on. These all describe the concentration of nitrogen in water or solid solution. You should recall from first-year chemistry how to convert between micromolar and microgram atom per litre (\\(\\mu\\)M to \\(\\mu\\)g atom L\\(^{-1}\\)), and vice versa. Be familiar with these conversions, as you will encounter them in tests.\nYou must also know the SI prefixes and the number of zeros associated with each—grammes, milligrammes, microgrammes, nanogrammes, picogrammes, et cetera. In plant physiology, a basic grasp of chemistry and SI unit prefixes is assumed.\n\nTypical Oceanic Nitrogen Concentrations\nHere’s a range of concentrations that nitrogen is available in the ocean:\n\nTropical regions (ca. \\(10^\\circ\\)S to \\(10^\\circ\\)N): Very low nitrogen; concentrations may be in the nano- to picogramme range (\\(\\mathrm{ng\\ L^{-1}}\\) – \\(\\mathrm{pg\\ L^{-1}}\\)).\nMost of the ocean: Microgramme to milligramme range.\nFreshwater systems: Often reach the milligramme range.\nUpwelling regions (e.g., the Benguela upwelling off South Africa, Canary Current off North Africa, Humboldt off South America, California Current off North America): Highest oceanic nutrient levels. Here, total inorganic nitrogen can reach up to \\(40\\ \\mu\\mathrm{mol\\ L^{-1}}\\), while inorganic phosphorus typically reaches \\(2\\ \\mu\\mathrm{mol\\ L^{-1}}\\).\n\nNote the ratio here, approximately 10:1, closely corresponding to the Redfield ratio.\nDuring active upwelling, nutrient concentrations rise to \\(20\\)–\\(40\\ \\mu\\mathrm{mol\\ L^{-1}}\\) nitrogen, \\(2\\ \\mu\\mathrm{mol\\ L^{-1}}\\) inorganic phosphorus. When upwelling subsides, these values drop below \\(4\\ \\mu\\mathrm{mol\\ L^{-1}}\\) for nitrate and \\(0.2\\ \\mu\\mathrm{mol\\ L^{-1}}\\) for inorganic phosphorus.\nThus, in the ocean, background nitrogen levels are not fixed but fluctuate, sometimes very rapidly (over tens of minutes) due to oceanographic processes such as upwelling. Algae have evolved various adaptations to cope with the intermittent and transient nature of nitrogen availability in the ocean—a stark contrast to terrestrial systems, where changes are usually slower.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#classification-of-oceanic-systems-based-on-nutrient-levels",
    "href": "BDC223/L08a-nutrient_uptake.html#classification-of-oceanic-systems-based-on-nutrient-levels",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Classification of Oceanic Systems Based on Nutrient Levels",
    "text": "Classification of Oceanic Systems Based on Nutrient Levels\nThe ocean can be classified into:\n\nOligotrophic: Low nutrient, e.g., tropical regions, almost no nitrogen.\nMesotrophic: Intermediate, e.g., upwelling zones, fluctuates temporally.\nEutrophic: High nutrients, often due to human influence—unnaturally high nitrogen.\n\nIn the ocean, nitrogen is typically limiting. However, excessive input causes problems—most notably, eutrophication.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#the-nitrogen-bomb-human-impact",
    "href": "BDC223/L08a-nutrient_uptake.html#the-nitrogen-bomb-human-impact",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "The Nitrogen Bomb: Human Impact",
    "text": "The Nitrogen Bomb: Human Impact\nFor your self-study: read the article “The Nitrogen Bomb” (available on Ecoma). The Haber-Bosch process has resulted in a huge problem worldwide in both terrestrial and aquatic systems. “Nitrogen bomb” is a metaphor for the disastrous potential of excessive and unwisely applied nitrogen, most sharply observed in eutrophication. This is examinable content.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#what-is-eutrophication",
    "href": "BDC223/L08a-nutrient_uptake.html#what-is-eutrophication",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "What is Eutrophication?",
    "text": "What is Eutrophication?\nEutrophication usually occurs when too much nitrogen is added to a body of water that would naturally be nitrogen limited. In these systems, certain algae or seaweeds respond rapidly to the enrichment. For instance, the three illustrated species all possess high surface area to volume ratios, meaning nearly every cell is exposed directly to the environment and can immediately take up available nutrients. This capacity allows rapid growth and biomass accumulation.\nMore complex seaweeds with lower surface area to volume ratios respond more slowly, if at all, to such nutrient pulses; the response is more distributed and structurally limited. In contrast, these high surface area opportunistic algae (sometimes called R-selected species) bloom excessively when nutrients are introduced, becoming nuisance species and disrupting the ecological balance.\nIn a natural system, there is a high diversity of plants and animals. After eutrophication, one species becomes dominant, reducing community composition, species richness, and causing the biomass of that one species to increase exponentially.\nIn severe cases, the system can become anoxic. Imagine a dense bloom of photosynthesising algae; at night, without photosynthesis, respiration consumes all the oxygen in the water. Species that require oxygen die off, and their decomposition consumes even more oxygen, eventually producing low-oxygen, or dystrophic, conditions.\nBacteria are key to these processes, as they drive decomposition and thus increase total ecosystem respiration and oxygen consumption.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#mitigating-eutrophication",
    "href": "BDC223/L08a-nutrient_uptake.html#mitigating-eutrophication",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Mitigating Eutrophication",
    "text": "Mitigating Eutrophication\nAs for what can be done: removing the blooming nuisance algae is not addressing the underlying issue. We must ensure that sources of nitrogen entering the water are addressed—by improving sewage treatment, reducing fertiliser runoff from agriculture, and proper waste management. Rivers seen as convenient dumping grounds simply transfer the problem downstream; the consequences are borne by someone else or by the ecosystem.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#linking-form-and-function-surface-area-to-volume-ratio",
    "href": "BDC223/L08a-nutrient_uptake.html#linking-form-and-function-surface-area-to-volume-ratio",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Linking Form and Function: Surface Area to Volume Ratio",
    "text": "Linking Form and Function: Surface Area to Volume Ratio\nI’ve mentioned that response to eutrophication is connected to both the morphology and physiology of algae. Those species with high surface area to volume ratios can absorb nutrients rapidly and outcompete others. This is where Littler and Littler’s “functional form model” comes into play, explaining why morphology is crucial to ecological dynamics under eutrophic conditions.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#summary-and-integration",
    "href": "BDC223/L08a-nutrient_uptake.html#summary-and-integration",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Summary and Integration",
    "text": "Summary and Integration\nIn summary, you should now connect previously disconnected ideas—such as surface area to volume ratio and nutrient uptake. Understanding this enables a more comprehensive view of how eutrophication alters ecosystems.\nIf you have questions or do not grasp a particular aspect, you’re welcome to ask on WhatsApp. Please ensure you read the assigned articles and refresh your knowledge of unit conversions and SI prefixes, as you will be expected to use this knowledge fluently.\nRead further on eutrophication. Much more can be said, but the core is straightforward biology playing out in an ecosystem that simplifies under stress—one species dominates as nutrients increase, reducing diversity and altering physiological and ecological processes within the system.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#introduction-1",
    "href": "BDC223/L08a-nutrient_uptake.html#introduction-1",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Introduction",
    "text": "Introduction\nRight, so today we will continue to talk about nutrient uptake. Last week, we spoke about nutrient uptake experiments, and I showed you how to derive information from the depletion curve—the relationship that shows uptake rate versus substrate concentration. When we plotted that relationship, the graph appeared as a hyperbolic tangent curve. At low concentrations, the uptake rate increases rapidly, and then at high nutrient concentrations, it reaches a plateau. This type of relationship is known as the Michaelis–Menten uptake relationship, and it serves as an example of one of three different uptake mechanisms called active uptake.\nOn Thursday, we will discuss passive transport and facilitated diffusion, which are two additional uptake mechanisms. Generally speaking, algae and most other plants can display one of these three mechanisms—active transport, passive transport, and facilitated diffusion. Today, our focus will remain on active uptake.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#review-of-active-uptake-parameters-refer-to-slide-text-if-available",
    "href": "BDC223/L08a-nutrient_uptake.html#review-of-active-uptake-parameters-refer-to-slide-text-if-available",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Review of Active Uptake Parameters (Refer to slide text if available)",
    "text": "Review of Active Uptake Parameters (Refer to slide text if available)\nLast week, after we explored the uptake curve—the \\(V\\) versus \\(S\\) relationship—of active uptake as determined for nitrate, I explained the various parameters: \\(V_{max}\\), \\(K_s\\), and \\(\\alpha\\). On this slide, you will find text discussing the ecological significance of these parameters. We have covered this before, so I will not repeat it in detail. Instead, let us delve a bit further into what active uptake involves.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#what-is-active-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#what-is-active-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "What is Active Uptake?",
    "text": "What is Active Uptake?\nActive uptake allows most plants to maintain nutrient concentrations inside their cells that are much greater than those found in the external environment. This process enables nutrients to be transported from an area where there is a lower concentration to an area inside the plant where there is a higher concentration—essentially moving against the concentration gradient. More formally, this occurs against the electrochemical potential gradient.\nFor most cases, external concentrations are in the micromolar range (\\(\\mu\\)mol), while internal concentrations inside the cell are in the millimolar range (mmol). This situation implies that passive diffusion alone cannot account for the movement of nutrients into the cell, because diffusion typically moves substances from high to low concentration—not the other way around.\nPassive diffusion is – at most – responsible for the movement of nutrients from the environment across the boundary layer. This specific process is determined by passive diffusion. However, the major uptake of nutrients into the cell is described by active uptake. For this to occur—from a region of low concentration to one of high concentration—cells must expend energy, namely metabolic energy.\nThe energy expended is generally light-dependent, with ATP being the most likely source. This is achieved by a system that involves proton-pumping ATPases, setting up a gradient between the external and internal environment in terms of pH. The proton gradient, or the pH gradient, establishes the electrochemical potential gradient, which then drives secondary ion transport. The secondary ion in question is the nutrient—such as nitrate, in our previous example.\nThis coupling between the pH gradient and the nutrient gradient facilitates the active uptake of nutrients. Coupled transport may arise from differing movements of ions at different sites, either in opposite or the same direction. When hydrogen ions are pumped out of the cell and nutrients are pumped in, this form of counter-transport is known as anti-port, or anti-porter transport. Conversely, some nutrients move in the same direction as the protons—this is called symport or co-transport.\nIn algae, the proton pump is linked to the co-transport of substances like sugars and thiourea, and there are also mechanisms involving the pumping of sodium ions, which can be responsible for the co-transport of other nutrients from the environment into the cell.\n\nKey Points of Active Uptake\nYou must remember that cellular energy, primarily derived from ATP, is required to drive active uptake. ATP drives the proton pump, which establishes the primary gradient, and then nutrients are brought into the cell by coupling to this gradient.\nThis is the mechanism by which an energetic, active process brings nutrients into the cell—by coupling with a proton pump generated by ATP and ATPases. While this is a complex physiological process, knowing this overview will suffice; we will not explore all the fine physiological details.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#characteristics-that-define-active-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#characteristics-that-define-active-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Characteristics that Define Active Uptake",
    "text": "Characteristics that Define Active Uptake\nBeyond its energy requirement, active uptake is defined by several additional characteristics:\n\nSelectivity for Particular Ions: Only specific ions are taken up via active transport, not all. For example, nitrate, phosphorus, and sulphates can be taken up in this way. One of the components of dissolved inorganic nitrogen (DIN), ammonium, is typically not taken up via active transport and is excluded here.\nSaturation of the Carrier System: There is a stage in the uptake process where the carrier system becomes saturated. At high nutrient concentrations, there is a portion of the curve where \\(V_{max}\\) is reached—meaning uptake rate will not increase, despite increasing external nutrient concentration. This is because the enzymatic systems responsible for transport become saturated and cannot operate any faster.\nMovement Against the Concentration Gradient: As previously discussed, active uptake involves the movement of ions against their concentration gradient.\n\nThese features—selectivity, saturation, and movement against the gradient—define active uptake. Note that some of these factors also apply to facilitated uptake, which we shall discuss later.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#graphical-representation-v-vs-s-curves",
    "href": "BDC223/L08a-nutrient_uptake.html#graphical-representation-v-vs-s-curves",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Graphical Representation: \\(V\\) vs \\(S\\) Curves",
    "text": "Graphical Representation: \\(V\\) vs \\(S\\) Curves\nWhen a graph of uptake rate (\\(V\\)) versus substrate concentration (\\(S\\)) displays a hyperbolic tangent curve—a steep rise at low concentrations and a plateau at high concentrations—this signifies active or facilitated uptake. The maximum rate of uptake (\\(V_{max}\\)) is primarily set by factors intrinsic to the algae, such as the enzymatic processes involved.\nConsequently, environmental factors that influence enzyme activity—like light intensity or temperature—will impact how high \\(V_{max}\\) can be. Thus, active uptake is largely determined and influenced by environmental conditions that promote growth, photosynthesis, and metabolic activity—for example, high temperatures and abundant light.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#surface-area-to-volume-ratio-and-uptake-parameters",
    "href": "BDC223/L08a-nutrient_uptake.html#surface-area-to-volume-ratio-and-uptake-parameters",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Surface Area to Volume Ratio and Uptake Parameters",
    "text": "Surface Area to Volume Ratio and Uptake Parameters\nSuppose we gather several seaweeds, spanning all six or seven different functional form categories outlined by Littler and Littler, and conduct uptake experiments. We would discover that \\(V_{max}\\) and \\(K_s\\) vary as a function of the surface area to volume ratio.\nPlants with flat, membranous, or highly filamentous forms tend to grow rapidly and thus have a much higher \\(V_{max}\\)—thanks to their high surface area to volume ratio, which allows every cell direct exposure to the nutrient-rich environment. They are also typically able to acquire nutrients effectively even in environments where nutrient levels are low, implying they often have a low \\(K_s\\) (and thus a high affinity for nutrients).\nOn the other end of the spectrum, algae with a low surface area to volume ratio—where the bulk of the cells are internal—grow more slowly, with reduced access to light and slower diffusion rates. Consequently, they possess a low \\(V_{max}\\) and often a higher \\(K_s\\), making them less able to acquire nutrients when these are scarce.\nIf these low \\(V_{max}\\) species are placed in a nutrient-rich (eutrophic) environment, it makes little difference, because their limitation is set by internal cellular processes, not external nutrient supply. In contrast, fast-growing, high-surface-area species with high \\(V_{max}\\) and low \\(K_s\\) will respond rapidly to eutrophic conditions. This helps explain why some algae become nuisance or problematic under excessive nutrient conditions—their physiological traits make them well-suited to exploit high nutrient environments.\nUnderstanding the ecological significance of high or low \\(V_{max}\\) and \\(K_s\\) is crucial; it explains the circumstances under which species will thrive and proliferate based on the environmental nutrient regime.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#the-three-phases-of-active-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#the-three-phases-of-active-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "The Three Phases of Active Uptake",
    "text": "The Three Phases of Active Uptake\nActive uptake can be described as having three phases:\n\nThe Surge Phase\nThe Internally Controlled Phase\nThe Externally Controlled Phase\n\nLet’s discuss each in detail.\n\nThe Surge Phase\nThe surge phase is observed at the very beginning of nutrient uptake. Imagine taking a seaweed that had not previously been exposed to nutrients and placing it into fresh seawater or a beaker with abundant nutrients. At the start, the environment is nutrient-rich, but the internal pools within the seaweed cells are nutrient-poor.\nImmediately after exposure, there is a rapid influx of nutrients—nutrients rush into the cellular pools (like vacuoles) which had been depleted. Once the concentrations equalise, the surge phase ends. This rapid initial movement is the surge phase, driven by a steep concentration gradient.\nThe surge phase only occurs at the beginning of exposure to high nutrient concentrations. Once the internal pools are filled, the diffusive flux equalises, and the rapid uptake stops.\n\n\nThe Internally Controlled Phase\nAfter the surge phase, once the internal pools are filled, the rate of nutrient conversion—transforming inorganic nutrients already inside the cell into organic compounds (such as amino acids or other macromolecules)—becomes the limiting step. This is the internally controlled phase.\nHere, the rate of nutrient uptake is governed by enzyme activity, and this phase sets the plateau seen in the uptake curve (\\(V_{max}\\)). The maximum rate is determined by how quickly the enzymes can process nutrients.\n\n\nThe Externally Controlled Phase\nIf the plant remains in the closed environment and continues to take up nutrients, eventually the external nutrient concentration will drop. At some point, the uptake rate is determined by the diffusion of nutrients from the environment to the cell—this is the externally controlled phase.\nAt very low ambient nutrient concentrations, the uptake rate also becomes low, because diffusion is limited. As nutrient concentrations increase, the concentration gradient increases, and so does the diffusive flux. In this region, the rate of nutrient uptake is determined by the difference in concentration across the boundary layer surrounding the organism.\nTwo main factors influence the movement of nutrients across the boundary layer:\n\nThe concentration gradient between the external environment and the cell.\nThe thickness of the boundary layer, which is influenced by water movement; high water movement results in a thin boundary layer and hence faster diffusion, while low water movement causes a thicker boundary layer which slows diffusion.\n\nAll these external physical factors combine to influence the maximum rate of diffusion across the boundary layer.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#surface-area-to-volume-ratio-and-growth-dynamics",
    "href": "BDC223/L08a-nutrient_uptake.html#surface-area-to-volume-ratio-and-growth-dynamics",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Surface Area to Volume Ratio and Growth Dynamics",
    "text": "Surface Area to Volume Ratio and Growth Dynamics\nIn seaweeds with high surface area to volume ratios—flat, membranous, or highly branched filamentous forms—cells are optimised for maximum exchange with the environment, efficient light harvesting, and rapid gas exchange. When these are placed into a nutrient medium, their uptake is extremely rapid because their enzymatic machinery can sustain a high \\(V_{max}\\). Biomass can, under certain conditions, double in just a day or two. For instance, if you have 1 gram of seaweed today, after a day or two you might have 2 grams—achievable because of the high \\(V_{max}\\) and the ample nutrient supply.\nOther seaweeds, at the opposite end of the functional form spectrum with a low surface area to volume ratio, do not respond instantaneously. There is often a lag. Some of these can engage in luxury consumption—taking up more nutrients than needed at that moment, storing them internally (in either organic or inorganic forms) for later use when growth conditions (e.g., light, other nutrients) are optimal. These plants respond more slowly, remobilising stored nutrients once other environmental factors are favourable.\nThis links back to our earlier discussions on surface area to volume ratio and the ecological and physiological implications for nuisance algae under eutrophic conditions.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#factors-influencing-nitrogen-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#factors-influencing-nitrogen-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Factors Influencing Nitrogen Uptake",
    "text": "Factors Influencing Nitrogen Uptake\nNitrogen (and other macronutrient) uptake is influenced by many factors:\n\nExternal Conditions: The thickness of the boundary layer and the actual concentration of nutrients in the water are crucial. These are influenced by physical conditions outside the plant.\nForm of Nutrient: Ammonium is taken up much faster than nitrate; nitrate requires active uptake, while ammonium is acquired via passive diffusion.\nNutrient Starvation History: Plants recently starved of nutrients will uptake rapidly when exposed to fresh supply; non-starved plants may not show a significant response.\nEnvironmental Conditions: High light intensity and high temperatures both enhance metabolism and, consequently, increase \\(V_{max}\\), speeding up uptake rates.\nSurface Area to Volume Ratio: As discussed, this has a substantial effect on uptake dynamics.\nMechanisms of Uptake: The presence of additional mechanisms (such as facilitated diffusion) can also influence overall nutrient uptake.\n\nAll of these together influence the rate at which macronutrients such as nitrogen and phosphorus are taken up from the environment.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#introduction-2",
    "href": "BDC223/L08a-nutrient_uptake.html#introduction-2",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Introduction",
    "text": "Introduction\nRight, so today we shall continue our discussion of nutrient uptake. Last week, we considered nutrient uptake experiments, and I demonstrated how to derive uptake rates from depletion curves—those curves that plot uptake rate versus substrate concentration. When we plotted that relationship, we observed a graph resembling a hyperbolic tangent curve. At low substrate concentrations, the uptake rate increases rapidly, and as concentration rises, this levels off to a plateau. This type of relationship is known as the Michaelis-Menten uptake relationship, and is representative of one of three main uptake mechanisms, namely active uptake.\nOn Thursday, we shall discuss passive transport and facilitated diffusion, which are the two other primary uptake mechanisms. Overall, algae and most other plants exhibit active uptake, passive transport, and facilitated diffusion. For today’s lecture, we are going to focus further on active uptake.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#recap-active-uptake-and-michaelis-menten-kinetics",
    "href": "BDC223/L08a-nutrient_uptake.html#recap-active-uptake-and-michaelis-menten-kinetics",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Recap: Active Uptake and Michaelis-Menten Kinetics",
    "text": "Recap: Active Uptake and Michaelis-Menten Kinetics\nPreviously, after introducing the uptake curve, or the \\(V\\) (uptake rate) versus \\(S\\) (substrate concentration) relationship for active uptake as determined for nitrate, I explained various parameters: \\(V_{max}\\), \\(K_s\\), and \\(\\alpha\\). On this slide, you will find text detailing the ecological significance of \\(V_{max}\\), \\(K_s\\), and \\(\\alpha\\). Since we’ve already discussed this, I’ll not repeat myself here, but let’s delve a bit deeper into active uptake.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#mechanism-of-active-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#mechanism-of-active-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Mechanism of Active Uptake",
    "text": "Mechanism of Active Uptake\nActive uptake is the process that accounts for why, in most plants, the internal concentration of a nutrient is much higher than the external concentration. This occurs because there is a cellular mechanism allowing the movement of nutrients from regions of low concentration (outside the cell) to regions of high concentration (inside the plant or cell), i.e., against the concentration gradient—or more properly, against the electrochemical potential gradient.\nTo quantify, external nutrient concentrations are usually in the micromolar range, while internal concentrations inside the cell are generally in the millimolar range—that is, from \\(\\,\\mu\\mathrm{mol}\\,\\mathrm{L}^{-1}\\) externally to \\(\\,\\mathrm{mmol}\\,\\mathrm{L}^{-1}\\) internally. This large difference suggests that passive diffusion alone is insufficient, as diffusion would only allow movement from high to low concentration. Passive diffusion is mainly responsible for moving nutrients across the boundary layer, but the main uptake into the cell interior is accounted for by active uptake.\nTherefore, moving nutrients from areas of low concentration to high concentration requires expenditure of cellular energy, typically metabolic energy. This process is generally light-dependent, and the primary energy source is ATP. The mechanism involves proton-pumping ATPases which establish a pH gradient between the exterior and interior of the cell. It is this proton (pH) gradient that sets up the electrochemical potential gradient necessary for secondary ion transport—the ‘secondary ion’ in this context is the nutrient being absorbed, for example nitrate in our previous examples.\nThis coupled transport can involve different ions moving in different directions. For instance, hydrogen ions are pumped out of the cell while nutrients are moved in; this is termed anti-porter or counter-transport. Alternatively, if nutrients are transported in the same direction as protons, this is termed symport or co-transport. In algae, the proton pump can be linked to the co-transport of molecules such as sugars and thiourea. There are also processes involving sodium ion pumping, which can similarly facilitate nutrient uptake from the environment into the cell.\nThe essential point to remember from this slide is that cellular energy is crucial for active uptake. ATP drives the proton pump, and the resultant proton gradient (established by ATPases) couples to nutrient uptake, allowing transport into the cell against the gradient.\nWe will not delve deeply into all physiological details here; it is sufficient, for now, to understand the concept as outlined above.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#characteristics-of-active-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#characteristics-of-active-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Characteristics of Active Uptake",
    "text": "Characteristics of Active Uptake\nApart from being energetically demanding, active uptake is also characterised by selectivity for particular ions—not all ions are absorbed via active transport, only some. In our prior example, nitrate is absorbed by active transport, but other ions, such as ammonium, are not.\nAnother key characteristic is saturation of the carrier system: as nutrient concentration increases, there is a stage where the system becomes saturated. This is observed at the plateau—\\(V_{max}\\)—where the uptake rate no longer increases with greater nutrient concentration, as the enzyme-catalysed process is working at maximal capacity.\nThe three primary characteristics of active uptake, therefore, are: - Movement against an electrochemical gradient, - Selectivity for particular ions, - Saturation of the carrier system at high substrate concentrations.\nThese considerations also apply, to some extent, to facilitated uptake—but we shall cover that separately.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#distinguishing-uptake-mechanisms-by-kinetics",
    "href": "BDC223/L08a-nutrient_uptake.html#distinguishing-uptake-mechanisms-by-kinetics",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Distinguishing Uptake Mechanisms by Kinetics",
    "text": "Distinguishing Uptake Mechanisms by Kinetics\nWhen uptake rate (\\(V\\)) is plotted against substrate concentration (\\(S\\)) and the curve shows a steep initial rise followed by a plateau—i.e., a hyperbolic tangent shape—we can infer the process is mediated by either active or facilitated uptake. The maximum uptake rate, \\(V_{max}\\), is controlled by intrinsic factors within the algae, specifically those that govern enzyme function. Thus, environmental factors that influence enzyme activity, such as light intensity and temperature, will affect \\(V_{max}\\).\nPut simply, environments promoting rapid growth—higher temperature, more light—will increase enzyme activities, resulting in a higher \\(V_{max}\\).",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#surface-areavolume-ratio-and-functional-morphology",
    "href": "BDC223/L08a-nutrient_uptake.html#surface-areavolume-ratio-and-functional-morphology",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Surface Area:Volume Ratio and Functional Morphology",
    "text": "Surface Area:Volume Ratio and Functional Morphology\nSuppose we conduct uptake experiments across the six or seven different functional form categories established by Littler and Littler. We would find that \\(V_{max}\\) and \\(K_s\\) vary with surface area:volume ratio. Fast-growing forms—flat membranous or highly filamentous algae—display high \\(V_{max}\\) and tend to have lower \\(K_s\\) values, indicating high affinity for nutrients. These forms, with large surface areas relative to their volume, excel at acquiring nutrients even in low-nutrient settings.\nConversely, algae with compact morphologies and low surface area:volume ratios (e.g., thick or bulky thalli), do not access light or diffuse nutrients as efficiently. They grow slowly and exhibit both low \\(V_{max}\\) and higher \\(K_s\\) values, so their affinity for nutrients is lower. In eutrophic conditions—where nutrients are abundant—such forms do not benefit as much as the high-surface area forms, since their \\(V_{max}\\) is constrained by internal physiological processes, not nutrient availability.\nTherefore, algae with high surface area:volume ratios and high \\(V_{max}\\) can rapidly respond to, and even become nuisances under, eutrophic conditions, while those with lower ratios are less responsive.\nThis illustrates the ecological relevance of \\(V_{max}\\) and \\(K_s\\), allowing us to predict under what conditions various algae will thrive or become problematic, based on their morphology and nutrient uptake physiology.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#the-three-phases-of-nutrient-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#the-three-phases-of-nutrient-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "The Three Phases of Nutrient Uptake",
    "text": "The Three Phases of Nutrient Uptake\nNow, considering active uptake, there are three phases to the process:\n\nThe Surge Phase\nThe Internally Controlled Phase\nThe Externally Controlled Phase\n\nLet’s discuss each in turn.\n\nSurge Phase\nThe surge phase occurs immediately after a previously nutrient-deprived alga is introduced into nutrient-rich medium (for example, moving a seaweed from low-nutrient water into a beaker of enriched seawater). At time \\(t = 0\\), there is ample nutrient outside but little inside the plant’s vacuoles and cellular pools. This difference drives a rapid influx of nutrients—‘surge uptake’—from the environment into these pools until the concentration gradients equilibrate and no further rapid uptake is possible. This phase is therefore very short and occurs at high external nutrient concentration.\nNote: Be mindful that, on the typical uptake curve, time runs in the opposite direction to substrate concentration; do not confuse the two.\n\n\nInternally Controlled Phase\nNext is the internally controlled phase. Once the cellular nutrient pools are filled, the rate of converting these newly imported nutrients (from inorganic to organic forms, such as amino acids and macromolecules) is limited by the enzymatic processing capacity—this is, \\(V_{max}\\). The rate of nutrient utilisation is set by the maximum rate of these metabolic pathways and is intrinsic to the species in question.\n\n\nExternally Controlled Phase\nIf the uptake experiment occurs in a closed system (e.g., seaweed in a beaker), and the algae continue to absorb nutrients, eventually the outer nutrient concentration drops as it is depleted from the water. At very low ambient concentrations, uptake is now determined by the concentration gradient and diffusion across the boundary layer is limiting—the ‘externally controlled phase.’ The steeper the concentration gradient, the greater the influx; as the gradient shallows, this uptake rate diminishes proportionately.\nTwo main factors affect the rate of nutrient movement across the boundary layer: - The concentration gradient between the external and internal environment, - The thickness of the boundary layer, which is itself influenced by water movement: rapid water flow yields a thin boundary layer and increased diffusion, whilst still water results in a thicker boundary layer and reduced diffusion.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#influence-of-morphology-and-environmental-factors",
    "href": "BDC223/L08a-nutrient_uptake.html#influence-of-morphology-and-environmental-factors",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Influence of Morphology and Environmental Factors",
    "text": "Influence of Morphology and Environmental Factors\nCertain seaweeds with high surface area:volume ratios—those with flat, membranous, or highly branched forms—are optimised for rapid nutrient uptake. When placed in nutrient-rich medium, these forms respond almost instantly, with rapid increases in biomass, sometimes doubling mass within a day or two, given high \\(V_{max}\\) and sufficient nutrients.\nOn the other hand, seaweeds with significantly lower surface area:volume ratios respond more slowly. They may exhibit ‘luxury consumption,’ taking up nutrient amounts exceeding immediate growth requirements and storing these for future use when growth conditions permit.\nConsequently, the physiological and morphological traits associated with fast uptake—high surface area:volume ratio, high \\(V_{max}\\), and low \\(K_s\\)—are precisely those that predispose certain species to become nuisance algae under eutrophic conditions.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#factors-affecting-nitrogen-and-other-nutrient-uptake",
    "href": "BDC223/L08a-nutrient_uptake.html#factors-affecting-nitrogen-and-other-nutrient-uptake",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Factors Affecting Nitrogen (and Other Nutrient) Uptake",
    "text": "Factors Affecting Nitrogen (and Other Nutrient) Uptake\nTo summarise, several factors determine nutrient uptake rates:\n\nPhysical environment: The concentration of nutrients in the water and the thickness of the boundary layer, affected by water motion.\nForm of nutrient: For example, ammonium is absorbed much more rapidly than nitrate. Nitrate must be taken up by active transport, whilst ammonium can diffuse passively into the cell.\nNutritional state of the plant: Nutrient-starved plants will take up nutrients rapidly when re-exposed, while replete plants will not show a significant response.\nGrowth environment: Higher light intensities and temperatures increase metabolic rates, raising \\(V_{max}\\) and thus enhancing nutrient uptake.\nMorphology: As discussed, a higher surface area to volume ratio increases both diffusion and uptake capacities.\nUptake mechanism: Combinations of uptake mechanisms (active, passive, facilitated) determine overall absorption rates.\n\nAll of these factors interact to control the rate at which nitrogen, phosphorus, or any other macronutrient can be absorbed from the environment.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#introduction-3",
    "href": "BDC223/L08a-nutrient_uptake.html#introduction-3",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Introduction",
    "text": "Introduction\nGood morning, everyone. Today is our last lecture, so I would just like to wrap up a few more slides. It’s not going to be a very long lecture. What we need to talk about today are the two remaining kinds of uptake mechanisms. We spoke at length about active uptake, which is characterised by the Michaelis-Menten equation, but there are also other kinds of uptake mechanisms, primarily passive uptake and facilitated uptake, and today we’re going to quickly talk about both of those.\nThe reason why we have different kinds of uptake mechanisms is because there are various different forms of nitrogen available in the environment. For some of the more complex molecules, such as nitrate, active uptake is necessary. However, there are more simple molecules also available that make up the total dissolved inorganic nitrogen (DIN) pool, and in this instance, we talk about the molecule ammonium or ammonia.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#passive-uptake-mechanism",
    "href": "BDC223/L08a-nutrient_uptake.html#passive-uptake-mechanism",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Passive Uptake Mechanism",
    "text": "Passive Uptake Mechanism\nRight, so the passive uptake mechanism is one that, when we talk about nitrogen uptake, is going to be mostly applicable to the uptake of ammonium from seawater into the seaweed itself.\nIf you want to know a little bit more about nutrient uptake in seaweeds—and it’s definitely recommended that you do—you can read that paper I wrote about 18 years ago, in 2002, that discusses nutrient uptake. It examines the nutrient uptake of ammonium and nitrate at various rates of external water movement and different temperatures in one particular kind of seaweed. So have a look; it’s going to give you additional information that might make the difference in your exams, enabling you to write an answer worth 100% versus one worth 80%. Every little bit of extra work you do by reading additional papers and so on is going to count in your favour.\nThe uptake of ammonium is established in the same way as nitrate uptake. In other words, we apply either multiple flask experiments or perturbation experiments, we establish a depletion curve, and from the depletion curve, we derive \\(V\\) versus \\(S\\), that is, the uptake kinetics graph. For ammonium, when you plot the uptake kinetics graph, you’ll notice that a straight line best describes the relationship between uptake rate and substrate concentration. Here, we see a nice straight line going through all the points.\nBut in the case below, when we look at the uptake of nitrate, also done by first establishing a depletion curve, and we translate those data into our uptake graph, we see a Michaelis-Menten-type curve is much better fitted. A linear relationship no longer describes the relationship between \\(V\\) and \\(S\\) for nitrate. In passive uptake, when we relate \\(V\\) to \\(S\\), we always find a linear relationship. That’s the primary difference in uptake kinetics between active uptake and passive uptake.\nPassive uptake, in the case of ammonium, is always going to give us, when we relate \\(V\\) to \\(S\\), a linear relationship. This implies that in passive uptake, no expenditure of metabolic energy is necessary, because the entire uptake process can be described by diffusion, and these usually involve the movement of uncharged molecules.\nNitrate is a charged molecule, as it has a negative charge, with extra electrons. Ammonia, on the other hand, is a non-charged, uncharged molecule, as is \\(\\mathrm{CO_2}\\), as is oxygen. So, uncharged molecules usually diffuse from the external environment into the plant, down the concentration gradient—that is, from where there’s plenty of it in the external culture medium, to where there’s less of it inside the plant. So it goes down the concentration gradient.\nIn active uptake, uptake typically goes against the concentration gradient, hence the necessity to use energy to drive that process. Here, the entire thing relies mostly on diffusion; therefore, external factors such as the rate of water movement, which affects the thickness of the boundary layer outside the thallus, are very important in affecting the rate at which uptake can take place.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#uptake-kinetics-and-the-affinity-coefficient",
    "href": "BDC223/L08a-nutrient_uptake.html#uptake-kinetics-and-the-affinity-coefficient",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Uptake Kinetics and the Affinity Coefficient",
    "text": "Uptake Kinetics and the Affinity Coefficient\nSo, when we have a linear relationship—for example, in passive uptake—at no point along our increasing range of external substrate concentrations is there any evidence that the rate of uptake slows down. In active uptake, the rate of uptake reaches a maximum, which is \\(V_{\\text{max}}\\), but in passive uptake, there’s no \\(V_{\\text{max}}\\), because it’s a straight line. If you increase the substrate concentration from \\(40\\) to \\(80\\) to \\(120\\), the line will just continue to go up, which means that the rate of uptake is proportional to the amount of nitrogen present in the external environment.\nA higher external concentration sets up a steeper concentration gradient, and when we have a steeper concentration gradient, the rate of diffusion increases. This is why, in a linear relationship, we cannot, as we do in the case of Michaelis-Menten kinetics, calculate the parameters \\(K_s\\) or \\(V_{\\text{max}}\\), because enzymes at no point, internal to the plant, influence the maximum rate of uptake in these cases. And the \\(K_s\\) relationship—that is, the substrate concentration at which uptake rate is half of \\(V_{\\text{max}}\\)—also cannot be calculated, because in order to calculate \\(K_s\\), we need a \\(V_{\\text{max}}\\).\nHowever, we can calculate a parameter called \\(\\alpha\\), and \\(\\alpha\\), in the case of a linear relationship, is simply the slope of that line. The slope of the line is directly equal to \\(\\alpha\\), so by calculating the slope from a linear regression, we can know what \\(\\alpha\\) is. Alpha has the same meaning as in the case of active uptake: it tells us about the affinity of the plant for a particular nutrient.\nSo, the steeper the \\(\\alpha\\), the more rapidly the uptake rate increases with a change in nutrient concentration. By contrast, with a low \\(\\alpha\\), or a very shallow slope, you need a far greater change in nutrient concentration to produce the same change in uptake rate.\nSeaweeds with a steep \\(\\alpha\\)—that is, a steep curve—are able to take up nutrients efficiently, even when the amount of nutrients in the external environment is low. Seaweeds with a low \\(\\alpha\\)—a shallow slope—will not take up nutrients effectively in low-nutrient environments. So, given a particular low nutrient concentration, if you put a seaweed with a low \\(\\alpha\\) next to one with a high \\(\\alpha\\) in the same water, the one with the high \\(\\alpha\\) will better sustain its nutritional needs and enable continued growth in those conditions.\nThis is a useful way to use knowledge of the steepness of that slope—in other words, the mathematical relationship that relates uptake rate to nutrient concentration in the water. This knowledge tells us about the ecological competitiveness of two different seaweeds with different alphas, in the same nutrient medium. It also tells us something about seaweeds likely to become nuisance species under eutrophic conditions. Those with a high \\(\\alpha\\) can respond rapidly to increased nutrient availability, and may become nuisance species when eutrophication occurs.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#factors-modifying-uptake-rates",
    "href": "BDC223/L08a-nutrient_uptake.html#factors-modifying-uptake-rates",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Factors Modifying Uptake Rates",
    "text": "Factors Modifying Uptake Rates\nThere are various things that can complicate the relationship between \\(V\\) and \\(S\\). For example, seaweeds that have been exposed to high nutrient concentrations over time will show a slower rate of uptake, as their nutritional requirements have already been met. But if you take a seaweed from oligotrophic (low nutrient) conditions and move it into water with more nutrients, it will show a very rapid rate of uptake.\nIn the case of active uptake, this can increase \\(V_{\\text{max}}\\). In passive uptake, this influences the steepness of the line—\\(\\alpha\\). The more deprived a seaweed is of nutrients, the greater the response in uptake when exposed to higher nutrient levels.\nAnother important factor is the light environment. For both linear (passive) and active uptake, a greater light environment—that is, more light—generally means more rapid photosynthesis. Photosynthesis takes up inorganic carbon and converts it into organic forms. In order to produce organic molecules inside the plant, nitrogen is also required to accompany carbon, hydrogen, oxygen, and sometimes phosphorus, in the molecule.\nSo, under higher light, the plant takes up more carbon, which increases demand for nitrogen. Typically, then, plants in high light environments will have a higher nutrient uptake rate than plants in low light.\nRelated to this is photoperiod—that is, the ratio of day to night, or how long light is present. The enzyme nitrate reductase, which converts nitrate into ammonium (before it can be incorporated into amino acids), is closely coupled to the photoperiod. The more light there is, the more active nitrate reductase is, and the faster the rate of nitrate uptake during daylight.\nTemperature is another key factor. As you may recall from discussions of Q\\(_{10}\\), the metabolic rate typically doubles for every \\(10\\ ^\\circ\\mathrm{C}\\) increase in temperature. As metabolic rate increases, more organic carbon can be formed, which requires more nitrogen uptake to support the synthesis of organic molecules. Temperature effects, though, are ion-specific and depend on the species, and will differ for uptake of nitrate or ammonium, for example.\nOther influences include surface area to volume ratio. You need to know, in detail, how the surface area to volume ratio modulates different physiological responses in seaweeds.\nEnvironmental factors such as desiccation, the type, and the concentration of nutrients also play a role. For instance, the type of nutrient—ammonium versus nitrate—dictates uptake mechanism: ammonium shows a linear, passive mechanism, while nitrate shows a Michaelis-Menten active mechanism.\nSome nutrients interact in their uptake. If ammonium and nitrate are both present in culture medium, seaweeds will preferentially take up ammonium; nitrate uptake does not occur until all ammonium has been depleted.\nBiological interactions, such as multiple species of plants or algae growing together, can influence nutrient uptake rates. Additionally, intrinsic adaptive factors, such as the production of hairlike hairs (small protrusions from the algal thallus), can increase surface area to volume ratio and thereby increase nutrient affinity, which is especially useful under low nutrient conditions.\nThe reproductive state matters too; reproductive seaweeds require greater nutrient uptake to sustain gamete and spore production. As a thallus ages and its growth slows, nutrient uptake decreases. Morphological changes—such as in seaweeds exhibiting heteromorphic alternation of generations—also affect nutrient uptake responses. Within the same species, genetic variation can also influence uptake kinetics.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#facilitated-uptake-mechanism",
    "href": "BDC223/L08a-nutrient_uptake.html#facilitated-uptake-mechanism",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Facilitated Uptake Mechanism",
    "text": "Facilitated Uptake Mechanism\nThe third type of uptake, after active and passive, is facilitated uptake. Facilitated uptake resembles passive uptake in that it moves nutrients down a concentration gradient—the external concentration is greater than inside the cell.\nHowever, unlike passive uptake, which relies entirely on diffusion, facilitated uptake uses a particular membrane protein that spans the cell membrane. This protein has an orientation across the membrane; its active site is external to the plant and specific for a particular molecule—say, for instance, sulfate. It binds to the sulfate outside, then flips around and releases it inside the cell.\nIn short, a protein collects something from outside, flips its conformation, and releases the molecule inside the cell—this is facilitated uptake. It is similar to passive uptake in being down the concentration gradient, but it is also similar to active uptake in showing a saturation response. That is, there is a maximum external concentration beyond which the transport protein cannot increase the rate of transport further—there is a \\(V_{\\text{max}}\\). Facilitated uptake is also very specific to particular nutrients, and is susceptible to competitive and non-competitive inhibition. For example, another molecule may compete with the primary substrate (such as sulfate) for the active site, displacing sulfate and preventing its uptake.\nThat’s essentially what facilitated uptake is about, and I will not go any further on that point.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/L08a-nutrient_uptake.html#conclusion",
    "href": "BDC223/L08a-nutrient_uptake.html#conclusion",
    "title": "Lecture 8a: Nutrient Uptake",
    "section": "Conclusion",
    "text": "Conclusion\nIf you need to know more about seaweed nutrient uptake, or nutrient uptake more generally (which can be generalised to plants), do look at the references provided. At the very least, I would like you to read the paper I wrote, as everything I have lectured on around this section is based on those experiments.\nAnd that brings me to the end of this nutrient uptake lecture, and indeed to the end of BDC223 as far as the plant component is concerned.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8a: Nutrient Uptake"
    ]
  },
  {
    "objectID": "BDC223/Lab1_SA_V.html",
    "href": "BDC223/Lab1_SA_V.html",
    "title": "Lab 1: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "",
    "text": "This Lab Accompanies the Following Lecture\n\n\n\n\nSlides: SA:V\nReading: Lecture 2. Surface Area to Volume (SA/V) Ratios in Biology\n\n\n\n\n\n\n\n\n\nDate\n\n\n\n\nLab Date: 16 September 2024 (Monday)\nDue Date: 7:00, 23 September 2024 (Monday)\n\n\n\n\n\n\n\n\n\nReading\n\n\n\n\nBarrett, D. 1983. Body size and temperature: an extended approach. J. Biol. Educ. 71:78.\nCohen, A, AB Moreh, R Chayoth. 1999. Hands-on method for teaching the concept of the ratio between surface area & volume. The American Biology Teacher 61: 691-696.\nDiamond, Jared. 1989. How cats survive falls from New York skyscrapers. Natural History, August, pp 20-26.\nHaldane, J.B.S. 1928. On Being the Right Size.\nStanek, Jr., J. A. (1983) Why don’t cells grow larger? American Biology Teacher 45:393-395.\n\n\n\nStudents will work as individuals; assignments are per individual. This lab is due on Monday 23 September 2024 at 7:00 on iKamva.\n\n1 Pre-Lab\nRead this lab, the associated reading in the box above, and the pertinent material in your text.\n\n\n2 Post-Lab\nUpon completion of this lab:\n\ntranscribe all tables and questions (Exercises A-E) to an electronic document and submit on iKamva. To submit online on Monday 23 September 2024 at 7:00.\n\n\n\n3 Objectives\nUpon completion of these exercises, the student will be able to:\n\ndescribe how organismal surface area and volume act together to influence S/V;\nperform various calculations involving surface are and volume;\nunderstand the relationship between S/V to biological form and ‘function’;\nunderstand how S/V relates to various rate processes in plants.\n\n\n\n4 Background\nThese exercises are designed to introduce you to the concept of surface-to-volume ratios (S/V) and their importance in plant biology. S/V refers to the amount of surface a structure has relative to its volume (bulk). To calculate the S/V, simply divide the surface area by the volume. We will first examine the effect of size, shape, flattening an object, elongating an object on S/V ratios.\n\n\n5 Exercise A: Influence of Size on S/V\nThe purpose of this exercise is to see how the S/V changes as an object gets larger. We will use a cube to serve as a model cell (or organism). Cubes are especially convenient because surface area (length × width × number of sides) and volume (length × width × height) calculations are easy to perform. To calculate the S/V divide the surface area by the volume. Complete the table below for a series of cubes of varying size:\n\nQuestions\n\nWhich cube has the greatest surface area? Volume? S/V?\nWhat happens to the surface area as the cubes get larger? What happens to the volume as the cubes get larger? What happens to the S/V as the cubes get larger?\nProportionately, which grows faster – surface area or volume? Explain.\nWhich cube has the most surface area in proportion to its volume?\nIf you cut a cube in half, how does the volume, surface area and S/V of one of the resultant halves compare to the original?\nAs the linear dimension of the cube triples, the surface area increases by the [square or cube?] of the linear dimension, and the volume increases by the [square or cube?] of the linear dimension.\nPlot the following: S/V vs cube size (length in mm); volume vs cube size (length in mm); and surface area vs cube size (length in mm).\n\n\n\n6 Exercise B: S/V Ratios in Flattened Objects\nIn this exercise we will explore how flattening an object impacts S/V. Consider a cube that is 8 × 8 × 8 mm on a side. Then, imagine that we can flatten the cube making it thinner and thinner (i.e. along one dimension, e.g. height) while maintaining the original volume. Complete the table below:\n\nQuestions\n\nWhat happens to the surface area and S/V as the box is flattened?\nExplain why some leaves are thin and flat (greater S/V). What could be the biological significance of this S/V relationship? Write a short essay to elaborate and include a few examples.\n\n\n\n7 Exercise C: Shape and S/V Ratios\nHere we will explore the impact of shape on surface to volume ratios. The three shapes given below have approximately the same volume. For each, calculate the volume, surface area and S/V and complete the table. The last column in the table, “Volume of environment extending to a distance of 1.0 mm of the object’s surface” is particularly important. Since the materials that an organism exchanges with its environment comes from its immediate surroundings, the greater this volume, the more material that can be exchanged.\n\nQuestions\n\nMake a sketch, to scale, of the three objects.\nWhich shape has the greatest surface area? Volume? S/V?\nIf you had to select a package with the greatest volume and smallest surface area, what shape would it be?\nExplain the implications of the last column in the table.\n\n\n\n8 Exercise D: Shape and S/V Ratios (continue)\nComplete the following tables (4-6), and for the data in each table, produce independent graphs of length (or diameter) vs surface area, length (or diameter) vs volume, surface area vs volume, and length (or diameter) vs S/V (i.e. there would be 12 figures in total).\nExplain the relationships in the table regarding metabolic efficiency, and define what this efficiency might entail and why it is crucial. Name representative groups of organisms that filamentous and spherical morphologies can characterise, and considering the metabolic ‘functioning’ of these groups, explain why their particular shapes matter.\n\n\n\n9 Exercise E: Other Plant Applications\nQuestions\n\nExplain why plants are essentially a cluster of filaments, whereas animals are blobs. In other words, why is a thin, elongated rectangle a good model for a plant, but a sphere a good model for an animal?\nExplain how S/V ratios relate to the form of plants that have evolved in mesic (moderate), xeric (dry) and hydric (aquatic) environments.\nExplain why the cells of the spongy mesophyll layer are irregular in shape whereas those of the palisade layer are more rectangular.\nDescribe the trends that have occurred in S/V during the evolution of plants from single cellular cyanobacteria to multicellular algae to mosses to ferns to angiosperms.\nObtain the leaf of a mesophytic plant. Record the scientific name and family of this species. Calculate the surface area of the leaf (ignore the edges of the leaf). Then, calculate the dimension of a cube that would have the same surface area.\nExplain why cells divide when they get large.\nExplain why the rate of cell growth slows as cells get larger.\nExplain why cats can fall off of tall buildings and survive. Why do people splat?\nDescribe the scientific inaccuracy in the story of Goldilocks and the porridge.\nExplain why lungs, gills and intestines have the shape they do.\nDescribe and explain the shape of a radiator?\nMice have large eyes relative to size, and elephant small ones. Explain why. Are large eyes better than small ones?\nEarth is geologically active (has a molten core; plate tectonics) but the moon is apparently no longer geologically active. Explain why using S/V.\nShrews have a reputation for being ferocious eaters. In other words, they must feed constantly. Explain why.\nWhy are there few small animals in the Arctic?\n\n\n\n10 Useful equations\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.,\n  author = {Smit, A. J.,},\n  title = {Lab 1: {Surface} {Area} to {Volume} {(SA/V)} {Ratios} in\n    {Biology}},\n  url = {http://tangledbank.netlify.app/BDC223/Lab1_SA_V.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. Lab 1: Surface Area to Volume (SA/V) Ratios in Biology. http://tangledbank.netlify.app/BDC223/Lab1_SA_V.html.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lab 1: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/Lab4_nitrogen_uptake.html",
    "href": "BDC223/Lab4_nitrogen_uptake.html",
    "title": "Lab 4: Uptake Kinetics – Michaelis-Menten",
    "section": "",
    "text": "This Lab Accompanies the Following Lecture\n\n\n\n\nSlides: Nutrient Uptake Kinetics\nReading: Lecture 9: Uptake Kinetics – Michaelis-Menten\n\n\n\n\n\n\n\n\n\nAdditional Reading\n\n\n\n\nPaper: Smit (2002)\n\n\n\n\n\n\n\n\n\nData For This Lab\n\n\n\n\nThe nutrient uptake data – BDC223_Lab_5_Rate calculations.xlsx\n\n\n\n\n\n\n\n\n\nDate\n\n\n\n\nLab Date: 7 October 2024 (Monday)\nDue Date: 7:00, 14 October 2024 (Monday)\n\n\n\nStudents will work as individuals; assignments are per individual. This lab is due on Monday 14 October 2024 at 7:00 on iKamva.\n\n1 Pre-Lab\nRead this lab and contextualise within the pertinent material in your text.\n\n\n2 Post-Lab\nUpon completion of this lab:\n\nsubmit online on Monday 14 October 2024 at 7:00.\n\n\n\n3 Task\nIn this practical, we will practice uptake kinetics calculations that plant biologists interested in nutrient uptake might encounter.\nPlease read the pertinent theory material in your text and listen to my recorded lectures. The relevant section dealing with the rate calculations is in the PDF slides from under the heading “Uptake kinetics experiments” to the end of “Michaelis-Menten,” but other material in the slides appears elsewhere in the document.\n\n\n4 Instructions\nYou may complete the assignment in your own time.\nAs part of the results presented in a properly formatted MS Word document, I would like to see:\n\nGraphs illustrating the depletion of nutrients over time.\nAll calculations in the spreadsheet in the columns where ‘???’ is indicated.\n\\(V\\) versus \\([S]\\) plots.\nEstimates for \\(V_{max}\\), \\(K_s\\) and \\(\\alpha\\)\n\nyou can either derive them from the \\(V\\) versus \\([S]\\) plot\nor, for extra credit, apply the Michaelis-Menten equation and provide parameter estimates along with estimates of their errors).\n\nAn abstract that summarises your findings, along with a physiological rationale for these findings.\n\nEnsure that your document is correctly structured (i.e., use headings relevant to the tasks mentioned above and present them in a logical sequence).\nRefer to Formatting requirements for all tasks.pdf for further guidance.FOR YOUR ASSIGNMENT, PLEASE SUBMIT YOUR SPREADSHEET. NAME THE FILE AS FOLLOWS: &lt;YOUR_SURNAME&gt;_UPTAKE_RATES.XLS\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2024,\n  author = {Smit, A. J.,},\n  title = {Lab 4: {Uptake} {Kinetics} -\\/- {Michaelis-Menten}},\n  date = {2024-09-18},\n  url = {http://tangledbank.netlify.app/BDC223/Lab4_nitrogen_uptake.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2024) Lab 4: Uptake Kinetics -- Michaelis-Menten. http://tangledbank.netlify.app/BDC223/Lab4_nitrogen_uptake.html.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lab 4: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html",
    "href": "BDC223/L02-SA_V.html",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "",
    "text": "This Lecture is Accompanied by the Following Lab\n\n\n\n\nLab 1: Surface Area to Volume (SA/V) Ratios in Biology",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#introduction",
    "href": "BDC223/L02-SA_V.html#introduction",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Introduction",
    "text": "Introduction\nSo let’s continue with our lectures today. Now, we’re going to talk a bit about surface area and volume ratio. You will have already encountered some of these calculations in your practical, in the very first lab that you had, so much of it should be quite intuitive to you by now. I think Brian would have also spoken a bit about the constraints imposed on animal behaviour, on its physiology, that stem from the relationship between the ratio of an animal’s surface area to its volume. The same kind of thing, of course, would happen in plants. It’s one of the most fundamental processes that places various different limits on the way in which various physiological rate processes work.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#surface-area-to-volume-ratio-definitions-and-importance",
    "href": "BDC223/L02-SA_V.html#surface-area-to-volume-ratio-definitions-and-importance",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Surface Area to Volume Ratio: Definitions and Importance",
    "text": "Surface Area to Volume Ratio: Definitions and Importance\nSo, when we talk about the surface area to volume ratio, we talk about the flat external surface—the skin, the total volume of skin around a human body, or the two sides of a leaf, which is typically measured in area, in square centimetres. The ratio of that quantity to the volume of something, which is the internal bulk of an organism—everything below your skin, or your meat, bones, and organs, or within, say, the leaf, where you might have one, two, or three layers of cells. So the ratio of these things—the surface area measured in square centimetres to the volume in cubic centimetres—is what’s commonly called the surface area to volume ratio.\nPlants in their day-to-day lives require various things that they need to do. They need to capture photons—they need to harvest light, in other words—so leaves provide a convenient two-dimensional flat surface, of which we can calculate the size, which translates directly to one of the units in the quantum measurement of light intensity, so metres squared. So the surface area of a leaf relates directly to the amount of area available for photon capture. It’s quite easy to derive the total amount of photons falling onto a leaf surface if you know the total area in square centimetres of that leaf.\nPlants must also acquire water and take up nutrients together with that water, and they must distribute all of these materials from the roots via the stems to the leaves. When they photosynthesise, one of the byproducts is oxygen. The oxygen must be released back into the atmosphere, and that’s happening via the leaves again. So, the more surface area there is, the greater the area available for oxygen exchange. Similarly, the same holds true for carbon dioxide—the greater the surface area available within the leaf, the greater the amount of area exposed to the atmosphere by which carbon dioxide can be released back, or taken up from the atmosphere, into the leaves. There are various different metabolic wastes that need to be disposed of, which typically happens in the roots and so on.\nAll of these operations require—or are based upon—various chemical, physical, and biological principles that impose various constraints on the rate at which these processes can operate. All of these functions are also constrained by the various parts of a plant body, the thallus. It helps us to also understand the construction—“construction” in inverted commas, because it hasn’t been constructed by a person, it just evolved. So we need to understand the various components that form the structure of a full plant body that permit these various different operations: CO₂ uptake, waste disposal, water uptake, and all of these.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#plant-structures-and-surface-area-to-volume-relationships",
    "href": "BDC223/L02-SA_V.html#plant-structures-and-surface-area-to-volume-relationships",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Plant Structures and Surface Area to Volume Relationships",
    "text": "Plant Structures and Surface Area to Volume Relationships\nWhen we talk about higher plants, these would typically be the things that we see outside the window. If you walk around in nature, these would be angiosperms and gymnosperms. All of them are comprised of roots, stems, and leaves.\nThe roots, of course, are the bits below the ground surface, which serves several purposes. From an ecophysiological point of view, the most important surface function is that it interacts with the soil, which is where the water is and where the nutrients are, which are taken up by the roots from the soils and transported up the stems to the leaves. Also, roots fulfil an anchorage purpose as well. You can imagine that things with a high surface area to volume ratio—adventitious roots with lots of fine root hairs—have more surface area in contact with the soil particles themselves. They become very effective at taking up water and nutrients from the soil and bringing it into the plant. So that’s a function of a high surface area to volume ratio—the high surface area is associated with direct contact with the soil.\nOn the other hand, as surface area to volume ratio decreases, the amount of bulk increases. That you typically see under the soil surface as tap roots—deep roots that are quite important for anchoring large plants. They can penetrate quite deep into the soil—maybe towards the water table, far down where it can access water. Anchorage structures, root structures that have a low surface area to volume ratio but more bulk relative to the surface, are very good at anchorage, whereas as soon as roots branch out more, reducing the amount of bulk relative to surface area, you have more contact with the soil, and can access nutrients and water.\nThe stems, similarly—in very fine ephemeral plants, would have a high surface area to volume ratio. They tend to be flimsy, not very strongly constructed, but as soon as the trees become larger, taller in stature, the surface area to volume ratio of the trunk decreases. There’s more volume relative to the external skin, the bark, and therefore it becomes far more strong in terms of its ability to sustain all the bulk above ground. So contrast very large, thick tree trunks to the very thin little stems of ephemeral plants—very different in terms of surface area and volume ratio, and also in terms of the structural strength of these various plant components.\nLeaves, of course, are just flat surfaces, very strongly packed with chlorophyll a. Some leaves might become increasingly bulky, with more internal volume relative to the amount of surface area, and so they’re able to store more water, becoming more adapted to drier climates. More ephemeral plants—things that have to grow a lot faster, like lettuces for instance—are very flimsy, and as soon as you leave them out in the sun, they wilt very quickly. That’s a function of a high surface area to volume ratio. Things like a cactus, for instance, have a huge amount of bulk; you leave it in the sun, and it sits there, fine, for weeks and months. That’s because there’s very little surface area relative to the amount of bulk through which water loss can take place.\nSo, it’s quite easy to understand how the surface area to volume ratio constraint imposes various different adaptive or evolutionary benefits to plants to survive under certain conditions and environments. It’s very easy to see when you walk around in nature, moving from forested areas at high latitudes to the tropics at low latitudes—the plants become very different in appearance and form, going from wet mesic areas to dry environments, to desert kinds of environments. The reason they look different is because there are major changes in the ratio of surface area relative to the volume of these plants.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#surface-area-to-volume-ratio-in-seaweeds-macroalgae",
    "href": "BDC223/L02-SA_V.html#surface-area-to-volume-ratio-in-seaweeds-macroalgae",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Surface Area to Volume Ratio in Seaweeds (Macroalgae)",
    "text": "Surface Area to Volume Ratio in Seaweeds (Macroalgae)\nPlant structure concepts can also be translated to seaweeds—algae that live in the ocean or in fresh water. They have similar structures, though called something else. In seaweeds, you typically have not roots but a holdfast which is just like a hand—sometimes called a hapteron (plural haptera)—looks like a hand that grabs onto something for anchorage. The only purpose of a hapteron is to anchor a seaweed onto the ground, and larger seaweeds have larger haptera in order to grab onto more surface area and more rock, so that they can anchor better, especially in wavy environments.\nThere is also a stipe, which is equivalent in terrestrial plants to the stem, but in seaweeds is called the stipe. Then there are the fronds, which are equivalent in function to the leaves in higher plants. We see various surface area to volume ratio variations across seaweed types, as outlined in one of the papers you’re meant to read.\nTogether, the holdfast, the stipes and the fronds are called the thallus (plural thalli).",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#littler-et-al.-functional-form-groups-in-macroalgae-reference-to-slidepaper",
    "href": "BDC223/L02-SA_V.html#littler-et-al.-functional-form-groups-in-macroalgae-reference-to-slidepaper",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Littler et al.: Functional Form Groups in Macroalgae (Reference to Slide/Paper)",
    "text": "Littler et al.: Functional Form Groups in Macroalgae (Reference to Slide/Paper)\nThis is the important paper that you need to read. It was published originally in the late 1970s by Mark and Diane Littler, but the one I want you to read for this lecture is on eConver—go and download it. It’s called “Primary Productivity of Marine Macroalgae or Marine Macroalgal Functional Form Groups from Southwestern North America.” What Mark and Diane Littler did, and Keith Arnold in later collaborations, was to look at functional forms of seaweeds, dividing them into different groups, each characterised by different surface area to volume ratios. The experiments clearly show that as surface area and volume ratio changes, various ecophysiological functions of the algae change.\nThere were about six groups of functional forms. At one extreme are very thin, sheet-like forms—think of a lettuce, like cos lettuce. In fact, one seaweed they studied is called sea lettuce—Ulva is the common name because it has a similar appearance. As the seaweeds become increasingly complex, like filamentous or coarsely branched groups, the internal bulk increases and the surface area relative to bulk decreases. So, going from sheet-like at the top of the table to crustose at the bottom, you see a decrease in surface area to volume ratio as complexity increases.\nWe can look at some examples. Sea lettuce (Ulva) looks very similar to lettuce, and Porphyra umbilicalis—Porphyra is the seaweed used to make nori for sushi, commonly seen in Asian foods [note: Porphyra is correct, but in South Africa, it is not native; it’s mainly an import] [attention]. Ulva and Porphyra are examples of the thin tubular and sheet-like group—characterised by high surface area to volume ratios. There’s much more surface compared to the bulk inside.\nIf you look at the measurement of photosynthesis—the rate of carbon fixation, so milligrams of carbon per gram dry mass per hour—you’ll see that the values for these groups extend to quite high ranges, with high averages around 5 or 6 mg C/g dry mass/hour. For more complex forms—delicately branched, coarsely branched—the average value is lower, and as you move to even more complex functional forms, the photosynthetic efficiency decreases. This is because surface area to volume ratio declines—less surface area is available to capture carbon and exchange with the environment, thus constraining the rate of photosynthesis.\nSo, as surface area to volume ratio decreases, the ability of the plant to harvest enough carbon for fast growth rates also decreases. In simple seaweeds, plenty of surface area means efficient carbon access for the one or two cell layers, but in complex forms, there’s less exchange and more constraints.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#broader-consequences-and-applications",
    "href": "BDC223/L02-SA_V.html#broader-consequences-and-applications",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Broader Consequences and Applications",
    "text": "Broader Consequences and Applications\nThis constraint acts not only on carbon dioxide uptake and photosynthesis, but also on oxygen release, water and nutrient uptake, metabolic wastes disposal, and so on. The same principle applies to terrestrial plants.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#surface-area-to-volume-ratio-in-seagrasses",
    "href": "BDC223/L02-SA_V.html#surface-area-to-volume-ratio-in-seagrasses",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Surface Area to Volume Ratio in Seagrasses",
    "text": "Surface Area to Volume Ratio in Seagrasses\nNow, let’s extend this analysis to seagrasses. Seagrasses are, of course, angiosperms. They are different from true grasses but have evolved from land plants to reoccupy marine spaces. If you dive in seagrass meadows, for instance in Australia where these are abundant, it looks like a lawn of grass underwater. These meadows are often dominated by one or a few seagrass species.\nSeagrasses display a spectrum from very simple to very complex. On the left, we have fast-growing halophila (high surface area to volume ratio); on the right, we have larger, more complex forms like Thalassia and Posidonia (low surface area to volume ratio). As you move from simple to complex, different evolutionary and ecological outcomes appear.\n\nOn the left (high surface area to volume ratio), seagrasses are ephemeral, growing quickly during favourable seasons, but they are fragile and highly accessible to grazing. That means that almost all the material is consumed quickly, with little left over as detritus, and they don’t stick around long enough for epiphytes to colonise. The consequence is a rapid turnover and open nutrient cycling; any unfavourable environmental change has rapid impacts on these plants.\nOn the right (low surface area to volume ratio), species like Posidonia and Thalassia are persistent, long-lived, often surviving for many decades. They invest more energy into below-ground rhizomes for persistence, less into seeds. Nutrient and carbon turnover is slow—nutrients taken up are stored and remobilised when needed— and so they show a closed nutrient cycling strategy. They are resilient to many perturbations, but, if damaged, recover slowly.\n\nIn terms of ecological interactions, simple, ephemeral seagrasses are readily grazed; complex, persistent ones are tougher, less palatable. In complex forms, large amounts of detrital material can accumulate, resisting decomposition for long periods, and their persistence creates stable habitats, allowing rich communities of epiphytes, plants, and animals. The longer the structural tissues, such as rhizomes, remain, the more they can accumulate attached organisms.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L02-SA_V.html#application-and-closing-remarks",
    "href": "BDC223/L02-SA_V.html#application-and-closing-remarks",
    "title": "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology",
    "section": "Application and Closing Remarks",
    "text": "Application and Closing Remarks\nSurface area to volume ratio, then, has major consequences for the ecophysiology, distribution, and ecological interactions of plants and algae. Whether you look at terrestrial or marine environments, you’ll see similar patterns: ephemeral, R-selected versus perennial, K-selected strategies. When you walk through nature, look for these surface area to volume ratio reasons for why different plants occupy particular habitats—often, the explanation lies in this fundamental geometric constraint.\nThere are some readings on ICOMVA for you, including some papers on surface area and volume ratios, as well as background on what seagrasses are. Remember, seagrasses are not true grasses; they look like grasses but represent a group of plants that have evolved on land and then returned to the sea—a separate evolutionary trajectory from seaweeds, which have always existed in the oceans. [Slide reference: left-hand side—Halophila as high surface area to volume, right-hand side—Posidonia sinuosa as low surface area to volume ratio. Note the presence of epiphytes on the latter.]\nSo, review this material, understand the consequences of surface area to volume differences, and their impacts across the spectrum of plant types and ecological circumstances. Be able to explain the consequences for distribution, ecophysiology, and ecological interactions.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 2: Surface Area to Volume (SA/V) Ratios in Biology"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html",
    "href": "BDC223/L06a-pigments_photosynthesis.html",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "Good morning everyone, welcome back to BDC 223. Today we’re going to follow on from our lectures on light, and we’re going to be talking about pigments and photosynthesis. Much of today’s discussion, and also the next couple of lectures about pigments as well as chromatic adaptation, is based on two papers I need you to read. The papers are both on eComva; you can find them there. Please read them while working through these lectures—it’s quite important that you understand their content. Everything I’m going to talk about today will be explained in a lot greater detail in those two papers.\n\n\n\nSo, in order to exploit the light available in the environment, plants and algae—indeed, all photo-oxygenic organisms—rely on a range of pigments that extract energy from light and convert it into chemical potential energy in the process of photosynthesis.\nThe predominant pigment in all photo-oxygenic production on Earth, and in all plants, algae, and cyanobacteria, is a molecule called chlorophyll-a. The chlorophyll-a pigment takes light energy and converts it into chemical energy. It is the only pigment that plays such a central role in photosynthesis. There are many other pigments called accessory pigments; they do not directly drive photosynthesis but support light harvesting.\n\n\n\nChlorophyll-a absorbs light mainly in the blue and red regions. In the previous lecture you saw that visible light falls between roughly \\(390\\) nm to around \\(760\\) nm. That’s the range of photosynthetically active radiation. However, within that range, not all light is equally effective at driving photosynthesis. This is because chlorophyll-a can maximally absorb light at \\(440\\) nm (blue light) and \\(675\\) nm (red light).\nRegardless of where these primary producers are—on land, in water, or elsewhere—they are sensitive to blue and red light. If they do not have sufficient light at precisely those wavelengths, their rate of photosynthesis will be impaired.\nHere are some graphs (Slide reference) that show the absorption for chlorophyll-a and chlorophyll-b. Chlorophyll-a, shown as the red line, has two main peaks: one around \\(425\\) nm (blue) and one in the red region around \\(660\\) to \\(675\\) nm. Chlorophyll-b has similar peaks but they are shifted: the blue peak sits nearer to \\(460\\) nm, closer to green, and the red peak falls slightly toward the orange region.\nChlorophyll-b does not drive photosynthesis directly, but it can harvest light and pass that energy to chlorophyll-a, thus broadening the range of light absorbed and utilised for photosynthesis. You’ll notice that, in the middle of these spectra, there is a gap—a region where light is available yet not absorbed by chlorophyll-a or b. This is often referred to as the “green gap” and is the reason why plants appear green: green light is not absorbed by the major photosynthetic pigments in most leaves, so it is reflected back into the environment and to our eyes.\n\n\n\nPlants have evolved various pigments to fill that green gap. Among the most notable of these are carotenoids, which include beta-carotene, and the phycobilins, such as phycoerythrin and phycocyanin. Carotenoids are also the pigments responsible for the orange colour in carrots, as indicated by the orange line on many absorption spectra.\nThe carotenoids and phycobilins absorb light in the green gap and pass that energy on to chlorophyll-a, enabling photosynthesis that would otherwise not occur at those wavelengths. These are called accessory pigments because they complement the absorption range of chlorophyll-a and make photosynthesis more effective in sub-optimal light conditions.\nAt first glance, the diversity of accessory pigments appears as vast as the diversity of light climates in the ocean, on land, and in freshwater. However, later experiments—especially those by Engelman, Haxo, and Blinks (to be discussed in your papers)—demonstrate that the diversity of accessory pigments does not necessarily correspond to the diversity of environmental light conditions.\n\n\n\nThere are three main pigment classes:\n\nChlorophylls – The major photosynthetic pigments. Chlorophyll-a is primary, with chlorophylls-b and -c acting as accessory pigments that transfer absorbed energy to chlorophyll-a.\nCarotenoids – Includes beta-carotene and xantho-phylls, which also serve as accessory pigments.\nPhycobilins – Reddish or purplish pigments, including phycocyanin and phycoerythrin, mainly found in certain algae and cyanobacteria.\n\nAcross all photoautotrophs, there are more than forty pigments involved. They bind differently to the proteins making up the photosynthetic machinery, expanding the plant’s ability to absorb different wavelengths, especially in the green gap, and maintain high photosynthetic efficiency in a range of environments.\nEspecially in algae, the types of pigments present can indicate taxonomic relationships and phylogenetic heritage. By extracting pigments from a seawater sample, for example, one can deduce the classes of algae present. Similar underpinnings occur in terrestrial plants, with certain pigments associated with specific plant types.\n\n\n\nPhotosynthesis is the conversion of light energy—radiant energy—into chemical potential energy. It drives carbon fixation: uptake of \\(\\mathrm{CO}_2\\) from the environment, splitting water, and releasing oxygen as a byproduct. The reactions occur in the photosystems I and II.\nAs a function of light intensity, photosynthesis responds with an increased rate—to a point. This relationship is described by the photosynthesis-irradiance (PI) curve.\n\n\n\nThe PI curve (Slide reference):\n\nY-Axis: Rate of photosynthesis, measured by carbon incorporation (e.g., mg C m\\(^{-2}\\) s\\(^{-1}\\), mg C m\\(^{-2}\\) hr\\(^{-1}\\), or mg C m\\(^{-2}\\) day\\(^{-1}\\)).\nX-Axis: Irradiance (light intensity).\n\nInitially, the PI curve is linear: as irradiance increases, photosynthesis increases at a rate defined by the slope \\(\\alpha\\) (alpha). Alpha reflects the plant’s sensitivity to changes in irradiance. A steep alpha (steep slope) means more sensitivity to small changes in light; a shallow slope indicates less sensitivity and a need for greater changes in light intensity to affect photosynthesis rate.\nAt a certain point, the rate reaches saturation, denoted as \\(I_k\\). This occurs where the extrapolated horizontal maximum rate (\\(P_{max}\\)) intersects with the linear part of the curve. Beyond this, increasing light does not increase photosynthetic rate since all the photosynthetic machinery is working at full capacity—much like pressing a car accelerator to the floor when the engine cannot go faster.\nShould irradiance keep increasing, photosynthesis can decline—a phenomenon called photoinhibition. Here, excessive light can cause actual damage, or trigger protective mechanisms within the photosynthetic apparatus to prevent damage, analogous to running a car engine past its operating limits.\nRespiration occurs at all times, consuming oxygen, while photosynthesis (in the presence of light) produces it. At low light, the rate of oxygen production by photosynthesis is less than the rate of consumption by respiration, resulting in net negative oxygen production. The light compensation point is the irradiance where net oxygen production is zero.\nNet photosynthesis: Above the compensation point—positive net oxygen evolution.\nGross photosynthesis: Total oxygen produced, regardless of respiration.\nUnderstanding these parameters—the light compensation point, \\(\\alpha\\), \\(P_{max}\\), \\(I_k\\), etc.—is crucial. We’ll revisit this concept in practical sessions where you will fit models to real data.\n\n\n\n\\(P_{max}\\) represents the maximum photosynthetic capacity, which is influenced by many stresses, including thermal stress, nutrient stress, and light stress. Any of these can reduce a plant’s capacity to sustain \\(P_{max}\\), and a reduction in this parameter is often the first sign of environmental stress. Measuring these rates gives insights into how and when plants become stressed.\nHere are some indicative values (Slide reference):\n\nIntertidal environments: light saturation might occur at \\(400\\)–\\(600\\;\\mu\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}\\)\nSublittoral species (deeper): saturated at \\(150\\)–\\(250\\;\\mu\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}\\)\nAs depth increases, saturation occurs at progressively lower irradiances.\nSome deep-water plants can become photoinhibited at what would seem to us like relatively dim light.\n\nThese values—\\(I_C\\), \\(I_K\\), \\(P_{max}\\), \\(\\alpha\\)—vary among species, being determined both by environmental adaptation and genetic heritage, and thus serve as good indicators of a plant’s typical habitat and stress response.\n\n\n\nBefore moving on, it’s critical to distinguish the absorption spectrum from the action spectrum.\n\nAbsorption spectrum: Measures the amount of light absorbed by all pigments at every wavelength—essentially, how much light is not reflected or transmitted.\nAction spectrum: For each wavelength, measures the biological effect—oxygen evolution rate—that results from absorption.\n\nTypically, the action and absorption spectra match well, but not perfectly. For instance, between about \\(450\\) and \\(500\\) nm, there is a mismatch. This occurs because, beyond the optimal absorption peak of chlorophyll-a, carotenoids start absorbing light. While they can capture light within this region, they are less efficient at passing the energy to chlorophyll-a, resulting in a lower action than absorption value.\nThis demonstrates that the ability of accessory pigments to pass energy to chlorophyll-a is not perfectly efficient; some energy is lost in the process. Nonetheless, the presence of carotenoids extends the range in which photosynthesis can be driven by chlorophyll-a.\nIn summary, accessory pigments are essential in harvesting a broader range of light and making photosynthesis effective under varied light environments, even if energy transfer from accessory to primary pigments is not perfectly efficient.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#introduction",
    "href": "BDC223/L06a-pigments_photosynthesis.html#introduction",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "Good morning everyone, welcome back to BDC 223. Today we’re going to follow on from our lectures on light, and we’re going to be talking about pigments and photosynthesis. Much of today’s discussion, and also the next couple of lectures about pigments as well as chromatic adaptation, is based on two papers I need you to read. The papers are both on eComva; you can find them there. Please read them while working through these lectures—it’s quite important that you understand their content. Everything I’m going to talk about today will be explained in a lot greater detail in those two papers.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#overview-of-pigments-in-photosynthetic-organisms",
    "href": "BDC223/L06a-pigments_photosynthesis.html#overview-of-pigments-in-photosynthetic-organisms",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "So, in order to exploit the light available in the environment, plants and algae—indeed, all photo-oxygenic organisms—rely on a range of pigments that extract energy from light and convert it into chemical potential energy in the process of photosynthesis.\nThe predominant pigment in all photo-oxygenic production on Earth, and in all plants, algae, and cyanobacteria, is a molecule called chlorophyll-a. The chlorophyll-a pigment takes light energy and converts it into chemical energy. It is the only pigment that plays such a central role in photosynthesis. There are many other pigments called accessory pigments; they do not directly drive photosynthesis but support light harvesting.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#absorption-properties-of-chlorophyll-a-and-accessory-pigments",
    "href": "BDC223/L06a-pigments_photosynthesis.html#absorption-properties-of-chlorophyll-a-and-accessory-pigments",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "Chlorophyll-a absorbs light mainly in the blue and red regions. In the previous lecture you saw that visible light falls between roughly \\(390\\) nm to around \\(760\\) nm. That’s the range of photosynthetically active radiation. However, within that range, not all light is equally effective at driving photosynthesis. This is because chlorophyll-a can maximally absorb light at \\(440\\) nm (blue light) and \\(675\\) nm (red light).\nRegardless of where these primary producers are—on land, in water, or elsewhere—they are sensitive to blue and red light. If they do not have sufficient light at precisely those wavelengths, their rate of photosynthesis will be impaired.\nHere are some graphs (Slide reference) that show the absorption for chlorophyll-a and chlorophyll-b. Chlorophyll-a, shown as the red line, has two main peaks: one around \\(425\\) nm (blue) and one in the red region around \\(660\\) to \\(675\\) nm. Chlorophyll-b has similar peaks but they are shifted: the blue peak sits nearer to \\(460\\) nm, closer to green, and the red peak falls slightly toward the orange region.\nChlorophyll-b does not drive photosynthesis directly, but it can harvest light and pass that energy to chlorophyll-a, thus broadening the range of light absorbed and utilised for photosynthesis. You’ll notice that, in the middle of these spectra, there is a gap—a region where light is available yet not absorbed by chlorophyll-a or b. This is often referred to as the “green gap” and is the reason why plants appear green: green light is not absorbed by the major photosynthetic pigments in most leaves, so it is reflected back into the environment and to our eyes.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#the-green-gap-and-accessory-pigments",
    "href": "BDC223/L06a-pigments_photosynthesis.html#the-green-gap-and-accessory-pigments",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "Plants have evolved various pigments to fill that green gap. Among the most notable of these are carotenoids, which include beta-carotene, and the phycobilins, such as phycoerythrin and phycocyanin. Carotenoids are also the pigments responsible for the orange colour in carrots, as indicated by the orange line on many absorption spectra.\nThe carotenoids and phycobilins absorb light in the green gap and pass that energy on to chlorophyll-a, enabling photosynthesis that would otherwise not occur at those wavelengths. These are called accessory pigments because they complement the absorption range of chlorophyll-a and make photosynthesis more effective in sub-optimal light conditions.\nAt first glance, the diversity of accessory pigments appears as vast as the diversity of light climates in the ocean, on land, and in freshwater. However, later experiments—especially those by Engelman, Haxo, and Blinks (to be discussed in your papers)—demonstrate that the diversity of accessory pigments does not necessarily correspond to the diversity of environmental light conditions.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#classification-and-function-of-pigments",
    "href": "BDC223/L06a-pigments_photosynthesis.html#classification-and-function-of-pigments",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "There are three main pigment classes:\n\nChlorophylls – The major photosynthetic pigments. Chlorophyll-a is primary, with chlorophylls-b and -c acting as accessory pigments that transfer absorbed energy to chlorophyll-a.\nCarotenoids – Includes beta-carotene and xantho-phylls, which also serve as accessory pigments.\nPhycobilins – Reddish or purplish pigments, including phycocyanin and phycoerythrin, mainly found in certain algae and cyanobacteria.\n\nAcross all photoautotrophs, there are more than forty pigments involved. They bind differently to the proteins making up the photosynthetic machinery, expanding the plant’s ability to absorb different wavelengths, especially in the green gap, and maintain high photosynthetic efficiency in a range of environments.\nEspecially in algae, the types of pigments present can indicate taxonomic relationships and phylogenetic heritage. By extracting pigments from a seawater sample, for example, one can deduce the classes of algae present. Similar underpinnings occur in terrestrial plants, with certain pigments associated with specific plant types.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#what-is-photosynthesis",
    "href": "BDC223/L06a-pigments_photosynthesis.html#what-is-photosynthesis",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "Photosynthesis is the conversion of light energy—radiant energy—into chemical potential energy. It drives carbon fixation: uptake of \\(\\mathrm{CO}_2\\) from the environment, splitting water, and releasing oxygen as a byproduct. The reactions occur in the photosystems I and II.\nAs a function of light intensity, photosynthesis responds with an increased rate—to a point. This relationship is described by the photosynthesis-irradiance (PI) curve.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#the-photosynthesis-irradiance-pi-curve",
    "href": "BDC223/L06a-pigments_photosynthesis.html#the-photosynthesis-irradiance-pi-curve",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "The PI curve (Slide reference):\n\nY-Axis: Rate of photosynthesis, measured by carbon incorporation (e.g., mg C m\\(^{-2}\\) s\\(^{-1}\\), mg C m\\(^{-2}\\) hr\\(^{-1}\\), or mg C m\\(^{-2}\\) day\\(^{-1}\\)).\nX-Axis: Irradiance (light intensity).\n\nInitially, the PI curve is linear: as irradiance increases, photosynthesis increases at a rate defined by the slope \\(\\alpha\\) (alpha). Alpha reflects the plant’s sensitivity to changes in irradiance. A steep alpha (steep slope) means more sensitivity to small changes in light; a shallow slope indicates less sensitivity and a need for greater changes in light intensity to affect photosynthesis rate.\nAt a certain point, the rate reaches saturation, denoted as \\(I_k\\). This occurs where the extrapolated horizontal maximum rate (\\(P_{max}\\)) intersects with the linear part of the curve. Beyond this, increasing light does not increase photosynthetic rate since all the photosynthetic machinery is working at full capacity—much like pressing a car accelerator to the floor when the engine cannot go faster.\nShould irradiance keep increasing, photosynthesis can decline—a phenomenon called photoinhibition. Here, excessive light can cause actual damage, or trigger protective mechanisms within the photosynthetic apparatus to prevent damage, analogous to running a car engine past its operating limits.\nRespiration occurs at all times, consuming oxygen, while photosynthesis (in the presence of light) produces it. At low light, the rate of oxygen production by photosynthesis is less than the rate of consumption by respiration, resulting in net negative oxygen production. The light compensation point is the irradiance where net oxygen production is zero.\nNet photosynthesis: Above the compensation point—positive net oxygen evolution.\nGross photosynthesis: Total oxygen produced, regardless of respiration.\nUnderstanding these parameters—the light compensation point, \\(\\alpha\\), \\(P_{max}\\), \\(I_k\\), etc.—is crucial. We’ll revisit this concept in practical sessions where you will fit models to real data.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#effects-of-environmental-stress",
    "href": "BDC223/L06a-pigments_photosynthesis.html#effects-of-environmental-stress",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "\\(P_{max}\\) represents the maximum photosynthetic capacity, which is influenced by many stresses, including thermal stress, nutrient stress, and light stress. Any of these can reduce a plant’s capacity to sustain \\(P_{max}\\), and a reduction in this parameter is often the first sign of environmental stress. Measuring these rates gives insights into how and when plants become stressed.\nHere are some indicative values (Slide reference):\n\nIntertidal environments: light saturation might occur at \\(400\\)–\\(600\\;\\mu\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}\\)\nSublittoral species (deeper): saturated at \\(150\\)–\\(250\\;\\mu\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}\\)\nAs depth increases, saturation occurs at progressively lower irradiances.\nSome deep-water plants can become photoinhibited at what would seem to us like relatively dim light.\n\nThese values—\\(I_C\\), \\(I_K\\), \\(P_{max}\\), \\(\\alpha\\)—vary among species, being determined both by environmental adaptation and genetic heritage, and thus serve as good indicators of a plant’s typical habitat and stress response.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/L06a-pigments_photosynthesis.html#absorption-spectrum-vs-action-spectrum",
    "href": "BDC223/L06a-pigments_photosynthesis.html#absorption-spectrum-vs-action-spectrum",
    "title": "Lecture 6a: Pigments and Photosynthesis",
    "section": "",
    "text": "Before moving on, it’s critical to distinguish the absorption spectrum from the action spectrum.\n\nAbsorption spectrum: Measures the amount of light absorbed by all pigments at every wavelength—essentially, how much light is not reflected or transmitted.\nAction spectrum: For each wavelength, measures the biological effect—oxygen evolution rate—that results from absorption.\n\nTypically, the action and absorption spectra match well, but not perfectly. For instance, between about \\(450\\) and \\(500\\) nm, there is a mismatch. This occurs because, beyond the optimal absorption peak of chlorophyll-a, carotenoids start absorbing light. While they can capture light within this region, they are less efficient at passing the energy to chlorophyll-a, resulting in a lower action than absorption value.\nThis demonstrates that the ability of accessory pigments to pass energy to chlorophyll-a is not perfectly efficient; some energy is lost in the process. Nonetheless, the presence of carotenoids extends the range in which photosynthesis can be driven by chlorophyll-a.\nIn summary, accessory pigments are essential in harvesting a broader range of light and making photosynthesis effective under varied light environments, even if energy transfer from accessory to primary pigments is not perfectly efficient.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6a: Pigments and Photosynthesis"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html",
    "href": "BDC223/BDC223_FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "This question was asked by 3rd years, but it is relevant here too.\nI just need some advice from sir. I’m starting to study today and I just wanted to know what do you think is the best way to study for sir’s exam. I’m asking because I studied really hard for the midterm tests but I still got low marks lol. So I just wanted to know if there is maybe a different approach that I can take.\n\nI was trying to memorise things for the midterm tests but I know sir said we must focus on understanding but when I focus on understanding then the work does not stick in my head😂.\n\nSo far I’ve just read through the articles but is there any advice that sir can give me please?\n\n\nThank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2).\nStructure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the things most important things (attributes) which informs who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer",
    "href": "BDC223/BDC223_FAQ.html#answer",
    "title": "FAQ",
    "section": "",
    "text": "Thank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2).\nStructure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the things most important things (attributes) which informs who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-1",
    "href": "BDC223/BDC223_FAQ.html#answer-1",
    "title": "FAQ",
    "section": "2.1 Answer",
    "text": "2.1 Answer\nClimatic envelopes are the suite of environmental conditions required for plant (or animal) growth that define the optimal niche area and hence the organism’s distribution.\nOne can model the future climatic envelopes using various statistical approaches, and hence so project the future distribution of the species (or ecosystems) whose distribution are linked to those envelopes. Such models are called bioclimatic models or niche models.\nThe process is called species distribution modelling. We will do this in Hons.\nEnough? The first little para I wrote is the definition and all you would put down if I asked.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-2",
    "href": "BDC223/BDC223_FAQ.html#answer-2",
    "title": "FAQ",
    "section": "3.1 Answer",
    "text": "3.1 Answer\nYes. But there’s only a certain range of env conditions plants can acclimatise to, and exceeding those limits will still cause stress.\nAcclimatisation can happen over minutes to hours to days. Or seasonally. But if env conditions exceed the normal range of variability they’ll become stressed.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-the-organic-foods-essay-topic",
    "href": "BDC223/BDC223_FAQ.html#question-about-the-organic-foods-essay-topic",
    "title": "FAQ",
    "section": "4.1 Question About the Organic Foods Essay Topic",
    "text": "4.1 Question About the Organic Foods Essay Topic\nI chose the organic food topic. My question is if I should find research papers for everything I state?\nE.g “Organic food has been a growing interest as people have become more concerned about their diet and what they chose to consume.”\nDo I need to search an article to support that or can I leave it as is since it’s something I’ve recently seen with friends, family and on social media platforms (how organic food is the “right food” to consume).\n\n4.1.1 Answer\nI think it’s commonly knowledge based on lived experience that organic foods have become more widely consumed. So no need to ref that. But the claims that people make about why organic foods are ‘better’ often do not have factual support. So, if you state that it’s better for whatever reason, that needs factual support. If no support is available, your conclusion would have to be that the claim is dogma, i.e., untested, unsubstantiated, wishful thinking, etc.\nScientific studies need to be done in order to prove some hypothesis. Without it the claim remains unsubstantiated despite how many people buy into the claim. Simply because 10 million people think it is good does not actually provide any evidence that the claim is fact.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-pigments",
    "href": "BDC223/BDC223_FAQ.html#question-about-pigments",
    "title": "FAQ",
    "section": "5.1 Question About Pigments",
    "text": "5.1 Question About Pigments\nGood day sir, I have a question about accessory pigments. I know they help pass light onto chlorophyll-a for photosynthesis right? And different chlorophylls, especially chlorophyll-a bind to proteins in different ways. Is that in order to absorb more more that the chrolophyll pigment itself wouldn’t be able to absorb?\n\n5.1.1 Answer\n“different chlorophylls, especially chlorophyll-a bind to proteins in different ways. Is that in order to absorb more more that the chlorophyll pigment itself wouldn’t be able to absorb” — No. If one would have to design something, then that would be the approach. But these molecules were not designed. They evolved. Evolution does not work by something functioning in a specific way in order for some other thing to do what it does. The specific protein binding between the pigments and proteins happened because, by chance, some configuration arose that happened to fill some need, that is, to fill the green gap. It happened by chance, not design.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-eutrophication",
    "href": "BDC223/BDC223_FAQ.html#question-about-eutrophication",
    "title": "FAQ",
    "section": "6.1 Question About Eutrophication",
    "text": "6.1 Question About Eutrophication\nGood day Professor, I was wondering if sir could clarify something. Is an anoxic water where there is no dissolve oxygen? And is that caused by oxygen-using bacteria that decompose dead organisms in eutrophic environments?\n\n6.1.1 Answer\nNot no oxygen. But very little. Usually anoxia is reached at O2 concentrations below 2mg/L. Before that low level it’s called hypoxia.\nYes. It is caused by bacterial respiration. Hypoxia/anoxia causes even more species to die, and further reduces O2 concentrations.\nEutrophic conditions can cause biomass accumulation of photoautotrophs. During night extremely dense biomass of such accumulations don’t photosynthesise but continue to respire. This is when low O2 first starts, and it causes the initial die-off.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-biofouling",
    "href": "BDC223/BDC223_FAQ.html#question-about-biofouling",
    "title": "FAQ",
    "section": "6.2 Question About Biofouling",
    "text": "6.2 Question About Biofouling\nHi Professor is biofouling and epiphytes the same or different things?\n\n6.2.1 Answer\nBiofouling is a process. It’s the process by which epiphytes colonise the surface of a basiphyte. The epiphytes in question might be macroalgae, but it’s most typically microalgae or bacteria (the latter two collectively called biofilm).",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-calculating-the-rate-of-uptake-v",
    "href": "BDC223/BDC223_FAQ.html#question-about-calculating-the-rate-of-uptake-v",
    "title": "FAQ",
    "section": "6.3 Question About Calculating the Rate of Uptake, V",
    "text": "6.3 Question About Calculating the Rate of Uptake, V\nGood day Professor, I am hoping sir could assist with my work. For the V column, does that represent the rate that N is being assimilated into the thallas? If so, then the values should be positive right? 😅.\nI’m asking because some students are getting negative values. Regards\nProfessors response.\n“Yes. Why do you think there’s a negative value? What does a negative rate mean—i.e. does it apply to the culture medium (where the concentration decreases) or to the seaweed (where it increases)?”\nI believe the values of the slope are negative because that shows the rate of N that leaves the solution. If I can put it like that\n\n6.3.1 Answer\nYes! And thus the rate of appearance of N in the seaweed is of the opposite sign, so simply take the absolute value.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-calculating-s-in-the-nutrient-uptake-experiments",
    "href": "BDC223/BDC223_FAQ.html#question-about-calculating-s-in-the-nutrient-uptake-experiments",
    "title": "FAQ",
    "section": "6.4 Question About Calculating S in the Nutrient Uptake Experiments",
    "text": "6.4 Question About Calculating S in the Nutrient Uptake Experiments\nSir, do we consider the only culture volume when calculate our S (substrate conc)? and we use μmol N or μg N units or it doesn’t much matter\n\n6.4.1 Answer\nIt is a function not so much of culture volume, but of the amount (micro moles or micrograms) of nutrients within a volume of seawater.\nVolume per se is not important: the concentration of a substance is the same in 1 ml or in 1 liter. The amount (moles or grams) of a substance is very different in that 1 ml or 1 liter, however. So, volume does not affect concentration, but it affect total amounts available in a volume.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-perturtbation-experiments",
    "href": "BDC223/BDC223_FAQ.html#question-about-perturtbation-experiments",
    "title": "FAQ",
    "section": "6.5 Question About Perturtbation Experiments",
    "text": "6.5 Question About Perturtbation Experiments\n(AJ?) Smit professor, with multiple flask experiment you said you can calculate update rate (so I’m assuming it’s a linear graph) and with perturbation you said it’s a depletion curve.\nWith the Michaelis- menten we measure substrate concentration against uptake rate but use perturbation methods (using the gradient for the uptake rate) Since multiple flask also shows uptake rate can you still use this methodology to generate a Michaelis-menten expression? Also wouldn’t it have been easier because then you don’t have the whole x-axis confusion\n\n6.5.1 Answer\nWhatsApp Ptt 2022-10-12 at 10.25.49 PM.ogg",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-about-ks-and-alpha",
    "href": "BDC223/BDC223_FAQ.html#question-about-ks-and-alpha",
    "title": "FAQ",
    "section": "6.6 Question About Ks and \\(\\alpha\\)",
    "text": "6.6 Question About Ks and \\(\\alpha\\)\nWith regards to Michaelis Menton expression and specifically the Ks and \\(\\alpha\\) does that specifically relate to diffusion ability?\nDoes a high Ks mean diffusion was rate limiting sooner whereas a low Ks meaning kinetics was rate limiting?\nOr am I completely misunderstanding the work?\n\n6.6.1 Answer\nYes. Ks and \\(\\alpha\\) relate to the externally controlled phases of nutrient uptake, so they are controlled by diffusion (and thus also water motion and nutrient concentration).",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#question-the-nitrogen-cycle",
    "href": "BDC223/BDC223_FAQ.html#question-the-nitrogen-cycle",
    "title": "FAQ",
    "section": "6.7 Question – The Nitrogen Cycle",
    "text": "6.7 Question – The Nitrogen Cycle\nI just wanted some clarification, is it correct to say that the definition of the nitrogen cycle is a biogeochemical process through which nitrogen is converted into many chemical forms circulating in the marine, terrestrial and atmospheric ecosystems?\n\n6.7.1 Answer\nN cycle. I’d say something like this:\nThe uptake, transformation, release, and transport of N-containing compounds through components of the Earth system, including the biosphere, geosphere, hydrosphere, cryosphere, and atmosphere. The underlying processes involve a series of biologically, physically, and chemically mediated processes which act on different compounds of inorganic and organic N.\nMore simply we can say the N cycle is N biogeochemistry, but less is explained by this short statement than by the longer one.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-12",
    "href": "BDC223/BDC223_FAQ.html#answer-12",
    "title": "FAQ",
    "section": "7.1 Answer",
    "text": "7.1 Answer\nVery nice question! It is a pity I already set the exams.\nSo why does Ulva not show saturation at some point?\nWithin the range of N concentrations typically present in the ocean, say up to 20μM N in upwelling systems, uptake should (can) theoretically remain unsaturated, PROVIDED THAT ALL OTHER ENVIRONMENTAL CONDITIONS REMAIN OPTIMAL. There always has to be sufficient amounts of light; the temperature must be optimal, and so on. As soon as the GROWTH RATE slows down because the alga cannot capture enough light to drive photosynthesis (for cellular replication and biomass growth), there will be an upper limit to the amount of N taken up sequestered. So, the high uptake rates promised by a fully rate-unsaturated uptake mechanism supported by diffusion are only possible if the alga can produce enough biomass quickly so it can assimilate N into biomass (protein). Algae can only assimilate N if enough C comes in (through photosynthesis) for sufficient amounts of the C compounds containing N in an organic form.\nTherefore, all suboptimal environmental conditions influencing C uptake will affect N uptake.\nOnly some environmental conditions are optimal for long enough for algae to sustain high N uptake through rapid growth rates. Only because of fast growth rates will N be maintained at low enough concentrations in the vacuoles to prevent feedback inhibition. When feedback inhibition happens, the rate of N uptake is limited. Under most natural conditions, there is likely an upper limit to N uptake. However, we can create optimised conditions in the lab to maximise the algal growth rate; thus, N uptake could remain unsaturated.\nEven passive uptake (N uptake through diffusion) can be rate limited if the amount of N building up inside the cells is so high that it reduces the concentration gradient across the cell from outside (water) to inside (vacuole). In this situation, there would also be a Vmax, determined by the rate at which the alga can bind N into an inorganic form, typically as protein (including some phycobilins).",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-13",
    "href": "BDC223/BDC223_FAQ.html#answer-13",
    "title": "FAQ",
    "section": "8.1 Answer",
    "text": "8.1 Answer\nIt’s as the question says:\nDesign an experiment that will provide insight into both the optimum ratio of N and P and the optimum concentration of potassium nitrate and orthophosphoric acid to feed the U. lactuca mass culture (i.e. with the aim to maximise biomass production).\nIn your answer, please pay specific attention to the experimental conditions during the acclimation phase (i.e. a period lasting two weeks prior to the experiment), as well as during the experimental phase. Provide a rationale and justification for all your decisions that ultimately inform your experiment.\nCalculations can only done after the experiment is completed, and the question simply asks that you design the experiment.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-14",
    "href": "BDC223/BDC223_FAQ.html#answer-14",
    "title": "FAQ",
    "section": "9.1 Answer",
    "text": "9.1 Answer\nThe American Geophysical Union does not recognise the Anthropocene as an actual geological epoch yet, so according to them we are still in the Holocene. But many people think that we have already deviated so far away from what was typical for Holocene into something very different, and that we should redefine the current era as the Anthropocene.\nWhat’s your personal view Prof?\nAnthropocene means ‘the age of humans’. So, humans have become so abundant that the signal of our activities have made an imprint on global biogeochemical systems such that in millennia from now when people no longer exist, ‘we’ (whatever replaces us or visits Earth) will be able to pick up signs of people’s existence in various geological strata on Earth.\nI think it makes sense to call where we are presently the Anthropocene, and I think Johan Rockström makes the same argument.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-15",
    "href": "BDC223/BDC223_FAQ.html#answer-15",
    "title": "FAQ",
    "section": "10.1 Answer",
    "text": "10.1 Answer\nRalph Keeling’s work is part of the justification. Much more has happened since, especially in the last decade. I don’t think a justification to use Anthropocene yet existed in the 1960s, but there’s plenty going on now to cause one to make that argument.\nSee The Keeling Curve for nice views into what constitutes the Keeling curve over various timescales.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-16",
    "href": "BDC223/BDC223_FAQ.html#answer-16",
    "title": "FAQ",
    "section": "11.1 Answer",
    "text": "11.1 Answer\nI guess I’m not so much interested in exact dates, but do knowing which part of which century things happened is important. And the correct order of events. The fact is, we know about climate change far longer than people give credit to.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "BDC223/BDC223_FAQ.html#answer-17",
    "href": "BDC223/BDC223_FAQ.html#answer-17",
    "title": "FAQ",
    "section": "12.1 Answer",
    "text": "12.1 Answer\nI gave you the answer on Friday [the one about N uptake, as seen above]. Something like that. Just adapt it for photosynthesis. You want to measure O2 production/consumption or CO2 production/consumption in stead of nutrients.\nJust pick your favourite plant or algal species. The experiment must be appropriate for plants or algae, of course. The difference is that plants live in air and algae in an aqueous medium, so the experiment must be set up appropriately.\nIn air we use an IRGA (infrared gas analyser) and in water we can use an O2 meter. Or we can use a C14-labelled source of CO2 and use scintillation counting to measure the appearance of a radioactive C for in the pool where CO2 accumulates.\nOtherwise, not too different from the N uptake answer, except we probably won’t use the perturbation method.\nAnd you probably want to measure net photosynthesis, so make sure you measure respiration too.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "FAQ"
    ]
  },
  {
    "objectID": "pages/assessment_theory.html",
    "href": "pages/assessment_theory.html",
    "title": "Assessment theory",
    "section": "",
    "text": "BCB744 and BCB743 thoroughly implement formative and summative assessments.\nFormative assessment is ‘academic speak’ for continuous assessment. It provides you with ongoing feedback that you can use to track your performance and to self-evaluate your understanding. Formative assessment also lets me see your development as we progress from simple to more complex topics. Since this is done daily with feedback the next day, I can identify and address any hurdles before they become problematic and impede progression. Formative assessments may include quizzes, discussions, observations, group activities, or small focussed tasks. They are designed to gauge your progress and identify areas of strength or weakness. Continual monitoring and feedback allow me to modify and adapt my instructional strategies in real-time to meet your needs as students better. I intend for this dynamic approach to assessment to create a more engaging, interactive, collaborative, and supportive learning environment, ultimately promoting deeper understanding and long-term retention of knowledge.\nSummative assessment is the second and final mode of assessment. It is designed to evaluate your understanding and mastery of subjects in their full complexity at the end of the learning period. These assessments are in the form of standardised tests or exams and may also comprise comprehensive integrative projects. This mode of assessment provides us (you, me, the BCB Department, and the UWC) with a view of attaining the desired teaching outcomes as stated in the modules’ preambles. It is also a yardstick we use to rate and rank the effectiveness of my instructional methods and the extent to which you have acquired knowledge.\nFormative and summative assessments must inform decisions regarding student advancement and future instructional needs. They contribute to the continuous improvement of the integrated educational program, the curriculum, and teaching practices.\nHere’s a summary of the two modes of assessment:\n\nPurpose Formative assessment mainly monitors your progress and provides feedback during the learning process. In contrast, summative assessment evaluates your performance and understanding at the end of a learning period.\nTiming Formative assessments frequently occur throughout a course or unit, allowing continuous feedback and adjustment. Summative assessments typically occur at the end of a course, unit, or semester.\nFeedback Formative assessment offers real-time, actionable feedback that enables you and me to adjust learning and teaching strategies. Summative assessment provides a more comprehensive evaluation of your knowledge and skills, which can inform future instruction or determine advancement.\nImpact on grades Formative assessments are often weighted less regarding how much tasks contribute to the final mark; it focuses instead on learning and improvement. Summative assessments typically count more towards the final grade and allow us to establish whether you have attained specific learning objectives.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{smit,_a._j.2023,\n  author = {Smit, A. J.,},\n  title = {Assessment Theory},\n  date = {2023-04-24},\n  url = {http://tangledbank.netlify.app/pages/assessment_theory.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2023) Assessment theory. http://tangledbank.netlify.app/pages/assessment_theory.html."
  },
  {
    "objectID": "pages/How_to_learn.html",
    "href": "pages/How_to_learn.html",
    "title": "How to learn",
    "section": "",
    "text": "This question was asked by 3rd years, but it is relevant here too.\nI just need some advice from sir. I’m starting to study today and I just wanted to know what do you think is the best way to study for sir’s exam. I’m asking because I studied really hard for the midterm tests but I still got low marks lol. So I just wanted to know if there is maybe a different approach that I can take.\n\nI was trying to memorise things for the midterm tests but I know sir said we must focus on understanding but when I focus on understanding then the work does not stick in my head😂.\n\nSo far I’ve just read through the articles but is there any advice that sir can give me please?\n\n\nThank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2). At some point later on you will also know what you don’t know…\n\n“Reports that say that something hasn’t happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don’t know we don’t know. And if one looks throughout the history of our country and other free countries, it is the latter category that tends to be the difficult ones.”\n-- Donald Rumsfeld, February 12, 2002\n\nYou already know things. Structure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles in a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the most important attributes which inform who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful."
  },
  {
    "objectID": "pages/How_to_learn.html#how-does-one-study-to-understand",
    "href": "pages/How_to_learn.html#how-does-one-study-to-understand",
    "title": "How to learn",
    "section": "",
    "text": "This question was asked by 3rd years, but it is relevant here too.\nI just need some advice from sir. I’m starting to study today and I just wanted to know what do you think is the best way to study for sir’s exam. I’m asking because I studied really hard for the midterm tests but I still got low marks lol. So I just wanted to know if there is maybe a different approach that I can take.\n\nI was trying to memorise things for the midterm tests but I know sir said we must focus on understanding but when I focus on understanding then the work does not stick in my head😂.\n\nSo far I’ve just read through the articles but is there any advice that sir can give me please?\n\n\nThank you for your email. Hmmm, a tricky question to ask of someone who wrote their last exam in 1993!\nI am the opposite to you. I cannot memorise things but I am able to understand things really well. Fortunately, in the process of figuring things out, the relevant bits of information/knowledge relating to the thing I am trying to understand also sticks in my mind, which is (for me) a useful side effect of figuring things out. For me, it is pointless having things to memorise unless I can apply it to something that needs figuring out. So, everything I know, I know because it is useful to me.\nHow does understanding come about? For me, I try and understand stuff because the challenge of a tricky problem is thrilling, so understanding is facilitated because the process aligns with what makes me ‘tick.’ Okay, so this does not answer how understanding comes about; it simply talks to who I am.\nI can tell you is how you can test your understanding. Explain the thing you are trying to understand to a friend or family member. If your explanation of the topic brings about an understanding in the other person, then you yourself understand it. At least this process will tell you where your own understanding starts to fail. As Richard Feynman said, “If you cannot explain something to a 7-year old, you don’t understand it yourself.” Or something like that.\nFor someone who finds it easier to memorise stuff and more challenging to understand… I don’t know what that feels like as I have no personal experience or frame of reference that allows me to place myself into your shoes. But here is the theory:\n\nKnow what it is you already know, and build upon that (see point 2). At some point later on you will also know what you don’t know…\n\n“Reports that say that something hasn’t happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don’t know we don’t know. And if one looks throughout the history of our country and other free countries, it is the latter category that tends to be the difficult ones.”\n-- Donald Rumsfeld, February 12, 2002\n\nYou already know things. Structure new and existing knowledge around the major concepts and principles of the module (broadly, the new concepts covered each week). This means that work done in your first and second years of your study, in ALL the modules you completed, remains relevant, and has to be used together with the new knowledge obtained in the new module, and structured around the broad concept and principle (there are many concepts and principles in a module).\nYou need to develop and understand the language used to communicate the topic. At the very basic level, this requires that you understand in intricate detail the individual words (ALL of them) that form the foundational language of your study discipline (biology and science more broadly). Only once you understand the definitions of individual words will you be able to develop more complex understanding. I think this is the primary reason why students fail to develop a deep understanding of a topic that requires explanation in multiple paragraphs (long answers and essays).\nAnother step involves knowing how all the steps that inform the thinking process are interrelated. This requires that you consider some of the following things: What does the assignment or task require me to do (i.e. unpack the problem)? What are the steps I need to follow to get there? What do I already know about it? What do I not understand and where do I get stuck? Why don’t I know it already? What about the problem causes me to get stuck? Where do I get the knowledge about what I don’t understand, and how can I use this to become unstuck? Okay, so now I am figuring things out… What does the problem remind me about? Have I encountered something similar before, and if so, how can I use that to develop further my current thinking about the problem? This whole process is called metacognition, which is thinking about thinking and learning. This kind of thing has to happen each time you see something new, come across a new piece of information, listen to someone speak, etc. It can be applied in your day to day life to the extent that it become implicit in how you approach life. Eventually, you’ll find yourself saying more often, “I am wondering…” Then you will arrive at critical thinking, which is what makes science special.\nHow well you are able to integrate the metacognitive skills in your life and learning depends unfortunately on your inherent abilities and prior experiences. It is easier for some than it is for others.\nA critical characteristic of good learning is that it informs your sense of self — this means that once you value learning as one of the most important attributes which inform who you are as a person, the easier it will become to learn, the less effort it will take, and the more learning itself will become the motivator (as opposed to search for motivation externally, like some reward, for having to learn).\nThe activities in your life, your friends, family and interests will also shape how much you learn, and what you learn.\nYou need to mix with people who values learning to the same extent that you do, so this social reinforcement further ‘snowballs’ into life-long learning and understanding.\n\nStep no. 4. is probably to most helpful."
  },
  {
    "objectID": "pages/reproducible_research.html",
    "href": "pages/reproducible_research.html",
    "title": "Reproducible research and the information economy",
    "section": "",
    "text": "The information economy refers to the modern-day and continually evolving economic system where information, knowledge, and data are the primary drivers of productivity, growth, and innovation. In this economy, the creation, distribution, and consumption of information are more valuable than in traditional industries. The information economy relies on technological advancements, particularly in information and communication technologies (ICTs), to enable efficient processing, storage, and data sharing. Tech companies, digital service providers, and knowledge-intensive industries are typically seen as key players in the information economy. As biologists, however, we often overlook how our information pipelines and knowledge-sharing approaches might benefit from the principles that are now deeply ingrained in just about every aspect of our daily lives."
  },
  {
    "objectID": "pages/reproducible_research.html#eresearch-and-the-information-economy",
    "href": "pages/reproducible_research.html#eresearch-and-the-information-economy",
    "title": "Reproducible research and the information economy",
    "section": "",
    "text": "The information economy refers to the modern-day and continually evolving economic system where information, knowledge, and data are the primary drivers of productivity, growth, and innovation. In this economy, the creation, distribution, and consumption of information are more valuable than in traditional industries. The information economy relies on technological advancements, particularly in information and communication technologies (ICTs), to enable efficient processing, storage, and data sharing. Tech companies, digital service providers, and knowledge-intensive industries are typically seen as key players in the information economy. As biologists, however, we often overlook how our information pipelines and knowledge-sharing approaches might benefit from the principles that are now deeply ingrained in just about every aspect of our daily lives."
  },
  {
    "objectID": "pages/reproducible_research.html#embracing-technological-advancements-a-pathway-to-enhanced-research-and-collaboration",
    "href": "pages/reproducible_research.html#embracing-technological-advancements-a-pathway-to-enhanced-research-and-collaboration",
    "title": "Reproducible research and the information economy",
    "section": "Embracing Technological Advancements: A Pathway to Enhanced Research and Collaboration",
    "text": "Embracing Technological Advancements: A Pathway to Enhanced Research and Collaboration\nOver the years, I have enthusiastically adopted various technological advancements, recognising their potential to elevate my research impact both locally and globally and to keep pace with the evolving global landscape. However, I have observed that not all scientists share my enthusiasm for technology, leading to a sense of alienation among some colleagues who prefer traditional research methods where buckets and spades still rule.\nIt appears that, for some individuals, particularly in fields such as biology or ecology, there is a belief that focusing solely on their discipline-specific subject matter is sufficient and that insights from Computer Science Departments hold little relevance. This narrow perspective, in my view, is limiting and stifles creativity.\nBy embracing technology, we can not only broaden our horizons but also enhance our research capabilities and expand collaboration. We must remain open-minded, explore the potential of interdisciplinary learning, and leverage technology to maximise the possibilities in our respective fields."
  },
  {
    "objectID": "pages/reproducible_research.html#the-interconnected-nature-of-science-and-technology-an-ongoing-journey",
    "href": "pages/reproducible_research.html#the-interconnected-nature-of-science-and-technology-an-ongoing-journey",
    "title": "Reproducible research and the information economy",
    "section": "The Interconnected Nature of Science and Technology: An Ongoing Journey",
    "text": "The Interconnected Nature of Science and Technology: An Ongoing Journey\nAs the practice of science has undergone dramatic changes in recent years, driven in part by Moore’s Law, we are now tackling global issues across vast timescales. This transformation is largely attributed to the availability of vast amounts of data, which has necessitated the development of efficient algorithms to establish connections, access subsets, and distil complex information using supervised and unsupervised data-analytical techniques.\nConcurrently, this data explosion has spurred the advancement of hardware capable of handling the computational, memory, and data transfer demands of big data. While it is debatable whether hardware development has facilitated the collection of increasing amounts of data or vice versa, the ultimate takeaway remains the same: technological progress is relentless, and the practice of science must adapt swiftly to keep up. By acknowledging this interconnected nature of science and technology, we can work with agility, ensuring we remain at the forefront of scientific discovery and innovation."
  },
  {
    "objectID": "pages/reproducible_research.html#navigating-modern-science-interdisciplinary-collaboration-transparency-and-data-sharing",
    "href": "pages/reproducible_research.html#navigating-modern-science-interdisciplinary-collaboration-transparency-and-data-sharing",
    "title": "Reproducible research and the information economy",
    "section": "Navigating Modern Science: Interdisciplinary Collaboration, Transparency, and Data Sharing",
    "text": "Navigating Modern Science: Interdisciplinary Collaboration, Transparency, and Data Sharing\nContemporary science is characterised by the convergence of diverse skill sets to address complex problems through interdisciplinary and transdisciplinary research. This approach, however, presents challenges in team dynamics, data sharing, and code management. Additionally, there is an increasing demand for transparency in research methodologies, as exemplified by the International Panel for Climate Change, and the emergence of reproducible research.\nCompliance with data and information-sharing policies, such as the FAIR principles, global standards, national legislative acts, and discipline-specific norms, has become essential. Recognising the value of metadata alongside primary datasets is now the norm. While software offers solutions to these challenges, only a fraction of us, primarily the tech-savvy, possess the willingness to keep pace and fully embrace the opportunities.\nTo advance modern science, it is imperative that we adapt and cultivate the skills necessary to navigate interdisciplinary collaboration, ensure transparency, and adhere to evolving data-sharing standards."
  },
  {
    "objectID": "pages/reproducible_research.html#embracing-modern-technologies-across-disciplines-for-a-future-ready-workforce",
    "href": "pages/reproducible_research.html#embracing-modern-technologies-across-disciplines-for-a-future-ready-workforce",
    "title": "Reproducible research and the information economy",
    "section": "Embracing Modern Technologies Across Disciplines for a Future-Ready Workforce",
    "text": "Embracing Modern Technologies Across Disciplines for a Future-Ready Workforce\nModern technologies are indispensable for those of us working with extensive datasets, whether in climate change, computational linguistics, or small-scale studies. My disregard for traditional disciplinary boundaries has enabled me to stay informed about relevant advancements, driving my determination to develop this website, The Tangled Bank. My motivation is further fuelled by the concern that many colleagues are failing to maintain the necessary interest for continuous advancement.\nA reluctance to embrace change not only affects ourselves but also has a domino effect on postgraduate and undergraduate students. By not nurturing the required skills in students, academics hinder their ability to become well-rounded graduates equipped for the modern workplace and to develop transferable skills that transcend disciplinary boundaries. It is crucial to remember that many graduates, particularly those with Bachelor and Honours degrees, will pursue careers unrelated to their original fields of study. Yet, they want to have a degree that provides skills anywhere their future selves might find themselves.\nTo foster a future-ready workforce, it is necessary that we embrace technological advancements and cultivate adaptable, interdisciplinary skill sets in the next generation of graduates."
  },
  {
    "objectID": "pages/reproducible_research.html#exemplifying-the-importance-of-reproducible-research-and-eresearch-frameworks",
    "href": "pages/reproducible_research.html#exemplifying-the-importance-of-reproducible-research-and-eresearch-frameworks",
    "title": "Reproducible research and the information economy",
    "section": "Exemplifying the Importance of Reproducible Research and eResearch Frameworks",
    "text": "Exemplifying the Importance of Reproducible Research and eResearch Frameworks\nConsider the challenge of conducting reproducible research, which, when addressed, can resolve many eResearch framework issues. A typical PhD student spends a few months writing their thesis, which often serves as the sole evidence of degree completion. However, the majority of the learning and methodological expertise developed during the rest of the degree remains undocumented and eventually forgotten. This wealth of knowledge is rarely shared, leading to repeated dead-ends in knowledge transfer as new candidates embark on similar journeys.\nMost research neglects the full data lifecycle, focusing mainly on the initial steps. The failure to share behind-the-scenes solutions results in non-reproducible research, making the scientific process opaque and fostering public mistrust. This opacity hinders collaboration among supervisors and co-investigators, increases error-proneness, and scales poorly as datasets and complexities grow. Additionally, the research process becomes less efficient due to inadequate documentation of data selection, filtering justifications, metadata tracking, data versions, and processing changes.\nAddressing these challenges is essential to promote reproducible research, enhance collaboration, and build public trust in science, ultimately contributing to a more efficient and transparent research process. This makes the research process extremely wasteful in as far as preserving the full complexity of what a typical student learns."
  },
  {
    "objectID": "pages/reproducible_research.html#promoting-reproducible-research-through-lab-notebooks-and-proper-workflow-management",
    "href": "pages/reproducible_research.html#promoting-reproducible-research-through-lab-notebooks-and-proper-workflow-management",
    "title": "Reproducible research and the information economy",
    "section": "Promoting Reproducible Research through Lab Notebooks and Proper Workflow Management",
    "text": "Promoting Reproducible Research through Lab Notebooks and Proper Workflow Management\nMany solutions exist to address research reproducibility, but I find lab notebooks using RStudio’s markdown (for R users) or Jupyter Lab/Notebooks (for Python users) particularly effective. Version tracking can be achieved using git, such as in GitHub. These notebooks integrate code with text, allowing automatic updates of tables and figures with new data. My students are proficient in this approach, ensuring their work is reproducible.\nI advocate for the widespread adoption of lab notebooks at universities, making them a prerequisite for thesis submission in applicable disciplines. The thesis can be a reproducible document written in markdown, and typeset to various formats such as PDF, HTML, MS Word, or eBook. This method also incorporates proper bibliography management.\nThis reproducible workflow complies with funding instruments requiring data and code sharing, reproducibility, and open publication per FAIR principles. It is already prevalent in disciplines like ecology. While this example focuses on paper or thesis writing, technology impacts research practice across disciplines, commerce, arts, and law. A comprehensive overview is beyond our scope, but the examples provided illustrate the broader possibilities."
  },
  {
    "objectID": "pages/AI4AI.html",
    "href": "pages/AI4AI.html",
    "title": "Artificial Intelligence For Academic Integrity (AI4AI)",
    "section": "",
    "text": "In OpenAI Desktop Application, you can follow the menu path Settings &gt; Personlization &gt; Customize ChatGPT to set up a custom prompt for the AI model. This is useful for setting up a prompt that you use frequently, such as for a specific course or project.\n\n\n\nSciSpace is a platform that allows researchers to read and summarise papers. It is a great tool for keeping up to date with the latest research in your field. You can use it to search for papers, read abstracts, and generate summaries of papers that you find interesting.\n\n\n\nNotebookLM\n\n\n\nChatGPT canvas (o1), accessible only in the OpernAI web application, is a great tool for writing and editing text. You can use it to write essays, reports, and other documents, and it provides a distraction-free environment for focusing on your writing."
  },
  {
    "objectID": "pages/AI4AI.html#prompts",
    "href": "pages/AI4AI.html#prompts",
    "title": "Artificial Intelligence For Academic Integrity (AI4AI)",
    "section": "",
    "text": "In OpenAI Desktop Application, you can follow the menu path Settings &gt; Personlization &gt; Customize ChatGPT to set up a custom prompt for the AI model. This is useful for setting up a prompt that you use frequently, such as for a specific course or project."
  },
  {
    "objectID": "pages/AI4AI.html#reading-and-summarising-papers",
    "href": "pages/AI4AI.html#reading-and-summarising-papers",
    "title": "Artificial Intelligence For Academic Integrity (AI4AI)",
    "section": "",
    "text": "SciSpace is a platform that allows researchers to read and summarise papers. It is a great tool for keeping up to date with the latest research in your field. You can use it to search for papers, read abstracts, and generate summaries of papers that you find interesting."
  },
  {
    "objectID": "pages/AI4AI.html#multidocument-chat",
    "href": "pages/AI4AI.html#multidocument-chat",
    "title": "Artificial Intelligence For Academic Integrity (AI4AI)",
    "section": "",
    "text": "NotebookLM"
  },
  {
    "objectID": "pages/AI4AI.html#writing-and-editing",
    "href": "pages/AI4AI.html#writing-and-editing",
    "title": "Artificial Intelligence For Academic Integrity (AI4AI)",
    "section": "",
    "text": "ChatGPT canvas (o1), accessible only in the OpernAI web application, is a great tool for writing and editing text. You can use it to write essays, reports, and other documents, and it provides a distraction-free environment for focusing on your writing."
  },
  {
    "objectID": "pages/promotion_index.html",
    "href": "pages/promotion_index.html",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "",
    "text": "About the square bracket `[]` notation\n\n\n\nA list of the links provided in my Case for Promotion document is provided here. The numbers in square brackets ‘[]’ refer to the footnote in the Case for Promotion document."
  },
  {
    "objectID": "pages/promotion_index.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "href": "pages/promotion_index.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department",
    "text": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department\n[5] I was instrumental in developing South Africa’s first undergraduate Marine Biology curriculum at the University of KwaZulu-Natal in 2007 (with Profs. Perissinotto and Schoeman)\n[6] See a discussion about how I allow modern technologies to influence and shape my teaching\n[7] Views on collaborative learning\n[8] Example exercises and bonus, designed to reward and incentivise continued learning towards advanced skills\n[9] Assessment policy for BCB744\n[10] Explanation of modes of assessment\n[11] Module-specific graduate attributes\n[12] The difference between science and data science\n[13] Thoughts about the learning process\n[14] Access to old test and exam questions"
  },
  {
    "objectID": "pages/promotion_index.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "href": "pages/promotion_index.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules",
    "text": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules\n[15] For an example of information rich text, see the example page\n[16] See the ‘vignettes’ menu at the top of The Tangled Bank.\n[17] For example, the FAQ page for BDC223\n[18] See feedback from colleagues about The Tangled Bank"
  },
  {
    "objectID": "pages/promotion_index.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "href": "pages/promotion_index.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught",
    "text": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught\n[19] Prof. Sophie von der Heyden’s feedback about BCB743 in 2022\n[20] Prof. Sophie von der Heyden’s feedback about BCB744 in 2022\n[21] BCB744 assessment policy\n[22] BCB743 assessment policy\n[23] BDC334 assessment policy\n[24] Student feedback about BDC223, BDC334, BCB744, and BCB743 are available at the links below:\n\nBDC223\nBDC334\nBCB744\nBCB743"
  },
  {
    "objectID": "pages/promotion_index.html#tangled-bank-vignettes-and-reproducible-research",
    "href": "pages/promotion_index.html#tangled-bank-vignettes-and-reproducible-research",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.1.4. Tangled Bank vignettes and reproducible research",
    "text": "4.1.4. Tangled Bank vignettes and reproducible research\n[25] See my essay on eResearch and reproducible research\n[26] Dr Robert Schlegel’s GitHub page\n[27] Ms Amieroh Abrahams’s GitHub page\n[28] Mr Ross Coppin’s GitHub page\n[29] Examples of vignettes may be access at The Tangled Bank under the ‘vignettes’ menu at the top. For example:\n\nRetrieving Chlorophyll-a Data from ERDDAP Servers\nWavelet analysis of diatom time series\nEvent horizon plots\n\nOther vignettes are at the heatwaveR website in the vignettes top menu."
  },
  {
    "objectID": "pages/promotion_index.html#successful-and-prolific-funding-attraction",
    "href": "pages/promotion_index.html#successful-and-prolific-funding-attraction",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.1. Successful and prolific funding attraction",
    "text": "4.2.1. Successful and prolific funding attraction\n[30] List of the more recent research funding received:\n\n2020 – 2022: Belmont Forum Collaborative Research Action on Transdisciplinary Research for Ocean Sustainability: Ecological and Economic impacts of the intensification of extreme events in the Benguela Upwelling System, Principal Investigator (EXEBUS) PDF\n2019 – 2021: SANOCEAN: Factors influencing the formation, fate and transport of microplastic in marine coastal ecosystems (FORTRAN) PDF\n2019 – 2021: SANOCEAN: Blue growth opportunities in changing kelp forests (BlueConnect) PDF\n2019 – 2023: Horizon 2020: iAtlantic, led by Prof. Murray Robert, own capacity as Regional Coordinator for the SE Atlantic PDF\n\n[31] List of older nationally funded research\n\n2019 – 2021: NRF Global Change Grand Challenge: Earth System Science Research Programme — Extreme Climatic Events in the Coastal Zone, Principal Investigator (ESS180920360856) PDF\nNRF GRANT for 2018 – 2020: Competitive Programme for Rated Researchers —Upwelling dynamics in kelp beds: implications for trophic function PDF\n2017: CHEC/CCT Joint Research Programme 2017: What can kelp loss processes and beach cast patterns tell us about the sandy beach management? PDF\n2015 – 2017: NRF COMPETITIVE PROGRAMME FOR RATED RESEARCHERS (CPRR) — Thermal characteristics of the South African nearshore: implications for biodiversity PDF\n2014 – 2016: NRF COMPETITIVE PROGRAMME FOR RATED RESEARCHERS (CPRR) — Kelps and climate change: South Africa in a global context PDF\n2014 – 2016: NRF GRANT FOR 2014: INCENTIVE FUNDING FOR RATED RESEARCHERS (IPRR) Grant No. IFR14020764026 PDF\n\n[32] My thoughts about the NRF rating system and maintaining my own rating"
  },
  {
    "objectID": "pages/promotion_index.html#development-of-r-packages-in-marine-heatwave-analysis",
    "href": "pages/promotion_index.html#development-of-r-packages-in-marine-heatwave-analysis",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.2. Development of R packages in marine heatwave analysis",
    "text": "4.2.2. Development of R packages in marine heatwave analysis\n[33] The RmarineHeatWaves documentation.\n[34] heatwaveR. Also see the GitHub page.\n[35] This number is hard to track, but a search in Google Scholar for the term “heatwaveR” (inverted commas included) yields at least 150 citations. A shorter list of the citations is provided at https://robwschlegel.github.io/heatwaveR/CITATIONS.html. Notable examples of high-impact publications are provided here:\n\nSmale, D. A., Wernberg, T., Oliver, E. C., Thomsen, M., Harvey, B. P., Straub, S. C., … & Moore, P. J. (2019). Marine heatwaves threaten global biodiversity and the provision of ecosystem services. Nature Climate Change, 9(4), 306-312.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Recent marine heatwaves in the North Pacific warming pool can be attributed to rising atmospheric levels of greenhouse gases. Communications Earth & Environment, 3(1), 131.\nThoral, F., Montie, S., Thomsen, M. S., Tait, L. W., Pinkerton, M. H., & Schiel, D. R. (2022). Unravelling seasonal trends in coastal marine heatwave metrics across global biogeographical realms. Scientific Reports, 12(1), 7740.\nBenedetti-Cecchi, L. (2021). Complex networks of marine heatwaves reveal abrupt transitions in the global ocean. Scientific Reports, 11(1), 1739.\nWoolway, R. I., Jennings, E., Shatwell, T., Golub, M., Pierson, D. C., & Maberly, S. C. (2021). Lake heatwaves under climate change. Nature, 589(7842), 402-407.\nGarcía Molinos, J., Hunt, H. L., Green, M. E., Champion, C., Hartog, J. R., & Pecl, G. T. (2022). Climate, currents and species traits contribute to early stages of marine species redistribution. Communications biology, 5(1), 1329.\nSmith, K. E., Burrows, M. T., Hobday, A. J., Sen Gupta, A., Moore, P. J., Thomsen, M., … & Smale, D. A. (2021). Socioeconomic impacts of marine heatwaves: Global issues and opportunities. Science, 374(6566), eabj3593.\n\n[36] Examples of cross-discipline research in marine heatwaves promoted by the heatwaveR package are provided here:\n\nSchlegel, R. W., Oliver, E. C., & Chen, K. (2021). Drivers of marine heatwaves in the Northwest Atlantic: The role of air–sea interaction during onset and decline. Frontiers in Marine Science, 8, 627970.\nHu, L. (2021). A global assessment of coastal marine heatwaves and their relation with coastal urban thermal changes. Geophysical Research Letters, 48(9), e2021GL093260.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Greenhouse Gas Forcing a Necessary Causation for Marine Heatwaves Over the Northeast Pacific Warming Pool.\n\n[37] Evidence of the application of the heatwaveR package outside of the initially intended field of application, marine science\n\nTassone, S. J., Besterman, A. F., Buelo, C. D., Ha, D. T., Walter, J. A., & Pace, M. L. (2023). Increasing heatwave frequency in streams and rivers of the United States. Limnology and Oceanography Letters, 8(2), 295-304.\nDiniz, F. R., Gonçalves, F. L. T., & Sheridan, S. (2020). Heat wave and elderly mortality: Historical analysis and future projection for metropolitan region of São Paulo, Brazil. Atmosphere, 11(9), 933.\nWoolway, R. I., Albergel, C., Frölicher, T. L., & Perroud, M. (2022). Severe Lake Heatwaves Attributable to Human‐Induced Global Warming. Geophysical Research Letters, 49(4), e2021GL097031.\nReynaert, S., De Boeck, H. J., Verbruggen, E., Verlinden, M., Flowers, N., & Nijs, I. (2021). Risk of short‐term biodiversity loss under more persistent precipitation regimes. Global Change Biology, 27(8), 1614-1626.\nWoolway, R. I., Anderson, E. J., & Albergel, C. (2021). Rapidly expanding lake heatwaves under climate change. Environmental Research Letters, 16(9), 094013.\nPaton, E. (2022). Intermittency analysis of dry spell magnitude and timing using different spell definitions. Journal of Hydrology, 608, 127645.\nMartinez-Baroja, L., Rey-Benayas, J. M., Perez-Camacho, L., & Villar-Salvador, P. (2022). Drivers of oak establishment in Mediterranean old fields from 25-year-old woodland islets planted to assist natural regeneration. European Journal of Forest Research, 141(1), 17-30.\nPappert, D., Barriendos, M., Brugnara, Y., Imfeld, N., Jourdain, S., Przybylak, R., … & Brönnimann, S. (2022). Statistical reconstruction of daily temperature and sea level pressure in Europe for the severe winter 1788/89. Climate of the Past, 18(12), 2545-2565.\nNgoungue Langue, C. G., Lavaysse, C., Vrac, M., & Flamant, C. (2023). Heat wave monitoring over West African cities: uncertainties, characterization and recent trends. Natural Hazards and Earth System Sciences, 23(4), 1313-1333.\n\n[38] For studies that have used metrics calculated by heatwaveR in support of policy development around the management of marine living resources, see this list\n\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2022). A climate-smart spatial planning framework.\nWegscheider, B., Linnansaari, T., Monk, W., Ndong, M., Haralampides, K., St-Hilaire, A., … & Allen, R. (2020). Quantitative modelling of fish habitat under future regulated and hydro-climatically driven flow regimes in the Saint John River (New Brunswick, Canada). Quantitative modelling of existing and future fish habitat in the Saint John River, NB, Canada, 184.\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2023). A metric-based framework for climate-smart conservation planning. Ecological Applications, e2852.\nMuñoz-Pizza, D. M., Sanchez-Rodriguez, R. A., & Manzano, E. G. Linking Climate Change to Urban Planning Through Vulnerability Assessment: The Case of Two Cities at the Mexico-Us Border. Available at SSRN 4348277.\n\n[39] Evidence of examples where such novel research questions and hypotheses have been addressed\n\nLeach, T. S., BuyanUrt, B., & Hofmann, G. E. (2021). Exploring impacts of marine heatwaves: paternal heat exposure diminishes fertilization success in the purple sea urchin (Strongylocentrotus purpuratus). Marine Biology, 168(7), 103.\nPegado, M. R., Santos, C. P., Raffoul, D., Konieczna, M., Sampaio, E., Maulvault, A. L., … & Rosa, R. (2020). Impact of a simulated marine heatwave in the hematological profile of a temperate shark (Scyliorhinus canicula). Ecological Indicators, 114, 106327.\nKraufvelin, L. (2021). Identification of marine heatwaves in the Archipelago Sea and experimental testing of their impacts on the non-indigenous Harris mud crab.\nOliveira, H., Maulvault, A. L., Santos, C. P., Silva, M., Bandarra, N. M., Valente, L. M., … & Anacleto, P. (2023). Can marine heatwaves affect the fatty acid composition and energy budget of the tropical fish Zebrasoma scopas?. Environmental Research, 224, 115504.\nLeach, T. S. (2022). The Role of Pre-and Post-Spawning Temperature Stress on Fertilization Dynamics Within Santa Barbara Channel Sea Urchin Species. University of California, Santa Barbara.\nMinuti, J. J., Byrne, M., Hemraj, D. A., & Russell, B. D. (2021). Capacity of an ecologically key urchin to recover from extreme events: Physiological impacts of heatwaves and the road to recovery. Science of the Total Environment, 785, 147281.\nClare, X. S., Kui, L., & Hofmann, G. E. (2022). Larval Thermal Tolerance of Kellet’s Whelk (Kelletia kelletii) as a Window into the Resilience of a Wild Shellfishery to Marine Heatwaves. Journal of Shellfish Research, 41(2), 283-290.\nMarochi, M. Z., De Grande, F. R., Pardo, J. C. F., Montenegro, Á., & Costa, T. M. (2022). Marine heatwave impacts on newly-hatched planktonic larvae of an estuarine crab. Estuarine, Coastal and Shelf Science, 278, 108122.\nVan Der Walt, K. A., Potts, W. M., Porri, F., Winkler, A. C., Duncan, M. I., Skeeles, M. R., & James, N. C. (2021). Marine Heatwaves Exceed Cardiac Thermal Limits of Adult Sparid Fish (Diplodus capensis, Smith 1884). Frontiers in Marine Science, 8, 702463.\n\n[40] Various online trackers of marine heatwaves use heatwaveR as the underlying processing engine, some of which are reported on my ePortfolio\n\nThe original marine heatwave tracker\nThe Physical Sciences Laboratory heatwave website\nWhaleMap"
  },
  {
    "objectID": "pages/promotion_index.html#student-supervision",
    "href": "pages/promotion_index.html#student-supervision",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.3. Student supervision",
    "text": "4.2.3. Student supervision\n[41] Extract from the NRFOnline system listing most of my post-graduate students"
  },
  {
    "objectID": "pages/promotion_index.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "href": "pages/promotion_index.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)",
    "text": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)\n[42] The The South African Coastal Seawater Temperature Network (SACTN) GitHub page from where data can be downloaded\n[43] Smit, A. J., Roberts, M., Anderson, R. J., Dufois, F., Dudley, S. F., Bornman, T. G., … & Bolton, J. J. (2013). A coastal seawater temperature dataset for biogeographical studies: large biases between in situ and remotely-sensed data sets around the coast of South Africa. PLoS One, 8(12), e81944.\n[44] A few personal well-cited publications that cite the SACTN:\n\nSchlegel, R. W., Oliver, E. C., Wernberg, T., & Smit, A. J. (2017). Nearshore and offshore co-occurrence of marine heatwaves and cold-spells. Progress in Oceanography, 151, 189-205.\nSchlegel, R. W., Oliver, E. C., Perkins-Kirkpatrick, S., Kruger, A., & Smit, A. J. (2017). Predominant atmospheric and oceanic patterns during coastal marine heatwaves. Frontiers in Marine Science, 4, 323."
  },
  {
    "objectID": "pages/promotion_index.html#editorial-contributions",
    "href": "pages/promotion_index.html#editorial-contributions",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.5. Editorial contributions",
    "text": "4.2.5. Editorial contributions\n[45] Associate Editor for Aquatic Botany\n[46] My Reviewer’s profile on Loop for editorial contributions to Frontiers in Ecology & Evolution"
  },
  {
    "objectID": "pages/promotion_index.html#committees-and-programmes",
    "href": "pages/promotion_index.html#committees-and-programmes",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.2.6. Committees and programmes",
    "text": "4.2.6. Committees and programmes"
  },
  {
    "objectID": "pages/promotion_index.html#academic-lead-kelp-scientific-collaboration-ppp",
    "href": "pages/promotion_index.html#academic-lead-kelp-scientific-collaboration-ppp",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP",
    "text": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP\n[47] Kelp Scientific Collaboration mission statement"
  },
  {
    "objectID": "pages/promotion_index.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "href": "pages/promotion_index.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp",
    "text": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp\n[48] Perceived Value of Kelp\n[49] Kelp, South Africa’s Golden Forests on YouTube\n[50] Akshata Mehta’s MPhil thesis"
  },
  {
    "objectID": "pages/promotion_index.html#blueconnect-engagements",
    "href": "pages/promotion_index.html#blueconnect-engagements",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.3. BlueConnect engagements",
    "text": "4.3.3. BlueConnect engagements\n[51] Invitation letter to the GEAK workshop held in Norway\n[52] BlueConnect March 2020 Field Course"
  },
  {
    "objectID": "pages/promotion_index.html#other-community-engagements-and-capacity-building-contributions",
    "href": "pages/promotion_index.html#other-community-engagements-and-capacity-building-contributions",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.4. Other community engagements and capacity-building contributions",
    "text": "4.3.4. Other community engagements and capacity-building contributions\n[53] See most recent invitation to participate in a capacity building initiative\n[54] Invitation quarterly Kogelberg Marine Working Group meeting"
  },
  {
    "objectID": "pages/promotion_index.html#covid-19-environmental-research-group",
    "href": "pages/promotion_index.html#covid-19-environmental-research-group",
    "title": "Ad Personam Promotion 2023: e-Portfolio",
    "section": "4.3.5. CoVID-19 Environmental Research Group",
    "text": "4.3.5. CoVID-19 Environmental Research Group\n[55] Smit et al (2020) about CoVID-19"
  },
  {
    "objectID": "pages/graduate_attributes.html",
    "href": "pages/graduate_attributes.html",
    "title": "Graduate attributes",
    "section": "",
    "text": "Key graduate attributes I emphasise in my BDC334, BCB744, and BCB743 syllabi are:\nBCB334, BCB744, and BCB743:\n\nAdvanced subject knowledge Deep understanding of the subject matter, its principles, and current research trends.\nCritical thinking Ability to evaluate scientific literature, identify gaps in knowledge, and propose novel research questions.\nCommunication skills Effective presentation of scientific concepts and research findings, both in written and oral formats, to diverse audiences.\nEthical awareness Understanding and adhering to ethical guidelines and principles in research, including responsible conduct of research, data management, and intellectual property rights.\n\nBCB744 and BCB743 additionally develop:\n\nProblem-solving Capacity to develop innovative solutions for complex scientific challenges.\nResearch skills Proficiency in experimental design, data collection, analysis, interpretation, and reporting of scientific findings.\nCollaboration Teamwork and interdisciplinary cooperation in research projects, fostering a productive scientific environment.\nAdaptability Flexibility and openness to new ideas, methods, and technologies, enabling continuous growth and development in the ever-evolving scientific landscape.\nProject management Planning, organising, and executing scientific projects while managing resources and time effectively.\nProfessional development Commitment to lifelong learning, networking, and career advancement through participation in conferences, workshops, and professional organisations.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{smit,_a._j.2023,\n  author = {Smit, A. J.,},\n  title = {Graduate Attributes},\n  date = {2023-04-24},\n  url = {http://tangledbank.netlify.app/pages/graduate_attributes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2023) Graduate attributes. http://tangledbank.netlify.app/pages/graduate_attributes.html."
  },
  {
    "objectID": "pages/technology_infusion.html",
    "href": "pages/technology_infusion.html",
    "title": "Technology infusion and reproducible research",
    "section": "",
    "text": "Coding skills supported by the intertwined technologies of R, RStudio IDE, and Quarto play a key role in my views on shaping modern-day learning and scientific processes. They equip students with the skills to become better collaborative learners and scientists. These technologies offer an extensive range of tools and libraries best known for data analysis, statistics, and visualisation. Coding skills and data analytical skills equip students to develop a deep understanding of complex data sets and derive meaningful insights from them, expanding their analytical thinking and problem-solving skills.\nRecently, Quarto has become tightly integrated into the R ‘ecosystem.’ The website states that Quarto is “an open-source scientific and technical publishing system.” At its heart, it is a dynamic document format based on R and Markdown. It enables students to create interactive, reproducible, well-documented reports, presentations, and websites that combine code, results, and narrative in a single document. The Tangled Bank was entirely developed within Quarto! This approach not only enhances students’ communication skills by encouraging clear and concise explanations of their findings but also promotes transparency and reproducibility in research. By integrating code and results seamlessly, Quarto reduces errors, simplifies the updating process, and ensures that results remain consistent with the underlying data and methods. Quarto is the de facto mode of reporting and communication that students must adopt in BCB744 and BCB743. I am exploring the feasibility of introducing it into BDC334, as feedback indicates that students are keen to develop their coding skills earlier in their undergraduate degrees.\nThe collaborative potential of R and Quarto further empowers students to work effectively in interdisciplinary teams. Students can easily share their code, data, and findings using version control systems, such as Git (as implemented in GitHub), alongside R and Quarto. This fosters a collaborative learning environment where students can collectively learn from each other’s expertise, troubleshoot problems, and develop innovative solutions. Moreover, creating and sharing well-documented Quarto reports improves communication among team members, ensuring everyone is on the same page and facilitating smoother project execution.\nIntegrating these collaborative, open, transparent coding technologies into the teaching, learning, and scientific processes cultivates essential skills in students, such as critical thinking, problem-solving, communication, and collaboration. By leveraging these technologies, students become better equipped to tackle the challenges of today’s data-driven research landscape, ultimately contributing to advancing science and developing innovative solutions to pressing global issues. These skills are also highly sought after in the workplace outside of science and academia and will significantly improve the employability of our graduates regardless of their future career paths.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{smit,_a._j.2023,\n  author = {Smit, A. J.,},\n  title = {Technology Infusion and Reproducible Research},\n  date = {2023-04-24},\n  url = {http://tangledbank.netlify.app/pages/technology_infusion.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2023) Technology infusion and reproducible research. http://tangledbank.netlify.app/pages/technology_infusion.html."
  },
  {
    "objectID": "pages/case_for_promotion.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "href": "pages/case_for_promotion.html#develop-level-3-module-bdc334-and-bsc-hons-modules-bcb744-and-bcb743-for-the-bcb-department",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department",
    "text": "4.1.1. Develop Level-3 module BDC334, and BSc (Hons) modules BCB744 and BCB743 for the BCB Department\nCapitalising on an extensive history of curriculum development5, I have played a vital role in revitalising the core BSc (Hons) module, BCB744 Biostatistics, and in creating the innovative elective BSc (Hons) module, BCB743 Quantitative Ecology. My deep fascination with biological, ecological, and environmental data underpins these modules, fuelling my passion for data processing, analysis, interpretation, and the invaluable insights that emerge from such data-driven enquiries.\n5 I was instrumental in developing South Africa’s first undergraduate Marine Biology curriculum at the University of KwaZulu-Natal in 2007 (with Profs. Perissinotto and Schoeman)6 See a discussion about how I allow modern technologies to influence and shape my teachingR, an open-source software ecosystem extensively adopted by ecologists, is the cornerstone of my core and elective BSc (Hons) modules. The increasing number of research papers and publications in biology and ecology utilising R and its packages attests to its importance. In academic settings such as UWC, Africa, and less developed countries, open-source software removes potential licensing obstacles presented by limited financial resources. This allows universal access to the software, enhancing scientific reporting, collaboration, and the principles of reproducible research, while fostering a culture of technological infusion6.\nAnother new module, BDC334 Global Biogeography & Macroecology, for which I share 50% of the credit for its development, is less data-intensive. This module lays the groundwork for engaging with species and environmental data matrices from which functional ecological processes can be extracted. Recent feedback from students who completed this module in 2022 indicated that exposure to more data-intensive coursework and an introduction to basic coding skills significantly alleviated the anxiety many students feel about coding (scripting). They further suggested that this exposure smoothed their transition into BCB744, the core module they undertake at the start of their BSc (Hons) degrees.\nCollaborative learning is a cornerstone of my teaching approach7, the benefits of which I discuss in my online teaching materials. I use engaging teaching tools to instil interest in my subjects. For example, figures and maps8 serve as critical heuristic devices throughout the modules. The visually appealing and information-rich outcomes of their learning efforts provide an immediate measure of success. In this way, students develop programming skills by breaking down problems into computable parts, whilst also enhancing their visual literacy skills. This engaging and interactive approach is deeply integrated with an agile assessment policy that evaluates teaching and learning9 10. My modules demystify coding, making it more accessible and enjoyable for beginners.\n7 Views on collaborative learning8 Example exercises and bonus, designed to reward and incentivise continued learning towards advanced skills9 Assessment policy for BCB74410 Explanation of modes of assessment11 Module-specific graduate attributes12 The difference between science and data scienceThe skills learnt and the graduate attributes11 developed are designed to produce competencies outside the narrow confines of Biodiversity and Conservation Biology. Transferable core skills include compartmentalising complex problems and finding analytical solutions to problems in diverse fields such as finance, market research, and data science. Many students who graduate with a BSc (Hons) course from the BCB Department will, without requiring further training, have the same skills as someone who has completed a data science course.12 Many of our graduates will not pursue a research-focused career, yet they would like to continue benefiting from the skills gained at the BCB Department.\nStructured outlines of the syllabus, timetables, course content, learning outcomes, required and recommended reading, assessment policies, advice for success (e.g. how to learn to understand13), model answers to old tests and exam questions (e.g. for BDC33414), and much else, are made available for all modules. During 2023 I will continue to build upon existing content and expand my approach to the other module I teach, BDC223 Plant Ecophysiology.\n13 Thoughts about the learning process14 Access to old test and exam questions"
  },
  {
    "objectID": "pages/case_for_promotion.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "href": "pages/case_for_promotion.html#develop-the-tangled-bank-website-in-support-of-undergraduate-and-bsc-hons-modules",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules",
    "text": "4.1.2. Develop the Tangled Bank website in support of undergraduate and BSc (Hons) modules\nWhile I’m not particularly fond of PowerPoint slides, I recognise their utility in structuring lectures. My preference leans towards long-form, information-rich text for delivering in-depth content15. Ideally, I would base my teaching on textbooks, but these are not accessible to all our students. In our fast-paced world, information can quickly become outdated, posing a challenge to addressing students’ evolving learning and knowledge needs. The reality is that many students are averse to reading. To overcome this, I’ve developed and continue to enhance The Tangled Bank, a teaching-oriented website tailored to the needs of students enrolled in my Level-3 and BSc (Hons) modules. Leveraging the website format, I can ensure timely updates of knowledge and technologies in response to the swiftly changing scientific landscape and students’ learning requirements and feedback.\n15 For an example of information rich text, see the example page16 See the ‘vignettes’ menu at the top of The Tangled Bank.17 For example, the FAQ page for BDC22318 See feedback from colleagues about The Tangled BankThe Tangled Bank serves as my main repository for lecture content and a continually expanding knowledge base for guiding research within my areas of focus. This website preserves invaluable behind-the-scenes insights16, contributes to the development of online textbooks, consolidates frequently asked questions about module content which ensures responsiveness to students17, and reinforces BCB Department modules by integrating relevant examples from my colleagues’ work18. The Tangled Bank aids peers in overcoming module-specific challenges, thereby enriching the learning experience.\nProviding students access to long-form written teaching materials and instilling an expectation to engage with this content are pivotal in preparing students for their undergraduate and graduate degree programs. Long-form content facilitates a thorough exploration of ideas, offering context, nuances, and essential background information that enable students to understand complex concepts. By immersing themselves in comprehensive texts, students can cultivate a profound understanding of intricate topics, empowering them to think critically and analytically.\nContrary to summarised bullet points, which can oversimplify and condense information, possibly omitting crucial details, long-form materials motivate students to delve deep into a subject and contemplate various perspectives. This approach fosters intellectual curiosity and instills a genuine interest in the subject, promoting a culture of lifelong learning. Engaging with long-form content allows the motivated student to build a robust knowledge base rooted in self-driven learning, forming a firm foundation for their future academic and professional pursuits. As an educator, this is my aspiration.\nFurthermore, interacting with long-form written materials enhances students’ reading comprehension skills. As they sift through dense texts, they learn to distinguish main ideas, supporting arguments, and potential counterarguments. This process refines their capacity to analyse and evaluate information—an essential skill in both academic and professional environments. Improving this skill is particularly crucial for the younger generation.\nBy supplying students with comprehensive content, I aim to foster a deeper appreciation for their chosen field, thus equipping them for success in their academic and professional journeys.\nLastly, The Tangled Bank strives to provide a detailed overview and breakdown of each module’s syllabus, including:\n\nan up-to-date timetable and links to each lecture’s material and assessments,\ninformation about the desired learning outcomes and graduate attributes,\nadditional supporting information,\nprerequisites,\nthe method of instruction,\nviews on the benefits of colaborative learning,\nattendance policies,\nassessment policies, and\nsupport.\n\nPlease refer to BDC33419, BCB744,20, and BCB74321 for the above-mentioned information.\n19 The BCB744 module syllabus and course outline20 The BCB743 syllabus and course outline21 The BDC334 syllabus and course outline"
  },
  {
    "objectID": "pages/case_for_promotion.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "href": "pages/case_for_promotion.html#feedback-from-external-peer-reviewers-and-students-about-above-modules-taught",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught",
    "text": "4.1.3. Feedback from external (peer) reviewers and students about above modules taught\nThe following feedback was received from Prof. Sophie von der Heyden for BCB74322 following her assessment of the module in her capacity of External Module Evaluator for the BCB BSc (Hons) Programme: “This is an excellent course; I really appreciate that everything is online and very easy to follow. The course is appropriate and challenging at the Honours level, but there also seems excellent support for the students. Really a standout module.” Further, she says, ”There was a wide range of marks, from 45 – 88%, with only one student […] failing this module. Given that students can really struggle with R, it was good to see how well the class did overall. I think part of this is the breakdown into the multiple assignments, which allows students to build on their knowledge as the tasks get more difficult, rather than being overwhelmed with one large assignment.”\n22 Prof. Sophie von der Heyden’s feedback about BCB743 in 202223 Prof. Sophie von der Heyden’s feedback about BCB744 in 2022About BCB744,23 she says, “As with BCB743, I was very impressed by this course, particularly how easy it is to navigate around the online component. I am sure that the students will be able to access all the necessary components fairly easily. The course is very much at the level of Honours and I hope that for the final projects the students utilize their learning from this course.”\nHowever, Prof von der Heydon’s comment on the question about whether the marks were assigned appropriately, she said, “This is a little difficult to comment on as I could not see how the marks were awarded, but given the consistency of marks for each student, I think that the marks are all appropriate.”\nSince the module content is continually being developed, expanded, and improved, I addressed Prof. von der Heydon’s concern about mark allocation by providing clear assessment policies for BCB74424, BCB74325, and BDC334.26 Further, the module content on The Tangled Bank has dramatically improved in all aspects since the modules were last evaluated at the end of 2022.\n24 BCB744 assessment policy25 BCB743 assessment policy26 BDC334 assessment policy27 Student feedback about BDC223, BDC334, BCB744, and BCB743 are available at on Google DriveFeedback from students about the modules is also available.27 Six students from a class of 14 responded to the module evaluation forms in 2022. Feedback about students’ experience with the module was positive for most of the questions, but 50% of the respondents felt that better feedback could be given to individual tasks. A third of the sample also indicated they felt uncertain about the module’s expectations.\nEighteen students took BCB744 in 2022, and eight provided feedback on the module. As with BCB743, the feedback was similar. Four students felt they could benefit from more comprehensive feedback, and three respondents felt somewhat uncertain about my expectations of them (including the quality of their work). Additionally, two students felt I could better explain concepts and give them more time to understand them. Another negative comment given by two students was that they could be better empowered to explore a variety of sources better to complete assessment tasks.\nThe BDC334 class comprised 41 students in 2022, and only five students tried to provide feedback. One person felt a mismatch between the assessment and the module’s content. Five students thought feedback on individual assessments could be better. There was also one instance of dissatisfaction with the following: sufficient time for communication, my effort to understand their challenges, and uncertainty about expectations. Feedback on BDC223 in 2022 was poor, with only nine responses. Their satisfaction with the module was mixed and polarised into two distinct groups. About 50% of respondents provided much of the same feedback as I received for BCB744, BCB743, and BDC334, and these people felt that feedback on individual assignments could be better. The other half had more negative experiences and I received negative feedback for several other questions. My experience with this class in 2022 was anomalous, as it is singular as the worst class I have ever taught at University. Ever."
  },
  {
    "objectID": "pages/case_for_promotion.html#tangled-bank-vignettes-and-reproducible-research",
    "href": "pages/case_for_promotion.html#tangled-bank-vignettes-and-reproducible-research",
    "title": "Ad Personam Promotion 2023",
    "section": "4.1.4. Tangled Bank vignettes and reproducible research",
    "text": "4.1.4. Tangled Bank vignettes and reproducible research\nInterdisciplinary research brings together a variety of expertise, resulting in challenges related to teamwork, data sharing, and coding. The importance of transparency in research methodologies, like reproducible research,28 is ever increasing. Conforming to FAIR principles, international standards, and discipline-specific norms is indispensable. Even though software provides solutions, numerous researchers require assistance to stay abreast and capitalise on new expectations and possibilities.\n28 See my essay on eResearch and reproducible researchPhD students typically devote 3-4 months to active thesis writing, which often serves as the only tangible evidence of degree completion. However, the vast majority of the learning and methodological skills developed over the remaining 33-44 months often become lost and unshared, leading to duplicated research efforts and restricted knowledge transfer. This failure to share behind-the-scenes solutions often results in non-reproducible research and collaboration difficulties, sometimes even contributing to public mistrust in science. Furthermore, better scalability is needed as datasets and complexities grow, and inefficiencies due to inadequate documentation of data selection, filtering, metadata tracking, and processing changes need addressing.\nThe Tangled Bank is designed to encourage knowledge retention and transfer, both of which are crucial for success in the information economy. To tackle these issues, my research students craft lab notebooks using tools like RStudio or Jupyter Lab/Notebooks and monitor version changes with git (e.g., GitHub). These notebooks combine code and text, automatically updating results as new data become available, thereby ensuring reproducibility in their work.29 30 31 I emphasise these same principles in both undergraduate and postgraduate courses I teach. The website also includes a series of vignettes32 that capture some of the analytical data workflows that often raise questions. These vignettes will continually be updated, and more examples documenting my own and my colleagues’ data and statistical analysis challenges will be preserved here for posterity.\n29 Dr Robert Schlegel’s GitHub page30 Ms Amieroh Abrahams’s GitHub page31 Mr Ross Coppin’s GitHub page32 Examples of vignettes may be accessed at The Tangled Bank under the ‘vignettes’ menu at the top.33 The heatwaveR website—see the vignettes in the top menu.Other vignettes are at the heatwaveR website.33"
  },
  {
    "objectID": "pages/case_for_promotion.html#successful-and-prolific-funding-attraction",
    "href": "pages/case_for_promotion.html#successful-and-prolific-funding-attraction",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.1. Successful and prolific funding attraction",
    "text": "4.2.1. Successful and prolific funding attraction\nMy H-index on Google Scholar is currently 2934, which ranks as the second highest in the BCB Department. As of 29 June 2023, the site has recorded a total of 4,167 citations, 2112 of which were garnered since 2018. Since joining UWC in 2014, my annual publication average stands at approximately five; however, this rate has somewhat dropped in light of the COVID-19 pandemic. With the induction of a new cohort of students into my postgraduate research group, I anticipate a resurgence in the publication rate.\n34 See my Google Scholar page35 List of national and international research funding receivedMy leadership and management skills, cultivated over the past eight years, are demonstrated by my significant success in securing funding from national and international research programmes35. Moreover, I’ve successfully seen these programmes through to completion, aligning with well-defined goals and objectives. Since 2014, these research endeavours have cumulatively raised an estimated ZAR 28.74 million, bolstering the sustainability of research efforts for myself, my collaborators, and my students.\nHistorically, I have primarily relied on the NRF for funding. However, in recent years, I have been diversifying my collaborations internationally. This strategy is facilitated by accessing global funding streams, such as those provided by the European Union, the Belmont Forum, and the SANOCEAN programme. These sources not only leverage funding from partnering countries, but they also foster a degree of collaboration that exceeds what is typically feasible with South Africa-centric funding.\nPreviously, I held a C2 rating, but chose to let it lapse after thoughtful consideration. I’ve expanded on my views regarding the rating system elsewhere36. Thus far, I’ve found that having an NRF rating does not necessarily enhance the likelihood of obtaining research funding.\n36 My thoughts about the NRF rating system and maintaining my own rating"
  },
  {
    "objectID": "pages/case_for_promotion.html#development-of-r-packages-in-marine-heatwave-analysis",
    "href": "pages/case_for_promotion.html#development-of-r-packages-in-marine-heatwave-analysis",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.2. Development of R packages in marine heatwave analysis",
    "text": "4.2.2. Development of R packages in marine heatwave analysis\nOne of my most distinctive and significant research contributions is the creation of two R packages: RmarineHeatWaves37 and heatwaveR38. These tools emerged as a response to the formal definition of marine heatwaves proposed by Alistair Hobday and his team in 2016. The algorithm to detect marine heatwaves based on standardised metrics was first published as an R package under the name RmarineHeatWaves, and later updated to heatwaveR in 2017. This software has since been downloaded more than 32k times39 by the international scientific community and has been cited in over 150 peer-reviewed papers since 201840. I, alongside Dr. Robert Schlegel, my former UWC PhD student, continue to maintain and enhance this package, introducing new functionalities in response to the needs of our user community.\n37 The RmarineHeatWaves documentation.38 heatwaveR. Also see the GitHub page39 The RmarineHeatWaves documentation.40 This number is hard to track, but a search in Google Scholar for the term “heatwaveR” (inverted commas included) yields at least 150 citations. A shorter list of the citations is provided on the heatwaveR website. Notable examples of high-impact publications are provided here41 Examples of cross-discipline research in marine heatwaves promoted by the heatwaveR package are provided here42 Evidence of the application of the heatwaveR package outside of the initially intended field of application, marine science, here.The influence of this R package on the global marine heatwave research community cannot be overstated. The standardisation of metrics it offers facilitates a more consistent global study of these events. Prior to its release, these tools were largely available only to physical oceanographers who primarily use Python; publishing it in R extended its reach to biologists and ecologists. This has sparked interdisciplinary collaboration across fields like oceanography, climatology, and ecology41. Interestingly, it is now being applied in areas beyond its initial intended marine scope, such as public health42, demonstrating its broad and unexpected utility.\nGiven the consistency in reporting Marine Heat Wave (MHW) metrics, the quality of decision-making by policy-makers and resource managers has been significantly enhanced. For instance, gaining a more refined understanding of MHWs aids in devising strategies to mitigate the environmental repercussions of extreme thermal events, as well as adapting to their influences on fisheries and other marine resources43.\n43 For studies that have used metrics calculated by heatwaveR in support of policy development around the management of marine living resources, see this list44 Evidence of examples where such novel research questions and hypotheses have been addressedFurther, heatwaveR also led to the development of novel research questions and hypotheses that better analyse and compare MHWs across different periods and regions and employ the metrics to design creative experiments that better link ecological impacts to precisely quantifiable properties of the temperature record.44\nFinally, the heatwaveR package raises public awareness about MHWs and their impacts on marine ecosystems by making it easier for researchers to communicate their findings to a broader audience. For example, the marine heatwave tracker built by Dr Schlegel uses the heatwaveR package in the background.45\n45 Various online trackers of marine heatwaves use heatwaveR as the underlying processing engine"
  },
  {
    "objectID": "pages/case_for_promotion.html#student-supervision",
    "href": "pages/case_for_promotion.html#student-supervision",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.3. Student supervision",
    "text": "4.2.3. Student supervision\nMy UWC student supervision record is provided in my e-Portfolio.46 The record indicates 16 BSc (Hons) graduates, 11 MSc/MPhil graduates, and 7 PhD graduates. Appearing on the online NRF online system as active and continuing is Mr Phumlile Cotiyane, a PhD candidate registered with SAEON’s Elwandle Node whom I am co-supervising. Including postgraduate supervision prior to my tenure at the UWC in 2014 brings my career total to 57 graduates, across all levels.\n46 Extract from the NRFOnline system listing most of my post-graduate studentsI have five active MSc students (Ms Cayley Cammel, Mr McQuwaen Moonoosamy, Mr Jesse Philips, Mr Tom Spencer-Hicken, and Ms Carlin Landsberg) and four active BSc (Hons) candidates, Ms Aailyah Samsodien, Ms Zoë-Angelique Petersen, Mr Taine Trimmel, and Mr Isma-eel Jattiem. Since these students receive free-standing bursaries from the NRF, their names do not yet appear in my NRF database under the list of students associated with my research profile. This also applies to Ms Zara Prew, an active PhD student in my research group.\nRoughly 49% of all the individuals, above, are of previously disadvantaged backgrounds, and 12% were with my role as co-supervisor.\nI have had three post-docs in my lab: Dr Rob Williamson, Dr Christo Rautenbach, and Dr David Dyer, and the latter will be with me until December 2023."
  },
  {
    "objectID": "pages/case_for_promotion.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "href": "pages/case_for_promotion.html#the-south-african-coastal-seawater-temperature-network-sactn",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)",
    "text": "4.2.4. The South African Coastal Seawater Temperature Network (SACTN)\nRelated to my interest in marine heatwaves, I have also been instrumental in developing the South African Coastal Seawater Temperature Network (SACTN).47 This work brings together, for the first time, the disparate seawater temperature records measured over up to 4 decades by the KwaZulu-Natal Sharks Board (KZNSB), Ezemvelo KZN Wildlife (EKZNW), the South African Weather Service (SAWS), the Department of Forestry, Fisheries and Environment (DFFE), the South African Environmental Observation Network (SAEON), and the UWC. 48 This paper has been cited 166 times and instrumental in several other of my own frequently cited publications49 and stimulated further avenues of research regarding the variability of ocean temperature, including the research on marine heatwaves.\n47 The The South African Coastal Seawater Temperature Network (SACTN) GitHub page from where data can be downloaded48 Smit et al (2013)49 Schlegel et al (2017a) and Schlegel et al (2017b)"
  },
  {
    "objectID": "pages/case_for_promotion.html#editorial-contributions",
    "href": "pages/case_for_promotion.html#editorial-contributions",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.5. Editorial contributions",
    "text": "4.2.5. Editorial contributions\n2018–present Associate Editor, Aquatic Botany.50\n50 Associate Editor for Aquatic Botany51 My Reviewer’s profile on Loop for editorial contributions to Frontiers in Ecology & Evolution2020–present Associate Editor Frontiers in Ecology & Evolution and Frontiers Topic Editor,51 Managing Deep-sea and Open Ocean Ecosystems at Ocean Basin Scale - Volume 2\n2023–present Guest Editor, Special Issue, Botanica Marina\nIn addition, reviewing done for Frontiers in Marine Science; Plos ONE; Proceedings of the National Academy of Sciences; Journal of Phycology; Estuarine Coastal & Shelf Science; African Journal of Marine Science; Hydrobiologia; Journal of Applied Phycology; Journal of Marine Systems; Marine Biology; Marine Ecology; Diversity & Distributions; Ecology & Evolution; Atmosfera; Big Earth Data; Botanica Marina; Environmental Pollution; Science of the Total Environment; Frontiers Ecology And Evolution; Meteorology and Atmospheric Physics; One Health; International Journal of Environmental Research and Public Health, Marine Pollution Bulletin."
  },
  {
    "objectID": "pages/case_for_promotion.html#future-research",
    "href": "pages/case_for_promotion.html#future-research",
    "title": "Ad Personam Promotion 2023",
    "section": "4.2.6. Future research",
    "text": "4.2.6. Future research\nMy future research endeavours will focus on investigating the interplay between coastal marine extreme events and the shifting climate. The objective is to ensure that this research is both relevant and beneficial to a broad spectrum of actors who gain from nature’s contributions. Building upon the foundation of my BlueConnect and EXEBUS programmes, the scope of my work will increasingly embody a transdisciplinary approach. This will be achieved through collaborations with experts in economics, sociology, and maritime law, rendering the research relevant to both society and industry. Within this field, my specific interests—the biogeochemical function of kelp and the detection and statistical analysis of extreme events in environmental time series—will be deployed to establish links between environmental drivers and their impacts on ecosystems and society."
  },
  {
    "objectID": "pages/case_for_promotion.html#academic-lead-kelp-scientific-collaboration-ppp",
    "href": "pages/case_for_promotion.html#academic-lead-kelp-scientific-collaboration-ppp",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP",
    "text": "4.3.1. Academic Lead, Kelp Scientific Collaboration PPP\nI have been the academic lead of the Kelp Scientific Collaboration52 consortium since September 2021 (ongoing). The consortium is a Public-Private-Partnership whose intention is to foster collaboration around kelp ecosystems for the betterment of sustainable practices that concern the industry and for scientific advancement on kelp ecological functioning.\n52 Kelp Scientific Collaboration mission statement"
  },
  {
    "objectID": "pages/case_for_promotion.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "href": "pages/case_for_promotion.html#a-research-project-funded-by-sanocean-blueconnect-about-the-perceived-value-of-south-african-kelp",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp",
    "text": "4.3.2. A research project, funded by SANOCEAN BlueConnect, about the perceived value of South African kelp\nThis project on the perceived value of kelp53 was heavily concerned with people’s relationship with kelp and produced several outputs:\n53 Perceived Value of Kelp\nJanuary 2022 Premier of Akshata Mehta’s movie, Kelp, South Africa’s Golden Forests (funded by myself through BlueConnect, and provided concept and oversight).54 The short film was first shown at the annual PSSA meeting in Arniston and subsequently entered into various nature documentary festivals. It is also on YouTube, where it has received 5.3k views.\nSeptember 2021 Supervise Akshata Mehta’s MPhil Thesis, “Golden Forests” of the Sea: Assessing Values and Perceptions of Kelp in the Western Cape Region of South Africa. This work continues to yield stakeholder engagements with community members and the seaweed industry of Southern Africa.55\n\n54 Kelp, South Africa’s Golden Forests on YouTube55 Akshata Mehta’s MPhil thesis"
  },
  {
    "objectID": "pages/case_for_promotion.html#blueconnect-engagements",
    "href": "pages/case_for_promotion.html#blueconnect-engagements",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.3. BlueConnect engagements",
    "text": "4.3.3. BlueConnect engagements\n\nContributing author to Chapter 3, UNEP report on global kelp forests.56\nGlobal Ecological Assessment of Kelp, June 15-17, 2022, in Arendal, Norway.57 This work stems directly from the SANOCEAN BlueConnect Programme, of which I am the South African PI. The work intended to bring together global kelp experts to evaluate kelp forests.\nBlueConnect Kelp Ecosystem 10-day Field Course, 16 – 26 March 2020, Cape Town and De Hoop Nature Reserve – this workshop was affected by COVID-19 and all field work was cancelled; it proceeded as an online course. Ten students from South Africa and Norway participated.58\nNovember 2019: Lead workshop with the kelp industry to gain perspectives about challenges they face about environmental and governance concerns they experience.\n\n56 United Nations Environment Programme, & Norwegian Blue Forests Network (2023). Into the Blue: Securing a Sustainable Future for Kelp Forests.57 Invitation letter to the GEAK workshop held in Norway58 BlueConnect March 2020 Field Course"
  },
  {
    "objectID": "pages/case_for_promotion.html#exebus-engagements",
    "href": "pages/case_for_promotion.html#exebus-engagements",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.4. EXEBUS engagements",
    "text": "4.3.4. EXEBUS engagements\nEXEBUS59 60 undertakes an Integrated Ecosystem Assessment (IEA) to establish the roles, trends, and range of variability and the extremities of natural and anthropogenic geophysical, biological, governance, socio-economic features and phenomena, and assess their impact on ecological, sociological, governance, and macroeconomic systems and processes in the Benguela Current Large Marine Ecosystem (BCLME) of South Africa (SA), Namibia, and Angola. The goal is to strengthen the rational basis for management on relevant spatial and temporal scales (up to 2070).\n59 Video on YouTube about EXEBUS60 EXEBUS websiteTo further these interests, my Team and I have had stakeholder engagements with (ongoing):\n\n2022 The Benguela Current Convention\n2022 The kelp industry in South Africa\n2022 An assortment of stakeholders (academia, the Ministry of Fisheries, University of Namibia academics)\n2023 Users and port operators of the Port of Cape Town"
  },
  {
    "objectID": "pages/case_for_promotion.html#other-community-engagements-and-capacity-building-contributions",
    "href": "pages/case_for_promotion.html#other-community-engagements-and-capacity-building-contributions",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.5. Other community engagements and capacity-building contributions",
    "text": "4.3.5. Other community engagements and capacity-building contributions\nI am currently involved with Cape Nature in initiatives aimed at building capacities among fishermen in the Helderberg region61. I am also an active participant in the Kogelberg Marine Working Group, which is dedicated to discussing and implementing conservation management initiatives in the Kogelberg region62.\n61 See most recent invitation to participate in a capacity building initiative62 Invitation quarterly Kogelberg Marine Working Group meetingSince 2017, I have been training students and budding scientists from previously disadvantaged Higher Education Institutions (HEIs) and NRF National Facilities. This includes teaching R courses at the University of Zululand, Walter Sisulu University, SAIAB, and SAEON. In the process of these collaborations, I regularly engage with young academics freshly appointed to their positions at these universities. The objective is to foster research proficiency and academic confidence, thereby amplifying their potential to positively influence subsequent generations of graduates.\nI have recently received and accepted an invitation from OceanHub Africa to spearhead a project at the Ocean Hackathon as a Challenge Owner. This platform allows me to interact with professional coders and jointly work towards data-driven solutions to address certain marine conservation and management challenges in the region63.\n63 See invitation letter"
  },
  {
    "objectID": "pages/case_for_promotion.html#covid-19-environmental-research-group",
    "href": "pages/case_for_promotion.html#covid-19-environmental-research-group",
    "title": "Ad Personam Promotion 2023",
    "section": "4.3.6. CoVID-19 Environmental Research Group",
    "text": "4.3.6. CoVID-19 Environmental Research Group\nDuring the first year of CoVID-19 I was part of the CoVID-19 Environmental reference Group (CERG) which aimed to establish the link between seasonality and the prevalence and spread of CoVID-19 in developing countries. An output of the work is the paper Smit et al. (2020).64\n64 Smit et al (2020) about CoVID-19"
  },
  {
    "objectID": "BDC334/Lab-03-biodiversity.html",
    "href": "BDC334/Lab-03-biodiversity.html",
    "title": "Lab 3. Quantifying Biodiversity",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.\nThe IUCN definition considers a diversity of diversity concepts. This module looks at diversity only at the species level (species diversity). However, we can also approach macroecological problems from phylogenetic and functional (and other) diversity concepts of view. Functional and phylogenetic diversity ideas will be introduced in the BDC743 module Quantitative Ecology.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 3. Quantifying Biodiversity"
    ]
  },
  {
    "objectID": "BDC334/Lab-03-biodiversity.html#preparation",
    "href": "BDC334/Lab-03-biodiversity.html#preparation",
    "title": "Lab 3. Quantifying Biodiversity",
    "section": "Preparation",
    "text": "Preparation\n\nThe South African Seaweed Data\nIn these examples, we will use the seaweed data of Smit et al. (2017). Please make sure that you read this paper. An additional file describing the background to the data is available here (Figure 1).\n\n\n\n\n\n\nFigure 1: The coastal sections and associated seawater temperature profile associated with the study by Smit et al. (2017).\n\n\n\nOne of the datasets, \\(Y\\) (in the file SeaweedSpp.csv), comprises updated distribution records of 847 macroalgal species within each of 58 × 50 km-long sections of the South African coast (Bolton and Stegenga 2002). The dataset captures ca. 90% of the known seaweed flora of South Africa, but excludes some very small and/or very rare species for which data are insufficient. The data are from verifiable literature sources and John Bolton and Rob Anderson’s collections, assembled from information collected by teams of phycologists over three decades (Bolton 1986; Stegenga et al. 1997; Bolton and Stegenga 2002; De Clerck et al. 2005). Another file, \\(E\\) (in env.csv), is a dataset of in situ coastal seawater temperatures derived from daily measurements over 40 years (Smit et al. 2013).\n\n\nSetting Up the Analysis Environment\nWe will use R, so first, we must find, install and load various packages. Some packages will be available on CRAN and can be accessed and installed the usual way, but you will need to download others from R Forge.\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(betapart)\nlibrary(BiodiversityR) # this package may at times be problematic to install\n\n\n\nA Look at the Data\nLet’s load the data and see how it is structured:\n\nspp &lt;- read.csv('../data/seaweed/SeaweedSpp.csv')\nspp &lt;- dplyr::select(spp, -1)\n\n# Lets look at the data:\ndim(spp)\n\n[1]  58 847\n\n\nWe see that our dataset has 58 rows and 847 columns. What is in the columns and rows? Start with the first five rows and five columns:\n\nspp[1:5, 1:5]\n\n  ACECAL ACEMOE ACRVIR AROSP1 ANAWRI\n1      0      0      0      0      0\n2      0      0      0      0      0\n3      0      0      0      0      0\n4      0      0      0      0      0\n5      0      0      0      0      0\n\n\nNow the last five rows and five columns:\n\nspp[(nrow(spp) - 5):nrow(spp), (ncol(spp) - 5):ncol(spp)]\n\n   WOMKWA WOMPAC WRAARG WRAPUR WURMIN ZONSEM\n53      0      0      1      0      0      0\n54      0      0      1      0      0      0\n55      0      0      1      0      0      0\n56      0      1      1      0      1      0\n57      1      0      1      0      1      0\n58      0      0      1      0      1      0\n\n\nSo, each row corresponds to a site (i.e. each of the coastal sections), and each column contains a species. We arrange the species alphabetically and use a six-letter code to identify them.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 3. Quantifying Biodiversity"
    ]
  },
  {
    "objectID": "BDC334/Lab-03-biodiversity.html#species-data",
    "href": "BDC334/Lab-03-biodiversity.html#species-data",
    "title": "Lab 3. Quantifying Biodiversity",
    "section": "Species Data",
    "text": "Species Data\nWhen ecologists talk about species diversity, they typically consider the characteristics of biological communities in a specific habitat, ecological community, or ecosystem. Species diversity considers three essential concepts about how species are distributed in space: their richness, abundance, and evenness. We can express each of these as biodiversity metrics that allow us to compare communities in space and time.\nWhen ecologists talk about ‘biodiversity’, they might not necessarily be interested in all the plants and animals and things that are neither plant nor animal that occur at a particular place. Some ecologists are interested in ants and moths. Others might find fish more insightful. Some even like marine mammals! I prefer seaweed. The analysis of biodiversity data might often be constrained to some higher-level taxon, such as all angiosperms in a landscape, reptiles, etc. (but we sample all species in the higher-level taxon). Some ecological questions benefit from comparisons of diversity assessments among selected taxa (avifauna vs small mammals, for example), as this focus might address some particular ecological hypothesis. The bird vs small mammal comparison might reveal how barriers such as streams and rivers structure biodiversity patterns. In our examples, we will use such focused datasets.\nHere we look at the various measures of biodiversity, viz. \\(\\alpha\\)-, \\(\\gamma\\)- and \\(\\beta\\)-diversity. David Zelený, in his Analysis of community data in R, provides deeper analysis and compulsory reading.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 3. Quantifying Biodiversity"
    ]
  },
  {
    "objectID": "BDC334/Lab-03-biodiversity.html#three-measures-of-biodiversity-alpha--gamma--beta-diversity",
    "href": "BDC334/Lab-03-biodiversity.html#three-measures-of-biodiversity-alpha--gamma--beta-diversity",
    "title": "Lab 3. Quantifying Biodiversity",
    "section": "Three Measures of Biodiversity: \\(\\alpha\\)-, \\(\\gamma\\)-, \\(\\beta\\)-Diversity",
    "text": "Three Measures of Biodiversity: \\(\\alpha\\)-, \\(\\gamma\\)-, \\(\\beta\\)-Diversity\nWhittaker (1972) coined three measures of biodiversity, and the concepts were ‘modernised’ by Jurasinski et al. (2009). The concepts represent the measurement of biodiversity across different spatial scales. \\(\\alpha\\)- and \\(\\gamma\\)-diversity express the total number of species in an area. The first, \\(\\alpha\\)-diversity, represents the number of species at the small (local) scale, such as, for example, within a sampling unit like a quadrat, transect, plot, or trawl. Alternatively, maybe the research question represents the local scale by several sampling units nesting within a small patch of landscape and defines the mean species richness within this patch as local. Multiples (sampling units or patches) are nested within a larger region (or ecosystem) and serve as replicates. The complete number of species across all of these replicates indicates the diversity at a larger scale—this is called \\(\\gamma\\)-diversity. \\(\\beta\\)-diversity refers to the change in species composition among samples (sites).\nBy now, you will have received a brief Introduction to R, and we can proceed with looking at some of the measures of biodiversity. We will start by using data on the seaweeds of South Africa to demonstrate some ideas around diversity measures. The vegan1 (for vegetation analysis) package (Oksanen et al. 2022) offers various functions to calculate diversity indices. I will demonstrate some of these functions below.\n1 I am by no means an advocate for veganism.\nAlpha-Diversity\nWe can represent \\(\\alpha\\)-diversity in three ways:\n\nas species richness, \\(S\\);\nas a univariate diversity index, such as the \\(\\alpha\\) parameter of Fisher’s log-series, Shannon diversity, \\(H'\\), Simpson’s diversity, \\(\\lambda\\); or\nSpecies evenness, e.g. Pielou’s evenness, \\(J\\).\n\nWe will work through each in turn.\n\nSpecies Richness, \\(S\\)\nFirst, is species richness, which we denote by the symbol \\(S\\). This is the simplest measure of \\(\\alpha\\)-diversity, counting the number of species (or another taxonomic level) present in a given community or sample. It doesn’t consider the abundance of species.\nIn the seaweed biodiversity data, I count the number of species within each of the sections. This is because we view each coastal section as the local scale (the smallest unit of sampling).\nThe preferred option for calculating species richness is the specnumber() function in vegan:\n\n1specnumber(spp, MARGIN = 1)\n\n\n1\n\nThe MARGIN = 1 argument tells R to calculate the number of species within each row (site).\n\n\n\n\n [1] 138 139 139 140 143 143 143 145 149 148 159 162 208 147 168 204 269 276 280\n[20] 265 265 283 269 279 281 295 290 290 299 295 311 317 298 299 301 315 308 327\n[39] 340 315 315 302 311 280 300 282 283 321 319 319 330 293 291 292 294 313 333\n[58] 316\n\n\nThe data output is easier to understand if we display it as a tibble():\n\nspp_richness &lt;- tibble(section = 1:58,\n                       richness = specnumber(spp, MARGIN = 1))\nhead(spp_richness)\n\n# A tibble: 6 × 2\n  section richness\n    &lt;int&gt;    &lt;int&gt;\n1       1      138\n2       2      139\n3       3      139\n4       4      140\n5       5      143\n6       6      143\n\n\nThe diversityresult() function in the BiodiversityR package can do the same (sometimes this package is difficult to install due to various software dependencies that might be required for the package to load properly—do not be sad if this method does not work):\n\nspp_richness &lt;- diversityresult(spp, index = 'richness',\n                                method = 'each site')\nhead(spp_richness)\n\nNow we make a plot seen in Figure 2:\n\nggplot(data = spp_richness, (aes(x = 1:58, y = richness))) +\n  geom_line(size = 1.2, colour = \"indianred\") +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Species richness\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nFigure 2: The seaweed species richness, \\(S\\), within each of the coastal sections along the shore of South Africa.\n\n\n\n\n\nIn other instances, it makes more sense to calculate the mean species richness of all the sampling units (e.g. quadrats) taken inside the ecosystem of interest. How you calculate and present species richness depend on your research question and so you will have to decide based on your data and study.\nIn the seaweed study, the mean ± SD species richness across all of the 58 coastal sections is:\n\nround(mean(spp_richness$richness), 2)\n\n[1] 259.24\n\nround(sd(spp_richness$richness), 2)\n\n[1] 68.03\n\n\n\n\nUnivariate Diversity Indices\nThe second way we can express \\(\\alpha\\)-diversity is to use one of the univariate diversity indices. The choice of which index to use should be informed by the extent to which one wants to emphasise richness or evenness. Species richness, \\(S\\), does not consider evenness as it is all about richness (obviously). Simpson’s \\(\\lambda\\) emphasises evenness a lot more. Shannon’s \\(H'\\) is somewhere in the middle.\nShannon’s \\(H'\\) is sometimes called Shannon’s diversity, the Shannon-Wiener index, the Shannon-Weaver index, or the Shannon entropy. This is a more nuanced measure that considers both species richness and evenness (how evenly individuals are distributed across different species).\nIt is calculated as:\n\\[H' = -\\sum_{i=1}^{S} p_{i} \\ln p_{i}\\] where \\(p_{i}\\) is the proportion of individuals belonging to the \\(i\\)th species, and \\(S\\) is the species richness.\nSimpson’s \\(\\lambda\\), or simply the Simpson index, is a measure that represents the probability that two individuals randomly selected from a sample will belong to the same species. It is calculated as:\n\\[\\displaystyle \\lambda = \\sum_{i=1}^{S} p_{i}^{2}\\] where \\(S\\) is the species richness and \\(p_{i}\\) is the relative abundance of the \\(i\\)th species.\nFisher’s \\(\\alpha\\) estimates the \\(\\alpha\\) parameter of Fisher’s logarithmic series (see functions fisher.alpha() and fisherfit()). The estimation is possible only for actual counts (i.e. integers) of individuals, so it will not work for per cent cover, biomass, and other measures that real numbers can express. It’s especially useful for comparing the diversity of samples with different total abundances. We will get to this function later under Fisher’s logarithmic series.\nExcept for Fisher’s-\\(\\alpha\\), we cannot calculate these for the seaweed data, because, in order to do so, we require abundance data—the seaweed data are presence-absence only. Let us load a fictitious dataset of the diversity of three different communities of plants, with each community corresponding to a different light environment (dim, mid, and high light):\n\nlight &lt;- read.csv(\"../data/light_levels.csv\")\nlight\n\n        Site    A    B    C    D    E    F\n1  low_light 0.75 0.62 0.24 0.33 0.21 0.14\n2  mid_light 0.38 0.15 0.52 0.57 0.28 0.29\n3 high_light 0.08 0.15 0.18 0.52 0.54 0.56\n\n\nWe can see above that instead of having data with 1s and 0s for presence-absence, here we have some values that indicate the relative number of individuals belonging to each of the species in the three light environments. We calculate species richness (as before), and also the Shannon and Simpson indices using vegan’s diversity() function:\n\nlight_div &lt;- tibble(\n  site = c(\"low_light\", \"mid_light\", \"high_light\"),\n  richness = specnumber(light[, 2:7], MARGIN = 1),\n  shannon = round(diversity(light[, 2:7], MARGIN = 1, index = \"shannon\"), 2),\n  simpson = round(diversity(light[, 2:7], MARGIN = 1, index = \"simpson\"), 2)\n)\nlight_div\n\n# A tibble: 3 × 4\n  site       richness shannon simpson\n  &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 low_light         6    1.62    0.78\n2 mid_light         6    1.71    0.81\n3 high_light        6    1.59    0.77\n\n\n\n\n\n\n\n\n\n\n\n\nEvenness refers to the shape of a species abundance distribution, which suggests the relative abundance of different species.\nOne index for evenness is Pielou’s evenness, \\(J\\):\n\\[J = \\frac{H^{\\prime}} {log(S)}\\]\nwhere \\(H'\\) is Shannon’s diversity index, and \\(S\\) the number of species (i.e. \\(S\\)).\nTo calculate Pielou’s evenness index for the light data, we can do this:\n\nH &lt;- diversity(light[, 2:7], MARGIN = 1, index = \"shannon\")\n\nJ &lt;- H/log(specnumber(light[, 2:7]))\nround(J, 2)\n\n[1] 0.91 0.95 0.89\n\n\nBerger-Parker Index indicates the proportion of the community that the most abundant species represents. It is given by the formula:\n\\[d = \\frac{N_{max}}{N}\\] where \\(N_{max}\\) is the number of individuals of the most common species and \\(N\\) is the total number of individuals in the sample.\nChao1 and ACE are estimators often used to predict the total species richness in a community based on the number of rare species observed in samples.\n\n\n\nGamma-Diversity\nReturning to the seaweed data, \\(Y\\), let us now look at \\(\\gamma\\)-diversity—this would be the total number of species along the South African coastline in all 58 coastal sections. Since each column represents one species, and the dataset contains data collected at each of the 58 sites (the number of rows), we can do:\n\n1ncol(spp)\n\n\n1\n\nThe number of columns gives the total number of species in this example.\n\n\n\n\n[1] 847\n\n\nWe can also use:\n\ndiversityresult(spp, index = 'richness', method = 'pooled')\n\n       richness\npooled      846\n\n\n\n\n\n\n\n\n\n\n\nLab 3\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nWhy is there a difference between the two?\nWhich is correct?\n\n\n\nThink before you calculate \\(\\gamma\\)-diversity for your own data as it might not be as simple as here!\n\n\nBeta-Diversity\n\nWhittaker’s \\(\\beta\\)-Diversity\nThe first measure of \\(\\beta\\)-diversity comes from Whittaker (1960) and is called true \\(\\beta\\)-diversity. In this instance, divide the \\(\\gamma\\)-diversity for the region by the \\(\\alpha\\)-diversity for a specific coastal section. We can calculate it all at once for the whole dataset and make a graph (Figure 3):\n\ntrue_beta &lt;- data.frame(\n  beta = specnumber(spp, MARGIN = 1) / ncol(spp),\n  section_no = c(1:58)\n)\n# true_beta\nggplot(data = true_beta, (aes(x = section_no, y = beta))) +\n  geom_line(size = 1.2, colour = \"indianred\") +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"True beta-diversity\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nFigure 3: Whittaker’s true β-diversity shown in the seaweed data.\n\n\n\n\n\nThe second measure of \\(\\beta\\)-diversity is absolute species turnover, and to calculate this, we subtract \\(\\alpha\\)-diversity for each section from the region’s \\(\\gamma\\)-diversity (Figure 4):\n\nabs_beta &lt;- data.frame(\n  beta = ncol(spp) - specnumber(spp, MARGIN = 1),\n  section_no = c(1:58)\n)\n# abs_beta\nggplot(data = abs_beta, (aes(x = section_no, y = beta))) +\n  geom_line(size = 1.2, colour = \"indianred\") +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Absolute beta-diversity\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nFigure 4: Whittaker’s absolute species turnover shown in action in the seaweed data.\n\n\n\n\n\n\n\nContemporary Definitions \\(\\beta\\)-Diversity\nContemporary definitions of \\(\\beta\\)-diversity rely on pairwise dissimilarity indices such as Bray-Curtis, Jaccard, or Sørensen dissimilarities—see Koleff et al. (2003) for many more; also see ?vegdist. However, discussing pairwise dissimilarities with \\(\\beta\\)-diversity makes more sense.\n\nDissimilarity indices\nDissimilarity indices are special cases of diversity indices that use pairwise comparisons between sampling units, habitats, or ecosystems.\nSpecies dissimilarities result in pairwise matrices similar to the pairwise correlation or Euclidian distance matrices we have seen in Lab 1. In Lab 2b you will have also learned how to calculate these ecological distances in R. These dissimilarity indices are multivariate and compare between sites, sections, plots, etc., and must therefore not be confused with the univariate diversity indices.\nWe use the Bray-Curtis and Jaccard indices with abundance data and the Sørensen dissimilarity with presence-absence data. The seaweed dataset is a presence-absence dataset that necessitates using the Sørensen index. The interpretation of the resulting square (number of rows = number of columns) dissimilarity matrices is the same regardless of whether we calculate it for an abundance or presence-absence dataset. The values in the matrix range from 0 to 1. A 0 means that the pair of sites we compare is identical (all species in common) but 1 means they are completely different (no species in common). In the square dissimilarity matrix, the diagonal is 0, which essentially (and obviously) means that any site is identical to itself. Elsewhere the values will range from 0 to 1. Since this is a pairwise calculation (each site compared to every other site), our seaweed dataset will contain (58 × (58 - 1))/2 = 1653 values, each one ranging from 0 to 1.\nThe first step involves the species table, \\(Y\\). First, we compute the Sørensen dissimilarity index, \\(\\beta_{\\text{sør}}\\), to compare the dissimilarity of all pairs of coastal sections using presence-absence data. The dissimilarity in species composition between two sections is calculated from three parameters, viz., b and c, which represent the number of species unique to each of the two sites, and a, the number of species in common between them. It is given by:\n\\[\\beta_\\text{sør}=\\frac{2a}{2a+b+c}\\] Where \\(a\\) is the number of species in common between two sites, and \\(b\\) and \\(c\\) are the number of species unique to each site. The Sørensen dissimilarity index ranges from 0 to 1, where 0 means that the pair of sites we compare is identical (all species in common) and 1 means they are completely different (no species in common).\nThe vegan function vegdist() provides access to the dissimilarity indices. We calculate the Sørensen dissimilarity index:\n\nsor &lt;- vegdist(spp, binary = TRUE) # makes the lower triangle matrix\nsor_df &lt;- round(as.matrix(sor), 4)\ndim(sor_df)\n\n[1] 58 58\n\nsor_df[1:10, 1:10] # the first 10 rows and columns\n\n        1      2      3      4      5      6      7      8      9     10\n1  0.0000 0.0036 0.0036 0.0072 0.0249 0.0391 0.0391 0.0459 0.0592 0.0629\n2  0.0036 0.0000 0.0000 0.0036 0.0213 0.0355 0.0355 0.0423 0.0556 0.0592\n3  0.0036 0.0000 0.0000 0.0036 0.0213 0.0355 0.0355 0.0423 0.0556 0.0592\n4  0.0072 0.0036 0.0036 0.0000 0.0177 0.0318 0.0318 0.0386 0.0519 0.0556\n5  0.0249 0.0213 0.0213 0.0177 0.0000 0.0140 0.0140 0.0208 0.0342 0.0378\n6  0.0391 0.0355 0.0355 0.0318 0.0140 0.0000 0.0000 0.0069 0.0205 0.0241\n7  0.0391 0.0355 0.0355 0.0318 0.0140 0.0000 0.0000 0.0069 0.0205 0.0241\n8  0.0459 0.0423 0.0423 0.0386 0.0208 0.0069 0.0069 0.0000 0.0136 0.0171\n9  0.0592 0.0556 0.0556 0.0519 0.0342 0.0205 0.0205 0.0136 0.0000 0.0034\n10 0.0629 0.0592 0.0592 0.0556 0.0378 0.0241 0.0241 0.0171 0.0034 0.0000\n\n\nWhat we see above is a square dissimilarity matrix. The most important characteristics of the matrix are:\n\nwhereas the raw species data, \\(Y\\), is rectangular (number rows ≠ number columns), the dissimilarity matrix is square (number rows = number columns);\nthe diagonal is filled with 0;\nthe matrix is symmetrical—it is comprised of symetrical upper and lower triangles.\n\nCreate a data.frame suitable for plotting:\n\nsor_df &lt;- data.frame(round(as.matrix(sor), 4))\n\n\n\n\n\n\n\nLab 3\n\n\n\n(To be reviewed by BCB743 student but not for marks)\nThese questions concern matrices produced from species data using any of the indices available in vegdist():\n\nWhy is the matrix square, and what determines the number of rows/columns?\nWhat is the meaning of the diagonal?\nWhat is the meaning of the non-diagonal elements?\nReferring to the seaweed species data specifically, take the data in row 1 or column 1 and create a line graph showing these values as a function of the section number.\nProvide a mechanistic (ecological) explanation for why this figure takes the shape that it does. Which community assembly process does this hint at? \n\n\n\n\n\n\n\n\n\n\nThere are different interpretations linked to \\(\\beta\\)-diversity, each telling us something different about community formation processes.\n\n\nSpecies turnover and nestedness-resultant \\(\\beta\\)-diversity\nThere are two kinds of \\(\\beta\\)-diversity: species turnover and nestedness-resultant \\(\\beta\\)-diversity. The former is the result of species replacement between sites, whereas the latter is the result of species loss or gain between sites. The Sørensen dissimilarity index, \\(\\beta_\\text{sør}\\), can be decomposed into these two components.\nHow do we calculate the turnover and nestedness-resultant components of \\(\\beta\\)-diversity? The betapart package (Baselga et al. 2022) comes to the rescue. We decompose the dissimilarity into the \\(\\beta_\\text{sim}\\) and \\(\\beta_\\text{sne}\\) components (Baselga 2010) using the betapart.core() and betapart.pair() functions. The outcomes of this partitioning calculation are placed into the matrices \\(Y1\\) and \\(Y2\\). These data can then be analysed further—e.g. we can apply a principal components analysis (PCA) or another multivariate analysis on \\(Y\\) to find the major patterns in the community data—we will do this in BCB743.\n\n# Decompose total Sørensen dissimilarity into turnover and nestedness-resultant\n# components:\nY.core &lt;- betapart.core(spp)\nY.pair &lt;- beta.pair(Y.core, index.family = \"sor\")\n\n# Let Y1 be the turnover component (beta-sim):\nY1 &lt;- data.frame(round(as.matrix(Y.pair$beta.sim), 3))\n\n# Let Y2 be the nestedness-resultant component (beta-sne):\nY2 &lt;- data.frame(round(as.matrix(Y.pair$beta.sne), 3))\n\nA portion of the turnover component matrix:\n\nY1[1:10, 1:10]\n\n      X1    X2    X3    X4    X5    X6    X7    X8    X9   X10\n1  0.000 0.000 0.000 0.000 0.007 0.022 0.022 0.022 0.022 0.029\n2  0.000 0.000 0.000 0.000 0.007 0.022 0.022 0.022 0.022 0.029\n3  0.000 0.000 0.000 0.000 0.007 0.022 0.022 0.022 0.022 0.029\n4  0.000 0.000 0.000 0.000 0.007 0.021 0.021 0.021 0.021 0.029\n5  0.007 0.007 0.007 0.007 0.000 0.014 0.014 0.014 0.014 0.021\n6  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.007\n7  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.007\n8  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.007\n9  0.022 0.022 0.022 0.021 0.014 0.000 0.000 0.000 0.000 0.000\n10 0.029 0.029 0.029 0.029 0.021 0.007 0.007 0.007 0.000 0.000\n\n\nA portion of the nestedness-resultant matrix:\n\nY2[1:10, 1:10]\n\n      X1    X2    X3    X4    X5    X6    X7    X8    X9   X10\n1  0.000 0.004 0.004 0.007 0.018 0.017 0.017 0.024 0.037 0.034\n2  0.004 0.000 0.000 0.004 0.014 0.014 0.014 0.021 0.034 0.030\n3  0.004 0.000 0.000 0.004 0.014 0.014 0.014 0.021 0.034 0.030\n4  0.007 0.004 0.004 0.000 0.011 0.010 0.010 0.017 0.030 0.027\n5  0.018 0.014 0.014 0.011 0.000 0.000 0.000 0.007 0.020 0.017\n6  0.017 0.014 0.014 0.010 0.000 0.000 0.000 0.007 0.021 0.017\n7  0.017 0.014 0.014 0.010 0.000 0.000 0.000 0.007 0.021 0.017\n8  0.024 0.021 0.021 0.017 0.007 0.007 0.007 0.000 0.014 0.010\n9  0.037 0.034 0.034 0.030 0.020 0.021 0.021 0.014 0.000 0.003\n10 0.034 0.030 0.030 0.027 0.017 0.017 0.017 0.010 0.003 0.000\n\n\nA portion of the nestedness-resultant matrix reformatted as a tibble()2:\n2 Note that the rows are no longer numbered in the tibble view, but it can easily be recreated by seq(1:58).\nY2_tib &lt;- as_tibble(Y2)\nhead(Y2_tib)\n\n# A tibble: 6 × 58\n     X1    X2    X3    X4    X5    X6    X7    X8    X9   X10   X11   X12   X13\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0     0.004 0.004 0.007 0.018 0.017 0.017 0.024 0.037 0.034 0.069 0.078 0.196\n2 0.004 0     0     0.004 0.014 0.014 0.014 0.021 0.034 0.03  0.065 0.074 0.193\n3 0.004 0     0     0.004 0.014 0.014 0.014 0.021 0.034 0.03  0.065 0.074 0.193\n4 0.007 0.004 0.004 0     0.011 0.01  0.01  0.017 0.03  0.027 0.062 0.071 0.19 \n5 0.018 0.014 0.014 0.011 0     0     0     0.007 0.02  0.017 0.052 0.061 0.181\n6 0.017 0.014 0.014 0.01  0     0     0     0.007 0.021 0.017 0.053 0.062 0.184\n# ℹ 45 more variables: X14 &lt;dbl&gt;, X15 &lt;dbl&gt;, X16 &lt;dbl&gt;, X17 &lt;dbl&gt;, X18 &lt;dbl&gt;,\n#   X19 &lt;dbl&gt;, X20 &lt;dbl&gt;, X21 &lt;dbl&gt;, X22 &lt;dbl&gt;, X23 &lt;dbl&gt;, X24 &lt;dbl&gt;,\n#   X25 &lt;dbl&gt;, X26 &lt;dbl&gt;, X27 &lt;dbl&gt;, X28 &lt;dbl&gt;, X29 &lt;dbl&gt;, X30 &lt;dbl&gt;,\n#   X31 &lt;dbl&gt;, X32 &lt;dbl&gt;, X33 &lt;dbl&gt;, X34 &lt;dbl&gt;, X35 &lt;dbl&gt;, X36 &lt;dbl&gt;,\n#   X37 &lt;dbl&gt;, X38 &lt;dbl&gt;, X39 &lt;dbl&gt;, X40 &lt;dbl&gt;, X41 &lt;dbl&gt;, X42 &lt;dbl&gt;,\n#   X43 &lt;dbl&gt;, X44 &lt;dbl&gt;, X45 &lt;dbl&gt;, X46 &lt;dbl&gt;, X47 &lt;dbl&gt;, X48 &lt;dbl&gt;,\n#   X49 &lt;dbl&gt;, X50 &lt;dbl&gt;, X51 &lt;dbl&gt;, X52 &lt;dbl&gt;, X53 &lt;dbl&gt;, X54 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nLab 3 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nPlot species turnover as a function of Section number, and provide a mechanistic explanation for the pattern observed.\nBased on an assessment of literature on the topic, provide a discussion of nestedness-resultant \\(\\beta\\)-diversity. Use either a marine or terrestrial example to explain this mode of structuring biodiversity (i.e. assembly of species into a community). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nThe Lab 3 assignment is due at 07:00 on Monday 12 August 2022.\nProvide a neat and thoroughly annotated R file which can recreate all the graphs and all calculations. Written answers must be typed in the same file as comments.\nPlease label the R file as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_3.R\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named R documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 3. Quantifying Biodiversity"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html",
    "href": "BDC334/BDC334_syllabus.html",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "",
    "text": "“Knowledge is not a resource we simply stumble upon. It’s not something that we pluck out of the air. Knowledge is created. It is coaxed into existence by thoughtful, creative people. It is not a free good. It comes only to the prepared mind.”\n— Frank H. T. Rhodes, Speed Bumps on the Road Ahead, Trusteeship, May/June 1999",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#timetable",
    "href": "BDC334/BDC334_syllabus.html#timetable",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Timetable",
    "text": "Timetable\nMy part of the BDC334 module runs in the 3rd term, from 22 July to 30 August 2024.\n\nLecture Timetable\n\n\n\nDay\nPeriods\nLocation\nNotes\n\n\n\n\nMonday\n3rd period (10:20–11:05)\n5th Floor BCB Dept.\nLecture\n\n\nTuesday\n2nd period (09:25–10:10)\n5th Floor BCB Dept\nLecture\n\n\nWednesday\n1st period (08:30–09:15)\n5th Floor BCB Dept\nUnused\n\n\nThursday\nperiods 6-8 (from 13:30)\n5th Floor BCB Dept\nLecture/Lab\n\n\n\nBelow, you are provided with reading material (lecture slides, PDFs for reading) and pre-recorded video lectures that you are expected to consume before the discussion classes on Thursdays. The weekly face-to-face sessions are essential for discussing the work you covered the previous two days, and it also allows you to be like real students, attending actual lectures, for real, in person. The discussion session is for free talk and bouncing of ideas. We can talk about anything related to the topic of biodiversity but will try and focus on the issues at hand.\nTypically, we will meet weekly, on Thursdays, in person on campus. The rest of the time, we will proceed with pre-recorded lecture material from wherever in the world you choose to be.\nHowever, on the first Monday of Term 3, we will all meet in person on campus in the lecture venue (again on the first Wednesday of Term 3). You can then meet me for the first time (even if you saw me online last year), and I will give an outline of my portion of the course. Prof Boatwright will take over in Term 4.\n\n\nLabs\n\n\n\nDay\nPeriods\nLocation\n\n\n\n\nThursday\nPeriods 6-8\n5th Floor BCB Dept\n\n\n\nThe Labs take place on Mondays during Periods 6-8 (starting at 13:30) in the 5th floor computer lab in Biodiversity and Conservation Biology Department (starts 22 July 2024).\nLabs are compulsory, and failing to attend will result in a penalty of 20% taken from your mark for the week.\nPlease ensure that you read through each Lab (accessible in the sidebar) before the start the Labs. You have until the following Monday at 07:00 to complete and submit all the material.\n\n\nClass Tests\nThere will be two class tests:\n\nThursday, 8 August 2025, 13:30-15:30\nThursday, 29 August 2025, 13:30-15:30\n\n\n\nEssay\nWrite a two page essay on:\n\n“The Promise in Our DNA: Science, the Essence of Being Human, and the Future I Choose to Build”.\n\nThe due date is:\n\nFriday, 1 August 2025, 23:59\n\nFormatting instructions for the essay are as follows.\n\nMaximum two pages (including references, if any) – everything on page three will be excluded from assessment.\n10 pt font, Times New Roman.\nSingle line spacing.\nLeft justified only – no full justification. Ever.\nA single blank line between paragraphs.\n2.54 cm margins all round.\nNo visual embellishments… stay professional.\nSee attached example – work from this example as your visual guide.\nNo internal section heaidngs, but ensure the title and your name appear in bold.\n\nPlease see the example layout format here.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#instructor-and-lab-assistant",
    "href": "BDC334/BDC334_syllabus.html#instructor-and-lab-assistant",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Instructor and Lab Assistant",
    "text": "Instructor and Lab Assistant\nTerm 3 of BDC334 is taught by me, Professor AJ Smit. You may find me in Office 4.103 in the BCB Department (4th floor). You’ll receive an introductory email from me, and you are welcome to contact me at that email address with questions or concerns. Please also use the WhatsApp group set up for this module to ask questions and share information.\nThe Lab Assistant for Term 3 is Ms. Siphe Kumalo. She will be available in the Lab during the Lab periods to assist you with any questions you may have.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#syllabus-overview-and-expectations",
    "href": "BDC334/BDC334_syllabus.html#syllabus-overview-and-expectations",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Syllabus, Overview, and Expectations",
    "text": "Syllabus, Overview, and Expectations\n\nSyllabus\nThese links point to online resources such as reading material in the form of publications, lecture slides, example workflows, datasets, and R scripts in support of the video and PDF lecture material. Note that the video material is housed on iKamva from where you may download it without incurring Internet costs; various PDFs for reading can also be found there. It is essential that you work through these examples and workflows.\nFor best results, use the BDC334 Lecture Transcript, which integrates most of the materials linked to in the table below.\n\n\n\nWk\nType\nTopic\nAdditional Reading\nClass/Lab\nExercise Due\n\n\n\n\nW1\nL\nLecture 1. Introductory Lecture\n\n21-24 Jul\n\n\n\n\nL\nLecture 2. Overview of Ecosystems\nLecture Transcript; PDF Slides on iKamva\n\n\n\n\n\nL\nKeith et al. (2012)\nReading\n\n\n\n\n\nL\nBDC334_Lecture_1_Introductiom_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_2a_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_2b_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_2c_720p30.mp4\niKamva\n\n\n\n\n\nP1\nLab 1. Ecological Data\nLecture Transcript; PDF Slides on iKamva\n24 Jul\n28 Jul\n\n\nW2\nL\nLecture 3. Ecological Gradients\nLecture Transcript; PDF Slides on iKamva\n28-31 Jul\n\n\n\n\nL\nLecture 4. Biodiversity COncepts\nLecture Transcript; PDF Slides on iKamva\n\n\n\n\n\nL\nNekola and White (1999)\nReading\n\n\n\n\n\nL\nSmit et al. (2017)\nReading\n\n\n\n\n\nL\nTittensor et al. (2010)\nReading\n\n\n\n\n\nL\nBDC334_Lecture_3a_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_3b_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_3c_720p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_3d_720p30.mp4\niKamva\n\n\n\n\n\nP2\nLab 2a. R & RStudio\n\n31 Jul\n4 Aug\n\n\n\nP2\nLab 2b. Environmental Distance\n\n31 Jul\n4 Aug\n\n\n\nL\nBDC334_Lecture_4a_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_4b_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_4c_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_5a_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_5b_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_5c_1080p30.mp4\niKamva\n\n\n\n\n\nEssay\nEssay due\n\n\n1 Aug\n\n\nW3\nL\nLecture 6. Unified Ecology\nLecture Transcript; PDF Slides on iKamva\n4-7 Aug\n\n\n\n\nL\nShade et al. (2018)\nReading\n\n\n\n\n\nL\nBDC334_Lecture_6a_1080p30.mp4\niKamva\n\n\n\n\n\nL\nBDC334_Lecture_6b_1080p30.mp4\niKamva\n\n\n\n\n\nP3\nLab 3. Quantifying Biodiversity\nLecture Transcript; PDF Slides on iKamva\n7 Aug\n11 Aug\n\n\n\nT1\nClass Test 1\n\n11 Aug\n\n\n\nW4\nL\nLecture 7: Impacts on Biodiversity\n\n11-14 Aug\n\n\n\n\nL\nChapin III et al. (2000)\nReading\n\n\n\n\n\nL\nGotelli and Chao (2013)\nReading\n\n\n\n\n\nL\nMaxwell et al. (2016)\nReading\n\n\n\n\n\nL\nTilman et al. (2017)\nReading\n\n\n\n\n\nP4\nLab 4. Species Distribution Patterns\nSlides\n14 Aug\n18 Aug\n\n\nW5\nL\nLecture 8: Nature’s Contribution to People\n\n18-21 Aug\n\n\n\n\nL\nCostanza et al. (1997)\nReading\n\n\n\n\n\nL\nCostanza et al. (2014)\nReading\n\n\n\n\n\nL\nBurger et al. (2012)\nReading\n\n\n\n\n\nP5\nWorksheet Completion (Prac Assessment)\nAssessment\n21 Aug\n21 Aug\n\n\nW6\nL\nRevision\n\n25-28 Aug\n\n\n\n\nT2\nClass Test 2\n\n25 Aug\n\n\n\n\n\n\nReading in support of the syllabus\nIn the table above, there are links to several key papers to read in preparation for each week’s theory. You must read these papers.\nI cite many other references in each chapter. These serve several functions in that they:\n\nadd additional theory relevant to some ecological concepts;\nprovide background to some of the datasets used in my examples;\ndiscuss derivations of some equations used to calculate diversity concepts;\nprovide example walkthroughs of some of the computational aspects of the methods covered in the Labs;\ncollectively supplement the discussion about these concepts covered in the lectures.\n\nActively engaging with these reading materials will make the difference between a 60% average mark for the module and a mark in excess of 80%.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#graduate-attributes",
    "href": "BDC334/BDC334_syllabus.html#graduate-attributes",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Graduate Attributes",
    "text": "Graduate Attributes\nThe graduate attributes resulting from completion of this modules alignment with the expectations of the workspace across diverse organisations and institutions where graduates typically find employment.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#course-resources-on-ikamva",
    "href": "BDC334/BDC334_syllabus.html#course-resources-on-ikamva",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Course Resources on iKamva",
    "text": "Course Resources on iKamva\nAll the lecture material for this module is on iKamva. You will find there the following under Course Resources:\n\nInteractive Sessions—These are screen recordings belonging to previous years’ teaching where I address some class questions. They might be interesting or helpful.\nPDF_Reading—The bulk of the ’teaching’ will happen in the form of reading material. In other words, learning will occur because you read the papers and understand them. My job will be to facilitate understanding, not to convey the content, which you can access yourselves by reading. Yes, reading is an important life skill.\nSlides—Some meagre slides to accompany your learning process… for what it’s worth.\nVideo—These are the actual video of me talking. I might record more as we work through the course.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#computer-access",
    "href": "BDC334/BDC334_syllabus.html#computer-access",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Computer Access",
    "text": "Computer Access\nYou are encouraged to provide your own laptops and install the necessary software before the module starts. Limited support can be provided if required. There are also computers with R and RStudio (and the essential add-on libraries) available in the 5th-floor lab in the BCB Department.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#attendance",
    "href": "BDC334/BDC334_syllabus.html#attendance",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Attendance",
    "text": "Attendance\n\nLabs\nThese Labs are hands-on. They can only deliver acceptable outcomes if you attend all Lab sessions. Sometimes an occasional absence cannot be avoided. Still, you need to provide evidence (affidavit, doctor’s note, or death certificate) for why you did not attend to avoid a non-attendance penalty. Please be courteous and notify the tutor or me before any absence. If you work with a partner in class, inform them too. Keep up with the reading assignments while you are away, and we will all work with you to get you back up to speed on what you miss. If you do miss a class, however, the assignments must still be submitted on time (also see Late submission of CA).\nSince you may decide to work in collaboration with a peer on tasks and assignments, please keep this person informed at all times in case some emergency makes you unavailable for some time. Someone might depend on your input and contributions—do not leave someone in the lurch so that they cannot complete a task in your absence.\n\n\nGeneral Considerations\nThe schedule is set and will not be changed. Sometimes an occasional absence cannot be avoided. Please be courteous and notify the tutor or me before any absence. If you work with a partner in class, inform them too. Keep up with the reading assignments while you are away, and we will all work with you to get you back up to speed on what you miss. However, if you miss a class, the assignments must still be submitted on time (also see ‘Late submissions’ below).\nSince you may decide to work in collaboration with a peer on tasks and assignments, please keep this person informed at all times in case some emergency makes you unavailable for some time. Someone might depend on your input and contributions—do not leave someone in the lurch so they cannot complete a task in your absence.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#sec-policy",
    "href": "BDC334/BDC334_syllabus.html#sec-policy",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Assessment",
    "text": "Assessment\nThe syllabus for Term 3 is comprised of the following mark-carrying components for Continuous Assessment (CA):\n\nWorksheet Completion (Prac Assessment) — [30%]\nEssay — [10%]\nQuizzes — [10%]\nTest 1 — [15%]\nTest 2 — [15%]\n\nThe CA and an exam will provide a final mark for the module. The weighting of the CA and the exam is 0.6 and 0.4, respectively.\n\nThursday 11 August\nThursday 25 August\n\nFor interest sake, I provide the questions and answers to previous years’ class tests.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#late-submission-of-ca",
    "href": "BDC334/BDC334_syllabus.html#late-submission-of-ca",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Late Submission of CA",
    "text": "Late Submission of CA\nLate assignments will be penalised 10% per day late. They will not be accepted more than 48 hours late unless evidence such as a doctor’s note, a death certificate or another documented emergency can be provided. If you know a submission will be late, please discuss this and seek prior approval. Class time is allocated to work on assignments, and students are expected to continue working on the projects outside class. Successfully completing (and passing) this module requires that you finish tasks based on what we have covered in the course by the following class period. Work diligently from the onset so that even if something unexpected happens at the last minute, you should already be close to done. This approach also allows rapid feedback to be provided to you, which can only be accomplished by returning assignments quickly and punctually.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#support",
    "href": "BDC334/BDC334_syllabus.html#support",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Support",
    "text": "Support\nIt’s expected that some tricky aspects of the module will take time to master, and the best way to master problematic material is to practice, practice some more, and then ask questions. Trying for 10 minutes and then giving up is not good enough. I’ll be more sympathetic to your cause if you can demonstrate having tried for a full day before giving up and asking me. When you ask questions about some challenges, this is the way to do it—explain to me your numerous attempts to solve the problem and how these various attempts have failed. I will not help you if you have not tried to help yourself first (maybe with advice from friends). There will be a time in class to do this, typically before we embark on a new topic.\nShould you require more time with me, find out when I am ‘free’ and set an appointment by sending me a calendar invitation. I am happy to have a personal meeting with you via Zoom, but I prefer face-to-face in my office.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#communication",
    "href": "BDC334/BDC334_syllabus.html#communication",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Communication",
    "text": "Communication\nAd-hoc communication is encouraged. Subscribe to the BDC334 WhatsApp group to openly discuss module content.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/BDC334_syllabus.html#advice-for-success",
    "href": "BDC334/BDC334_syllabus.html#advice-for-success",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "Advice for Success",
    "text": "Advice for Success\nYour success on this course depends very much on you and the effort you put into it. The module has been organised so that the burden of learning is on you, mainly by reading scientific publications on the week’s lecture topics. Your TAs and I will help you by providing you with materials and answering questions, and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class. This includes reading and working through the lecture slides.\nAsk questions. Engage with your peers and me. In a class or away from it. Use the WhatsApp group set up for this module and the comments section on the website. Surround yourself with people who are brighter than you, and make your conversations about ideas, not people and things. If you get a question wrong on an assessment, ask why. If you’re not sure about the Lab assignments, ask. If you hear something on the news that sounds related to what we discussed, raise it as a topic for discussion in class. If the reading is confusing, ask.\nDo all assignments and Labs, attend, and don’t be late. The earlier you start, the better. You should ask yourself how these exercises relate to earlier material and imagine how they might be changed (to make questions for an exam, for example.) It’s not enough to just mechanically plough through the exercises.\nTo learn how to translate your human thoughts into computer language (coding), you should work with computer and R multiple times each week—ideally daily.\nDon’t procrastinate. If something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, and eventually, you won’t know where to begin asking questions. Don’t end a week with unanswered questions. But if you fall behind and don’t know where to start asking, come to my office, and let me help you identify a good (re)starting point.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "**About**"
    ]
  },
  {
    "objectID": "BDC334/Lec-04-biodiversity.html",
    "href": "BDC334/Lec-04-biodiversity.html",
    "title": "Lecture 4: Biodiversity Concepts",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 4: Biodiversity Concepts"
    ]
  },
  {
    "objectID": "BDC334/Lec-04-biodiversity.html#quantifying-diversity",
    "href": "BDC334/Lec-04-biodiversity.html#quantifying-diversity",
    "title": "Lecture 4: Biodiversity Concepts",
    "section": "Quantifying Diversity",
    "text": "Quantifying Diversity\nWhen we talk about ‘biodiversity,’ we typically refer to the variety of life in a given area or ecosystem. This encompasses species diversity, genetic diversity within species, and the diversity of ecosystems or habitats. To quantify biodiversity, we use metrics that capture various aspects, including:\n\nThe variability and characteristics of the environment.\nThe species present in a given area (species lists).\nThe relative abundance of each of the species.\nThe spatial distribution of species across different habitats or ecosystems.\n\nIn this lecture, we will explore some of the most common metrics used to quantify biodiversity. We’ll delve into the concepts of species richness, evenness, and diversity, and how these metrics can be applied to compare different habitats or ecosystems.\nBiodiversity metrics can be broadly categorised into three groups based on the type of information they provide:\n\nBiodiversity metrics (\\(\\alpha\\)-diversity, \\(\\beta\\)-diversity, \\(\\gamma\\)-diversity).\nDiversity indices (e.g., Shannon’s Entropy, Gini Index, Herfindahl-Hirschman Index (HHI)).\nDistance measures (e.g., Euclidean, Manhattan) and Dissimilarity indices (e.g., Bray-Curtis, Jaccard, Sørensen).\n\nThe first two categories—biodiversity metrics and diversity indices—offer simplified representations of biodiversity through synthetic metrics or indices. In contrast, distance measures and dissimilarity indices provide more nuanced and detailed insights by exposing the full multivariate information within our datasets. This allows for a deeper examination of the processes driving community formation and the resulting structures that describe biodiversity patterns across landscapes.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 4: Biodiversity Concepts"
    ]
  },
  {
    "objectID": "BDC334/Lec-04-biodiversity.html#biodiversity-metrics",
    "href": "BDC334/Lec-04-biodiversity.html#biodiversity-metrics",
    "title": "Lecture 4: Biodiversity Concepts",
    "section": "Biodiversity Metrics",
    "text": "Biodiversity Metrics\n\n\\(\\alpha\\)-Diversity (Species Richness)\nAlpha diversity quantifies the diversity of species within a specific, localised area or community. This could be a site, plot, quadrat, a field, or any other small unit of (typically) replication in the study. This measure provides information about the ecological structure and complexity of a given habitat at a fine scale.\nThere are several ways to represent \\(\\alpha\\)-diversity. The simplest and most straightforward measure is species richness, which is simply a count of the number of different species present in the sampling area. Simply put, this is a list of species within the local scale. If we have multiple local scale sites, we can calculate the average species richness across all sites (Figure 1).\n\n\n\n\n\n\nFigure 1: Alpha-diversity in one sense is the simple expression of the average species richness (number of species) across a landscape.\n\n\n\nSpecies richness is easy to understand and implement, but it doesn’t account for the relative abundance of each species within the community. To address this limitation we make use of univariate indices. Shannon’s H’ (Shannon’s Diversity Index) and Simpson’s \\(\\lambda\\) (Simpson’s Diversity Index) are such univariate diversity indices. These indices place various amounts of emphasis on the abundance and evenness of species present.\nChoosing Shannon’s or Simpson’s is a bit controversial and it often depends on who is using it. According to Jari Oksanen, author of the vegan package in R, the choice between Shannon’s and Simpson’s index is a matter of personal preference. He writes:\n\nBetter stories can be told about Simpson’s index than about Shannon’s index, and still grander narratives about rarefaction (Hurlbert 1971). However, these indices are all very closely related (Hill 1973), and there is no reason to despise one more than others (but if you are a graduate student, don’t drag me in, but obey your Professor’s orders). In particular, the exponent of the Shannon index is linearly related to inverse Simpson (Hill 1973) although the former may be more sensitive to rare species.\n\nBoth Shannon’s H’ or Simpson’s \\(\\lambda\\) can be applied to the local scale, or averaged across multiple sites to get a regional scale measure of the average \\(\\alpha\\)-diversity. We will revisit Shannon’s H’ or Simpson’s \\(\\lambda\\) lower down in this section as they also crop up in under the heading of Diversity Indices (another logical place to classify the same concepts).\n\n\n\\(\\beta\\)-Diversity (Variation in Diversity)\nA related concept of diversity is one that considers the variation between sites (Figure 2). This is known as \\(\\beta\\)-diversity. \\(\\beta\\)-diversity refers to the measure of diversity between different communities or ecosystems within a larger region. It quantifies the variation in species composition from one habitat or site to another and captures the degree of differentiation or turnover of species across spatial scales. \\(\\beta\\)-diversity helps to understand how species diversity is distributed across different environments and can indicate the impact of environmental gradients, habitat fragmentation, and ecological processes on community composition. It links local (\\(\\alpha\\)-diversity) and regional (\\(\\gamma\\)-diversity) scales and offers a processed-based view on biodiversity formation.\n\n\n\n\n\n\nFigure 2: Beta-diversity quantifies the variation in species richness (number of species) and composition (number of individuals of a particular species) across the landscape.\n\n\n\n\\(\\beta\\)-diversity has a long history in ecology and has undergone several conceptual revisions over the years. The concept was first introduced by Whittaker (1960) to describe the variation in species composition between different sites.\nWhittaker’s initial idea was that of true \\(\\beta\\)-diversity (hence it sometimes being called Whittaker’s \\(\\beta\\)-diversity), which is often defined as the effective number of distinct communities in a region. It can be calculated as the ratio of \\(\\gamma\\)-diversity to \\(\\alpha\\)-diversity when these are expressed as Hill numbers or effective numbers of species. Mathematically, this is expressed as:\n\\[\\beta = \\frac{\\gamma}{\\alpha}\\]\nwhere \\(\\beta\\) is true \\(\\beta\\)-diversity, \\(\\gamma\\) is the total diversity of the region, and \\(\\alpha\\) is the mean diversity of the individual communities.\nAnother approach is absolute species turnover, which is a measure of the total amount of species change between communities or along environmental gradients. It can be calculated in various ways, but one common approach is to use the Whittaker’s \\(\\beta\\)-diversity index:\n\\[\\beta_w = \\frac{S}{\\alpha} - 1\\]\nwhere \\(S\\) is the total number of species in all communities combined (\\(\\gamma\\)-diversity), and \\(\\alpha\\) is the average number of species found in all the local scale samples that comprise the region.\nThis measure of turnover ranges from 0 (when all communities have identical species composition) to a maximum value that depends on the number of communities being compared. It provides a quantitative measure of how much species composition changes across communities or sites.\nContemporary views of \\(\\beta\\)-diversity were developed by Nekola and White (1999), Baselga (2010), and Anderson et al. (2011). This information is encapsulated with pairwise matrices of dissimilarity indices (see the section below on dissimilarity indices where the various dissimilarity indices are presented in more detail) calculated for each pair of sites within the studied system. The broad implication is the same as how it was traditionally applied: that is, \\(\\beta\\)-diversity describes how species formation (into communities) measured within the ecosystem of interest vary from place to place, e.g. between the various transects or quadrats used to sample the ecosystem. But, these modern interpretations of \\(\\beta\\)-diversity extract from these views of habitat heterogeneity some deeper insights about the mechanisms responsible for driving the community formation process, viz. the role of gradients (Process 1: niche theory) and stochastic processes (Process 2: neutral theory).\nProcess 1: If a region comprises the species A, B, C, …, M (i.e. \\(\\gamma\\)-diversity is 13), a subset of the regional flora captured by one quadrat might be species A, D, E. In another quadrat species A, D, F might be present. \\(\\alpha\\)-diversity is three in both instances, and heterogeneity (and hence \\(\\beta\\)-diversity) results from the fact that the first quadrat has species E, but the other has species F. In other words, here, we have the same number of species in both quadrats, but only two of the species are the same. The process responsible for this form of \\(\\beta\\)-diversity is species turnover, \\(\\beta_\\text{sim}\\). Turnover refers to processes that cause communities to differ due to species being lost or gained from section to section, i.e. the species composition changes between sections without corresponding changes in \\(\\alpha\\)-diversity. The function beta() in the R package BAT calls this form of \\(\\beta\\) diversity replacement and use the symbol \\(\\beta_{repl}\\) (Cardoso et al. 2015).\nProcess 2: Consider again species A, B, C, …, M. Now we have a quadrat with species A, B, C, D, G, H (\\(\\alpha\\)-diversity is six) but another quadrat has a subset of these species, e.g. only species A, B, G (\\(\\alpha\\)-diversity three). Here, \\(\\beta\\)-diversity is high even though the quadrats share some species, but the number of species differs among the quadrats (i.e. from place to place) due to one quadrat capturing only a subset of species present in the other. This form of \\(\\beta\\)-diversity is called nestedness-resultant \\(\\beta\\)-diversity, \\(\\beta_\\text{sne}\\), and it refers to processes that cause species to be gained or lost, and the community with the lowest \\(\\alpha\\)-diversity is a subset of the richer community. The function beta() in the R package BAT calls this form of \\(\\beta\\) diversity *richness difference** and uses the symbol \\(\\beta_{rich}\\) (Cardoso et al. 2015).\nThe above two examples show that \\(\\beta\\)-diversity is coupled not only with the identity of the species in the quadrats but also \\(\\alpha\\)-diversity—with species richness in particular.\nWe express \\(\\beta\\)-diversity as nestedness-resultant, \\(\\beta_\\text{sne}\\), and turnover, \\(\\beta_\\text{sim}\\), components to be able to distinguish between these two processes. It allows us to make inferences about the two possible drivers of \\(\\beta\\)-diversity. Turnover refers to processes that cause communities to differ due to species being lost or gained from section to section, i.e. the species composition changes between sections without corresponding changes in \\(\\alpha\\)-diversity. The nestedness-resultant component implies processes that cause species to be gained or lost without replacement, and the community with the lowest \\(\\alpha\\)-diversity is a subset of the richer community.\nAccording to Nekola and White (1999) on p. 868, there are two causes of ecological distance decay. ‘Ecological’ is key to the first cause—it is environmental filtering results in a decrease in similarity as the distance between sites increases. We sometimes call this the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography.\nThe second cause of distance decay sensu Nekola and White (1999) involves aspects of the spatial configuration, context of the habitats, and some temporal considerations. Here, the evolutionary differences between species—specifically around those traits that affect their ability to disperse—are more at play and are the primary influences of distance decay rates that might vary between species. Let us first consider some properties of a hypothetical homogeneous landscape. The landscape creates some impediment (resistance) to the propagation of some species (hypothetically species A, B, and C) across its surface, but which are less effective in impeding others (D, E, and F). For argument’s sake, all species (A, …, F) share similar environmental tolerances to the prevailing environmental conditions, so one can argue that the niche difference model (environmental filtering) does not explain distributional patterns. Given a particular founding or disturbance event, species D, E, and F will, in a relatively shorter period, be able to become evenly distributed (relatively similar abundances everywhere) across this landscape. However, the less vagile (in terms of dispersal ability), species A, B, and C will develop a steeper gradient of decreasing species abundances away from the founding populations (resulting from, for example, adaptive radiation). They will require more time to become homogeneously dispersed across the landscape. In this regard, historical events set up striking distributional patterns that can be mistaken for gradients, which exist because insufficient time has passed to ensure complete dispersal. Studying the influence of such past events is called ‘historical biogeography.’ In reality, landscapes are seldom homogeneous in their spatial template (e.g. there are hills and valleys), and variable dispersal mechanisms and abilities will interact with this heterogeneous landscape to form interesting patterns of communities. The ecologist will have an exciting time figuring out the relative importance of actual gradients vs those that result from evolved traits that affect their dispersal ability and interact with the environment. I have not said anything about ‘neutral theories’ (but which are seen in the \\(\\beta_\\text{sne}\\) form of \\(\\beta\\)-diversity as in Smit et al. 2017), nor biological interactions that might affect community structure.\n\n\n\\(\\gamma\\)-Diversity (Regional Diversity)\nWhile \\(\\alpha\\)-diversity focuses on the local scale, representing the species richness within a specific area or community, the concept of species richness changes as we broaden our scope of observation. This brings us to the concept of \\(\\gamma\\)-diversity, which refers to the overall diversity of a larger area or region encompassing multiple local-scale units of observation or quantification (Figure 3). The transition from \\(\\alpha\\)- to \\(\\gamma\\)-diversity occurs as we aggregate data from multiple sampling units or sites within a broader landscape or ecosystem. \\(\\gamma\\)-diversity captures the total species diversity across all the local communities in a region. It is not merely the average \\(\\alpha\\)-diversity or total \\(\\alpha\\)-diversity aggregated over individual sites; rather, it reflects the combined diversity, including both the diversity within each local community (\\(\\alpha\\)-diversity) and the diversity between communities (\\(\\beta\\)-diversity).\n\n\n\n\n\n\nFigure 3: Gamma-diversity is the total species list (number of species) across a landscape taking into account all sampling units representative of that landscape.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 4: Biodiversity Concepts"
    ]
  },
  {
    "objectID": "BDC334/Lec-04-biodiversity.html#sec-diversity-indices",
    "href": "BDC334/Lec-04-biodiversity.html#sec-diversity-indices",
    "title": "Lecture 4: Biodiversity Concepts",
    "section": "Diversity Indices",
    "text": "Diversity Indices\nA diversity index is a metric that quantifies species diversity within a community. While species richness simply refers to the number of species present, diversity indices also consider the relative abundances of these species. For instance, consider two communities: community A comprises 10 individuals of each of 10 species (totalling 100 individuals) and community B has 9 species with 1 individual each, and a 10th species with 91 individuals (also totalling 100 individuals). Which community is more diverse? To address this, diversity indices incorporate both richness and evenness information and provides a more comprehensive assessment of diversity than species richness alone.\n\nMargalef’s Index\nMargalef’s Index is a simple measure of species richness that accounts for the number of species in a community and the total number of individuals. The formula for Margalef’s Index is:\n\\[\nD = \\frac{S - 1}{\\ln(N)}\n\\]\nwhere \\(S\\) is the total number of species in the community, and \\(N\\) is the total number of individuals. A higher value of \\(D\\) indicates greater diversity.\n\n\nShannon’s Entropy\nShannon’s Entropy, or Shannon’s H’, comes out of the field of information theory and was developed by Claude Shannon. It measures the uncertainty or diversity within a system. It is a general measure of information content and is applicable to a variety of data types beyond species diversity, such as genetic diversity, linguistic diversity, or even the distribution of different types of land use in a landscape. The formula for Shannon’s H’ is as used by ecologists is:\n\\[\nH' = -\\sum_{i=1}^{S} p_i \\ln(p_i)\n\\]\nwhere \\(S\\) is the total number of species in the community, and \\(p_i\\) is the proportion of individuals belonging to species \\(i\\). A higher H’ value indicates greater diversity, with values typically ranging from 0 to about 4.5, rarely exceeding 5 in extremely diverse communities. We use this index to help us understand the evenness and richness of species within a community, and it is used when we need to emphasise the contribution of rare species.\n\n\nSimpson’s Indices\nSimpson’s Indices are a group of related diversity measures developed by Edward H. Simpson. These indices focus on the dominance or evenness of species in a community, giving more weight to common species and being less sensitive to species richness compared to Shannon’s H’.\n\nSimpson’s Dominance Index\nSimpson’s Dominance Index (\\(\\lambda\\)) measures the probability that two individuals randomly selected from a sample will belong to the same species. The formula for Simpson’s Dominance Index is:\n\\[\n\\lambda = \\sum_{i=1}^{S} p_i^2\n\\]\nwhere \\(S\\) is the total number of species, and \\(p_i\\) is the proportion of individuals belonging to species \\(i\\). Values range from 0 to 1, with higher values indicating lower diversity (higher dominance). A value of 1 represents no diversity (only one species present), while a value approaching 0 indicates very high diversity.\n\n\nSimpson’s Diversity Index\nTo make the index more intuitive we prefer to use Simpson’s Diversity Index, which is calculated as:\n\\[\n1 - \\lambda = 1 - \\sum_{i=1}^{S} p_i^2\n\\]\nThis form ensures that the index increases with increasing diversity. Values range from 0 to 1, with higher values indicating higher diversity.\n\n\nSimpson’s Reciprocal Index\nAnother common form is Simpson’s Reciprocal Index, calculated as:\n\\[\n\\frac{1}{\\lambda} = \\frac{1}{\\sum_{i=1}^{S} p_i^2}\n\\]\nThis index starts with a value of 1 as the lower limit, representing a community containing only one species. The upper limit is the number of species in the sample (S). Higher values indicate greater diversity.\nSimpson’s Indices are less sensitive to species richness and more sensitive to evenness compared to Shannon’s Entropy. They are useful when you want to give more weight to common species in your diversity assessment.\n\n\n\nGini Index\nThe Gini Index, or Gini Coefficient, should be fimiliar to all South Africans—South Africa is infamous for having the highest Gini Coefficient in the world. The Gini Index is a measure of inequality within a distribution, and is typically used in economics to assess income or wealth inequality. Since its purpose is to evaluate disparity, it is also suited to ecological systems because, here too, the distribution in abundance differs among species. The formula for the Gini Index is:\n\\[\nG = \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{N} |x_i - x_j|}{2N^2 \\bar{x}}\n\\]\nwhere \\(N\\) is the total number of observations, \\(x_i\\) and \\(x_j\\) are the values of the observations, and \\(\\bar{x}\\) is the mean of the values. In ecological studies, a high Gini Index indicates a large disparity in species abundances, with few species dominating the community, whereas a low Gini Index suggests a more even distribution of individuals among species.\n\n\nHerfindahl-Hirschman Index (HHI)\nThe Herfindahl-Hirschman Index (HHI) is a measure of market concentration commonly used in economics to assess the level of competition within an industry. It is calculated as the sum of the squares of the market shares of all firms in the market. Ecologists sometimes use the HHI to assess species dominance or the concentration of individuals within species. The formula for HHI is:\n\\[\nHHI = \\sum_{i=1}^{N} s_i^2\n\\]\nwhere \\(N\\) is the total number of species, and \\(s_i\\) is the proportion of individuals belonging to species \\(i\\). Here, a higher HHI indicates a higher concentration of individuals in a few species, signifying lower diversity. Conversely, a lower HHI reflects a more even distribution of individuals across species, indicating higher diversity.\nHere’s a corrected and improved version of the text:",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 4: Biodiversity Concepts"
    ]
  },
  {
    "objectID": "BDC334/Lec-04-biodiversity.html#sec-resemblance-metrics",
    "href": "BDC334/Lec-04-biodiversity.html#sec-resemblance-metrics",
    "title": "Lecture 4: Biodiversity Concepts",
    "section": "Ecological Resemblance",
    "text": "Ecological Resemblance\nResemblance matrices are mathematical representations used to quantify the similarity or dissimilarity between pairs of samples, communities, or ecological sampling units based on various criteria such as species composition, abundance, functional traits, phylogenetic relatedness, or environmental properties. Well-structured raw data about species composition typically come in the form of a table with rows representing sites or samples, and columns representing species. Similarly, data about environmental variables are structured as a table with rows representing sites or samples, and columns representing environmental variables.\nThe diagram below (Figure 4) summarises the species and environmental data tables, and what we can do with them. These tables are the starting points of many additional analyses, and we will explore some of these deeper insights later in this module.\n\n\n\n\n\n\nFigure 4: Species and environmental tables, resemblance matrices, and deeper analyses possible from the various kinds of ecological data. The ordinations (e.g. PCA, CA, nMDS, etc.) will only be covered in BCB743 in your Honours year.\n\n\n\n\n\n\n\n\n\nTerminology: matrices and tables\n\n\n\nAlthough we often use the terms ‘matrix’ and ‘table’ interchangeably, in this book I use matrix to refer to a mathematical object with rows and columns and with the cell content derived from calculations of distances and dissimilarities. In these situations they tend to be square and symmetrical. I then use the term table to refer to a more general data structure, also with rows and columns, but here representing samples or sites (as rows) and columns representing species or environmental variables. My use of ‘table’ generally refers to the raw data we use as a starting point for our calculations (including of the matrices).\nThis is my notations and authors such as Borcard et al. (2011), David Zelený, and Michael Palmer may not make this distinction and use both terms to refer to a rectangular data structure.\n\n\nWhen the focus is on comparing sites (i.e., the information about objects in the rows of site × species or site × environment tables) based on their species composition or environmental characteristics, we call this type of analysis an R-mode analysis. Such resemblance matrices typically manifest as square matrices, with rows and columns representing the samples or units being compared.\nOther cases of square resemblance matrices include: i) Species-by-species matrices (association matrices), where both rows and columns represent species, and the values in the matrix represent the association between each pair of species. ii) Environmental-by-environmental matrices (correlation matrices), where both rows and columns represent environmental variables, and the values in the matrix represent the correlation between each pair of variables. In these cases, the focus falls onto the information initially contained in the columns (species or descriptors) of the sites × species table or the sites × environmental variables table. This is called a Q-mode analysis.\nEnvironmental resemblance matrices, or environmental distance matrices, are used to quantify the similarity between pairs of sites based on their environmental variables. They can also be used in more advanced analyses, such as various kinds of ordinations and clustering. These matrices have zeros down the diagonal, as the distance between a site and itself is zero. The subdiagonal values are typically the same as the superdiagonal values, as the dissimilarity between samples \\(i\\) and \\(j\\) is the same as the dissimilarity between samples \\(j\\) and \\(i\\), i.e., the matrices are symmetrical. The off-diagonal values represent the distance between pairs of sites, with higher values indicating greater dissimilarity.\nIn species dissimilarity matrices (species resemblance matrices), the values represent the degree of dissimilarity between each pair of samples. Dissimilarity matrices are characterised by a diagonal filled with zeros, because the dissimilarity between a sample and itself is zero. The off-diagonal values represent the dissimilarity between pairs of samples, with higher values indicating greater dissimilarity. They are also symmetrical for the same reasons given for the environmental matrices. Species dissimilarity matrices are used in various multivariate analyses, such as cluster analysis, ordination, and diversity partitioning.\nLegendre and Legendre (2012) provide a full chapter (Chapter 7) on ecological resemblance, including an in-depth look at the various kinds of ‘association coefficients,’ which is what we will cover next. The next two sub-sections will thus introduce a few frequently used association coefficients to study species dissimilarity and environmental distances across the landscape.\n\nEnvironmental Distance\nSometimes we need to quantify the environmental similarities or differences between sampling sites, such as plots, quadrats, or transects. This is typically achieved through the use of distance matrices (one kind of resemblance matrix), which provide an overall view of how all the sites relate to one another. These matrices are derived from data tables containing information on environmental variables (sites in rows and variables in columns).\nThere are several kinds of distance metrics available for use with environmental data. Regardless of which index one chooses, the resulting matrix provides pairwise differences (or distances) or similarities in a metric that relates to the ecological distance between all sites (and which might also link to their community composition, which is the thing we are trying to determine). Such pairwise matrices are foundational for various multivariate analyses and can reveal patterns in ecological data that might not be apparent from raw measurements of individual variables alone.\nEuclidean distance is in my experience the commonly used in spatial analysis. It defined as the straight-line distance between two points in Euclidean space. In its simplest form, it applies to a planar area such as a graph with \\(x\\)- and \\(y\\)-axes, but it can be extended to higher dimensions. In two or three dimensions, it gives the Cartesian distance between points on a plane (\\(x\\), \\(y\\)) or in a volume (\\(x\\), \\(y\\), \\(z\\)), and this concept can be further extended to higher-dimensional spaces. Euclidean distance conforms to our intuitive physical concept of distance, making it useful for applications like measuring short geographic distances between points on a map. However, over large distances on Earth’s surface, Euclidean distance loses accuracy due to the Earth’s spherical shape. In such cases, great circle distances, calculated using formulas like the Haversine formula, provide more accurate measurements.\nMathematically, Euclidean distance is calculated using the Pythagorean theorem. This method squares the differences between coordinates, which means that single large differences become disproportionately important in the final distance calculation. While this property makes Euclidean distance useful for environmental data, where it effectively calculates the ‘straight-line distance’ between two points in multidimensional space (with each dimension representing an environmental variable), it is ill suited to species data.\nThe Euclidean distance between two points \\(A\\) and \\(B\\) in a \\(n\\)-dimensional space is calculated as:\n\\[\nd_{jk} = \\sqrt{\\sum_{i=1}^{n} (j_i - k_i)^2}\n\\]\nwhere \\(j_i\\) and \\(k_i\\) are the values of the \\(i\\)-th variable at points \\(j\\) and \\(k\\), respectively.\nOther distance metrics are the Mahalanobis Distance, Manhattan Distance, Canberra Distance, Gower Distance, and Bray-Curtis Dissimilarity. I’ll not discuss them here and you can refer to Chapter 3 in the book by Borcard et al. (2011) for more information. Additionally, vegan’s vegdist() function does a very good job of providing a wide range of distance metrics and you can find a discussion of many of them in the function’s help file, which you can access as ?vegan::vegdist.\n\n\nSpecies Dissimilarities\nEcological similarity between sites is fundamentally tied to their species composition, which is a function of both species richness and abundance. Sites that share similar species compositions are considered ecologically similar and exhibit a low dissimilarity metric. The factors influencing this similarity are complex and influenced by many properties of the environment and processes operating there.\nAs we have already seen, the degree of similarity between sites can be attributed to measurable environmental differences (i.e. hopefully captured in the environmental distance matrices we saw above) that directly influence species composition. These might include variables like soil type, climate, or topography. However, similarity can also be affected by unmeasured, often overlooked influences that are not immediately apparent or easily quantifiable. Additionally, some degree of variation may simply be attributed to ecological ‘noise’—random fluctuations or stochastic events that affect species distributions.\nIt is our role to disentangle these various influences and determine the primary drivers of similarity or dissimilarity among sites. To aid in this analysis, we use a class of matrices known as dissimilarity matrices (a type of resemblance matrix). These matrices quantify the dissimilarity between sites based on their species composition.\nVarious indices have been developed to compare the composition of different groups or communities. These diversity indices quantify how different or similar groups are based on their attributes, primarily species richness and/or relative abundances. While the simplest application is to compare the species composition of two sites, these indices can be extended to compare multiple groups or communities. They are core to the study of β-diversity, which examines the variation in species composition among sites within a geographic area.\nI’ll present the Bray-Curtis dissimilarity as an example, which is a widely-used metric for comparing species composition between two sites. For abundance data, it is calculated as follows:\n\\[\nd_{jk} = \\frac{\\sum_i |x_{ij} - x_{ik}|}{\\sum_i (x_{ij} + x_{ik})}\n\\]\nwhere \\(x_{ij}\\) and \\(x_{ik}\\) are the abundances of species \\(i\\) (the columns) at sites \\(j\\) and \\(k\\) (the rows) respectively.\nFor presence-absence data, the Bray-Curtis dissimilarity simplifies to:\n\\[\nd_{AB} = \\frac{A+B-2J}{A+B-J}\n\\]\nwhere \\(J\\) is the number of species present in both sites being compared, \\(A\\) is the number unique to site A, and \\(B\\) is the number unique to site B.\nThe Bray-Curtis dissimilarity ranges from 0 (indicating identical species compositions) to 1 (indicating completely different compositions). This metric can be used to construct dissimilarity matrices for multivariate analyses, where each cell in the matrix represents the ecological distance between a pair of sites based on their species composition.\nIn practice, these dissimilarity indices and distances can be calculated using the vegan R package’s vegdist() function. Refer to ?vegan::vegdist for information and a deeper look.\nCommon dissimilarities suited to presence-absence data are the Jaccard Dissimilarity, Sørensen-Dice index, and Ochiai index. For abundance data, we have already seen the Bray-Curtis dissimilarity, but you also have the Morisita-Horn index, which is also commonly used. The Raup-Crick index is used to compare the dissimilarity between two groups to the expected dissimilarity between two random groups, whilst the Chao-Jaccard and Chao-Sørensen indices are probabilistic versions of the Jaccard and Sørensen indices that account for unseen shared species.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 4: Biodiversity Concepts"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html",
    "href": "BDC334/Lab-02b-env_dist.html",
    "title": "Lab 2b. Environmental Distance",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html#set-up-the-analysis-environment",
    "href": "BDC334/Lab-02b-env_dist.html#set-up-the-analysis-environment",
    "title": "Lab 2b. Environmental Distance",
    "section": "Set Up the Analysis Environment",
    "text": "Set Up the Analysis Environment\n\nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(geodist) # to calculate geographic distances between lats/lons\nlibrary(ggpubr) # to arrange the multipanel graphs",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html#revisiting-euclidean-distance",
    "href": "BDC334/Lab-02b-env_dist.html#revisiting-euclidean-distance",
    "title": "Lab 2b. Environmental Distance",
    "section": "Revisiting Euclidean Distance",
    "text": "Revisiting Euclidean Distance\nThe toy data have arbitrary columns to demonstrate the Euclidean distance calculation:\n\\[ d(a,b) = \\sqrt{(a_x - b_x)^2 + (a_y - b_y)^2 + (a_z - b_z)^2} \\]\nThe distance is found between every pair of sites named a to g whose locations are marked by the ‘coordinates’ \\(x\\), \\(y\\), and \\(z\\)—i.e. this is an example of 3-dimensional data (a space or volume, as opposed to 2D data situated on a \\(x\\), \\(y\\) place). We might also call each coordinate a ‘variable’ (sometimes called a ‘dimension’) and hence we have multivariate or multidimensional data.\nLet’s load the dataset and find the size of the dataframe:\n\nxyz &lt;- read.csv(\"../data/Euclidean_distance_demo_data_xyz.csv\")\ndim(xyz)\n\n[1] 7 4\n\n\nThere are seven rows and four columns.\nThe data look like:\n\nxyz\n\n  site x y z\n1    a 4 1 3\n2    b 5 5 5\n3    c 6 6 4\n4    d 1 4 9\n5    e 2 3 8\n6    f 8 3 1\n7    g 9 1 5\n\n\nThe first column contains the site names and it must be excluded from subsequent calculations. The remaining three columns will be used below.\nCalculate the Euclidean distance using vegan’s vegdist() function and view the lower triangle with the diagonal:\n\nxyz_euc &lt;- round(vegdist(xyz[, 2:4], method = \"euclidian\",\n                         upper = FALSE, diag = TRUE), 4)\n# selected only cols 2, 3 and 4\nxyz_euc\n\n        1       2       3       4       5       6       7\n1  0.0000                                                \n2  4.5826  0.0000                                        \n3  5.4772  1.7321  0.0000                                \n4  7.3485  5.7446  7.3485  0.0000                        \n5  5.7446  4.6904  6.4031  1.7321  0.0000                \n6  4.8990  5.3852  4.6904 10.6771  9.2195  0.0000        \n7  5.3852  5.6569  5.9161  9.4340  7.8740  4.5826  0.0000\n\n\nConvert to a dataframe and view it:\n\nxyz_df &lt;- as.data.frame(as.matrix(xyz_euc))\nxyz_df\n\n       1      2      3       4      5       6      7\n1 0.0000 4.5826 5.4772  7.3485 5.7446  4.8990 5.3852\n2 4.5826 0.0000 1.7321  5.7446 4.6904  5.3852 5.6569\n3 5.4772 1.7321 0.0000  7.3485 6.4031  4.6904 5.9161\n4 7.3485 5.7446 7.3485  0.0000 1.7321 10.6771 9.4340\n5 5.7446 4.6904 6.4031  1.7321 0.0000  9.2195 7.8740\n6 4.8990 5.3852 4.6904 10.6771 9.2195  0.0000 4.5826\n7 5.3852 5.6569 5.9161  9.4340 7.8740  4.5826 0.0000\n\n\nDistance matrices have the same properties as dissimilarity matrices, i.e.:\n\nThe distance matrix is square (number rows = number columns).\nThe diagonal is filled with 0.\nThe matrix is symmetrical—it is comprised of symmetrical upper and lower triangles.\n\nIn terms of the meaning of the cell values, their interpretation is also analogous with that of the species dissimilarities. A value of 0 means the properties of the sites (or sections, plots, transects, quadrats, etc.) in terms of their environmental conditions are identical (this is always the case the the diagonal). The larger the number (which may be &gt;1) the more different sites are in terms of their environmental conditions.\nSince each column, \\(x\\), \\(y\\), and \\(z\\), is a variable, we can substitute them for actual variables or properties of the environment within which species are present. Let’s load such data (again fictitious):\n\nenv_fict &lt;- read.csv(\"../data/Euclidean_distance_demo_data_env.csv\")\nhead(env_fict, 2) # print first two rows only\n\n  site temperature depth light\n1    a           4     1     3\n2    b           5     5     5\n\n\nThese are the same data as in Euclidean_distance_demo_data_xyz.csv but I simply renamed the columns to names of the variables temperature, depth, and light intensity. I won’t repeat the analysis here as the output remains the same.\nNow apply vegdist() as before. The resultant distances are called ‘environmental distances’.\nLet us now use some real data.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html#a-look-at-the-seaweed-environmental-data",
    "href": "BDC334/Lab-02b-env_dist.html#a-look-at-the-seaweed-environmental-data",
    "title": "Lab 2b. Environmental Distance",
    "section": "A Look at the Seaweed Environmental Data",
    "text": "A Look at the Seaweed Environmental Data\nThese data accompany the analysis of the South African seaweed flora (Smit et al. 2017).\n\nload(\"../data/seaweed/SeaweedEnv.RData\")\n\n# lets look at the data\ndim(env)\n\n[1] 58 18\n\n\nWe see that the data have 58 rows and 18 columns… the same number of rows as the seaweed.csv data. What is in the first five rows?\n\nround(env[1:5, 1:5], 4)\n\n  febMean  febMax  febMed  febX95 febRange\n1 13.0012 18.7204 12.6600 16.8097   6.0703\n2 13.3795 18.6190 13.1839 17.0724   5.8893\n3 13.3616 17.8646 13.2319 16.6111   5.4314\n4 13.2897 17.1207 13.1028 16.1214   5.0490\n5 12.8113 16.3783 12.4003 15.5324   4.9779\n\n\nAnd the last five rows?\n\nround(env[(nrow(env) - 5):nrow(env), (ncol(env) - 5):ncol(env)], 4)\n\n   annRange  febSD  augSD annChl augChl febChl\n53   4.3707 1.0423 0.7735 4.3420 4.3923 4.6902\n54   4.3358 1.1556 0.9104 1.6469 2.2654 1.6930\n55   4.4104 1.1988 0.8427 0.2325 0.6001 0.5422\n56   4.6089 1.1909 0.6631 0.1321 0.4766 0.3464\n57   4.9693 1.1429 0.4994 0.1339 0.5845 0.3185\n58   5.5743 1.0000 0.3494 0.1486 0.7363 0.4165\n\n\nSo, each of the rows corresponds to a site (i.e. each of the coastal sections), and the columns each contains an environmental variable. The names of the environmental variables are:\n\ncolnames(env)\n\n [1] \"febMean\"  \"febMax\"   \"febMed\"   \"febX95\"   \"febRange\" \"augMean\" \n [7] \"augMin\"   \"augMed\"   \"augX5\"    \"augRange\" \"annMean\"  \"annSD\"   \n[13] \"annRange\" \"febSD\"    \"augSD\"    \"annChl\"   \"augChl\"   \"febChl\"  \n\n\nAs we have seen, there are 18 variables (or dimensions). These data are truly multidimensional in a way that far exceeds our brains’ limited ability to spatially visualise. For mathematicians these data define an 18-dimensional space, but all we can do is visualise 3-dimensions.\nWe select only some of the thermal variables; the rest are collinear with some of the ones I import:\n\n  env1 &lt;- dplyr::select(env, febMean, febRange, febSD, augMean,\n                    augRange, augSD, annMean, annRange, annSD)\n\nLet us make a quick graph of annMean as a function of distance along the coast (Figure 1).\n\nggplot(env1, aes(x = 1:58, y = annMean)) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  labs(x = \"Coastal section (west to east)\",\n       y = \"Temperature (°C)\") +\n  theme_linedraw()\n\n\n\n\n\n\nFigure 1: Line plot showing the trend in the mean annual seawater temperature along the coast from the west at Section 1 to Section 58 in the East.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html#z-scores",
    "href": "BDC334/Lab-02b-env_dist.html#z-scores",
    "title": "Lab 2b. Environmental Distance",
    "section": "\nz-Scores",
    "text": "z-Scores\nHere we need to do something new that was not necessary with the toy data. We calculate z-scores, and the process is called ‘standardisation’. Standardisation is necessary when the variables are measured in different units—e.g. the unit for temperature is °C whereas Ch-a is measured in mg Chl-a/m3.\n\nE1 &lt;- round(decostand(env1, method = \"standardize\"), 4)\nE1[1:5, 1:5]\n\n  febMean febRange   febSD augMean augRange\n1 -1.4915  -0.0443 -0.2713 -1.3765  -0.4735\n2 -1.4014  -0.1432 -0.1084 -1.4339  -0.0700\n3 -1.4057  -0.3932 -0.1720 -1.5269   0.0248\n4 -1.4228  -0.6020 -0.3121 -1.5797  -0.0508\n5 -1.5368  -0.6408 -0.4096 -1.5464  -0.0983\n\n\nFor comparison with the previous plot showing the raw data, let us now plot the standardised annMean data (Figure 2).\n\nggplot(E1, aes(x = 1:58, y = annMean)) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  labs(x = \"Coastal section (west to east)\",\n       y = \"Standardised temperature\")+\n  theme_linedraw()\n\n\n\n\n\n\nFigure 2: Line plot showing the trend in the standardised mean annual seawater temperature along the coast from the west at Section 1 to Section 58 in the East.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html#euclidean-distance",
    "href": "BDC334/Lab-02b-env_dist.html#euclidean-distance",
    "title": "Lab 2b. Environmental Distance",
    "section": "Euclidean Distance",
    "text": "Euclidean Distance\n\nE1_euc &lt;- round(vegdist(E1, method = \"euclidian\", upper = TRUE), 4)\nE1_df &lt;- as.data.frame(as.matrix(E1_euc))\nE1_df[1:10, 1:10]\n\n        1      2      3      4      5      6      7      8      9     10\n1  0.0000 0.7040 1.0006 1.1132 0.9902 0.9124 0.7849 0.7957 2.7901 2.0327\n2  0.7040 0.0000 0.3769 0.6126 0.6553 0.7726 0.6291 0.5565 2.2733 1.7509\n3  1.0006 0.3769 0.0000 0.2818 0.4729 0.7594 0.7164 0.7939 2.2692 1.8055\n4  1.1132 0.6126 0.2818 0.0000 0.3662 0.7566 0.7911 0.9708 2.4523 1.9019\n5  0.9902 0.6553 0.4729 0.3662 0.0000 0.4094 0.5261 0.9860 2.4847 2.1376\n6  0.9124 0.7726 0.7594 0.7566 0.4094 0.0000 0.2862 1.0129 2.4449 2.3483\n7  0.7849 0.6291 0.7164 0.7911 0.5261 0.2862 0.0000 0.7678 2.3035 2.1656\n8  0.7957 0.5565 0.7939 0.9708 0.9860 1.0129 0.7678 0.0000 2.2251 1.5609\n9  2.7901 2.2733 2.2692 2.4523 2.4847 2.4449 2.3035 2.2251 0.0000 2.8476\n10 2.0327 1.7509 1.8055 1.9019 2.1376 2.3483 2.1656 1.5609 2.8476 0.0000\n\n\nWe already know how to read this matrix. Let’s plot it as a function of the coastal section’s number (Figure 3).\n\nggplot(data = E1_df, (aes(x = 1:58, y = `1`))) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Environmental distance\")+\n  theme_linedraw()\n\n\n\n\n\n\nFigure 3: Line plot showing the trend in environmental distance along the coast from the west at Section 1 to Section 58 in the East.\n\n\n\n\n\n\n\n\n\n\nLab 2\n\n\n\n(To be reviewed by BCB743 student but not for marks)\nUse the Doubs River environmental data for this exercise.\n\nStandardise these data using R and display a portion of the resultant standardised data file.\nDiscuss why standardisation was necessary for these data. Use the content of the actual ‘raw’ data file in your discussion.\nUsing R, calculate the Euclidean distances for these data and display a portion of the resultant distance matrix.\nDiscuss the ecological conclusions you are able to draw from these Euclidean distances. Provide a few graphs to substantiate your answer.\n\n\n\nWe will explore distance and dissimilarity matrices in more detail in later sections.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html#pairwise-correlations",
    "href": "BDC334/Lab-02b-env_dist.html#pairwise-correlations",
    "title": "Lab 2b. Environmental Distance",
    "section": "Pairwise Correlations",
    "text": "Pairwise Correlations\nIt is easy to calculate pairwise correlation matrices for the above data:\n\nenv1_cor &lt;- round(cor(env1), 2)\nenv1_cor\n\n         febMean febRange febSD augMean augRange augSD annMean annRange annSD\nfebMean     1.00    -0.27 -0.28    0.90    -0.10 -0.16    0.98     0.74  0.41\nfebRange   -0.27     1.00  0.79   -0.32     0.14  0.14   -0.29    -0.08  0.48\nfebSD      -0.28     0.79  1.00   -0.16     0.35  0.46   -0.26    -0.33  0.31\naugMean     0.90    -0.32 -0.16    1.00    -0.01 -0.05    0.96     0.37  0.13\naugRange   -0.10     0.14  0.35   -0.01     1.00  0.91   -0.10    -0.20  0.06\naugSD      -0.16     0.14  0.46   -0.05     0.91  1.00   -0.17    -0.27  0.08\nannMean     0.98    -0.29 -0.26    0.96    -0.10 -0.17    1.00     0.60  0.29\nannRange    0.74    -0.08 -0.33    0.37    -0.20 -0.27    0.60     1.00  0.68\nannSD       0.41     0.48  0.31    0.13     0.06  0.08    0.29     0.68  1.00\n\n\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nExplain in s short (1/3 page paragraph) what is meant by ‘environmental distance’.\nDescribe to your grandmother how to interpret the above correlation matrix, and also mention what the major conclusions are that can be drawn from studying the matrix. Add a mechanistic explanation to demonstrate to her what your thought processes are for reaching your conclusion.\nExplain why the same general trend is seen in the raw or standardised environmental data for annMean (Figure 1 and 2) and that of environmental distance (Figure 3).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lab-02b-env_dist.html#euclidean-distance-of-geographical-data",
    "href": "BDC334/Lab-02b-env_dist.html#euclidean-distance-of-geographical-data",
    "title": "Lab 2b. Environmental Distance",
    "section": "Euclidean Distance of Geographical Data",
    "text": "Euclidean Distance of Geographical Data\nWhen we calculate Euclidean distances between geographic lat/lon coordinate, the relationship between sections will be the same (but scaled) as actual geographic distances.\n\ngeo &lt;- read.csv(\"../data/seaweed/SeaweedSites.csv\")\ndim(geo)\n\n[1] 58  2\n\n\n\nhead(geo)\n\n   Latitude Longitude\n1 -28.98450  16.72429\n2 -29.38053  16.94238\n3 -29.83253  17.08194\n4 -30.26426  17.25928\n5 -30.67874  17.47638\n6 -31.08580  17.72167\n\n\n\nCalculate geographic distances (in meters) between coordinate pairs (Figure 4).\n\ndists &lt;- geodist(geo, paired = TRUE, measure = \"geodesic\")\ndists_df &lt;- as.data.frame(as.matrix(dists))\ncolnames(dists_df) &lt;- seq(1:58)\ndists_df[1:5, 1:5]\n\n          1         2         3         4         5\n1      0.00  48752.45 100201.82 151021.75 201380.00\n2  48752.45      0.00  51894.01 102638.03 152849.90\n3 100201.82  51894.01      0.00  50822.71 101197.22\n4 151021.75 102638.03  50822.71      0.00  50457.53\n5 201380.00 152849.90 101197.22  50457.53      0.00\n\n\n\nplt1 &lt;- ggplot(data = dists_df, (aes(x = 1:58, y = `1`/1000))) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Distance (km)\") +\n  ggtitle(\"Actual geographic distance\")+\n  theme_linedraw()\n\n\ndists_euc &lt;- vegdist(geo, method = \"euclidian\")\ndists_euc_df &lt;- round(as.data.frame(as.matrix(dists_euc)), 4)\ndists_euc_df[1:5, 1:5]\n\n       1      2      3      4      5\n1 0.0000 0.4521 0.9204 1.3871 1.8537\n2 0.4521 0.0000 0.4731 0.9388 1.4037\n3 0.9204 0.4731 0.0000 0.4667 0.9336\n4 1.3871 0.9388 0.4667 0.0000 0.4679\n5 1.8537 1.4037 0.9336 0.4679 0.0000\n\n\n\nplt2 &lt;- ggplot(data = dists_euc_df, (aes(x = 1:58, y = `1`))) +\n  geom_line(colour = \"indianred\", size = 1.2) +\n  xlab(\"Coastal section, west to east\") +\n  ylab(\"Euclidean distance\") +\n  ggtitle(\"Euclidean distance\")+\n  theme_linedraw()\n\nggarrange(plt1, plt2, ncol = 2)\n\n\n\n\n\n\nFigure 4: Line plots showing the relationship between Euclidean and geographical distance.\n\n\n\n\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nDo a full analysis of the Doubs River environmental data using Euclidean distances and correlations. Demonstrate graphically any clear spatial patterns that you might find, and offer a full suite of mechanistic explanations for the patterns you see. It is sufficient to submit a fully annotated R script (not a MS Word or Excel file).\n\n\n\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nThe Lab 2 assignment on Ecological Data was discussed on Monday 8 August and is due at 07:00 on Monday 5 August 2024.|\nProvide a neat and thoroughly annotated R file which can recreate all the graphs and all calculations. Written answers must be typed in the same file as comments.\nPlease label the R file as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_2.R\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named R documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2b. Environmental Distance"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html",
    "href": "BDC334/Lec-01-introduction.html",
    "title": "Lecture 1: Introductory Lecture",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#main-content",
    "href": "BDC334/Lec-01-introduction.html#main-content",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Main Content",
    "text": "Main Content\n\nProfessor Smit (Term 3)\n\nLatitudinal gradients in diversity.\nInteractions of body and population size on diversity and distribution.\nEarth as a system\nThe physical nature of environmental drivers of biogeography.\nGlobal change: the distinction between natural variability and anthropogenically-driven change.\nOverview of the biological responses to global change.\nBasic data collection and analytical methods in biogeography.\n\n\n\nProfessor Boatwright (Term 4)\n\nGlobal biogeography: key principles and concepts.\nContinental drift and glaciation.\nTheories of biogeography and biogeographic reconstruction.\nPhylogeography\nIsland biogeography theory and its applications for conservation.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#main-outcomes",
    "href": "BDC334/Lec-01-introduction.html#main-outcomes",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Main Outcomes",
    "text": "Main Outcomes\nOn completion of this module the student should be able to:\n\nDiscuss the past, present and projected future patterns of global biogeography.\nExamine the distribution of past floras, faunas and climate with respect to plate tectonics and compare them with current distributions.\nExplain the role that the major environmental drivers play in driving these biogeographical patterns.\nUnderstand the physical basis underpinning the components of global change.\nRecognise the central importance that humans play in bringing about global change.\nUnderstand the ecological, physiological and behavioural basis for biogeographical change.\nContrast the fundamental differences between ecological biogeography and historical biogeography.\nConsider the biogeography of key extant plant and animal lineages.\nApply the appropriate concepts to collect, analyse and interpret multivariate environmental and ecological data.\nPresent their position on the above in discussion or in written format.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#lecture-1.-overview-of-ecosystems",
    "href": "BDC334/Lec-01-introduction.html#lecture-1.-overview-of-ecosystems",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Lecture 1. Overview of Ecosystems",
    "text": "Lecture 1. Overview of Ecosystems\nThis lecture.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#lecture-2.-overview-of-ecosystems",
    "href": "BDC334/Lec-01-introduction.html#lecture-2.-overview-of-ecosystems",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Lecture 2. Overview of Ecosystems",
    "text": "Lecture 2. Overview of Ecosystems\nIn Lecture Two, “Overview of Ecosystems,” ecosystems are characterised as dynamic assemblies whose presence at particular places and times is explained by underlying environmental drivers—gradients in abiotic variables such as temperature, photoperiod and seasonality spanning tropical to polar regions and even local thermal shifts, such as those around Cape Point. I then turn my attention to ecosystem structure and contrasts these natural gradients with anthropogenic impacts that reshape community form and function. In doing so, we explore a selection of terrestrial and marine systems through both lenses. I conclude the lecture by mapping the module’s trajectory (from defining biodiversity and deploying similarity‐matrix methods to developing unifying macroecological theory) and extends consideration to planetary gradients beyond Earth and future applications in global‐change and infectious‐disease contexts.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#lecture-3.-ecological-gradients",
    "href": "BDC334/Lec-01-introduction.html#lecture-3.-ecological-gradients",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Lecture 3. Ecological Gradients",
    "text": "Lecture 3. Ecological Gradients\nIn Lecture Three, I begin by situating macroecology within the study of environmental gradients, and frame our inquiry around how life arranges itself according to shifts in temperature, moisture, and other abiotic drivers across the planet’s surface. I then define an environmental gradient as the progressive change in a variable, whether it’s the temperature drop from Johannesburg to Cape Town or the rainfall decline from KwaZulu-Natal into the Northern Cape, and show how these gradients dictate where organisms can thrive. Turning to species‐level patterns, I illustrate the classic unimodal response in which a species’ abundance peaks at its optimal temperature and wanes away from that “sweet spot,” and remind us that the same principle applies equally to gradients of humidity, soil nutrients, and beyond. To capture the true complexity of natural systems, I introduce coenoclines, coenoplanes, and coenospaces, which are conceptual tools that overlay multiple gradients into multidimensional maps of species distributions. Alongside these conceptual advances, I weave in lessons from classical quadrat-and-transect sampling methods and from ecophysiology to show how nutrient- and energy-flow processes at organismal levels scale up to community structure. Finally, I place all of this within the broader context of global change: ongoing shifts in climate and biogeochemical cycles will continually reshape these gradient-driven processes and, ultimately, the patterns we observe in biogeography.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#lecture-4.-biodiversity-concepts",
    "href": "BDC334/Lec-01-introduction.html#lecture-4.-biodiversity-concepts",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Lecture 4. Biodiversity Concepts",
    "text": "Lecture 4. Biodiversity Concepts\nIn Lecture Four, I introduce the three “Greek-letter metrics” of biodiversity (alpha, beta, and gamma) explaining that alpha diversity measures richness at the smallest sampling unit, beta diversity captures species turnover among units, and gamma diversity represents the total number of species across an entire study area. I explain that univariate indices such as Shannon’s and Simpson’s quantify not only species counts but also the relative abundances of those species, then I detail how alpha diversity measured via quadrat sampling (counting species per plot) and introduce dissimilarity indices like Bray–Curtis and Jaccard for pairwise community comparisons. Shifting to beta diversity, I characterise it as “species turnover” to illustrate how it quantifies landscape heterogeneity along environmental gradients and summarise its role in measuring compositional change. Finally, I show how gamma diversity scaled from reserve-wide quadrat totals to global species estimates, and emphasise that researchers must define alpha and gamma scales relative to their study extents, whether local quadrats or continental surveys, before selecting appropriate diversity metrics.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#lecture-5.-multivariate-data",
    "href": "BDC334/Lec-01-introduction.html#lecture-5.-multivariate-data",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Lecture 5. Multivariate Data",
    "text": "Lecture 5. Multivariate Data\nIn Lecture Five, I introduce the multivariate nature of ecological data by describing how we sample eight quadrats (“sites A” through “H”) and count species to distinguish abundance data (where zeros indicate absence and positive integers record quantity) from presence–absence data, which simply flags occurrence with ones and zeros. I explain that our goal is to determine how similar these sites are in species composition, whether they share the same taxa or differ in relative abundances, and that similarity can arise from both shared presence and uneven abundance patterns. To quantify these relationships, I introduce distance and dissimilarity matrices, using Euclidean distance for environmental variables and indices such as Bray–Curtis, Sørensen or Jaccard for species‐composition data. I then detail the Euclidean formula by extending the Pythagorean theorem to three and \\(n\\) dimensions to calculate environmental distances across any number of variables. I conclude by emphasising that ecological datasets inhabit a space defined by as many dimensions as there are measurements, and that constructing these multivariate distance matrices provides the foundation for all further analyses in the module.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lec-01-introduction.html#lecture-6.-unified-ecology",
    "href": "BDC334/Lec-01-introduction.html#lecture-6.-unified-ecology",
    "title": "Lecture 1: Introductory Lecture",
    "section": "Lecture 6. Unified Ecology",
    "text": "Lecture 6. Unified Ecology\nIn Lecture Six, I begin by reviewing foundational papers that drive the ambition of unified macroecology, which is asking whether patterns identified in multicellular organisms also hold for microbial life across scales. I then highlight advances in genetic and computational tools (high-throughput sequencing and big-data analyses) that allow us to treat soil or water samples as ecological quadrats, before unpacking metabolic-scaling laws: the three-quarters power law of multicellular organisms, the linear 1:1 scaling in protists, and the doubling rule in bacteria, each of which reveals physiological underpinnings of ecosystem function. Turning to Shade et al. (2018), I explore the challenges of defining ‘individuals’ and ‘species’ in microbes and set out a unified accounting framework that quantitatively links species richness and abundance to spatial scale and sampling effort. The lecture then demonstrates key empirical tools: rank-abundance curves show many rare versus few dominant species, occupancy–abundance relationships illustrate how prevalence rises with local abundance, species–area curves depict the plateau of cumulative richness with additional samples, and distance-decay analyses reveal how community similarity wanes over space. All of these are presented as indispensable analytics for a truly unified macroecological theory.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 1: Introductory Lecture"
    ]
  },
  {
    "objectID": "BDC334/Lab-04-biodiversity.html",
    "href": "BDC334/Lab-04-biodiversity.html",
    "title": "Lab 4. Species Distribution Patterns",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.\nIn this Lab, we will calculate the various species distribution patterns included in the paper by Shade et al. (2018).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 4. Species Distribution Patterns"
    ]
  },
  {
    "objectID": "BDC334/Lab-04-biodiversity.html#the-data",
    "href": "BDC334/Lab-04-biodiversity.html#the-data",
    "title": "Lab 4. Species Distribution Patterns",
    "section": "The Data",
    "text": "The Data\nWe will calculate each for the Barro Colorado Island Tree Counts data that come with vegan. See ?vegan::BCI for a description of the data contained with the package, as well as a selection of publications relevant to the data and analyses. The primary publication of interest is Condit et al. (2002).\n\nlibrary(tidyverse)\nlibrary(vegan)\n\n\n#library(vegan) # already loaded\n#library(tidyverse) # already loaded\ndata(BCI) # data contained within vegan\n\n# make a head-tail function\nht &lt;- function(d) rbind(head(d, 7), tail(d, 7))\n\n# Lets look at a portion of the data:\nht(BCI)[1:7,1:7]\n\n  Abarema.macradenia Vachellia.melanoceras Acalypha.diversifolia\n1                  0                     0                     0\n2                  0                     0                     0\n3                  0                     0                     0\n4                  0                     0                     0\n5                  0                     0                     0\n6                  0                     0                     0\n7                  0                     0                     0\n  Acalypha.macrostachya Adelia.triloba Aegiphila.panamensis\n1                     0              0                    0\n2                     0              0                    0\n3                     0              0                    0\n4                     0              3                    0\n5                     0              1                    1\n6                     0              0                    0\n7                     0              0                    1\n  Alchornea.costaricensis\n1                       2\n2                       1\n3                       2\n4                      18\n5                       3\n6                       2\n7                       0\n\n\nSpecies-Abundance Distribution\nThe species abundance distribution (SAD) is a fundamental pattern in ecology. Typical communities have a few species that are very abundant, whereas most of them are quite rare; indeed—this is perhaps a universal law in ecology. SAD represents this relationship graphically by plotting the abundance rank on the \\(x\\)-axis and the number of species (or some other taxonomic level) along \\(y\\), as was first done by Fisher et al. (1943). He then fitted the data by log series that ideally capture situations where most of the species are quite rare with only a few very abundant ones—called Fisher’s log series distribution—and is implemented in vegan by the fisherfit() function (Figure 1). The curve in Fisher’s logarithmic series shows the expected number of species \\(f\\) with \\(n\\) observed individuals. In fact, the interpretation of the curve is the same for all species-abundance models shown below, and it is only the math and rationale that differ.\n\n# take one random sample of a row (site):\n# for this website's purpose, this function ensure the same random\n# sample is drawn each time the web page is recreated\nset.seed(13) \nk &lt;- sample(nrow(BCI), 1)\nfish &lt;- fisherfit(BCI[k,])\nfish\n\n\nFisher log series model\nNo. of species: 95 \nFisher alpha:   39.87659 \n\nplot(fish)\n\n\n\n\n\n\nFigure 1: Fisher’s log series distribution calculated for the Barro Colorado Island Tree Counts data.\n\n\n\n\nPreston (1948) showed that when data from a thoroughly sampled population are transformed into octaves along the \\(x\\)-axis (number of species binned into intervals of 1, 2, 4, 8, 16, 32 etc.), the SAD that results is approximated by a symmetric Gaussian distribution. This is because more thorough sampling makes species that occur with a high frequency more common and those that occur only once or are very rare become either less common will remain completely absent. This SAD is called Preston’s log-normal distribution. In the vegan package there is an updated version of Preston’s approach with a mathematical improvement to better handle ties. It is called prestondistr() (Figure 2):\n\npres &lt;- prestondistr(BCI[k,])\npres\n\n\nPreston lognormal model\nMethod: maximized likelihood to log2 abundances \nNo. of species: 95 \n\n      mode      width         S0 \n 0.9234918  1.6267630 26.4300640 \n\nFrequencies by Octave\n                0        1        2        3        4        5         6\nObserved 19.00000 27.00000 21.50000 17.00000 7.000000 2.500000 1.0000000\nFitted   22.49669 26.40085 21.23279 11.70269 4.420327 1.144228 0.2029835\n\nplot(pres)\n\n\n\n\n\n\nFigure 2: Preston’s log-normal distribution demonstrated for the BCI data.\n\n\n\n\nWhittaker (1965) introduced rank abundance distribution curves (RAD; sometimes called a dominance-diversity curve or Whittaker plots). Here the \\(x\\)-axis has species ranked according to their relative abundance, with the most abundant species at the left and rarest at the right. The \\(y\\)-axis represents relative species abundances (sometimes log-transformed). The shape of the profile as—influenced by the steepness and the length of the tail—indicates the relative proportion of abundant and scarce species in the community. In vegan we can accomplish fitting this type of SAD with the radfit() function. The default plot is somewhat more complicated as it shows broken-stick, preemption, log-Normal, Zipf and Zipf-Mandelbrot models fitted to the ranked species abundance data (Figure 3):\n\nrad &lt;- radfit(BCI[k,])\nrad\n\n\nRAD models, family poisson \nNo. of species 95, total abundance 392\n\n           par1      par2     par3    Deviance AIC      BIC     \nNull                                   56.3132 324.6477 324.6477\nPreemption  0.042685                   55.8621 326.1966 328.7504\nLognormal   0.84069   1.0912           16.1740 288.5085 293.6162\nZipf        0.12791  -0.80986          21.0817 293.4161 298.5239\nMandelbrot  0.66461  -1.2374   4.1886   6.6132 280.9476 288.6093\n\nplot(rad)\n\n\n\n\n\n\nFigure 3: Whittaker’s rank abundance distribution curves demonstrated for the BCI data.\n\n\n\n\nWe can also fit the rank abundance distribution curves to several sites at once (previously we have done so on only one site) (Figure 4):\n\nm &lt;- sample(nrow(BCI), 6)\nrad2 &lt;- radfit(BCI[m, ])\nrad2\n\n\nDeviance for RAD models:\n\n                  3       37       10       13        6       22\nNull        86.1127  93.5952  77.2737  52.6207  72.1627 114.1747\nPreemption  58.9295 104.0978  62.7210  57.7372  54.7709 110.5156\nLognormal   29.2719  19.0653  20.4770  15.8218  19.5788  26.2510\nZipf        50.1262  11.3048  39.7066  22.8006  32.4630  15.5222\nMandelbrot   5.7342   8.9107   9.8353  12.1701   5.5973   9.6047\n\nplot(rad2)\n\n\n\n\n\n\nFigure 4: Rank abundance distribution curves fitted to several sites.\n\n\n\n\nAbove, we see that the model selected for capturing the shape of the SAD is the Mandelbrot, and it is plotted individually for each of the randomly selected sites. Model selection works through Akaike’s or Schwartz’s Bayesian information criteria (AIC or BIC; AIC is the default—select the model with the lowest AIC).\nBiodiversityR (and here and here) also offers options for rank abundance distribution curves; see rankabundance() (Figure 5):\n\nlibrary(BiodiversityR)\nrankabund &lt;- rankabundance(BCI)\nrankabunplot(rankabund, cex = 0.8, pch = 0.8, col = \"indianred4\")\n\n\n\n\n\n\nFigure 5: Rank-abundance curves for the BCI data.\n\n\n\n\nRefer to the help files for the respective functions to see their differences.\nOccupancy-Abundance Curves\nOccupancy refers to the number or proportion of sites in which a species is detected. Occupancy-abundance relationships are used to infer niche specialisation patterns in the sampling region. The hypothesis (almost a theory) is that species that tend to have high local abundance within one site also tend to occupy many other sites (Figure 6).\n\nlibrary(ggpubr)\n\n# A function for counts:\n# count number of non-zero elements per column\ncount_fun &lt;- function(x) {\n  length(x[x &gt; 0])\n}\n\nBCI_OA &lt;- data.frame(occ = apply(BCI, MARGIN = 2, count_fun),\n                     ab = apply(BCI, MARGIN = 2, mean))\n\nggplot(BCI_OA, aes(x = ab, y = occ/max(occ))) +\n  geom_point(colour = \"indianred3\") +\n  scale_x_log10() +\n  # scale_y_log10() +\n  labs(title = \"Barro Colorado Island Tree Counts\",\n     x = \"Log (abundance)\", y = \"Occupancy\") +\n  theme_linedraw()\n\n\n\n\n\n\nFigure 6: Occupancy-abundance relationships seen in the BCI data.\n\n\n\n\nSpecies-Area (Accumulation)\nSpecies accumulation curves (species area relationships, SAR) try and estimate the number of unseen species. These curves can be used to predict and compare changes in diversity over increasing spatial extent. Within an ecosystem type, one would expect that more and more species would be added (accumulates) as the number of sampled sites increases (i.e. extent increases). This continues to a point where no more new species are added as the number of sampled sites continues to increase (i.e. the curve plateaus). Species accumulation curves, as the name suggests, accomplishes this by adding (accumulation or collecting) more and more sites and counting the average number of species along \\(y\\) each time a new site is added. See Roeland Kindt’s description of how species accumulation curves work (on p. 41). In the community matrix (the sites × species table), we can do this by successively adding more rows to the curve (seen along the \\(x\\)-axis). The specaccum() function has many different ways of adding the new sites to the curve, but the default ‘exact’ seems to be a sensible choice. BiodiversityR has the accumresult() function that does nearly the same. Let’s demonstrate using vegan’s function (Figure 7, Figure 8, and Figure 9):\n\nsp1 &lt;- specaccum(BCI)\nsp2 &lt;- specaccum(BCI, \"random\")\n\n# par(mfrow = c(2,2), mar = c(4,2,2,1))\n# par(mfrow = c(1,2))\nplot(sp1, ci.type = \"polygon\", col = \"indianred4\", lwd = 2, ci.lty = 0,\n     ci.col = \"steelblue2\", main = \"Default: exact\",\n     ylab = \"No. of species\")\n\n\n\n\n\n\nFigure 7: Species-area accumulation curves seen in the BCI data.\n\n\n\n\n\nmods &lt;- fitspecaccum(sp2, \"arrh\")\nplot(mods, col = \"indianred\", ylab = \"No. of species\")\nboxplot(sp2, col = \"yellow\", border = \"steelblue2\", lty = 1, cex = 0.3, add = TRUE)\nsapply(mods$models, AIC)\n\n  [1] 311.4642 303.7835 346.3668 320.0786 338.7978 320.2538 325.6968 346.2671\n  [9] 320.3900 343.8570 318.2509 369.8303 335.9936 350.8711 327.9831 348.1287\n [17] 328.2393 347.8133 324.3837 314.8555 333.1390 340.5678 332.6836 360.5208\n [25] 335.3660 325.3150 347.4324 336.7498 336.6374 276.1878 349.9283 295.0268\n [33] 308.4656 315.8304 303.0776 329.8425 356.2393 368.4302 318.0514 359.5975\n [41] 327.4228 335.7604 259.8340 318.0063 335.7753 285.8790 323.5174 300.3546\n [49] 327.1448 355.2747 288.2583 366.5995 287.4120 327.5877 362.6487 323.5904\n [57] 339.5650 321.2264 336.6331 353.1295 317.9578 311.6528 336.3613 337.8327\n [65] 328.4787 311.6842 345.8035 367.5620 319.0269 305.6546 338.7805 321.8859\n [73] 330.6029 326.7097 345.8923 338.4755 352.8710 355.8038 307.7327 329.2355\n [81] 341.6628 340.1687 333.4771 348.3144 321.4417 317.4331 339.2211 313.1990\n [89] 305.3069 342.4581 318.0308 299.7067 294.7851 324.3237 333.5849 349.2749\n [97] 369.8287 323.0041 332.6820 329.3875\n\n\n\n\n\n\n\nFigure 8: Fit Arrhenius models to all random accumulations\n\n\n\n\n\naccum &lt;- accumresult(BCI, method = \"exact\", permutations = 100)\naccumplot(accum)\n\n\n\n\n\n\nFigure 9: A species accumulation curve.\n\n\n\n\nSpecies accumulation curves can also be calculated with the alpha.accum() function of the BAT package (Figure 10). In addition, the BAT package can also apply various diversity and species distribution assessments to phylogenetic and functional diversity. See the examples provided by Cardoso et al. (2015).\n\nlibrary(BAT)\nBCI.acc &lt;- alpha.accum(BCI, prog = FALSE)\n\npar(mfrow = c(1,2))\nplot(BCI.acc[,2], BCI.acc[,17], col = \"indianred\",\n     xlab = \"Individuals\", ylab = \"Chao1P\")\nplot(BCI.acc[,2], slope(BCI.acc)[,17], col = \"indianred\",\n     xlab = \"Individuals\", ylab = \"Slope\")\n\n\n\n\n\n\nFigure 10: A species accumulation curve made with the alpha.accum() function of BAT.\n\n\n\n\nRarefaction Curves\nLike species accumulation curves, rarefaction curves also try to estimate the number of unseen species. Rarefaction, meaning to scale down (Heck Jr et al. 1975), is a statistical technique used by ecologists to assess species richness (represented as S, or diversity indices such as Shannon diversity, \\(H'\\), or Simpson’s diversity, \\(\\lambda\\)) from data on species samples, such as that which we may find in site × species tables. Rarefaction can be used to determine whether a habitat, community, or ecosystem has been sufficiently sampled to fully capture the full complement of species present.\nRarefaction curves may seem similar to species accumulation curves, but there is a difference as I will note below. Species richness, S, accumulates with sample size or with the number of individuals sampled (across all species). The first way that rarefaction curves are presented is to show species richness as a function of number of individuals sampled. Here the principle demonstrated is that when only a few individuals are sampled, those individuals may belong to only a few species; however, when more individuals are present more species will be represented. The second approach to rarefaction is to plot the number of samples along \\(x\\) and the species richness along the \\(y\\)-axis (as in SADs too). So, rarefaction shows how richness accumulates with the number of individuals counted or with the number of samples taken. Rarefaction curves rise rapidly at the start when few species have been sampled and the most common species have been found; the slope then decreases and eventually plateaus suggesting that the rarest species remain to be sampled.\nBut what really distinguishes rarefaction curves from SADs is that rarefaction randomly re-samples the pool of \\(N\\) samples (that is equal or less than the total community size) a number of times, \\(n\\), and plots the average number of species found in each resample (1,2, …, \\(n\\)) as a function of individuals or samples. The rarecurve() function draws a rarefaction curve for each row of the species data table. All these plots are made with base R graphics Figure 11, but it will be a trivial exercise to reproduce them with ggplot2.\n\n# Example provided in ?vegan::rarefy\n# observed number of species per row (site)\nS &lt;- specnumber(BCI) \n\n# calculate total no. individuals sampled per row, and find the minimum\n(raremax &lt;- min(rowSums(BCI)))\n\n[1] 340\n\nSrare &lt;- rarefy(BCI, raremax, se = FALSE)\npar(mfrow = c(1,2))\nplot(S, Srare, col = \"indianred3\",\n     xlab = \"Sample size\\n(observed no. of individuals)\", ylab = \"No. species found\")\nrarecurve(BCI, step = 20, sample = raremax, col = \"indianred3\", cex = 0.6,\n          xlab = \"Sample size\\n(observed no. of individuals)\", ylab = \"No. species found\")\n\n\n\n\n\n\nFigure 11: Rarefaction curves for the BCI data.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\niNEXT\nWe can also use the iNEXT package for rarefaction curves. From the package’s Introduction Vignette:\niNEXT focuses on three measures of Hill numbers of order q: species richness (q = 0), Shannon diversity (q = 1, the exponential of Shannon entropy) and Simpson diversity (q = 2, the inverse of Simpson concentration). For each diversity measure, iNEXT uses the observed sample of abundance or incidence data (called the “reference sample”) to compute diversity estimates and the associated 95% confidence intervals for the following two types of rarefaction and extrapolation (R/E):\n\nSample‐size‐based R/E sampling curves: iNEXT computes diversity estimates for rarefied and extrapolated samples up to an appropriate size. This type of sampling curve plots the diversity estimates with respect to sample size.\nCoverage‐based R/E sampling curves: iNEXT computes diversity estimates for rarefied and extrapolated samples with sample completeness (as measured by sample coverage) up to an appropriate coverage. This type of sampling curve plots the diversity estimates with respect to sample coverage.\n\niNEXT also plots the above two types of sampling curves and a sample completeness curve. The sample completeness curve provides a bridge between these two types of curves.\nFor information about Hill numbers see David Zelený’s Analysis of community data in R and Jari Oksanen’s coverage of diversity measures in vegan.\nThere are four datasets distributed with iNEXT and numerous examples are provided in the Introduction Vignette. iNEXT has an ‘odd’ data format that might seem foreign to vegan users. To use iNEXT with dataset suitable for analysis in vegan, we first need to convert BCI data to a species × site matrix (Figure 12):\n\nlibrary(iNEXT)\n\n# transpose the BCI data: \nBCI_t &lt;- list(BCI = t(BCI))\nstr(BCI_t)\n\nList of 1\n $ BCI: int [1:225, 1:50] 0 0 0 0 0 0 2 0 0 0 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:225] \"Abarema.macradenia\" \"Vachellia.melanoceras\" \"Acalypha.diversifolia\" \"Acalypha.macrostachya\" ...\n  .. ..$ : chr [1:50] \"1\" \"2\" \"3\" \"4\" ...\n\nBCI_out &lt;- iNEXT(BCI_t, q = c(0, 1, 2), datatype = \"incidence_raw\")\nggiNEXT(BCI_out, type = 1, color.var = \"Order.q\")\n\n\n\n\n\n\n\n\n\nThe warning is produced because the function expects incidence data (presence-absence), but I’m feeding it abundance (count) data. Nothing serious, as the function converts the abundance data to incidences.\n\n\n\nFigure 12: Demonstration of iNEXT capabilities.\nDistance-Decay Curves\nThe principles of distance decay relationships are clearly captured in analyses of \\(\\beta\\)-diversity—see specifically turnover, \\(\\beta_\\text{sim}\\). Distance decay is the primary explanation for the spatial pattern of \\(\\beta\\)-diversity along the South African coast in Smit et al. (2017). A deeper dive into distance decay calculation can be seen in Deep Dive into Gradients.\nElevation and Other Gradients\nIn once sense, an elevation gradient can be seen as specific case of distance decay. The Doubs River dataset offer a nice example of data collected along an elevation gradient. Elevation gradients have many similarities with depth gradients (e.g. down the ocean depths) and latitudinal gradients.\n\n\n\n\n\n\nLab 4\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\n\nProduce the following figures for the species data indicated in [square brackets]:\n\nspecies-abundance distribution [mite];\noccupancy-abundance curves [mite];\nspecies-area curves [seaweed]—note, do not use the BAT package’s alpha.accum() function as your computer might fall over;\nrarefaction curves [mite].\n\nAnswer each under its own heading. For each, also explain briefly what the purpose of the analysis is (i.e. what ecological insights might be provided), and describe the findings of your own analysis as well as any ecological implications that you might be able to detect.\n\nUsing the biodiversityR package, find the most dominant species in the Doubs River dataset.\nDiscuss how elevation, depth, or latitudinal gradients are similar in many aspects to distance decay relationships.\n\n\n\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nThe Lab 4 assignment is due at 07:00 on Monday 19 August 2024.\nProvide a neat and thoroughly annotated R file which can recreate all the graphs and all calculations. Written answers must be typed in the same file as comments.\nPlease label the R file as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_4.R\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named R documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 4. Species Distribution Patterns"
    ]
  },
  {
    "objectID": "BDC334/Lec-02-ecosystems.html",
    "href": "BDC334/Lec-02-ecosystems.html",
    "title": "Lecture 2. Overview of Ecosystems",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.\n\n\n\n\n\n\n\n\nBDC334 Lecture Transcript\n\n\n\nPlease see the BDC334 Lecture Transcript for the main content of all lectures.\n\n\n\nEcological Concepts\nWhen we talk about ‘ecology’, central to our discussion is the concept of biodiversity. The Convention on Biological Diversity defines biodiversity as:\n\n“The variability among living organisms from all sources including, inter alia, terrestrial, marine and other aquatic ecosystems and the ecological complexes of which they are part; this includes diversity within species, between species and of ecosystems.”\n\nIn this lecture, we will work towards an understanding of macroecology by working through these topics:\n\n‘Traditional’ ecology—focus on the ‘local’ scale.\nThe distinction between populations and communities.\nA definition for what ecology is.\nThe concept of the ‘niche’ (fundamental and realised).\nThe concept of ‘species’.\nDescribe the properties of communities, viz. mainly structure and function.\nUsing measures of diversity to understand structure.\nArrive at the concept of macroecology.\n\nIn this module, we’ll rely on thinking emerging from a unifying field of ecology called macroecology. According to Keith et al. (2012), macroecology is:\n\n“…the study of the mechanisms underlying general patterns of ecology across scales.”\n\n\n\nMacroecology: Ecology Across Scales\n\nFor a deeper dive into macroecology, please see the paper Shade et al. (2018). I provide some additional views on macroecology to supplement the insights you extract from this publication.\n\nMacroecology explores ecological patterns and processes across a wide range of scales (from microbes to blue whales, from the Cape Flats Nature Reserve to the whole of Earth, and from the Pleistocene to 2100). To the best of my knowledge, the term ‘macroecology’ was coined by Brown and Maurer (1989), who used it to study continental biotas. The term has since then undergone much growth and evolution in recent decades. More recently, it has led to attempts to develop unified theories of ecology Shade et al. (2018), driven by a convergence of technological and methodological advancements, building upon earlier foundations laid by disciplines such as ‘phytosociology.’\nPhytosociology (phytocoenology or plant sociology) studies and classifies plant communities. It has greatly influenced modern ecological research. Phytosociologists emphasise systematic vegetation classification and the understanding of plant community structure, which prepared the ground for many concepts in macroecology. For example, the Braun-Blanquet method, a cornerstone method in phytosociology since its development by Josias Braun-Blanquet (1884-1980) (Dengler et al. 2008; Dengler 2016), still forms a standardised approach to vegetation sampling. The method has been adapted and expanded in the study of, amongst other things, benthic (limnetic and marine) communities, as it is well suited to sampling communities comprised mainly of sessile organisms.\nRecent progress in macroecology was achieved through advances in several key areas. Molecular phylogenetics provided new insights into evolutionary relationships, while high-resolution datasets of abiotic and biotic variables offered unprecedented detail about environmental conditions and species distributions. Today’s vast (and rapidly growing) computational power allows the processing and analysis of these complex datasets, and novel numerical approaches and a robust statistical framework provide tools to extract insightful patterns from the data.\nIncreased knowledge sharing and access to open data have further accelerated the growth of macroecology. Wider collaborative networks of ecologists now provide a more integrated understanding of ecological systems across broad spatial and temporal scales. We can now tackle complex questions that were previously out of reach.\nSome of these fundamental questions include inquiries about variations in body size across species and regions, the patterns of biodiversity at global spatial scales and over geological timescales, abundance distributions across size classes of organisms, geographical range dynamics as we experience the various pressures of global change and the role of neutral processes in shaping ecological communities.\nWhile ‘traditional’ ecology primarily focuses on describing natural patterns, macroecology has shifted towards finding mechanistic explanations for the processes resulting in observed biodiversity patterns. This transition advances our understanding of ecological systems, moving beyond mere description to explanation and prediction. Ecological systems are also increasingly coupled with Earth system models to offer projections of ecological structure and function in the future.\nToday’s ecology students must reconcile their biological observations and knowledge with hypotheses about patterns and processes and understand the statistical models used to explain them. New frameworks are being developed to integrate biological theory with sophisticated statistical techniques, and we can conduct more robust and meaningful analyses of large-scale ecological data.\nA key insight from this approach is the recognition that local species interactions can explain broad-scale patterns in species distributions. This understanding bridges the gap between small-scale ecological studies and large-scale macroecological patterns and provides a more cohesive view of how ecosystems function across different spatial scales.\nThis growing recognition that links local processes to global patterns has led some ecologists to try and find unified theories of ecology. These theories aim to be predictive by offering explanations for observed patterns and the ability to forecast future ecological scenarios. Such unified theories represent a holy grail in ecology and potentially provide an integrated framework for understanding and predicting ecological phenomena across scales and systems.\nThe advancements in macroecology have significantly enhanced our comprehension of biodiversity, ecosystem functioning, and ecological responses to global change. Notably, macroecology has exerted a remarkably wide-ranging and transformative impact at the intersection of scientific research and policy-making. This influence is especially evident in land-use management, climate change mitigation and adaptation strategies, and efforts to address biodiversity loss.\n\n\n\n\n\n\n\nReferences\n\nBrown JH, Maurer BA (1989) Macroecology: The division of food and space among species on continents. Science 243:1145–1150.\n\n\nDengler J (2016) Phytosociology. International Encyclopedia of Geography: People, the Earth, Environment and Technology: People, the Earth, Environment and Technology 1–6.\n\n\nDengler J, Chytrý M, Ewald J (2008) Phytosociology. In: Jørgensen SE, Fath BD (eds) Encyclopedia of ecology. Academic Press, Oxford, pp 2767–2779\n\n\nKeith SA, Webb TJ, Böhning-Gaese K, Connolly SR, Dulvy NK, Eigenbrod F, Jones KE, Price T, Redding DW, Owens IP, others (2012) What is macroecology?\n\n\nShade A, Dunn RR, Blowes SA, Keil P, Bohannan BJ, Herrmann M, Küsel K, Lennon JT, Sanders NJ, Storch D, others (2018) Macroecology to unite all life, large and small. Trends in ecology & evolution 33:731–744.\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2024,\n  author = {Smit, A. J.,},\n  title = {Lecture 2. {Overview} of {Ecosystems}},\n  date = {2024-07-19},\n  url = {http://tangledbank.netlify.app/BDC334/Lec-02-ecosystems.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2024) Lecture 2. Overview of Ecosystems. http://tangledbank.netlify.app/BDC334/Lec-02-ecosystems.html.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 2. Overview of Ecosystems"
    ]
  },
  {
    "objectID": "BCB744/intro_r/05-graphics.html#geom_-the-pipe-or-and-the-sign",
    "href": "BCB744/intro_r/05-graphics.html#geom_-the-pipe-or-and-the-sign",
    "title": "5. Graphics with ggplot2\n",
    "section": "\ngeom_*(), the pipe (%>% or |>), and the + sign",
    "text": "geom_*(), the pipe (%&gt;% or |&gt;), and the + sign\nAs part of the tidyverse (as we saw briefly on Day 1, and will go into in depth on Day 4), the ggplot2 package endeavours to use a clean, easy for humans to understand syntax that relies heavily on functions that do what they say. For example, the function geom_point() makes points on a figure. Need a line plot? geom_line() is the way to go! Need both at the same time? No problem. In ggplot2 we may seamlessly merge a nearly limitless number of objects together to create startlingly sophisticated figures. Before we go over the code below, it is very important to note the use of the + signs. This is different from the pipe symbol (|&gt; or %&gt;%) used elsewhere in the tidyverse. The + sign indicates that one set of geometric features is added to another, each building on top of what came before. In other words, we add one geometry on top of the next, and in such a way we can arrive at complex graphical representations of data. Effectively, each line of code represents one new geometric feature with its own aesthetic appearance of the figure. It is designed this way so as to make it easier for the human eye to read through the code.\n\n\n\n\n\n\n+ signs in ggplot() code\n\n\n\nOne may see below that the code naturally indents itself if the previous line ended with a + sign. This is because R knows that the top line is the parent line and the indented lines are it’s children. This is a concept that will come up again when we learn about tidying data. What we need to know now is that a block of code that has + signs, like the one below, must be run together. As long as lines of code end in +, R will assume that you want to keep adding lines of code (more geometric features). If we are not mindful of what we are doing we may tell R to do something it cannot and we will see in the console that R keeps expecting more + signs. If this happens, click inside the console window and push the esc button to cancel the chain of code you are trying to enter.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "5. Graphics with **ggplot2**"
    ]
  },
  {
    "objectID": "BCB744/intro_r/05-graphics.html#aes",
    "href": "BCB744/intro_r/05-graphics.html#aes",
    "title": "5. Graphics with ggplot2\n",
    "section": "aes()",
    "text": "aes()\nAnother recurring function within the parent ggplot() function or the associated geom_*() is aes(). The aes() function in ggplot2 is used to specify the mapping between variables in a dataframe and visual properties of a plot. aes() stands for ‘aesthetic,’ which refers to the visual elements of a plot, such as colour, size, shape, etc. In ggplot2, the aesthetics of a plot are defined inside the aes() function, which is passed as an argument to the base ggplot() function or its associated geometry.\nFor example, if you have a dataframe with two variables x and y, you can create a scatterplot of x against y by calling ggplot(data, aes(x, y)) + geom_point(). The aes(x, y) function maps the variables (columns) in the dataframe to the x and y positions of the points in the scatterplot. Similarly, we can map variables in the dataframe to aesthetic properties of the geometric features, such as colour (e.g. a colour might be more internse as the magnitude of the values in a column increase), size (larger symbols for bigger values), transparency, etc.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "5. Graphics with **ggplot2**"
    ]
  },
  {
    "objectID": "BCB744/intro_r/13-tidier.html",
    "href": "BCB744/intro_r/13-tidier.html",
    "title": "13. Tidier Data",
    "section": "",
    "text": "“Knowing where things are, and why, is essential to rational decision making.”\n— Jack Dangermond\n\n\n“The mind commands the body and it obeys. The mind orders itself and meets resistance.”\n— Frank Herbert, Dune\n\nOn Day 1 already you worked through a tidy workflow. You saw how to import data, how to manipulate it, run a quick analysis or two, and create figures. In the previous session you filled in the missing piece of the workflow by also learning how to tidy up your data within R. For the remainder of today you will be revisiting the ‘transform’ portion of the tidy workflow. In this session you are going to go into more depth on what you learned in Day 1, and in the last session you will learn some new tricks. Over these two sessions you will also become more comfortable with the pipe command %&gt;%, while practising writing tidy code.\nThere are five primary data transformation functions that you will focus on here:\n\nArrange observations (rows) with arrange()\n\nFilter observations (rows) with filter()\n\nSelect variables (columns) with select()\n\nCreate new variables (columns) with mutate()\n\nSummarise variables (columns) with summarise()\n\n\nYou will use the full South African Coastal Temperature Network dataset for these exercises. Before you begin, however, you will need to cover two new concepts.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load the data from a .RData file\nload(\"../../data/SACTNmonthly_v4.0.RData\")\n\n# Copy the data as a dataframe with a shorter name\nSACTN &lt;- SACTNmonthly_v4.0\n\n# Remove the original\nrm(SACTNmonthly_v4.0)\n\nComparison and logical (Boolean) operators\nThe assignment operator (&lt;-) is a symbol that we use to assign some bit of code to an object in your environment. Likewise, comparison operators are symbols we use to compare different objects. This is how you tell R how to decide to do many different things. You will see these symbols often out in the ‘real world’ so let’s spend a moment now getting to know them better. Most of these should be very familiar to you already:\n\n\n&gt;: Greater than\n\n\n&gt;=: Greater than or equal to\n\n&lt;: Less than\n\n&lt;=: Less than or equal to\n\n==: Equal to\n\n!= Not equal to\n\nIt is important here to note that == is for comparisons and = is for maths. They are not interchangeable, as we may see in the following code chunk. This is one of the more common mistakes one makes when writing code. Luckily the error message this creates should provide us with the clues we need to figure out that we have made this specific mistake.\n\nSACTN %&gt;% \n  filter(site = \"Amanzimtoti\")\n\nR&gt; Error in `filter()`:\nR&gt; ! We detected a named input.\nR&gt; ℹ This usually means that you've used `=` instead of `==`.\nR&gt; ℹ Did you mean `site == \"Amanzimtoti\"`?\n\n\nThe comparison operators are often used together with Boolean operators. Boolean operators are used for logical operations and can compare values, resulting in either TRUE or FALSE. Here they are:\n\n\n!: NOT - Negates a true value to false, and a false value to true.\n\n&: AND - Returns TRUE if both operands are true, and FALSE otherwise.\n\n|: OR - Returns TRUE if at least one of the operands is true.\n\n&&: AND (element-wise for vectors) - Similar to &, but it only evaluates the first element of each vector operand.\n\n||: OR (element-wise for vectors) - Similar to |, but it only evaluates the first element of each vector operand.\n\nThe %in% operator in R is a special operator used to test if elements of a vector or data object are contained in another vector or data object. It returns a Boolean vector (TRUE or FALSE) indicating whether each element of the first vector is found in the second vector. This operator is particularly useful for subsetting or filtering data based on matching values. For example, x %in% y will check for each element of x if there is a match in y, and return a logical vector indicating the presence or absence of each x element in y.\nSo, comparison operators are used to make direct comparisons between specific things, but logical operators are used more broadly when making logical arguments. Logic is central to most computing so it is worth taking the time to cover these symbols explicitly here. R makes use of the same Boolean logic symbols as many other platforms, including Google, so some (or all) of these will likely be familiar.\nWhen writing a line of tidy code you tend to use these logical operator to combine two or more arguments that use comparison operators. For example, the following code chunk uses the filter() function to find all temperatures recorded at Pollock Beach during December or January. Don’t worry if the following line of code is difficult to piece out, but make sure you can locate which symbols are comparison operators and which are logical operators. Please note that for purposes of brevity all of the outputs in this section are limited to ten lines, but when you run these code chunks on your own computer they will be much longer.\n\nSACTN %&gt;% \n  filter(site == \"Pollock Beach\", month(date) == 12 | month(date) == 1)\n\n\n\nR&gt;             site  src       date     temp depth   type\nR&gt; 1  Pollock Beach SAWS 1999-12-01 19.95000     0 thermo\nR&gt; 2  Pollock Beach SAWS 2000-01-01 19.03333     0 thermo\nR&gt; 3  Pollock Beach SAWS 2000-12-01 19.20000     0 thermo\nR&gt; 4  Pollock Beach SAWS 2001-01-01 18.32667     0 thermo\nR&gt; 5  Pollock Beach SAWS 2001-12-01 20.59032     0 thermo\nR&gt; 6  Pollock Beach SAWS 2002-01-01 21.47097     0 thermo\nR&gt; 7  Pollock Beach SAWS 2002-12-01 19.78065     0 thermo\nR&gt; 8  Pollock Beach SAWS 2003-01-01 20.64516     0 thermo\nR&gt; 9  Pollock Beach SAWS 2003-12-01 20.48710     0 thermo\nR&gt; 10 Pollock Beach SAWS 2004-01-01 21.34839     0 thermo\n\n\nYou will look at the interplay between comparison and logical operators in more depth in the following session after you have reacquainted yourself with the main transformation functions you need to know.\nArrange observations (rows) with arrange()\n\nFirst up in our greatest hits reunion tour is the function arrange(). This very simply arranges the observations (rows) in a dataframe based on the variables (columns) it is given. If you are concerned with ties in the ordering of our data you provide additional columns to arrange(). The importance of the columns for arranging the rows is given in order from left to right.\n\nSACTN %&gt;% \n  arrange(depth, temp)\n\n\n\nR&gt;             site  src       date      temp depth   type\nR&gt; 1      Sea Point SAWS 1990-07-01  9.635484     0 thermo\nR&gt; 2     Muizenberg SAWS 1984-07-01  9.708333     0 thermo\nR&gt; 3     Doringbaai SAWS 2000-12-01  9.772727     0 thermo\nR&gt; 4  Hondeklipbaai SAWS 2003-06-01  9.775000     0 thermo\nR&gt; 5      Sea Point SAWS 1984-06-01 10.000000     0 thermo\nR&gt; 6     Muizenberg SAWS 1992-07-01 10.193548     0 thermo\nR&gt; 7  Hondeklipbaai SAWS 2005-07-01 10.333333     0 thermo\nR&gt; 8  Hondeklipbaai SAWS 2003-07-01 10.340909     0 thermo\nR&gt; 9      Sea Point SAWS 2000-12-01 10.380645     0 thermo\nR&gt; 10    Muizenberg SAWS 1984-08-01 10.387097     0 thermo\n\n\nIf you would rather arrange your data in descending order, as is perhaps more often the case, you simply wrap the column name you are arranging by with the desc() function as shown below.\n\nSACTN %&gt;% \n  arrange(desc(temp))\n\n\n\nR&gt;             site   src       date     temp depth type\nR&gt; 1        Sodwana   DEA 2000-02-01 28.34648    18  UTR\nR&gt; 2        Sodwana   DEA 1999-03-01 28.04890    18  UTR\nR&gt; 3        Sodwana   DEA 1998-03-01 27.87781    18  UTR\nR&gt; 4        Sodwana   DEA 1998-02-01 27.76452    18  UTR\nR&gt; 5        Sodwana   DEA 1996-02-01 27.73637    18  UTR\nR&gt; 6        Sodwana   DEA 2000-03-01 27.52637    18  UTR\nR&gt; 7        Sodwana   DEA 2000-01-01 27.52291    18  UTR\nR&gt; 8  Leadsmanshoal EKZNW 2007-02-01 27.48132    10  UTR\nR&gt; 9        Sodwana EKZNW 2005-01-01 27.45619    12  UTR\nR&gt; 10       Sodwana EKZNW 2007-02-01 27.44054    12  UTR\n\n\nIt must also be noted that when arranging data in this way, any rows with NA values will be sent to the bottom of the dataframe. This is not always ideal and so must be kept in mind.\nFilter observations (rows) with filter()\n\nWhen simply arranging data is not enough, and you need to remove rows of data you do not want, filter() is the tool to use. For example, you can select all monthly temperatures recorded at the site Humewood during the year 1990 with the following code chunk:\n\nSACTN %&gt;% \n  filter(site == \"Humewood\", year(date) == 1990)\n\n\n\nR&gt;        site  src       date     temp depth   type\nR&gt; 1  Humewood SAWS 1990-01-01 21.87097     0 thermo\nR&gt; 2  Humewood SAWS 1990-02-01 18.64286     0 thermo\nR&gt; 3  Humewood SAWS 1990-03-01 18.61290     0 thermo\nR&gt; 4  Humewood SAWS 1990-04-01 17.30000     0 thermo\nR&gt; 5  Humewood SAWS 1990-05-01 16.35484     0 thermo\nR&gt; 6  Humewood SAWS 1990-06-01 15.93333     0 thermo\nR&gt; 7  Humewood SAWS 1990-07-01 15.70968     0 thermo\nR&gt; 8  Humewood SAWS 1990-08-01 16.09677     0 thermo\nR&gt; 9  Humewood SAWS 1990-09-01 16.41667     0 thermo\nR&gt; 10 Humewood SAWS 1990-10-01 17.14194     0 thermo\n\n\nRemember to use the assignment operator (&lt;-, keyboard shortcut alt -) if you want to create an object in the environment with the new results.\n\nhumewood_90s &lt;- SACTN %&gt;% \n  filter(site == \"Humewood\", year(date) %in% seq(1990, 1999, 1))\n\nIt must be mentioned that filter() also automatically removes any rows in the filtering column that contain NA values. Should you want to keep rows that contain missing values, insert the is.na() function into the line of code in question. To illustrate this let’s filter the temperatures for the Port Nolloth data collected by the DEA that were at or below 11°C OR were missing values. You’ll put each argument on a separate line to help keep things clear. Note how R automatically indents the last line in this chunk to help remind you that they are in fact part of the same argument. Also note how I have put the last bracket at the end of this argument on it’s own line. This is not required, but I like to do so as it is a very common mistake to forget the last bracket.\n\nSACTN %&gt;% \n  filter(site == \"Port Nolloth\", # First give the site to filter\n         src == \"DEA\", # Then specify the source\n         temp &lt;= 11 | # Temperatures at or below 11°C OR\n           is.na(temp) # Include missing values\n         )\n\nSelect variables (columns) withselect()\n\nWhen you load a dataset that contains more columns than will be useful or required, it is preferable to shave off the excess. You do this with the select() function. In the following four examples you are going to remove the depth and type columns. There are many ways to do this and none are technically better or faster. So it is up to the user to find a favourite technique.\n\n# Select columns individually by name\nSACTN %&gt;% \n  select(site, src, date, temp)\n\n# Select all columns between site and temp like a sequence\nSACTN %&gt;% \n  select(site:temp)\n\n# Select all columns except those stated individually\nSACTN %&gt;% \n  select(-date, -depth)\n\n# Select all columns except those within a given sequence\n  # Note that the '-' goes outside of a new set of brackets\n  # that are wrapped around the sequence of columns to remove\nSACTN %&gt;% \n  select(-(date:depth))\n\nYou may also use select() to reorder the columns in a dataframe. In this case the inclusion of the everything() function may be a useful shortcut as illustrated below.\n\n# Change up order by specifying individual columns\nSACTN %&gt;% \n  select(temp, src, date, site)\n\n# Use the everything function to grab all columns \n# not already specified\nSACTN %&gt;% \n  select(type, src, everything())\n\n# Or go bananas and use all of the rules at once\n  # Remember, when dealing with tidy data,\n  # everything may be interchanged\nSACTN %&gt;% \n  select(temp:type, everything(), -src)\n\n\n\n\n\n\n\nThe square bracket [] notation\n\n\n\nThe square bracket [] notation may also be used for indexing and subsetting data structures such as vectors, matrices, data frames, and lists. Before tidyverse existed, this was the only way to do so. Square brackets allows you to select elements from these data structures based on their positions, conditions, or names. The use of square brackets can vary slightly depending on the data structure being accessed. Here’s a brief overview:\n\nVectors: When used with vectors, square brackets allow you to select elements by their numeric position or a logical vector indicating which elements to select. For example, vector[c(1, 3)] returns the first and third elements of the vector.\nMatrices: For matrices, square brackets take two dimensions [row, column] to select elements. You can select entire rows, columns, or individual elements. Specifying a row and column as empty (e.g., [,]) selects everything in that dimension.\nDataframes: Similar to matrices, square brackets can be used to subset data frames by row and column. However, since dataframes can have column names, you can also use these names for selection, e.g., df[1,] selects the first row of the dataframe, and df[, \"columnName\"] selects all rows of a specific column.\nLists: Lists can be subsetted by numeric or character indices corresponding to their elements. For example, list[[1]] selects the first element of the list. Note the double brackets, which are used to extract elements from a list directly. Single brackets, e.g., list[1], return a sublist containing the first element.\n\n\n\n\n\n\n\n\n\nData structures and square brackets\n\n\n\nDo this now: provide examples of i) the various data structures available in R, and ii) how to use square brackets to subset each of them. You may use any of the built-in datasets to do so.\n\n\nCreate new variables (columns) with mutate()\n\nWhen you are performing data analysis/statistics in R this is likely because it is necessary to create some new values that did not exist in the raw data. The previous three functions you looked at (arrange(), filter(), select()) will prepare you to create new data, but do not do so themselves. This is when you need to use mutate(). You must however be very mindful that mutate() is only useful if we want to create new variables (columns) that are a function of one or more existing columns (well, that’s how it’s mainly used). Any new column you create with mutate() must always have the same number of rows as the dataframe you are working with. In order to create a new column you must first tell R what the name of the column will be, in this case let’s create a column named kelvin. The second step is to then tell R what to put in the new column. As you may have guessed, you are going to convert the temp column which contains degrees Celsius (°K) into Kelvin (°K) by adding 273.15 to every row.\n\nSACTN %&gt;% \n  mutate(kelvin = temp + 273.15))\n\n\n\nR&gt;            site src       date     temp depth type   kelvin\nR&gt; 1  Port Nolloth DEA 1991-02-01 11.47029     5  UTR 284.6203\nR&gt; 2  Port Nolloth DEA 1991-03-01 11.99409     5  UTR 285.1441\nR&gt; 3  Port Nolloth DEA 1991-04-01 11.95556     5  UTR 285.1056\nR&gt; 4  Port Nolloth DEA 1991-05-01 11.86183     5  UTR 285.0118\nR&gt; 5  Port Nolloth DEA 1991-06-01 12.20722     5  UTR 285.3572\nR&gt; 6  Port Nolloth DEA 1991-07-01 12.53810     5  UTR 285.6881\nR&gt; 7  Port Nolloth DEA 1991-08-01 11.25202     5  UTR 284.4020\nR&gt; 8  Port Nolloth DEA 1991-09-01 11.29208     5  UTR 284.4421\nR&gt; 9  Port Nolloth DEA 1991-10-01 11.37661     5  UTR 284.5266\nR&gt; 10 Port Nolloth DEA 1991-11-01 10.98208     5  UTR 284.1321\n\n\nThis is a very basic example and mutate() is capable of much more than simple addition. You will get into some more exciting examples during the next session.\nSummarise variables (columns) with summarise()\n\nFinally this brings you to the last tool for this section. To create new columns you use mutate(), but to calculate any sort of summary/statistic from a column that will return fewer rows than the dataframe has you will use summarise(). This makes summarise() much more powerful than the other functions in this section, but because it is able to do more, it can also be more unpredictable, making it’s use potentially more challenging. You will almost always end op using this function in our work flows. The following chunk very simply calculates the overall mean temperature for the entire SACTN.\n\nSACTN %&gt;% \n  summarise(mean_temp = mean(temp, na.rm = TRUE))\n\nR&gt;   mean_temp\nR&gt; 1  19.26955\n\n\nNote how the above chunk created a new dataframe. This is done because it cannot add this one result to the previous dataframe due to the mismatch in the number of rows. If you were to create additional columns with other summaries, you may do so within the same summarise() function. These multiple summaries are displayed on individual lines in the following chunk to help keep things clear.\n\nSACTN %&gt;% \n  summarise(mean_temp = mean(temp, na.rm = TRUE),\n            sd_temp = sd(temp, na.rm = TRUE),\n            min_temp = min(temp, na.rm = TRUE),\n            max_temp = max(temp, na.rm = TRUE)\n            )\n\nR&gt;   mean_temp  sd_temp min_temp max_temp\nR&gt; 1  19.26955 3.682122 9.136322 28.34648\n\n\nCreating summaries of the entire SACTN dataset in this way is not appropriate as you should not be combining time series from such different parts of the coast. In order to calculate summaries within variables you will need to learn how to use group_by(), which in turn will first require you to learn how to chain multiple functions together within a pipe (%&gt;%). That is how you will begin the next session for today. You will finishing with several tips on how to make your data the tidiest that it may be.\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {13. {Tidier} {Data}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/13-tidier.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 13. Tidier Data. http://tangledbank.netlify.app/BCB744/intro_r/13-tidier.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "13. Tidier Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/17-base_r.html",
    "href": "BCB744/intro_r/17-base_r.html",
    "title": "17. Base R Primer",
    "section": "",
    "text": "Please note that the following chapter departs from the syntax employed by the tidyverse, as utilised throughout this workshop, in favour of the base R syntax. This may be changed in the future, but has been left for now in order to better highlight the fundamental machinations of the R language, upon which the tidyverse is based.\nDataframes\nThe ‘workhorse’ data-containing structures you will use extensively in R are called dataframes. In fact, almost all of the work you do in R will be done directly with dataframes or will involve converting data into a dataframe. A dataframe is used for storing data as tables, with a table defined by a collection of vectors of similar or dissimilar data types but all of the same length. Don’t worry if any of those terms are unknown or daunting. We will cover them in detail just now. But first we need to see what a dataframe looks like in order to provide context for all of the parts they consist of. After we have covered all of the terms used for data in R we will learn some methods of creating our own dataframes.\nTo load a dataframe into R is quite simple when the data are already in the ‘.Rdata’ format. Let’s load a small dataframe that was prepared for this class and see. The file extension ‘.Rdata’ does not mean necessarily that the data are in a dataframe (table) format. This file extensions is actually a form of data compression unique to R and could hold anything from a single letter to the results of a complex species distribution model. For the following line of code to work we must make sure we are in the ‘Intro_R_Workshop’ project.\n\nload(\"../../data/intro_data.Rdata\")\n\nUpon loading the data frame we see in the Environment tab that there is a little blue circle next to our object. If we click on that we see a summary of each column. First it says what the data type for that column is and then shows the first several values therein.\nIf you click on the ‘intro_data’ word in your Environment tab it will open it in your Source Editor and allow you to click on the columns to organise them by ascending or descending order. Note that this does not change the dataframe, it is only a visual aid.\nBasic data types\nThere are several basic R data types that you frequently encounter in daily work. These include but are not limited to numeric, integer, logical, character, factor and date classes. All of these data types are present in our ‘intro_data’ dataframe for us to see practical examples. We will create our own examples as we go along.\nNumeric\nNumeric data with decimal values are called numeric in R. It is the default computational data type. If we look at our data frame we see that the following columns are numeric: lon, lat, NA.perc, mean, min and max. What sort of data are these?\nLet’s create our own numeric object by assigning a decimal value to a variable x as follows, x will be of numeric type:\n\nx &lt;- 1.2 # assign 1.2 to x\nx # print the value of x\n\n[1] 1.2\n\nclass(x) # what is the class of x?\n\n[1] \"numeric\"\n\n\nFurthermore, even if we assign a number to a variable k that doesn’t have a decimal place, it is still being saved as a numeric value:\n\nk &lt;- 1\nk\n\n[1] 1\n\nclass(k)\n\n[1] \"numeric\"\n\n\nIf we want to really be certain that k is or is not an integer we use is.integer():\n\nis.integer(k) # is k an integer?\n\n[1] FALSE\n\n\nInteger\nAn integer in R is a numeric value that does not have a decimal place. It may only be a round whole number. Integers are often used for count data and when converting qualitative data to numbers for data analysis. In our dataframe we may see that we have two integer columns: depth and length. Why are these integers?\nIn order to create your own integer variable(s) in R, we use the as.integer(). We can be assured that y is indeed an integer by checking with is.integer():\n\ny &lt;- as.integer(13)\ny\n\n[1] 13\n\nclass(y)\n\n[1] \"integer\"\n\nis.integer(y) # is it an integer?\n\n[1] TRUE\n\n\nIf we really have to, we can coerce a numeric value into an integer with the same as.integer() function:\n\nz &lt;- as.integer(pi)\nz\n\n[1] 3\n\nclass(z)\n\n[1] \"integer\"\n\nis.integer(z) # is it an integer?\n\n[1] TRUE\n\n\nLogical\nThere are several logic values in R. We are mostly going to be concerned with the two main values we will be encountering: TRUE and FALSE. Note that all letters must be upper case. In our dataframe we see that only the ‘thermo’ column is logical. This column tells us whether or not the data were collected with a thermometer or not.\nLogical values (TRUE or FALSE) are often created via comparison between variables:\n\nx &lt;- 1; y &lt;- 2 # sample values\nz &lt;- x &gt; y\nz\n\n[1] FALSE\n\nclass(z)\n\n[1] \"logical\"\n\n\nIn order to perform logical operations we mostly use & (and), | (or), and ! (negation):\n\nu &lt;- TRUE; v &lt;- FALSE; w &lt;- TRUE; x &lt;- FALSE\nu & v\n\n[1] FALSE\n\nu & w\n\n[1] TRUE\n\nv & x\n\n[1] FALSE\n\nu | v\n\n[1] TRUE\n\n!u\n\n[1] FALSE\n\n\nAlthough these logical operators can be immensely useful in more advanced R programming, we will not go into too much detail in this introductory course. For more information on the logical operators, see the R help material:\n\nhelp(\"&\")\n\nOne final thing to note about logic in R is that it can be useful to perform arithmetic on logical values. TRUE has the value 1, while FALSE has value 0:\n\nas.integer(TRUE) # the numeric value of TRUE\n\n[1] 1\n\nas.integer(FALSE) # the numeric value of FALSE\n\n[1] 0\n\nsum(as.integer(intro_data$thermo))\n\n[1] 10\n\n\nWhat is this telling us?\nCharacter\nIn our dataframe we see that only the ‘src’ column has the character values. This column is showing us which government body etc. collected the data in that row. At the use of a very familiar word, character, one may think this data type must be the most straightforward. This is not necessarily so as character values are used to represent string values in R. Because computers do not understand text the same way we do, they tend to handle this information differently. This allows us to do some pretty wild stuff with character values, but we won’t be getting into that in this course as it quickly becomes very technical and generally speaking isn’t very useful in a daily application.\nIf however we wanted to convert an object to a character value we would do so with as.character():\n\nd &lt;- as.character(pi)\nclass(d)\n\n[1] \"character\"\n\n\nThis can be useful if you have data that you want to be characters, but for one reason or another R has decided to make it a different data type.\nIf you want to join two character objects they can be concatenated with the paste() function:\n\na &lt;- \"fluffy\"; b &lt;- \"bunny\"\npaste(a, b)\n\n[1] \"fluffy bunny\"\n\npaste(a, b, sep = \"-\")\n\n[1] \"fluffy-bunny\"\n\n\nMore functions for string manipulation can be found in the R documentation — type help(\"sub\") at the command prompt. You may also wish to install Hadley Wickham’s nifty stringr package for more cool ways to work with character strings.\nFactor\nFactor values are somewhat difficult to explain and often even more difficult to understand. Factor values appear the same as character values when we look at them in a spreadsheet. But they are not the same. This will lead to much wailing and gnashing of teeth. So why then do factors exist and why would we use them? Factors allow us to numerically order names non-alphabetically, for example. This then allows one to order a list of research sites in geographical order.\nWe will see many examples of factors during this course but for now look at the ‘site’ column in our dataframe. If we click on this column a couple of times we see that it reorders all the data based on ascending or descending order of the sites. But that order is not alphabetical, it is based on the levels within the factor column. Each factor value in a column is assigned a level integer value (e.g. 1, 2, 3, 4, etc.). If multiple values in a factor column are the same, they receive the same level value as well.\nIf we want to see what the levels within a factor column are we use levels():\n\nlevels(intro_data$site)\n\n [1] \"Port Nolloth\"  \"St Helena Bay\" \"Saldanha Bay\"  \"Muizenberg\"   \n [5] \"Cape Agulhas\"  \"Mossel Bay\"    \"Tsitsikamma\"   \"Humewood\"     \n [9] \"Hamburg\"       \"Durban\"        \"Richards Bay\"  \"Sodwana\"      \n\n\nWe will discuss in the next session what that $ means. But for now, are you able to see what the pattern is in the levels of the site listing?\nIf we want to create our own factors we will use as.factor():\n\nf &lt;- as.factor(letters[1:5])\nlevels(f)\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\nAnd if we want to change the order of our factor levels we use factor():\n\nf &lt;- factor(f, levels = c(\"b\", \"a\", \"c\", \"e\", \"d\"))\nlevels(f)\n\n[1] \"b\" \"a\" \"c\" \"e\" \"d\"\n\n\nAnother reason for using factors to re-order our data, as we shall see tomorrow, is that this allows us to control the order in which values are plotted.\nDates\nSee the next chapter about dates.\n\n\nDates.\n\nVectors\nA vector, by definition, is a one-dimensional sequence of data elements of the same basic type (class). Members in a vector are officially called components. Basically, a vector is a column. Indeed, a dataframe is nothing more than a collection of vectors stuck together. If we wanted to create a vector from our dataframe we would do this:\n\nlonely_vector &lt;- intro_data$NA.perc\n\nNotice that we may not click on the object lonely_vector in our Environment tab. This is because it is no longer two-dimensional. If we want to visualise the data we need to enter it into the console or run it from our script:\n\nlonely_vector\n\n [1]  6 41 32  4 28 26  8  3  6 67 38 16\n\n\nLet’s create some vectors of our own:\n\nprimes1 &lt;- c(3, 5, 7)\nprimes1\n\n[1] 3 5 7\n\nclass(primes1)\n\n[1] \"numeric\"\n\np1 &lt;- pi\np2 &lt;- 5\np3 &lt;- 7\n\nprimes2 &lt;- c(p1, p2, p3)\nprimes2\n\n[1] 3.141593 5.000000 7.000000\n\nclass(primes2)\n\n[1] \"numeric\"\n\nis.numeric(primes2)\n\n[1] TRUE\n\nis.integer(primes2) # integers coerced into floating point numbers\n\n[1] FALSE\n\n\nWe can also have vectors of logical values or character strings, and we can use the function length() to see how many components each has:\n\ntf &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE)\ntf\n\n[1]  TRUE FALSE  TRUE FALSE FALSE\n\nlength(tf)\n\n[1] 5\n\ncs &lt;- c(\"Mary\", \"has\", \"a\", \"silly\", \"lamb\")\ncs\n\n[1] \"Mary\"  \"has\"   \"a\"     \"silly\" \"lamb\" \n\nlength(cs)\n\n[1] 5\n\n\nOf course one would seldom enter data into R using the c() (combine) function, but it is useful for short calculations. More often than not one would import data from Excel (urgh!) or something more reputable. The kinds of data one can read into R are remarkable. We will get to that later on.\nWe can also combine vectors in many ways, and the simplest way is the append one after the other:\n\nprimes12 &lt;- c(primes1, primes2)\nprimes12\n\n[1] 3.000000 5.000000 7.000000 3.141593 5.000000 7.000000\n\nnonSense &lt;- c(primes12, cs)\nnonSense\n\n [1] \"3\"                \"5\"                \"7\"                \"3.14159265358979\"\n [5] \"5\"                \"7\"                \"Mary\"             \"has\"             \n [9] \"a\"                \"silly\"            \"lamb\"            \n\nclass(nonSense)\n\n[1] \"character\"\n\n\nIn the code fragment above, notice how the numeric values are being coerced into character strings when the two vectors of dissimilar class are combined. This is necessary so as to maintain the same primitive data type for members in the same vector.\nVector indices\nWhat if we want to extract one or a few components from the vector? Easy… We retrieve values in a vector by declaring an index inside a single square bracket [] operator. For example, the following shows how to retrieve a vector component. Since the vector index is 1-based (i.e. the first component in a vector is numbered 1), we use the index position 7 for retrieving the seventh member:\n\nnonSense[7] # find the seventh component in the vector\n\n[1] \"Mary\"\n\n# or combine them in interesting ways...\npaste(nonSense[7], nonSense[8], nonSense[4], nonSense[10], \"bunnies\", sep = \" \")\n\n[1] \"Mary has 3.14159265358979 silly bunnies\"\n\n\nIf the index given is negative, it will remove the value whose position has the same absolute value as the negative index. For example, the following creates a vector slice with the third member removed. However, if an index is out-of-range, a missing value will be reported via the symbol NA:\n\na &lt;- c(2, 6, 3, 8, 13)\na\n\n[1]  2  6  3  8 13\n\na[-3]\n\n[1]  2  6  8 13\n\na[10]\n\n[1] NA\n\n\nVector creation\nR has many funky ways of creating vectors. This process is important to understand because we will need to build on it to create our own dataframes. Here are some examples of vector creation:\n\nseq(1:10) # assign them to a variable if you want to...\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from = 0, to = 100, by = 10)\n\n [1]   0  10  20  30  40  50  60  70  80  90 100\n\nseq(0, 100, len = 10) # one may omit from and to\n\n [1]   0.00000  11.11111  22.22222  33.33333  44.44444  55.55556  66.66667\n [8]  77.77778  88.88889 100.00000\n\nseq(1, 9, by = pi)\n\n[1] 1.000000 4.141593 7.283185\n\nrep(13, times = 13)\n\n [1] 13 13 13 13 13 13 13 13 13 13 13 13 13\n\nrep(seq(1:5), times = 6)\n\n [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\na &lt;- rnorm(20, mean = 13, sd = 0.13) # random numbers with known mean and sd\nrep(a, 5) # one may omit the times argument\n\n  [1] 12.91882 13.16600 13.07493 12.89111 13.22153 12.72786 13.12804 13.03413\n  [9] 13.18092 13.19653 13.13261 12.97597 13.07407 13.00502 12.94179 13.08436\n [17] 12.97347 13.18023 13.07841 12.79849 12.91882 13.16600 13.07493 12.89111\n [25] 13.22153 12.72786 13.12804 13.03413 13.18092 13.19653 13.13261 12.97597\n [33] 13.07407 13.00502 12.94179 13.08436 12.97347 13.18023 13.07841 12.79849\n [41] 12.91882 13.16600 13.07493 12.89111 13.22153 12.72786 13.12804 13.03413\n [49] 13.18092 13.19653 13.13261 12.97597 13.07407 13.00502 12.94179 13.08436\n [57] 12.97347 13.18023 13.07841 12.79849 12.91882 13.16600 13.07493 12.89111\n [65] 13.22153 12.72786 13.12804 13.03413 13.18092 13.19653 13.13261 12.97597\n [73] 13.07407 13.00502 12.94179 13.08436 12.97347 13.18023 13.07841 12.79849\n [81] 12.91882 13.16600 13.07493 12.89111 13.22153 12.72786 13.12804 13.03413\n [89] 13.18092 13.19653 13.13261 12.97597 13.07407 13.00502 12.94179 13.08436\n [97] 12.97347 13.18023 13.07841 12.79849\n\nrep(c(\"A\", \"B\", \"C\"), 3)\n\n[1] \"A\" \"B\" \"C\" \"A\" \"B\" \"C\" \"A\" \"B\" \"C\"\n\nrep(c(\"A\", \"B\", \"C\"), each = 3)\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\nx &lt;- c(\"01-31-1960\", \"02-13-1960\", \"06-23-1977\", \"01-01-2013\")\nclass(x)\n\n[1] \"character\"\n\nz &lt;- as.Date(x, \"%m-%d-%Y\")\nclass(z) # introducing the date class\n\n[1] \"Date\"\n\nseq(as.Date(\"2013-12-30\"), as.Date(\"2014-01-04\"), by = \"days\")\n\n[1] \"2013-12-30\" \"2013-12-31\" \"2014-01-01\" \"2014-01-02\" \"2014-01-03\"\n[6] \"2014-01-04\"\n\nseq(as.Date(\"2013-12-01\"), as.Date(\"2016-01-31\"), by = \"months\")\n\n [1] \"2013-12-01\" \"2014-01-01\" \"2014-02-01\" \"2014-03-01\" \"2014-04-01\"\n [6] \"2014-05-01\" \"2014-06-01\" \"2014-07-01\" \"2014-08-01\" \"2014-09-01\"\n[11] \"2014-10-01\" \"2014-11-01\" \"2014-12-01\" \"2015-01-01\" \"2015-02-01\"\n[16] \"2015-03-01\" \"2015-04-01\" \"2015-05-01\" \"2015-06-01\" \"2015-07-01\"\n[21] \"2015-08-01\" \"2015-09-01\" \"2015-10-01\" \"2015-11-01\" \"2015-12-01\"\n[26] \"2016-01-01\"\n\nseq(as.Date(\"2000/1/1\"), by = \"month\", length.out = 12)\n\n [1] \"2000-01-01\" \"2000-02-01\" \"2000-03-01\" \"2000-04-01\" \"2000-05-01\"\n [6] \"2000-06-01\" \"2000-07-01\" \"2000-08-01\" \"2000-09-01\" \"2000-10-01\"\n[11] \"2000-11-01\" \"2000-12-01\"\n\n# and many more...\n\nVector arithmetic\nArithmetic operations of vectors are performed component-by-component, i.e., componentwise. For example, suppose we have vectors a and b:\n\na &lt;- c(1, 3, 5, 7)\nb &lt;- c(1, 2, 4, 8)\n\nThen we multiply a by 5…\n\na * 5\n\n[1]  5 15 25 35\n\n\n… and see that each component of a is multiplied by 5. In other words, the shorter vector (here 5) is recycled. Now multiply a with b…\n\na * b\n\n[1]  1  6 20 56\n\n\n…and we see that the components in one vector matches those in the other one-for-one. Similarly for subtraction, addition and division, we get new vectors via componentwise operations. Try this here now a few times with your own vectors.\nBut what if one vector is somewhat shorter than the other? The recycling rule comes into play. If two vectors are of unequal length, the shorter one will be recycled in order to match the longer vector. For example, the following vectors u and v have different lengths, and their sum is computed by recycling values of the shorter vector u:\n\nv &lt;- rep(2, len = 13)\nu &lt;- rep(c(1, 20), len = 5)\nv + u\n\nWarning in v + u: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  3 22  3 22  3  3 22  3 22  3  3 22  3\n\n\nDataframe creation\nThe most rudimentary way to create a dataframe is to create several vectors and then assemble them into a dataframe using cbind() — this is a function that combines by column. For instance:\n\n# create three vectors of different types\nvec1 &lt;- rep(c(\"A\", \"B\", \"C\"), each = 5) # a character vector (a facctor)\nvec2 &lt;- seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                 length.out = length(vec1)) # date vector\nvec3 &lt;- rnorm(n = length(vec1), mean = 0, sd = 0.35) # numeric vector\n# now assemble dataframe\ndf1 &lt;- cbind(vec1, vec2, vec3)\nhead(df1)\n\n     vec1 vec2   vec3                 \n[1,] \"A\"  \"4018\" \"0.0760282758843187\" \n[2,] \"A\"  \"4019\" \"0.123285889065449\"  \n[3,] \"A\"  \"4020\" \"0.392826291465943\"  \n[4,] \"A\"  \"4021\" \"0.094771745990392\"  \n[5,] \"A\"  \"4022\" \"0.00624499810711309\"\n[6,] \"B\"  \"4023\" \"-0.101524142025691\" \n\n\nAnother way to achieve the same thing is to use the data.frame() function that will allow you to achieve all of the above steps at once. Here is the example:\n\ndf2 &lt;- data.frame(vec1 = rep(c(\"A\", \"B\", \"C\"), each = 5),\n                  vec2 = seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                                  length.out = length(vec1)),\n                  vec3 = rnorm(n = length(vec1), mean = 2, sd = 0.75))\nhead(df2, 2)\n\n  vec1       vec2     vec3\n1    A 1981-01-01 1.631724\n2    A 1981-01-02 2.856046\n\n\nWhat about the names of the dataframe that you just created? Are you happy that they are descriptive enough? If you aren’t, don’t fear. There are several different ways in which we can change it. We can assign the existing separate vectors vec1, vec2 and vec3 to more user-friendly names using the data.frame() function, like this:\n\ndf1 &lt;- data.frame(level = vec1,\n                  sample.date = vec2,\n                  measurement = vec3)\nhead(df1, 2)\n\n  level sample.date measurement\n1     A  1981-01-01  0.07602828\n2     A  1981-01-02  0.12328589\n\n\nAnother way is to change the name after you have created the dataframe using the colnames() assignment function, as in:\n\ncolnames(df2) &lt;- c(\"level\", \"sample.date\", \"measurement\")\nhead(df2, 2)\n\n  level sample.date measurement\n1     A  1981-01-01    1.631724\n2     A  1981-01-02    2.856046\n\nnames(df2)\n\n[1] \"level\"       \"sample.date\" \"measurement\"\n\n\nDataframes are very versatile and we can do many operations on them. A common requirement is to add a column to a dataframe that contains the outcome of some calculation. We could create a new column in the dataframe ‘on the fly’, as in:\n\ndf2.1 &lt;- df1 # copy the dataframe\ndf2.1$meas.anom &lt;- df1$measurement - mean(df1$measurement)\ndf2.1$meas.diff &lt;- df2.1$measurement - df2.1$meas.anom\nhead(df2.1, 2)\n\n  level sample.date measurement  meas.anom meas.diff\n1     A  1981-01-01  0.07602828 -0.1132522 0.1892805\n2     A  1981-01-02  0.12328589 -0.0659946 0.1892805\n\n\nWe can also combine dataframes in different ways. Perhaps you have two (or more) dataframe that conform to the same layout, i.e. they have the same number of columns (although the length of the dataframes may differ), they have the same data type in those columns and the names of those columns are the same. Also, the order of the columns must be identical in all the dataframes. Two separate dataframe with the same structure may, for example, result from two identical experiments that were repeated at different times. We can then stack one on top (e.g. combine our experiments) of the other using the row bind function rbind(), as in:\n\nnrow(df1) # check the number of rows first\n\n[1] 15\n\nnrow(df2)\n\n[1] 15\n\ndf3 &lt;- rbind(df1, df2)\nnrow(df3) # number of rows in the combined dataframe\n\n[1] 30\n\nhead(df3, 2)\n\n  level sample.date measurement\n1     A  1981-01-01  0.07602828\n2     A  1981-01-02  0.12328589\n\n\nBut now how do we know how the portions of the stacked dataframe relate to the experiments that resulted in the data in the first place? There is no label to distinguish one experiment from the other. We can fix this by adding a new column to the stacked dataframe that contains the coding for the two experiments. We can achieve it like this:\n\ndf3$exp.no &lt;- rep(c(\"exp1\", \"exp2\"), each = nrow(df1))\nhead(df3, 2)\n\n  level sample.date measurement exp.no\n1     A  1981-01-01  0.07602828   exp1\n2     A  1981-01-02  0.12328589   exp1\n\ntail(df3, 2)\n\n   level sample.date measurement exp.no\n29     C  1981-01-14   2.2702602   exp2\n30     C  1981-01-15   0.7551167   exp2\n\n\nWe can combine dataframes in another way — that is, bind columns side-by-side using the function cbind(). We used it before to place vectors of the same length next to each other to create a dataframe. This function is similar to rbind(), but where rbind() fusses over the names of the columns, cbind() does not. What does concern cbind(), however, is that the number of rows in the two (or more) dataframes that will be ‘glued’ side-by-side is the same. Try it yourself with your own dataframes.\nDataframe indices\nRemember that weird $ symbol we saw a little while ago? That symbol tells R that you want to see a column (vector) within a dataframe. For example, if we wanted to perform an operation on only one column in intro_data in order to ascertain the mean depth (m) of sampling:\n\nround(mean(intro_data$depth),2)\n\n[1] 1.33\n\n\nIf we want to subset only specific values in a dataframe, as we have seen how to do with vectors, we need to consider that we are now working with two dimensions and not one. We still use [] but now we must do a little extra. If we want to see how long the time series for Sodwana is we could do this in several ways, here are the three most common in an improving order:\n\n# Subset a dataframe using [,]\nintro_data[12,9]\n\n[1] 4606\n\n# Subset only one column using []\nintro_data$length[12]\n\n[1] 4606\n\n# Subset from one column using logic for another column\nintro_data$length[intro_data$site == \"Sodwana\"]\n\n[1] 4606\n\n\nThe important thing to remember here is that when one needs to use a comma when subsetting, the row number is always on the left, and the column number is always on the right. Rows then columns! Tattoo that onto your brain. Or fore-arm if you are the adventurous type. We will go into the subsetting and analysis of dataframes in much more detail in the following session.\nOne must keep in mind that data in R can become substantially more complex than what we have covered, and the software also distinguishes several other kinds of data ‘containers’: in addition to vectors and dataframes, we also have lists, matrices, time series and arrays. The more complex ones, such as arrays, may have more dimensions than the two (rows along dimension 1, columns along dimension 2) that most people are familiar with. We will not delve into these here as they are bit more advanced than the goals of this course.\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {17. {Base} {R} {Primer}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/17-base_r.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 17. Base R Primer. http://tangledbank.netlify.app/BCB744/intro_r/17-base_r.html."
  },
  {
    "objectID": "BCB744/intro_r/08-mapping.html",
    "href": "BCB744/intro_r/08-mapping.html",
    "title": "8. Mapping With ggplot2\n",
    "section": "",
    "text": "“There’s no map to human behaviour.”\n— Bjork\n\n\n“Here be dragons.”\n— Unknown\n\nYesterday you learned how to create ggplot2 figures, change their aesthetics, labels, colour palettes, and facet/arrange them. Now you are going to look at how to create maps.\nMost of the work that you will perform as environmental/biological scientists involves going out to a location and sampling information there. Sometimes only once, and sometimes over a period of time. All of these different sampling methods lend themselves to different types of figures. One of those, collection of data at different points, is best shown with maps. As you will see over the course of Day 3, creating maps in ggplot2 is very straight forward and is extensively supported. For that reason you are going to have plenty of time to also learn how to do some more advanced things. Your goal in this chapter is to produce the figure below.\n\n\nToday’s goal.\n\nUsing prepared data\nBefore you begin let’s go ahead and load the packages you will need, as well as the several dataframes required to make the final product.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\n\n# Load data\nload(\"../../data/south_africa_coast.Rdata\")\nload(\"../../data/sa_provinces.RData\")\nload(\"../../data/rast_annual.Rdata\")\nload(\"../../data/MUR.Rdata\")\nload(\"../../data/MUR_low_res.RData\")\n\n# Choose which SST product you would like to use\nsst &lt;- MUR_low_res\n# OR\nsst &lt;- MUR\n\n# The colour palette we will use for ocean temperature\ncols11 &lt;- c(\"#004dcd\", \"#0068db\", \"#007ddb\", \"#008dcf\", \"#009bbc\",\n            \"#00a7a9\", \"#1bb298\", \"#6cba8f\", \"#9ac290\", \"#bec99a\")\n\nA new concept?\nThe idea of creating a map in R may be daunting to some, but remember that a basic map is nothing more than a simple figure with an x and y axis. We tend to think of maps as different from other scientific figures, whereas in reality they are created the exact same way. Let’s compare a dot plot of the chicken weight data against a dot plot of the coastline of South Africa.\nChicken dots:\n\nggplot(data = ChickWeight, aes(x = Time, y = weight)) +\n  geom_point()\n\n\n\nDot plot of chicken weight data.\n\n\n\nSouth African coast dots:\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_point()\n\n\n\nDot plot off South African coast.\n\n\n\nDoes that look familiar? Notice how the x and y axis tick labels look the same as any map you would see in an atlas. This is because they are. But this isn’t a great way to create a map. Rather it is better to represent the land mass with a polygon. With ggplot2 this is a simple task.\nLand mask\nNow that you have seen that a map is nothing more than a bunch of dots and shapes on specific points along the x and y axes you are going to look at the steps you would take to build a more complex map. Don’t worry if this seems daunting at first. You are going to take this step by step and ensure that each step is made clear along the way. The first step is to create a polygon. Note that you create an aesthetic argument inside of geom_polygon() and not ggplot() because some of the steps you will take later on will not accept the group aesthetic. Remember, whatever aesthetic arguments we put inside of ggplot() will be inserted into all of our other geom_...() lines of code.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) # The land mask\n\n\n\nThe map of South Africa. Now with province borders!\n\n\n\nBorders\nThe first thing you will add is the province borders as seen in Figure @ref(fig:map-goal). Notice how you only add one more line of code to do this.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) # The province borders\n\n\n\nThe map of South Africa. Now with province borders!\n\n\n\nForce lon/lat extent\nUnfortunately when you added our borders it increased the plotting area of our map past what you would like. To correct that you will need to explicitly state the borders you want.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) + \n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0) # Force lon/lat extent\n\n\n\nThe map, but with the extra bits snipped off.\n\n\n\nOcean temperature\nThis is starting to look pretty fancy, but it would be nicer if there was some colour involved. So let’s add the ocean temperature. Again, this will only require one more line of code. Starting to see a pattern? But what is different this time and why?\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) + # The ocean temperatures\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\nOcean temperature (°C) visualised as an ice cream spill.\n\n\n\nThat looks… odd. Why do the colours look like someone melted a big bucket of ice cream in the ocean? This is because the colours you see in this figure are the default colours for discrete values in ggplot2. If you want to change them we may do so easily by adding yet one more line of code.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) + # Set the colour palette\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\nOcean temperatures (°C) around South Africa.\n\n\n\nThere’s a colour palette that would make Jacques Cousteau swoon. When you set the colour palette for a figure in ggplot2 you must use that colour palette for all other instances of those types of values, too. What this means is that any other discrete values that will be filled in, like the ocean colour above, must use the same colour palette (there are some technical exceptions to this rule that you will not cover in this course). You normally want ggplot2 to use consistent colour palettes anyway, but it is important to note that this constraint exists. Let’s see what I mean. Next you will add the coastal pixels to our figure with one more line of code. You won’t change anything else. Note how ggplot2 changes the colour of the coastal pixels to match the ocean colour automatically.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  geom_tile(data = rast_annual, aes(x = lon, y = lat, fill = bins), \n            colour = \"white\", size = 0.1) + # The coastal temperature values\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\nMap of South Africa showing in situ temeperatures (°C) as pixels along the coast.\n\n\n\nFinal touches\nYou used geom_tile() instead of geom_rast() to add the coastal pixels above so that you could add those little white boxes around them. This figure is looking pretty great now. And it only took a few rows of code to put it all together! The last step is to add several more lines of code that will control for all of the little things you want to change about the appearance of the figure. Each little thing that is changed below is annotated for your convenience.\n\nfinal_map &lt;- ggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  geom_tile(data = rast_annual, aes(x = lon, y = lat, fill = bins), \n            colour = \"white\", size = 0.1) +\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0) +\n  scale_x_continuous(position = \"top\") + # Put x axis labels on top of figure\n  theme(axis.title = element_blank(), # Remove the axis labels\n        legend.text = element_text(size = 7), # Change text size in legend\n        legend.title = element_text(size = 7), # Change legend title text size\n        legend.key.height = unit(0.3, \"cm\"), # Change size of legend\n        legend.background = element_rect(colour = \"white\"), # Add legend background\n        legend.justification = c(1, 0), # Change position of legend\n        legend.position = c(0.55, 0.4) # Fine tune position of legend\n        )\nfinal_map\n\n\n\nThe cleaned up map of South Africa. Resplendent with coastal and ocean temperatures (°C).\n\n\n\nThat is a very clean looking map so go ahead and save it on your local disk.\n\nggsave(plot = final_map, \"figures/map_complete.pdf\", height = 6, width = 9)\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {8. {Mapping} {With} **Ggplot2**},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/08-mapping.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 8. Mapping With **ggplot2**. http://tangledbank.netlify.app/BCB744/intro_r/08-mapping.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. Mapping With **ggplot2**"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html",
    "href": "BCB744/intro_r/14-tidiest.html",
    "title": "14. Tidiest Data",
    "section": "",
    "text": "“Conducting data analysis is like drinking a fine wine. It is important to swirl and sniff the wine, to unpack the complex bouquet and to appreciate the experience. Gulping the wine doesn’t work.”\n— Daniel B. Wright\nIn the previous session you covered the five main transformation functions you would use in a typical tidy workflow. But to really unlock their power you need to learn how to use them with group_by(). This is how you may calculate statistics based on the different grouping variables within your data, such as sites or species or soil types, for example. Let’s begin by loading the tidyverse package and the SACTN data if you haven’t already.\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# load the data from a .RData file\nload(\"../../data/SACTNmonthly_v4.0.RData\")\n\n# Copy the data as a dataframe with a shorter name\nSACTN &lt;- SACTNmonthly_v4.0\n\n# Remove the original\nrm(SACTNmonthly_v4.0)",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html#grouping-by-multiple-variables",
    "href": "BCB744/intro_r/14-tidiest.html#grouping-by-multiple-variables",
    "title": "14. Tidiest Data",
    "section": "Grouping by multiple variables",
    "text": "Grouping by multiple variables\nAs you may have guessed by now, grouping is not confined to a single column. One may use any number of columns to perform elaborate grouping measures. Let’s look at some ways of doing this with the SACTN data.\n\n# Create groupings based on temperatures and depth\nSACTN_temp_group &lt;- SACTN %&gt;% \n  group_by(round(temp), depth)\n\n# Create groupings based on source and date\nSACTN_src_group &lt;- SACTN %&gt;% \n  group_by(src, date)\n\n# Create groupings based on date and depth\nSACTN_date_group &lt;- SACTN %&gt;% \n  group_by(date, depth)\n\nNow that you’ve created some grouped dataframes, let’s think of some ways to summarise these data.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html#ungrouping",
    "href": "BCB744/intro_r/14-tidiest.html#ungrouping",
    "title": "14. Tidiest Data",
    "section": "Ungrouping",
    "text": "Ungrouping\nOnce you level up our tidyverse skills you will routinely be grouping variables while calculating statistics. This then poses the problem of losing track of which dataframes are grouped and which aren’t. Happily, to remove any grouping we just use ungroup(). No arguments required, just the empty function by itself. Too easy.\n\nSACTN_ungroup &lt;- SACTN_date_group %&gt;% \n  ungroup()",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html#rename-variables-columns-with-rename",
    "href": "BCB744/intro_r/14-tidiest.html#rename-variables-columns-with-rename",
    "title": "14. Tidiest Data",
    "section": "Rename variables (columns) with rename()\n",
    "text": "Rename variables (columns) with rename()\n\nYou have seen that you select columns in a dataframe with select(), but if you want to rename columns you have to use, you guessed it, rename(). This functions works by first telling R the new name you would like, and then the existing name of the column to be changed. This is perhaps a bit back to front, but such is life on occasion.\n\nSACTN %&gt;% \n  rename(source = src)\n\n\n\nR&gt;            site source       date     temp depth type\nR&gt; 1  Port Nolloth    DEA 1991-02-01 11.47029     5  UTR\nR&gt; 2  Port Nolloth    DEA 1991-03-01 11.99409     5  UTR\nR&gt; 3  Port Nolloth    DEA 1991-04-01 11.95556     5  UTR\nR&gt; 4  Port Nolloth    DEA 1991-05-01 11.86183     5  UTR\nR&gt; 5  Port Nolloth    DEA 1991-06-01 12.20722     5  UTR\nR&gt; 6  Port Nolloth    DEA 1991-07-01 12.53810     5  UTR\nR&gt; 7  Port Nolloth    DEA 1991-08-01 11.25202     5  UTR\nR&gt; 8  Port Nolloth    DEA 1991-09-01 11.29208     5  UTR\nR&gt; 9  Port Nolloth    DEA 1991-10-01 11.37661     5  UTR\nR&gt; 10 Port Nolloth    DEA 1991-11-01 10.98208     5  UTR",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html#create-a-new-dataframe-for-a-newly-created-variable-column-with-transmute",
    "href": "BCB744/intro_r/14-tidiest.html#create-a-new-dataframe-for-a-newly-created-variable-column-with-transmute",
    "title": "14. Tidiest Data",
    "section": "Create a new dataframe for a newly created variable (column) with transmute()\n",
    "text": "Create a new dataframe for a newly created variable (column) with transmute()\n\nIf for whatever reason you wanted to create a new variable (column), as you would do with mutate(), but you do not want to keep the dataframe from which the new column was created, the function to use is transmute().\n\nSACTN %&gt;% \n  transmute(kelvin = temp + 273.15)\n\n\n\nR&gt;  [1] 284.6203 285.1441 285.1056 285.0118 285.3572 285.6881 284.4020 284.4421\nR&gt;  [9] 284.5266 284.1321\n\n\nThis makes a bit more sense when paired with group_by() as it will pull over the grouping variables into the new dataframe. Note that when it does this for you automatically it will provide a message in the console.\n\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  transmute(kelvin = temp + 273.15)\n\n\n\nR&gt; # A tibble: 10 × 3\nR&gt; # Groups:   site, src [1]\nR&gt;    site         src   kelvin\nR&gt;    &lt;fct&gt;        &lt;chr&gt;  &lt;dbl&gt;\nR&gt;  1 Port Nolloth DEA     285.\nR&gt;  2 Port Nolloth DEA     285.\nR&gt;  3 Port Nolloth DEA     285.\nR&gt;  4 Port Nolloth DEA     285.\nR&gt;  5 Port Nolloth DEA     285.\nR&gt;  6 Port Nolloth DEA     286.\nR&gt;  7 Port Nolloth DEA     284.\nR&gt;  8 Port Nolloth DEA     284.\nR&gt;  9 Port Nolloth DEA     285.\nR&gt; 10 Port Nolloth DEA     284.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html#count-observations-rows-with-n",
    "href": "BCB744/intro_r/14-tidiest.html#count-observations-rows-with-n",
    "title": "14. Tidiest Data",
    "section": "Count observations (rows) with n()\n",
    "text": "Count observations (rows) with n()\n\nYou have already seen this function sneak it’s way into a few of the code chunks in the previous session. You use n() to count any grouped variable automatically. It is not able to be given any arguments, so you must organise our dataframe in order to satisfy it’s needs. It is the diva function of the tidyverse; however, it is terribly useful as you usually want to know how many observations your summary stats are based. First you will run some stats and create a figure without documenting n. Then you will include n and see how that changes your conclusions.\n\n SACTN_n &lt;- SACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %&gt;% \n  arrange(mean_temp) %&gt;% \n  ungroup() %&gt;% \n  select(mean_temp) %&gt;% \n  unique()\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point() +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\n\n\nDot plot showing range of mean temperatures for the time series in the SACTN dataset.\n\n\n\nThis looks like a pretty linear distribution of temperatures within the SACTN dataset. But now let’s change the size of the dots to show how frequently each of these mean temperatures is occurring.\n\n SACTN_n &lt;- SACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %&gt;% \n  ungroup() %&gt;% \n  select(mean_temp) %&gt;% \n  group_by(mean_temp) %&gt;% \n  summarise(count = n())\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point(aes(size = count)) +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\n\n\nDot plot showing range of mean temperatures for the time series in the SACTN dataset with the size of each dote showing the number of occurences of each mean.\n\n\n\nYou see now when you include the count (n) of the different mean temperatures that this distribution is not so even. There appear to be humps around 17°C and 22°C. Of course, you’ve created dot plots here just to illustrate this point. In reality if you were interested in a distribution like this one would use a histogram, or better yet, a density polygon.\n\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))\n            ) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(x = mean_temp)) +\n  geom_density(fill = \"seagreen\", alpha = 0.6) +\n  labs(x = \"Temperature (°C)\")\n\n\n\nFrequency distribution of mean temperature for each time series in the SACTN dataset.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html#select-observations-rows-by-number-with-slice",
    "href": "BCB744/intro_r/14-tidiest.html#select-observations-rows-by-number-with-slice",
    "title": "14. Tidiest Data",
    "section": "Select observations (rows) by number with slice()\n",
    "text": "Select observations (rows) by number with slice()\n\nIf you want to select only specific rows of a dataframe, rather than using some variable like you do for filter(), you use slice(). The function expects us to provide it with a series of integers as seen in the following code chunk. Try playing around with these values and see what happens\n\n# Slice a seqeunce of rows\nSACTN %&gt;% \n  slice(10010:10020)\n\n# Slice specific rows\nSACTN %&gt;%\n  slice(c(1,8,19,24,3,400))\n\n# Slice all rows except these\nSACTN %&gt;% \n  slice(-(c(1,8,4)))\n\n# Slice all rows except a sequence\nSACTN %&gt;% \n  slice(-(1:1000))\n\nIt is discouraged to use slice to remove or select specific rows of data as this does not discriminate against any possible future changes in ones data. Meaning that if at some point in the future new data are added to a dataset, re-running this code will likely no longer be selecting the correct rows. This is why filter() is a main function, and slice() is not. This auxiliary function can however still be quite useful when combined with arrange.\n\n# The top 5 variable sites as measured by SD\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(sd_temp = sd(temp, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(sd_temp)) %&gt;% \n  slice(1:5)\n\nR&gt; # A tibble: 5 × 3\nR&gt;   site       src   sd_temp\nR&gt;   &lt;fct&gt;      &lt;chr&gt;   &lt;dbl&gt;\nR&gt; 1 Muizenberg SAWS     2.76\nR&gt; 2 Stilbaai   SAWS     2.72\nR&gt; 3 Mossel Bay SAWS     2.65\nR&gt; 4 De Hoop    DAFF     2.51\nR&gt; 5 Mossel Bay DEA      2.51",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/14-tidiest.html#summary-functions",
    "href": "BCB744/intro_r/14-tidiest.html#summary-functions",
    "title": "14. Tidiest Data",
    "section": "Summary functions",
    "text": "Summary functions\nThere is a near endless sea of possibilities when one starts to become comfortable with writing R code. You have seen several summary functions used thus far. Mostly in straightforward ways. But that is one of the fun things about R, the only limits to what you may create are within your mind, not the program. Here is just one example of a creative way to answer a straightforward question: ‘What is the proportion of recordings above 15°C per source?’. Note how you may refer to columns you have created within the same chunk. There is no need to save the intermediate dataframes if we choose not to.\n\nSACTN %&gt;% \n  na.omit() %&gt;% \n  group_by(src) %&gt;%\n  summarise(count = n(), \n            count_15 = sum(temp &gt; 15)) %&gt;% \n  mutate(prop_15 = count_15/count) %&gt;% \n  arrange(prop_15)\n\nR&gt; # A tibble: 7 × 4\nR&gt;   src   count count_15 prop_15\nR&gt;   &lt;chr&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;\nR&gt; 1 DAFF    641      246   0.384\nR&gt; 2 SAWS   8636     4882   0.565\nR&gt; 3 UWC      12        7   0.583\nR&gt; 4 DEA    2087     1388   0.665\nR&gt; 5 SAEON   596      573   0.961\nR&gt; 6 EKZNW   369      369   1    \nR&gt; 7 KZNSB 15313    15313   1",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Tidiest Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html",
    "href": "BCB744/intro_r/01-RStudio.html",
    "title": "1. R & RStudio",
    "section": "",
    "text": "In this Lecture we will cover:",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#general-settings",
    "href": "BCB744/intro_r/01-RStudio.html#general-settings",
    "title": "1. R & RStudio",
    "section": "General Settings",
    "text": "General Settings\nBefore we start using RStudio (which is a code editor and environment that runs R) let’s first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’) and select the tick boxes as shown in the figure below.\n\n\nRStudio preferences",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#customising-appearance",
    "href": "BCB744/intro_r/01-RStudio.html#customising-appearance",
    "title": "1. R & RStudio",
    "section": "Customising Appearance",
    "text": "Customising Appearance\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g. Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code.\n\n\nAppearance settings",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#configuring-panes",
    "href": "BCB744/intro_r/01-RStudio.html#configuring-panes",
    "title": "1. R & RStudio",
    "section": "Configuring Panes",
    "text": "Configuring Panes\nYou cannot rearrange panes (see below) in RStudio by dragging them, but you can alter their position via the Pane Layout tab in the ‘Tools’/‘Global Options’ (‘RStudio’/‘Preferences’ – for Mac). You may arrange the panes as you would prefer; however, we recommend that during the duration of this workshop you leave them in the default layout.\n\n\nRearranging the panes",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#source-editor",
    "href": "BCB744/intro_r/01-RStudio.html#source-editor",
    "title": "1. R & RStudio",
    "section": "Source Editor",
    "text": "Source Editor\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it isn’t already)\nNow double click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (on Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon or Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18 and re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now. Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nThe # symbol\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#console",
    "href": "BCB744/intro_r/01-RStudio.html#console",
    "title": "1. R & RStudio",
    "section": "Console",
    "text": "Console\nThis is where you can type code that executes immediately. This is also known as the command line. Throughout the notes, we will represent code for you to execute in R as a different font.\n\n\n\n\n\n\nType it in!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really shouldn’t. The first reason is that you might unwittingly copy invisible PDF formatting codes into R, which will make your script fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your errors. This is an invaluable way of learning and taking shortcuts now will only hurt you in the long run.\n\n\nEntering code in the command line is intuitive and easy. For example, we can use R as a calculator by typing into the Console (and pressing Enter after each line). Note that the output for every line of calculation (e.g. 6 * 3) is indicated by R&gt;, as we see here:\n\n6 * 3\n\nR&gt; [1] 18\n\n5 + 4\n\nR&gt; [1] 9\n\n2 ^ 3\n\nR&gt; [1] 8\n\n\nNote that spaces are optional around simple calculations, but I encourage their use to adhere to the R style guidelines.\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it; we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\nR&gt; [1] 9\n\n\nTo type the assignment operator (&lt;-) push the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\nWe can create a vector in R by using the combine c() function:\n\napples &lt;- c(5.3, 3.8, 4.5)\n\nA vector is a one-dimensional array (i.e., a list of numbers), and this is the simplest form of data used in R (you can think of a single value in R as just a very short vector). We’ll talk about more complex (and therefore more powerful) types of data structures as we go along.\nIf you want to display the value of apples type:\n\napples\n\nR&gt; [1] 5.3 3.8 4.5\n\n\nFinally, there are default functions in R for nearly all basic statistical analyses, including mean() and sd() (standard deviation):\n\nmean(apples)\n\nR&gt; [1] 4.533333\n\nsd(apples)\n\nR&gt; [1] 0.7505553\n\n\n\n\n\n\n\n\nVariable names\n\n\n\nIt is best not to use c as the name of a value or array. Why? What other words might not be good to use?\n\n\nOr try this:\n\nround(sd(apples), 2)\n\nR&gt; [1] 0.75\n\n\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters app and then the Tab key. What happens?\nThe code completion feature also provides brief inline help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google (see the code in: BONUS/mapping_yourself.Rmd). On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl+Up to review the list (Cmd+Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane (see below).\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source pane using the buttons at the top-right or by double-clicking the title bar)",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#environment-and-history-panes",
    "href": "BCB744/intro_r/01-RStudio.html#environment-and-history-panes",
    "title": "1. R & RStudio",
    "section": "Environment and History Panes",
    "text": "Environment and History Panes\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values and functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\nR&gt; [1] \"a\"        \"apples\"   \"b\"        \"pkgs_lst\" \"url\"\n\n\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/01-RStudio.html#files-plots-packages-help-and-viewer-panes",
    "href": "BCB744/intro_r/01-RStudio.html#files-plots-packages-help-and-viewer-panes",
    "title": "1. R & RStudio",
    "section": "Files, Plots, Packages, Help, and Viewer Panes",
    "text": "Files, Plots, Packages, Help, and Viewer Panes\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where graphics you create will appear. The Packages tab shows you the packages that are installed and those that can be installed (more on this just now). The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console.\nMethods of getting help from the Console include:\n\n?mean\n\n…or:\n\nhelp(mean)\n\nWe will go into this in more detail in the next session.\nTo reproduce Figure Figure 1 in the Plot tab, simply copy and paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.01)\ny &lt;- 2 * sin(2 * pi * (x - 1/4))\nggplot() +\n  geom_point(aes(x = x, y = y), shape = 21, col = \"salmon\", fill = \"white\")\n\n\n\n\n\n\nFigure 1: A plot assembled with ggplot2.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. R & RStudio"
    ]
  },
  {
    "objectID": "BCB744/intro_r/06-faceting.html",
    "href": "BCB744/intro_r/06-faceting.html",
    "title": "6. Faceting Figures",
    "section": "",
    "text": "So far we have only looked at single panel figures. But as you may have guessed by now, ggplot2 is capable of creating any sort of data visualisation that a human mind could conceive. This may seem like a grandiose assertion, but we’ll see if we can’t convince you of it by the end of this course. For now however, let’s just take our understanding of the usability of ggplot2 two steps further by first learning how to facet a single figure, and then stitch different types of figures together into a grid. In order to aid us in this process we will make use of an additional package, ggpubr. The purpose of this package is to provide a bevy of additional tools that researchers commonly make use of in order to produce publication quality figures. Note that library(ggpubr) will not work on your computer if you have not yet installed the package.\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Faceting Figures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/06-faceting.html#line-graph",
    "href": "BCB744/intro_r/06-faceting.html#line-graph",
    "title": "6. Faceting Figures",
    "section": "Line graph",
    "text": "Line graph\n\nline_1 &lt;- ggplot(data = ChickWeight, aes(x = Time, y = weight, colour = Diet)) +\n  geom_point() +\n  geom_line(aes(group = Chick)) +\n  labs(x = \"Days\", y = \"Mass (g)\") +\n  theme_minimal()\nline_1\n\n\n\nLine graph for the progression of chicken weights (g) over time (days) based on four different diets.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Faceting Figures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/06-faceting.html#smooth-gam-model",
    "href": "BCB744/intro_r/06-faceting.html#smooth-gam-model",
    "title": "6. Faceting Figures",
    "section": "Smooth (GAM) model",
    "text": "Smooth (GAM) model\n\nlm_1 &lt;- ggplot(data = ChickWeight, aes(x = Time, y = weight, colour = Diet)) +\n  geom_point() +\n  geom_smooth(method = \"gam\") +\n  labs(x = \"Days\", y = \"Mass (g)\") +\n  theme_minimal()\nlm_1\n\n\n\nLinear models for the progression of chicken weights (g) over time (days) based on four different diets.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Faceting Figures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/06-faceting.html#histogram",
    "href": "BCB744/intro_r/06-faceting.html#histogram",
    "title": "6. Faceting Figures",
    "section": "Histogram",
    "text": "Histogram\n\n# Note that we are using 'ChickLast', not 'ChickWeight'\nhistogram_1 &lt;- ggplot(data = ChickLast, aes(x = weight)) +\n  geom_histogram(aes(fill = Diet), position = \"dodge\", binwidth = 100) +\n  labs(x = \"Final Mass (g)\", y = \"Count\") +\n  theme_minimal()\nhistogram_1\n\n\n\nHistogram showing final chicken weights (g) by diet.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Faceting Figures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/06-faceting.html#boxplot",
    "href": "BCB744/intro_r/06-faceting.html#boxplot",
    "title": "6. Faceting Figures",
    "section": "Boxplot",
    "text": "Boxplot\n\n# Note that we are using 'ChickLast', not 'ChickWeight'\nbox_1 &lt;- ggplot(data = ChickLast, aes(x = Diet, y = weight)) +\n  geom_boxplot(aes(fill = Diet)) +\n  labs(x = \"Diet\", y = \"Final Mass (g)\") +\n  theme_minimal()\nbox_1\n\n\n\nViolin plot showing the distribution of final chicken weights (g) by diet.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Faceting Figures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/10-mapping_rnaturalearth.html",
    "href": "BCB744/intro_r/10-mapping_rnaturalearth.html",
    "title": "10. Mapping With Natural Earth",
    "section": "",
    "text": "“In the beginning there was nothing, which exploded.”\n— Terry Pratchett\n\n\n“Biology is the study of complicated things that have the appearance of having been designed with a purpose.”\n— Richard Dawkins\n\nWeb resources about R for Spatial Applications\nNow that we are upgrading to better, more powerful maps, you’ll need to refer to industrial-strength documentation for detailed help. Please refer to links below for information about the vast array of functions available for spatial computations and graphics.\n\n\n\n\n\n\nWeb resources about spatial methods in R\n\n\n\n\n\nAUTHOR\nTITLE\n\n\n\nSpatial R\n\n\n\nEdzer Pebesma\nSimple Features for R\n\n\nEdzer Pebesma, Roger Bivand\nSpatial Data Science with applications in R\n\n\nRobin Lovelace et al.\nGeocomputation with R\n\n\nManuel Gimond\nIntro to GIS and Spatial Analysis\n\n\nWasser et al.\nIntroduction to Geospatial Raster and Vector Data with R\n\n\nTaro Mieno\nR as GIS for Economists\n\n\n\n\n\nThe sf package\nThe sf package in R is a package for handling and processing spatial data. In recent years it has become the de facto package to use for many mapping application, replacing older packages such as sp and including the C libraries GEOS 3, GDAL, and PROJ. It provides classes for storing and manipulating simple feature geometries, and functions for working with spatial data. ‘Simple features’ refer to a standardised way of encoding vector data, including points, lines, and polygons, that are widely used in geographic information systems (GIS).\nThe sf package was created to provide a fast and efficient way to work with vector data in R, and it is designed to integrate with other packages in the tidyverse, such as dplyr and ggplot2, allowing for seamless processing and visualisation of spatial data. The package provides a variety of functions for data import, transformation, manipulation, and analysis, making it a valuable tool for working with spatial data in R.\nIn addition to its core functionality, the sf package also provides a set of methods for converting between different data representations, such as data frames, matrices, and lists, making it a versatile tool for working with spatial data in a variety of formats.\nWhile sf works with vector data, raster data require the well-known but old raster package, or its modern replacements terra and stars. I will not work with raster data in this Chapter.\nMaps with rnaturalearth\n\nNatural Earth is a public domain map dataset that provides high-quality, general-purpose base maps for the world at various scales. It was designed to be a visually pleasing alternative to other public domain datasets, and its creators aim to provide the data in a form that is useful for a wide range of applications and to make it easy to use and integrate with other data.\nThe dataset includes a variety of geographic features, including coastlines, rivers, lakes, and political boundaries, as well as cultural features like cities, roads, and railways. The data are available in several different formats, including vector and raster, and it can be used with a variety of software, including GIS and mapping applications. Within R we can access these map layers using the rnaturalearth package.\nOne of the key benefits of Natural Earth is its public domain status, which means that anyone can use and distribute the data without restrictions or licensing fees. This makes it an ideal choice for individuals who need high-quality base maps for their projects but may not have the resources or expertise to create them from scratch. I am not convinced that students actually read this. The first person to send me a WhatsApp mentioning the phrase “Know your maps” will get a Lindt chocolate.\nIn addition to its public domain status, Natural Earth is also regularly updated with new data to ensure that the maps remain accurate and up-to-date. This makes it a valuable resource for anyone who needs reliable and up-to-date geographic data.\nInstall packages and set things up\n\n# install.packages(\"rnaturalearth\", \"rnaturalearthdata\", \"sf\")\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n# for the buffer to work as I expect, swith off\n# the functions for spherical geometry:\nsf_use_s2(FALSE)\n\nFirst, I define the extent of the map region:\n\n# the full map extent:\nxmin &lt;- 12; ymin &lt;- -36.5; xmax &lt;- 40.5; ymax &lt;- -10\nxlim &lt;- c(xmin, xmax); ylim &lt;- c(ymin, ymax)\n\n# make a bounding box for cropping:\nbbox &lt;- st_bbox(c(xmin = xmin, ymin = ymin,\n  xmax = xmax, ymax = ymax))\n\n# might be useful for zooming into a smaller region (False Bay and \n# the Cape Peninsula):\nxlim_zoom &lt;- c(17.8, 19); ylim_zoom &lt;- c(-34.5, -33.2)\n\nLoad the data and make maps\n\n# load the countries:\nsafrica_countries &lt;- ne_countries(returnclass = 'sf',\n  continent = \"Africa\",\n  country = c(\"South Africa\", \"Mozambique\",\n    \"Namibia\", \"Zimbabwe\", \"Botswana\",\n    \"Lesotho\", \"Eswatini\"),\n  scale = \"large\")\n\nLet us see what is inside the safrica_countries object:\n\nclass(safrica_countries)\n\nR&gt; [1] \"sf\"         \"data.frame\"\n\n# safrica_countries\n\nAs you can see, it is a data.frame and tbl (tibble), amongst other classes, and so you can apply many of the tidyverse functions to it, including select(), filter(), summarise() and so on. The class() argument additionally indicates that it has some simple features properties, so some functions provided by the sf package also becomes available to use. You can see some of these functions in action, below.\n\n\n\n\n\n\nThe sf class\n\n\n\nsf indicates that the object is of class simple features. In sf language, what would be called columns (variables) in normal tidyverse speak becomes known as attributes—these are the properties of the map features, with the features being the types of geometrical representations of geographical objects.\n\n\nLet us plot the entire safrica_countries object to see all the attributes of all of the features. This kind of figure a called a choropleth map:\n\nplot(safrica_countries)\n\n\n\n\n\n\n\nYou probably don’t want to plot all of them. Let us select one:\n\nplot(safrica_countries[\"sovereignt\"])\n\n\n\n\n\n\n\nYou might achieve the same in a more familiar way:\n\nsafrica_countries |&gt; \n  select(sovereignt) |&gt; \n  plot()\n\n\n\n\n\n\n\nOr you may want to plot the estimate of the population size, which is contained in the attribute pop_est:\n\nsafrica_countries |&gt; \n  select(pop_est) |&gt; \n  plot()\n\n\n\n\n\n\n\nThe names of the countries are in the rows down the safrica_countries object, and so they become accessible with filter(). Let us only plot some attribute for South Africa:\n\nsafrica_countries |&gt; \n  dplyr::filter(sovereignt == \"South Africa\") |&gt; \n  select(sovereignt) |&gt; \n  plot()\n\n\n\n\n\n\n\nYou can continue to add additional operations to create a new map:\n\nsafrica_countries_new &lt;- safrica_countries |&gt; \n  group_by(continent) |&gt; \n  summarise() |&gt; \n  st_crop(bbox) |&gt;\n  st_combine()\n\nplot(safrica_countries_new)\n\n\n\n\n\n\n\nSo far you have relied on the base R plot function made for the simple features. You can also plot the map in ggplot using a more familiar and more customisable interface:\n\nggplot() +\n  geom_sf(data = safrica_countries,\n    colour = \"indianred\", fill = \"beige\") +\n  coord_sf(xlim = xlim,\n           ylim = ylim)\n\n\n\n\n\n\n\nNow you can layer another feature:\n\nbuffer &lt;- safrica_countries_new %&gt;%\n  st_buffer(0.4)\n\nggplot() +\n  geom_sf(data = buffer, fill = \"lightblue\", col = \"transparent\") +\n  geom_sf(data = safrica_countries, colour = \"indianred\", fill = \"beige\") +\n  theme_minimal()\n\n\n\n\n\n\n\nExample\nHere are examples that use the built-in Fiji earthquake data or the Kaggle earthquake data.\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {10. {Mapping} {With} {Natural} {Earth}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/10-mapping_rnaturalearth.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 10. Mapping With Natural Earth. http://tangledbank.netlify.app/BCB744/intro_r/10-mapping_rnaturalearth.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "10. Mapping With Natural Earth"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html",
    "href": "BCB744/intro_r/03-data-in-R.html",
    "title": "3. Data Classes & Structures",
    "section": "",
    "text": "“That which can be destroyed by the truth should be.”\n— P.C. Hodgell",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#numeric-variables",
    "href": "BCB744/intro_r/03-data-in-R.html#numeric-variables",
    "title": "3. Data Classes & Structures",
    "section": "Numeric variables",
    "text": "Numeric variables\nNumeric data in the context of biostatistics refers to quantitative data that can be expressed in numerical form, typically obtained from field and laboratory measurements, or from field sampling campaigns. Examples of numeric data in biostatistics include the height and mass of a animals, concentrations of nutrients, laboratory test results such as respiration rates, or the number of limpets in a quadrat. Numeric data can be further categorised as discrete or continuous.\nDiscrete variables\nDiscrete data are whole (integer) numbers that represent counts of items or events. Integer data usually answer the question, “how many?” For example, in the biological and Earth sciences, discrete data are commonly encountered in the form of counts or integers that represent the presence or absence of certain characteristics or events. For example, the number of individuals of some species in a population, the number of chromosomes in a cell, or the number of earthquakes occurring in a region within a given time frame. Other examples of discrete data in these sciences include the number of mutations in a gene, the number of cells in a tissue sample, or the number of species present in an ecosystem. These types of data are often analysed using statistical techniques such as frequency distributions, contingency tables, and chi-square tests.\nContinuous variables\nContinuous data, on the other hand, are measured on a continuous scale. These usually represent measured quantities such as something’s heat content (temperature, measured in degrees Celsius) or distance (measured in metres or similar), etc. They can be rational numbers including integers and fractions, but typically they have an infinite number of ‘steps’ that depend on rounding (they can even be rounded to whole integers) or considerations such as measurement precision and accuracy. Often, continuous data have upper and lower bounds that depend on the characteristics of the phenomenon being studied or the measurement being taken.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#dates",
    "href": "BCB744/intro_r/03-data-in-R.html#dates",
    "title": "3. Data Classes & Structures",
    "section": "Dates",
    "text": "Dates\n\n\n\n\nWe often encounter date data when dealing with time-related data. For example, in ecological research, data collection may involve recording the date of a particular observation or sampling event, such as the date when a bird was sighted, or when water samples were taken from a stream. The purpose of using date (or time) data in biology and ecology is to enable us to understand and analyse temporal patterns and relationships in their response variables. This can include exploring seasonal trends, understanding the impact of environmental changes over time, or tracking the growth and development of organisms.\nBy analysing date data, we can gain insights into long-term trends and patterns that may not be apparent when looking at the data in aggregate. They can also use this information to make predictions about future trends, develop more effective management strategies, and identify potential areas for further research.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#character-data",
    "href": "BCB744/intro_r/03-data-in-R.html#character-data",
    "title": "3. Data Classes & Structures",
    "section": "Character data",
    "text": "Character data\nCharacter data are used to describe qualitative variables or descriptive text that are not numerical in nature. Character data can be entered as descriptive character strings, and internally, they are translated into a vector of characters in R. They are often used to represent categorical variables, such as the type of plant species, the colour of a bird’s feathers, or the name of a some gene. Social scientists will sometimes use character data fields to record the names of people or places, or other descriptive information, such as a narrative that will later be subjected to, for example, a sentiment analysis. For convenience, I’ll call these data narrative style data to distinguish them from the qualitative data that are the main focus of the present discussion.\nSince narrative style data are not directly amenable to statistical analsysis, in this module, we will mainly concern ourselves with qualitative data which are typically names of things, or categories of objects, classes of behaviours, properties, characteristics, and so on. Qualitative data typically refer to non-numeric data collected from observations, experimental treatment groups, or other sources. They tend to be textual and are often used to describe characteristics or properties of living organisms, ecosystems, or other biological phenomena. Examples may include the colour of flowers, the type of habitat where an animal is found, the behaviour of animals, or the presence or absence of certain traits or characteristics in a population.\nQualitative data can be further classified into nominal or ordinal data types. Ordinal and nominal data are both amenable to statistical interpretation.\nNominal variables\nNominal data are used to describe qualitative variables that do not have any inherent order or ranking. Examples of nominal data in biology may include the type of plant or animal species, or the presence or absence of certain genetic traits. Another term for nominal data is categorical data. Because there are well-defined categories, the number of members belonging to each of the category can be counted. For example, there are three red flowers, 66 purple flowers, and 13 yellow flowers.\nOrdinal variables\nOrdinal data refer to a type of data that can be used to describe qualitative categorical variables that have a natural order or ranking. It is used when we need to arrange things in a particular order, such as from worst to best or from least to most. However, the differences between the values cannot be measured or quantified exactly, making them somewhat subjective. Examples of ordinal data include the different stages of development of an organism or the performance of a species to different fertilisers. Ordinal data can be entered as descriptive character strings, and internally, they are translated into an ordered vector of integers in R. For example, we can use a scale of 1 for terrible, 2 for ‘so-so’, 3 for average, 4 for good, and 5 for brilliant.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#binary-variables",
    "href": "BCB744/intro_r/03-data-in-R.html#binary-variables",
    "title": "3. Data Classes & Structures",
    "section": "Binary variables",
    "text": "Binary variables\nLife can be boiled down to a series of binary decisions: should I have pizza for dinner, yes or no? Should I go to bed early, TRUE or FALSE? Should I start that new series on Netflix, accept or reject? Am I present or absent? You get the gist… This kind of binary decision-making is known as ‘logical’, and in R they can only take on the values of TRUE or FALSE (remember to mind your case!). In the computing world, logical data are often represented by 1 for TRUE and 0 for FALSE. So basically, your life’s choices can be summarised as a string of 1s and 0s. Who knew it was that simple?\n\n\n\n\nWhen it comes down to it, everything in life is either black or white, right or wrong, good or bad. It’s like a cosmic game of “Would You Rather?” — and we’re all just playing along.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#missing-values",
    "href": "BCB744/intro_r/03-data-in-R.html#missing-values",
    "title": "3. Data Classes & Structures",
    "section": "Missing values",
    "text": "Missing values\nIt’s unfortunate to admit that one of the most reliable aspects of any biological dataset is the presence of missing data (the presence of something that’s missing?!). It is a stark reminder of the fragility of life. How can we say that something contains missing data? It seems counter intuitive, as if the data were never there in the first place. However, as we remember the principles of tidy data, we see that every observation must be documented in a row, and each column in that row must contain a value. This organisation allows us to create a matrix of data from multiple observations. Since the data are presented in a two-dimensional format, any missing values from an observation will leave a gaping hole in the matrix. We call these ‘missing values.’ It’s a somber reality that even the most meticulous collection of data can be marred by the loss of information.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#complex-numbers",
    "href": "BCB744/intro_r/03-data-in-R.html#complex-numbers",
    "title": "3. Data Classes & Structures",
    "section": "Complex numbers",
    "text": "Complex numbers\n\n“And if you gaze long enough into an abyss, the abyss will gaze back into you.”\n— Friedrich Nietzsche\n\nAs we draw to a close on the topic of data types, we cling desperately to the threads of our sanity, hoping against hope that they remain tightly stitched. But let it be known, to those who dare enter further into the realm of data, that beneath the surface lie countless rocks, and around every corner lurk a legion of complex data types, waiting to ensnare the unwary. These shadows of information are as enigmatic as they are perilous, for they challenge the very essence of our understanding. It is not until the final chapter of our journey, when we confront the elusive art of modeling, that we will face these data demons head-on. But fear not, for we shall arm ourselves with the knowledge and techniques acquired on this treacherous path, and with each step forward, we shall move closer to mastering the darkness that awaits us.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#numeric",
    "href": "BCB744/intro_r/03-data-in-R.html#numeric",
    "title": "3. Data Classes & Structures",
    "section": "numeric",
    "text": "numeric\nIn R, the numeric data class represents either integers or floating point (decimal) values. Numerical data are quantitative in nature as they represent things that can be objectively counted, measured, or calculated—the measured variables.\nNumeric data are one of the most common types of data used in statistical and mathematical analysis. In R, numeric data are represented by the class numeric, which includes both integers and floating-point numbers. Numeric data can be used in a variety of operations and calculations, including arithmetic operations, statistical analyses, and visualisations. One important feature of the numeric data class in R is that it supports vectorisation, which allows for efficient and concise operations on large sets of numeric data. Additionally, R provides a wide range of built-in functions for working with numeric data, including functions for calculating basic statistical measures such as mean, median, and standard deviation.\nIn R integer (discrete) data are called int or &lt;int&gt; while continuous data are denoted num or &lt;dbl&gt;.\nExample of integer data Suppose you have a dataset of the number of rats in different storm water drains in a neighbourhood. The number of rats is a discrete variable because it can only take on integer values (you can’t own a fraction of a rat).\nHere’s how you could create a vector of this data in R:\n\n# Create a vector of the number of pets owned by each household\nnum_rats &lt;- c(0, 1, 2, 2, 3, 1, 4, 0, 2, 1, 2, 2, 0, 3, 2, 1, 1, 4, 2, 0)\nnum_rats\n\n [1] 0 1 2 2 3 1 4 0 2 1 2 2 0 3 2 1 1 4 2 0\n\nclass(num_rats)\n\n[1] \"numeric\"\n\n\nIn this example, the data are represented as a vector called num_rats of class numeric (as revealed by class(num_rats)). Each element of the vector represents the number of rats in one storm water drain. For example, the first element of the vector (num_rats[1]) is 0, which means that the first drain in the dataset is free of rats. The fourth element of the vector (num_rats[4]) is 2, indicating that the fourth drain in the dataset is occupied by 2 rats.\nOne can also explicitly create a vector of integer using the as.integer() function:\n\nnum_rats_int &lt;- as.integer(num_rats)\nnum_rats_int\n\n [1] 0 1 2 2 3 1 4 0 2 1 2 2 0 3 2 1 1 4 2 0\n\nclass(num_rats_int)\n\n[1] \"integer\"\n\n\nAbove we coerced the class numeric data to class integer. But we can take floating point numeric and convert them to integers too with the as.integer() function. As we see, the effect is that the whole part of the number is retained and the rest discarded:\n\npies &lt;- pi * seq(1:5)\npies\n\n[1]  3.141593  6.283185  9.424778 12.566371 15.707963\n\nclass(pies)\n\n[1] \"numeric\"\n\nas.integer(pies)\n\n[1]  3  6  9 12 15\n\n\nEffectively, what happened above is more-or-less equivalent to what the floor() function would return:\n\nfloor(pies)\n\n[1]  3  6  9 12 15\n\n\nBe careful when coercing floating point numbers to integers. If rounding is what you expect, this is not what you will get. For rounding, use round() instead:\n\nround(pies, 0)\n\n[1]  3  6  9 13 16\n\n\nExample of continuous data Here are some randomly generated temperature data assigned to an object called temp_data:\n\n# Generate a vector of 50 normally distributed temperature values\ntemp_data &lt;- round(rnorm(n = 50, mean = 15, sd = 3), 2)\ntemp_data\n\n [1] 14.18 20.21 15.95  7.26 10.20 17.08 21.95 13.82 16.02 15.23 14.68  9.82\n[13] 16.26 13.63 12.76 15.45 15.81 15.34 13.43 20.26 11.77 20.24  9.84 16.22\n[25] 12.53 13.88 14.55 19.23 15.01  8.66 12.60  6.28 17.44 15.21 17.49 16.56\n[37] 14.73 14.20 14.76 10.37 17.08 13.90 17.46 16.69 16.17  9.93 13.76 12.45\n[49] 15.75 19.20\n\nclass(temp_data)\n\n[1] \"numeric\"",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#character",
    "href": "BCB744/intro_r/03-data-in-R.html#character",
    "title": "3. Data Classes & Structures",
    "section": "character",
    "text": "character\nIn R, the character data class represents textual data such as words, sentences, and paragraphs. Character data can be created using either single or double quotes, and it can include letters, numbers, and other special characters. In addition, character data can be concatenated using the paste() function or other string manipulation functions.\nOne important feature of the character data class in R is its versatility in working with textual data. For instance, it can be used to store and manipulate text data, including text-based datasets, text-based files, and text-based visualisations. Additionally, R provides a wide range of built-in functions for working with character data, including functions for manipulating strings, searching for patterns, and formatting output. Overall, the character data class in R is a fundamental data type that is critical for working with textual data in a variety of contexts. You will most frequently use character values are often used to represent labels, names, or descriptions.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#factor",
    "href": "BCB744/intro_r/03-data-in-R.html#factor",
    "title": "3. Data Classes & Structures",
    "section": "factor",
    "text": "factor\nIn R, the factor data class is used to represent discrete categorical variables. Factors are often used in statistical analyses to represent class or group belonging. Factor values are categorical data, such as levels or categories of a variable. Factor variables are most commonly also character data, but they can be numeric too if coded correctly as factors. Factor values can be ordered (ordinal) or unordered (categorical or nominal).\nCategorical variables take on a limited number of distinct values, often corresponding to different groups or levels. For example, a categorical variable might represent different colours, size classes, or species. Factors in R are represented as integers with corresponding character levels, where each level corresponds to a distinct category. The levels of a factor can be defined explicitly using the factor() function or automatically using the cut() function. One important feature of the factor data class in R is that it allows for efficient and effective data manipulation and analysis, particularly when working with large datasets. For instance, factors can be used in statistical analyses such as regression models or ANOVA, and they can also be used to create visualisations such as bar or pie graphs. The factor data class in R is a fundamental data type that is critical for representing and working with categorical variables in data analysis and visualisation.\nThe factor data class of data in an R data.frame structure (or in a tibble) is indicated by Factor or &lt;fctr&gt;. Ordered factors are denoted by columns named Ord.factor or &lt;ord&gt;.\nNominal data One example of nominal factor data that ecologists might encounter is the type of vegetation in a particular area, such as ‘grassland’, ‘forest’, or ‘wetland’. Here’s an example of how to generate a vector of nominal data in R using the sample() function:\n\n# Generate a vector of vegetation types\nvegetation &lt;- sample(c(\"grassland\", \"forest\", \"wetland\"), size = 50, replace = TRUE)\n\n# View the vegetation data\nvegetation\n\n [1] \"grassland\" \"wetland\"   \"grassland\" \"forest\"    \"forest\"    \"grassland\"\n [7] \"forest\"    \"forest\"    \"forest\"    \"wetland\"   \"grassland\" \"wetland\"  \n[13] \"grassland\" \"forest\"    \"wetland\"   \"forest\"    \"forest\"    \"grassland\"\n[19] \"wetland\"   \"forest\"    \"grassland\" \"forest\"    \"wetland\"   \"forest\"   \n[25] \"forest\"    \"forest\"    \"grassland\" \"forest\"    \"forest\"    \"forest\"   \n[31] \"grassland\" \"grassland\" \"wetland\"   \"forest\"    \"wetland\"   \"wetland\"  \n[37] \"grassland\" \"forest\"    \"wetland\"   \"forest\"    \"forest\"    \"forest\"   \n[43] \"wetland\"   \"grassland\" \"forest\"    \"forest\"    \"forest\"    \"forest\"   \n[49] \"forest\"    \"grassland\"\n\nclass(vegetation)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\nThe sample() function\n\n\n\nNote that the sample() function is not made specifically for nominal data; it can be used on any kind of data class.\n\n\nOrdinal data Here’s an example vector of ordinal data in R that could be encountered by ecologists:\n\n# Vector of ordinal data representing the successional stage of a forest\nsuccession &lt;- c(\"Early Pioneer\", \"Late Pioneer\",\n                \"Young Forest\", \"Mature Forest\",\n                \"Old Growth\")\nsuccession\n\n[1] \"Early Pioneer\" \"Late Pioneer\"  \"Young Forest\"  \"Mature Forest\"\n[5] \"Old Growth\"   \n\nclass(succession)\n\n[1] \"character\"\n\n# Convert to ordered factor\nsuccession &lt;- factor(succession, ordered = TRUE,\n                     levels = c(\"Early Pioneer\", \"Late Pioneer\",\n                                \"Young Forest\", \"Mature Forest\",\n                                \"Old Growth\"))\nsuccession\n\n[1] Early Pioneer Late Pioneer  Young Forest  Mature Forest Old Growth   \n5 Levels: Early Pioneer &lt; Late Pioneer &lt; Young Forest &lt; ... &lt; Old Growth\n\nclass(succession)\n\n[1] \"ordered\" \"factor\" \n\n\nIn this example, the successional stage of a forest is represented by an ordinal scale with five levels ranging from ‘Early Pioneer’ to ‘Old Growth’. The factor() function is used to convert the vector to an ordered factor, with the ordered argument set to TRUE and the levels argument set to the same order as the original vector. This ensures that the levels are properly represented as an ordered factor.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#logical",
    "href": "BCB744/intro_r/03-data-in-R.html#logical",
    "title": "3. Data Classes & Structures",
    "section": "logical",
    "text": "logical\nIn R, the logical data class represents binary or Boolean data. Logical data are used to represent variables that can take on only two possible values, TRUE or FALSE. In addition to TRUE and FALSE, logical data can also take on the values of NA or NULL, which represent missing or undefined values.\nLogical data can be created using logical operators such as ==, !=, &gt;, &lt;, &gt;=, and &lt;=. Logical data are commonly used in R for data filtering and selection, conditional statements, and logical operations. For example, logical data can be used to filter a dataset to include only observations that meet certain criteria or to perform logical operations such as AND (&) and OR (|). The logical data class in R is a fundamental data type that is critical for representing and working with binary or Boolean variables in data analysis and programming.\nExample logical (binary) data Here’s an example of generating a vector of binary or logical data in R, which represents the presence or absence of a particular species in different ecological sites:\n\n# Generate a vector of 1s and 0s to represent the presence\n# or absence of a species in different ecological sites\nspecies_presence &lt;- sample(c(0,1), 10, replace = TRUE)\nspecies_presence\n\n [1] 0 0 0 1 1 1 1 1 0 0\n\n\nWe can also make a formal logical class data:\n\nspecies_presence_logi &lt;- as.logical(species_presence)\nclass(species_presence_logi)\n\n[1] \"logical\"\n\n\nIn this example, we again use the sample() function to randomly generate a vector of 10 values, each either 0 or 1, to represent the presence or absence of a species in 10 different ecological sites. However, it is often not necessary to coerce to class logical, as we see in the presence-absence datasets we will encounter in BCB743: Quantitative Ecology.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#date",
    "href": "BCB744/intro_r/03-data-in-R.html#date",
    "title": "3. Data Classes & Structures",
    "section": "date",
    "text": "date\nIn R, the POSIXct, POSIXlt, and Date classes are commonly used to represent date and time data. These classes each have unique characteristics that make them useful for different purposes.\nThe POSIXct class is a date/time class that represents dates and times as a numerical value, typically measured in seconds since January 1st, 1970. This class provides a high level of precision, with values accurate to the second. It is useful for performing calculations and data manipulation involving time, such as finding the difference between two dates or adding a certain number of seconds to a given time. An example of how to generate a POSIXct object in R is as follows:\n\nmy_time &lt;- as.POSIXct(\"2022-03-10 12:34:56\")\nclass(my_time)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nmy_time\n\n[1] \"2022-03-10 12:34:56 SAST\"\n\n\nThe POSIXlt class, on the other hand, typically represents dates and times in a more human-readable format. It stores date and time information as a list of named elements, including year, month, day, hour, minute, and second. This format is useful for displaying data in a more understandable way and for extracting specific components of a date or time. An example of how to generate a POSIXlt object in R is as follows:\n\nmy_time &lt;- as.POSIXlt(\"2022-03-10 12:34:56\")\nclass(my_time)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\nmy_time\n\n[1] \"2022-03-10 12:34:56 SAST\"\n\n\nThe Date class is used to represent dates only, without any time information. Dates are typically stored as the number of days since January 1st, 1970. This class provides functions for performing arithmetic operations and comparisons between dates. It is useful for working with time-based data that is only concerned with the date component, such as daily sales or stock prices. An example of how to generate a Date object in R is as follows:\n\nmy_date &lt;- as.Date(\"2022-03-10\")\nclass(my_date)\n\n[1] \"Date\"\n\nmy_date\n\n[1] \"2022-03-10\"\n\n\nTo generate a vector of dates in R with daily intervals, we can use the seq() function to create a sequence of dates, specifying the start and end dates and the time interval. Here’s an example:\n\n# Generate a vector of dates from January 1, 2022 to December 31, 2022\ndates &lt;- seq(as.Date(\"2022-01-01\"), as.Date(\"2022-12-31\"), by = \"day\")\n\n# View the first 10 dates in the vector\nhead(dates, 10)\n\n [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\n [6] \"2022-01-06\" \"2022-01-07\" \"2022-01-08\" \"2022-01-09\" \"2022-01-10\"\n\nclass(dates)\n\n[1] \"Date\"\n\n\nUnderstanding the characteristics of these date and time classes in R is essential for effective data analysis and manipulation in fields where time-based data is a critical component.\nDate and time data in R can be manipulated using various built-in functions and packages such as lubridate and chron. Additionally, date and time data can be visualised using different types of graphs such as time series plots, heatmaps, and Hovmöller diagrams. The date and time data classes in R are essential for working with temporal data and conducting time-related analyses in various biological and environmental datasets.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#missing-values-na",
    "href": "BCB744/intro_r/03-data-in-R.html#missing-values-na",
    "title": "3. Data Classes & Structures",
    "section": "Missing values, NA\n",
    "text": "Missing values, NA\n\nMissing values can be encountered in vectors of all data classes. To demonstrate some data that contains missing values, I will generate a data sequence containing 5% missing values. We can use the rnorm() function to generate a sequence of random normal numbers and then randomly assign 5% of the values as missing using the sample() function. The indices of the missing values are stored in missing_indices, and we use them to assign NA to the corresponding elements of the data sequence. Here’s some code to achieve this:\n\n# Set the length of the sequence\nn &lt;- 100\n\n# Generate a sequence of random normal numbers with\n# mean 0 and standard deviation 1\ndata &lt;- rnorm(n, mean = 0, sd = 1)\n\n# Randomly assign 5% of the values as missing\nmissing_indices &lt;- sample(1:n, size = round(0.05*n))\ndata[missing_indices] &lt;- NA\nlength(data)\n\n[1] 100\n\ndata\n\n  [1] -0.43893229  0.01974387 -1.33186453 -0.00604415 -0.27340465 -1.96578150\n  [7]  0.80519266 -0.26910291 -1.18716061  0.14724895 -2.06307132  0.26743487\n [13]  1.24799667  0.94861069  0.34213769  1.00450813 -0.96998471 -1.08868347\n [19]  0.46311791 -0.85428187 -0.17499040  1.16796146  0.57246644  0.30265309\n [25] -1.29163397  0.58347529  0.84277752 -1.79853900  0.89689587  2.41061414\n [31]  0.94718411          NA  0.56382487  1.76054848 -1.22831988  2.06285169\n [37] -0.25518641  0.75181897  0.75635293 -2.05972401  2.51167272  0.79864421\n [43]          NA -0.13677150 -0.78365331          NA  0.47501091  0.57645090\n [49]  0.42552069  0.26871437 -0.31719160 -0.43283729  0.14214261  1.55236447\n [55] -0.50016935  1.73194340  1.14015057 -0.99746387  0.93999175  0.82959245\n [61] -0.79652555 -0.73916175 -1.04763073  1.46964595  0.12744893 -1.27652921\n [67]  0.04616807  0.59675766  0.58859599 -0.52205117 -0.01057440  0.14606629\n [73] -0.25300451  0.10965145          NA -1.28487686  1.81380033  0.87177567\n [79]  1.61475708  0.42301453  1.24437290  0.13025747  1.05024325 -0.31187800\n [85] -1.59583920  1.51996711 -1.48988572 -0.05826837 -0.50812472          NA\n [91]  0.51904718  0.42054858 -0.18533010  0.80120952  1.74426119  0.25357084\n [97]  0.64195957  2.29072398 -0.80525184  0.19655255\n\n\nTo remove all NAs from the vector of data we can use na.omit():\n\ndata_sans_na &lt;- na.omit(data)\nlength(data_sans_na)\n\n[1] 95\n\ndata_sans_na\n\n [1] -0.43893229  0.01974387 -1.33186453 -0.00604415 -0.27340465 -1.96578150\n [7]  0.80519266 -0.26910291 -1.18716061  0.14724895 -2.06307132  0.26743487\n[13]  1.24799667  0.94861069  0.34213769  1.00450813 -0.96998471 -1.08868347\n[19]  0.46311791 -0.85428187 -0.17499040  1.16796146  0.57246644  0.30265309\n[25] -1.29163397  0.58347529  0.84277752 -1.79853900  0.89689587  2.41061414\n[31]  0.94718411  0.56382487  1.76054848 -1.22831988  2.06285169 -0.25518641\n[37]  0.75181897  0.75635293 -2.05972401  2.51167272  0.79864421 -0.13677150\n[43] -0.78365331  0.47501091  0.57645090  0.42552069  0.26871437 -0.31719160\n[49] -0.43283729  0.14214261  1.55236447 -0.50016935  1.73194340  1.14015057\n[55] -0.99746387  0.93999175  0.82959245 -0.79652555 -0.73916175 -1.04763073\n[61]  1.46964595  0.12744893 -1.27652921  0.04616807  0.59675766  0.58859599\n[67] -0.52205117 -0.01057440  0.14606629 -0.25300451  0.10965145 -1.28487686\n[73]  1.81380033  0.87177567  1.61475708  0.42301453  1.24437290  0.13025747\n[79]  1.05024325 -0.31187800 -1.59583920  1.51996711 -1.48988572 -0.05826837\n[85] -0.50812472  0.51904718  0.42054858 -0.18533010  0.80120952  1.74426119\n[91]  0.25357084  0.64195957  2.29072398 -0.80525184  0.19655255\nattr(,\"na.action\")\n[1] 32 43 46 75 90\nattr(,\"class\")\n[1] \"omit\"\n\n\n\n\n\n\n\n\nDealing with NAs in functions\n\n\n\nMany functions have specific arguments to deal with NAs in data. See for example the na.rm = TRUE argument given to mean(), median(), min(), lm(), etc.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#vector-array-and-matrix",
    "href": "BCB744/intro_r/03-data-in-R.html#vector-array-and-matrix",
    "title": "3. Data Classes & Structures",
    "section": "\nvector, array, and matrix\n",
    "text": "vector, array, and matrix\n\nVectors In R, a vector is a one-dimensional array-like data structure that can hold a sequence of values of the same atomic mode, such as numeric, character, logical values, or Date and times. A vector can be created using the c() function, which stands for ‘combine’ or ‘concatenate,’ and is used to combine a sequence of values into a vector. Vectors can also be created by using the seq() function to generate a sequence of numbers, or the rep() function to repeat a value or sequence of values. Here is an example of a numeric vector:\n\n# create a numeric vector\nmy_vector &lt;- c(1, 2, 3, 4, 5)\n\n# coerce to vector\nmy_vector &lt;- as.vector(c(1, 2, 3, 4, 5))\nclass(my_vector) # but it doesn't change the class from numeric\n\n[1] \"numeric\"\n\n# print the vector\nmy_vector\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n\n\nCoercion to vector\n\n\n\nThe behaviour is such that the output of coercion to vector is that one the atomic modes (the basic data types) is returned.\n\n\nOne of the advantages of using vectors in R is that many of the built-in functions and operations work on vectors, allowing us to easily manipulate and analyse large amounts of data. Additionally, R provides many functions specifically designed for working with vectors, such as mean(), median(), sum(), min(), max(), and many others.\nMatrices A matrix (again, this terminology may be different for other languages), on the other hand, is a special case of an array that has two dimensions (rows and columns). It is also a multi-dimensional data structure that can hold elements of the same data type, but it is specifically designed for handling data in a tabular format. A matrix can be created using the matrix() function in R.\n\n# create a numeric matrix\nmy_matrix &lt;- matrix(1:6, nrow = 2, ncol = 3)\n\n# print the matrix\nmy_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nclass(my_matrix)\n\n[1] \"matrix\" \"array\" \n\n\nWe can query the size or dimensions of the matrix as follows:\n\ndim(my_matrix)\n\n[1] 2 3\n\nncol(my_matrix)\n\n[1] 3\n\nnrow(my_matrix)\n\n[1] 2\n\n\nCoercion of matrices to vectors A matrix can be coerced to a vector:\n\nas.vector(my_matrix)\n\n[1] 1 2 3 4 5 6\n\n\nArrays In R (as opposed to in python or some other languages), an array specifically refers to a multi-dimensional data structure that can hold elements of the same data type. It can have any number of dimensions (1, 2, 3, etc.), and its dimensions can be named. An array can be created using the array() function in R.\n\n# create a 2-dimensional array\nmy_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n# print the array\nmy_array\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27\n\nclass(my_array)\n\n[1] \"array\"\n\n\nWe can figure something out about the size or dimensions of the array:\n\ndim(my_array)\n\n[1] 3 3 3\n\nncol(my_array)\n\n[1] 3\n\nnrow(my_array)\n\n[1] 3\n\n\nCoercion of arrays to vectors The array can be coerced to a vector:\n\nas.vector(my_array)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27\n\n\nThe key difference between vectors, arrays, and a matrices in R is their dimensions. A vector has one dimension, an array can have any number of dimensions, while a matrix is limited to two dimensions. Additionally, a matrix is often used to store data in a tabular format, while an array is used to store multi-dimensional data in general. A commonly encountered kind of matrix is seen in multivariate statistics is a distance or dissimilarity matrix.\nIn R, vectors, arrays, and matrices share a common characteristic: they do not have row or column names. Therefore, to refer to any element, row, or column, one must use their corresponding index. How?\nAccessing elements, rows, columns, and matrices In R, the square bracket notation is used to access elements, rows, columns, or matrices in arrays. The notation takes the form of [i, j, k, ...], where i, j, k, and so on, represent the indices of the rows, columns, or matrices to be accessed.\nSuppose we have the following array:\n\n\nmy_array &lt;- array(data = round(rnorm(n = 60, mean = 13, sd = 2), 1),\n                  dim = c(5, 4, 3))\nmy_array\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,] 11.0 12.1 12.2 13.3\n[2,] 13.6 11.9 11.9 10.8\n[3,] 10.3 13.1 11.7 10.9\n[4,] 15.2 10.2 17.2 12.3\n[5,] 14.8 11.8 13.7 12.1\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,] 11.3 14.5 14.7 13.8\n[2,] 12.4 13.5 15.0 12.1\n[3,] 14.8  9.9 13.3 12.1\n[4,] 14.4 15.7 14.8 12.3\n[5,] 12.7 10.0 14.3  7.3\n\n, , 3\n\n     [,1] [,2] [,3] [,4]\n[1,]  8.9 15.1 14.2 12.3\n[2,] 15.4 13.1 12.7 12.0\n[3,] 15.0 11.8 11.2 13.1\n[4,] 13.5 10.7 12.0 15.5\n[5,] 15.4 12.7 11.8 10.3\n\ndim(my_array)\n\n[1] 5 4 3\n\n\nThis creates a \\(5\\times4\\times3\\) array with values from 1 to 60.\nWhen working with multidimensional arrays, it is possible to omit some of the indices in the square bracket notation. This results in a subset of the array, which can be thought of as a lower-dimensional array obtained by fixing the omitted dimensions. For example, consider a 3-dimensional array my_array above with dimensions dim(my_array) = c(5,4,3). If we use the notation my_array[1,,], we would obtain a 2-dimensional array with dimensions dim(my_array[1,,]) = c(4,3) obtained by fixing the first index at 1:\n\ndim(my_array[1,,])\n\n[1] 4 3\n\nmy_array[1,,]\n\n     [,1] [,2] [,3]\n[1,] 11.0 11.3  8.9\n[2,] 12.1 14.5 15.1\n[3,] 12.2 14.7 14.2\n[4,] 13.3 13.8 12.3\n\n\nHere are some more examples of how to use square brackets notation with arrays in R:\nTo access a single element in the array, use the notation [i, j, k], where i, j, and k are the indices along each of the three dimensions, which in combination, uniquely identifies each element. Below we return the element in the second row, third column, and first matrix:\n\nmy_array[2, 3, 1]  \n\n[1] 11.9\n\n\nTo access a single row in the array, use the notation [i, , ], where i is the index of the row. This will return the second rows and all of the columns of the first matrix:\n\nmy_array[2,,1]\n\n[1] 13.6 11.9 11.9 10.8\n\n\nTo access a single column in the array, use the notation [ , j, ], where j is the index of the column. Here we will return all the elements in the row of column two and matrix three:\n\nmy_array[ , 2, 3]\n\n[1] 15.1 13.1 11.8 10.7 12.7\n\n\nTo access a single matrix in the array, use the notation [ , , k], where k is the index of the matrix:\n\nmy_array[ , , 2]\n\n     [,1] [,2] [,3] [,4]\n[1,] 11.3 14.5 14.7 13.8\n[2,] 12.4 13.5 15.0 12.1\n[3,] 14.8  9.9 13.3 12.1\n[4,] 14.4 15.7 14.8 12.3\n[5,] 12.7 10.0 14.3  7.3\n\n\nTo obtain a subset of the array, use the notation [i, j, k] with i, j, or k omitted to obtain a lower-dimensional array:\n\nmy_array[1, , ]\n\n     [,1] [,2] [,3]\n[1,] 11.0 11.3  8.9\n[2,] 12.1 14.5 15.1\n[3,] 12.2 14.7 14.2\n[4,] 13.3 13.8 12.3\n\nmy_array[ , 2:3, ]\n\n, , 1\n\n     [,1] [,2]\n[1,] 12.1 12.2\n[2,] 11.9 11.9\n[3,] 13.1 11.7\n[4,] 10.2 17.2\n[5,] 11.8 13.7\n\n, , 2\n\n     [,1] [,2]\n[1,] 14.5 14.7\n[2,] 13.5 15.0\n[3,]  9.9 13.3\n[4,] 15.7 14.8\n[5,] 10.0 14.3\n\n, , 3\n\n     [,1] [,2]\n[1,] 15.1 14.2\n[2,] 13.1 12.7\n[3,] 11.8 11.2\n[4,] 10.7 12.0\n[5,] 12.7 11.8",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#data.frame",
    "href": "BCB744/intro_r/03-data-in-R.html#data.frame",
    "title": "3. Data Classes & Structures",
    "section": "data.frame",
    "text": "data.frame\nA dataframe is perhaps the most commonly-used ‘container’ for data in R because they are so convenient and serve many purposes. A dataframe is not a data class—more correctly, it is a form of tabular data (like a table in MS Excel), with each vector (a variable or column) comprising the table sharing the same length. What makes a dataframe versatile is that its variables can be any combination of the atomic data types. It may even include list columns (we will not cover list columns in this module). Applying the class() function to a dataframe shows that it blongs to class data.frame.\nHere’s an example of an R data.frame with Date, numeric, and categorical data classes:\n\n# Create a vector of dates\ndates &lt;- as.Date(c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\",\n                   \"2022-01-04\", \"2022-01-05\"))\n\n# Create a vector of numeric data\nnumeric_data &lt;- rnorm(n = 5, mean = 0, sd = 1)\n\n# Create a vector of categorical data\ncategorical_data &lt;- c(\"A\", \"B\", \"C\", \"A\", \"B\")\n\n# Combine the vectors into a data.frame\nmy_dataframe &lt;- data.frame(dates = dates,\n                           numeric_data = numeric_data,\n                           categorical_data = categorical_data)\n\n# Print the dataframe\nmy_dataframe\n\n       dates numeric_data categorical_data\n1 2022-01-01  -0.14354983                A\n2 2022-01-02   0.47971805                B\n3 2022-01-03  -0.59996086                C\n4 2022-01-04   0.06288558                A\n5 2022-01-05  -1.01865179                B\n\nclass(my_dataframe)\n\n[1] \"data.frame\"\n\nstr(my_dataframe)\n\n'data.frame':   5 obs. of  3 variables:\n $ dates           : Date, format: \"2022-01-01\" \"2022-01-02\" ...\n $ numeric_data    : num  -0.1435 0.4797 -0.6 0.0629 -1.0187\n $ categorical_data: chr  \"A\" \"B\" \"C\" \"A\" ...\n\nsummary(my_dataframe)\n\n     dates             numeric_data      categorical_data  \n Min.   :2022-01-01   Min.   :-1.01865   Length:5          \n 1st Qu.:2022-01-02   1st Qu.:-0.59996   Class :character  \n Median :2022-01-03   Median :-0.14355   Mode  :character  \n Mean   :2022-01-03   Mean   :-0.24391                     \n 3rd Qu.:2022-01-04   3rd Qu.: 0.06289                     \n Max.   :2022-01-05   Max.   : 0.47972                     \n\n\nDataframes may also have row names:\n\nrownames(my_dataframe) &lt;- paste(rep(\"row\", 5), seq = 1:5)\nmy_dataframe\n\n           dates numeric_data categorical_data\nrow 1 2022-01-01  -0.14354983                A\nrow 2 2022-01-02   0.47971805                B\nrow 3 2022-01-03  -0.59996086                C\nrow 4 2022-01-04   0.06288558                A\nrow 5 2022-01-05  -1.01865179                B\n\n\nTypically we will create a dataframe by reading in data from a .csv file, but it is useful to be able to construct one from scratch.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#tibble",
    "href": "BCB744/intro_r/03-data-in-R.html#tibble",
    "title": "3. Data Classes & Structures",
    "section": "tibble",
    "text": "tibble\nIn R, a dataframe and a tibble are both data structures used to store tabular data. Although tibbles are also dataframes, but they differ subtly in several ways.\n\nA tibble is a relatively new addition to the R language and forms part of the tidyverse suite of packages. They are designed to be more user-friendly than traditional data frames and have several additional features, such as more informative error messages, stricter data input and output rules, and better handling of NA.\nUnlike a dataframe, a tibble never automatically converts strings to factors or changes column names, which can help avoid unexpected behavior when working with the data.\nA tibble does not have row names.\nA tibble has a slightly different and more compact printing method than a dataframe, which makes them easier to read and work with.\nFinally, a tibble has better performance than dataframes for many tasks, especially when working with large datasets.\n\nWhile a dataframe is a core data structure in R, a tibble provides additional functionality and are becoming increasingly popular among R users, particularly those working with tidyverse packages. Applying the class() function to a tibble revelas that it belongs to the classes tbl_df, tbl and data.frame.\nWe can convert our dataframe my_dataframe to a tibble, and present the output with the print() function that applies nicely to tibbles:\n\nlibrary(tidyverse) # we need to load the tidyverse package\nmy_tibble &lt;- as_tibble(my_dataframe)\nclass(my_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(my_tibble)\n\n# A tibble: 5 × 3\n  dates      numeric_data categorical_data\n  &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;           \n1 2022-01-01      -0.144  A               \n2 2022-01-02       0.480  B               \n3 2022-01-03      -0.600  C               \n4 2022-01-04       0.0629 A               \n5 2022-01-05      -1.02   B               \n\n\nThis very simple tibble looks identical to a dataframe, but as we start using more complex sets of data you’ll learn to appreciate the small convenience that tibbles offer.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/intro_r/03-data-in-R.html#list",
    "href": "BCB744/intro_r/03-data-in-R.html#list",
    "title": "3. Data Classes & Structures",
    "section": "list",
    "text": "list\nThis is also not actually a data class, but rather another way of representing a collection of objects of different types, all the way from numerical vectors to dataframes. Lists are useful for storing complex data structures and can also be accessed using indexing.\nAs an example, we create another dataframe:\n\ndates &lt;- as.Date(c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\",\n                   \"2022-01-04\", \"2022-01-05\"))\n\n# Create a vector of numeric data\nnumeric_data &lt;- rnorm(n = 5, mean = 1, sd = 1)\n\n# Create a vector of categorical data\ncategorical_data &lt;- c(\"C\", \"D\", \"D\", \"F\", \"A\")\n\n# Combine the vectors into a data.frame\nmy_other_dataframe &lt;- data.frame(dates = dates,\n                                  numeric_data = numeric_data,\n                                  categorical_data = categorical_data)\n\nmy_list &lt;- list(A = my_dataframe,\n                B = my_other_dataframe)\nmy_list\n\n$A\n           dates numeric_data categorical_data\nrow 1 2022-01-01  -0.14354983                A\nrow 2 2022-01-02   0.47971805                B\nrow 3 2022-01-03  -0.59996086                C\nrow 4 2022-01-04   0.06288558                A\nrow 5 2022-01-05  -1.01865179                B\n\n$B\n       dates numeric_data categorical_data\n1 2022-01-01    3.4982586                C\n2 2022-01-02    0.1419428                D\n3 2022-01-03    0.5939201                D\n4 2022-01-04    1.2093531                F\n5 2022-01-05    1.5751606                A\n\nclass(my_list)\n\n[1] \"list\"\n\nstr(my_list)\n\nList of 2\n $ A:'data.frame':  5 obs. of  3 variables:\n  ..$ dates           : Date[1:5], format: \"2022-01-01\" \"2022-01-02\" ...\n  ..$ numeric_data    : num [1:5] -0.1435 0.4797 -0.6 0.0629 -1.0187\n  ..$ categorical_data: chr [1:5] \"A\" \"B\" \"C\" \"A\" ...\n $ B:'data.frame':  5 obs. of  3 variables:\n  ..$ dates           : Date[1:5], format: \"2022-01-01\" \"2022-01-02\" ...\n  ..$ numeric_data    : num [1:5] 3.498 0.142 0.594 1.209 1.575\n  ..$ categorical_data: chr [1:5] \"C\" \"D\" \"D\" \"F\" ...\n\n\nWe can access one of the dataframes is the list as follows:\n\nmy_list[[2]]\n\n       dates numeric_data categorical_data\n1 2022-01-01    3.4982586                C\n2 2022-01-02    0.1419428                D\n3 2022-01-03    0.5939201                D\n4 2022-01-04    1.2093531                F\n5 2022-01-05    1.5751606                A\n\nmy_list[[\"A\"]]\n\n           dates numeric_data categorical_data\nrow 1 2022-01-01  -0.14354983                A\nrow 2 2022-01-02   0.47971805                B\nrow 3 2022-01-03  -0.59996086                C\nrow 4 2022-01-04   0.06288558                A\nrow 5 2022-01-05  -1.01865179                B\n\n\nTo access a variable within one of the elements of the list we can do something like:\n\nmy_list[[\"B\"]]$numeric_data\n\n[1] 3.4982586 0.1419428 0.5939201 1.2093531 1.5751606",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Data Classes & Structures"
    ]
  },
  {
    "objectID": "BCB744/BCB744_index.html",
    "href": "BCB744/BCB744_index.html",
    "title": "BCB744: Introduction to R, & Biostatistics",
    "section": "",
    "text": "Venue, Timetable, and Content\nThe venue for the module is the 5th Floor Computer Lab, BCB Department, University of the Western Cape. The module will run from 09:00 to 16:30 on the days indicated in the table below.\nThe module coordinator and lecturer is Prof AJ Smit (Room 4.103), and the teaching assistant for the module is Chané Claassen (4142581@myuwc.ac.za). For queries about the Honours programme in general, please consult Prof Bryan Maritz (Room 4.105).\n\n\nIntro to R: From 3 to 7 February 2025.\n\nBiostatistics: From 31 March to 4 April 2025 (during the mid-semester break of Semester 1).\n\nImportant links:\n\nSelf-Assessments\nPresentations\nBCB744 Data\n\n\n\n\n\nWk\nLecture\nTopic\nClass Date\nTasks/Assessments\nTask/Assess. due\n\n\n\n\n\nINTRO R\n\n\n\n\n\nWk1\nL1\nAbout the Module\n3 Feb 25\nTask A\n4 Feb 25\n\n\n\n\n1. R and RStudio\n\n\n\n\n\n\n\n2. Working With Data and Code\n\n\n\n\n\n\n\n3. Data Classes and Structures in R\n\n\n\n\n\n\n\n4. R Workflows\n\n\n\n\n\n\nL2\n5. Graphics With ggplot2\n4 Feb 25\nTask B\n5 Feb 25\n\n\n\n\n6. Faceting Figures\n\n\n\n\n\n\n\n7. Brewing Colours\n\n\n\n\n\n\nL3\n8. Mapping With ggplot2\n5 Feb 25\nTask C\n6 Feb 25\n\n\n\n\n9. Mapping With style\n\n\n\n\n\n\n\n10. Mapping With Natural Earth and the sf Package\n\n\n\n\n\n\nSelf\n11. The Fiji Earthquake data\n\nBonus Task\n31 Mar 25\n\n\n\nL4\n12. Tidy Data\n6 Feb 25\nTask D\n7 Feb 25\n\n\n\n\n13. Tidier Data\n\n\n\n\n\n\n\n14. Tidiest Data\n\n\n\n\n\n\nL5\nRecap\n7 Feb 25\n\n\n\n\n\n\nTest 1\n17 Mar 25\n\nTBA\n\n\n\n\nBIOSTATISTICS\n\n\n\n\n\nWk10\nL1\nThe History of Scientific Inquiry\n31 Mar 25\n\n\n\n\n\n\n1. Rmarkdown and Quarto\n\n\n\n\n\n\n\n2. Exploring With Summaries and Descriptions\n\nTask E\n1 Apr 25\n\n\n\n\n3. Exploring With Figures\n\nTask E\n1 Apr 25\n\n\n\nL2\n4. Data Distributions\n1 Apr 25\n\n\n\n\n\n\n5. Statistical Inference and Hypothesis Testing\n\n\n\n\n\n\n\n6. Assumptions\n\n\n\n\n\n\n\n7. Inferences About One or Two Populations\n\nTask F\n2 Apr 25\n\n\n\nL3\n8. Analysis of Variance (ANOVA)\n2 Apr 25\nTask G\n3 Apr 25\n\n\n\n\n9. Simple Linear Regressions\n\nTask H\n4 Apr 25\n\n\n\n\n10. Correlations\n\nTask H\n4 Αpr 25\n\n\n\nL4\n11. A Guide to Selecting the Right Parametric Test\n3 Apr 25\n\n\n\n\n\n\n12. Non-Parametric Statistics\n\n\n\n\n\n\n\n13. Confidence Intervals\n\n\n\n\n\n\n\n14. Data Transformations\n\n\n\n\n\n\n\nTest 2\n7-11 Apr 25\n\nTBA\n\n\n\n\nExam\nTBA\n\nTBA\n\n\n\n\n\nCourse Description\nTheoretical Content\nStatistical Content\n\n\n\nYes, the comma in this page’s title is correct: “BCB744: Introduction to R, and Biostatistics.” The module provides an introduction to the R software and language. I will also teach biostatistics.\nThis is a core module in your Honours programme. You will learn to use R for data analysis, visualisation, and statistical inference. You will also learn fundamental biostatistics concepts, such as hypothesis testing, probabilities, confidence intervals, regression analysis, Analysis of Variance, and other staples of biostatistics. I will use real-world datasets from the biological, ecological, and environmental fields that you can use to practice applying your R and biostatistics skills.\nThe approach taken in this Workshop is not dissimilar from a course in Data Science. However, in this Workshop, we won’t do data science, but we will use R to actually do science. There is a difference! Any scientist that can use R is also ideally equipped to be a data scientist, and some people who have completed this module actually do just that. The difference between the two ideas, philosophies, careers is provided in the box immediately below.\n\n\n\n\n\n\nReal Scientists and Data ‘Scientists’\n\n\n\nA Scientist able to apply their intermediate to advanced R skills is by default also a ‘Data Scientist’. The opposite is generally not true: Data Scientists are not real Scientists—especially after only having completed ‘traditional’ courses in data science.\nScience refers to the application of the scientific method of conducting research, where hypotheses are proposed, experiments are designed and conducted to test these hypotheses, and data are collected and analysed to draw conclusions. The aim of Science is to generate new knowledge and understanding of the natural world. A Scientist will typically be equipped to work through all of these steps.\nData Science, on the other hand, involves the use of computational and statistical tools to extract knowledge and insights from data. These datasets typically already exist because someone (companies, industries, NGOs, etc.) collected them. Data Science focuses on analysing large and complex datasets to uncover patterns, trends, and relationships that can be used to inform decision-making. The Data Scientist is not typically involved in generating the data from de novo.\nThese key aspects summarise the difference between the two fields:\n\nApproach Science is hypothesis-driven, while Data Science is data-driven. Science begins with a hypothesis that is tested through experiments, while Data Science begins with data and uses statistical and computational methods to uncover insights.\nGoals Science aims to generate new knowledge and understanding of the natural world, while Data Science aims to uncover insights and make predictions based on existing data. Scientist focus on understanding the underlying mechanisms of natural phenomena and their area of focus is the real world, while Data Scientists focus on extracting knowledge and insights from data, often in the realm of business.\nMethods Science involves making observations of the world, conducting experiments, collecting and analysing data, and drawing conclusions based on the results. Data Science only involves using statistical and computational tools to analyse data and uncover patterns and relationships.\nContext Science is typically focused on a specific domain, such as biology, chemistry, or physics. Data Science can be applied to any domain that involves data, including business, finance, healthcare, and social media.\n\n\n\n\n\nThe Intro R Workshop focuses on the functionality offered by the tidyverse suite of packages. I designed the Workshop to introduce you to a powerful set of tools for data manipulation, exploration, and visualisation. The tidyverse is a collection of R packages that work together to provide a cohesive set of functions for manipulating data. This course will cover the most popular packages in the tidyverse, including tidyr for data reshaping, dplyr for data ‘wrangling’, and ggplot2 for data visualisation. You will learn how to clean, transform, and visualise data, as well as how to use these tools to build reproducible and informative data analysis pipelines. With a focus on practical application and hands-on exercises, you will gain the skills and knowledge needed to effectively use the tidyverse in your own data analysis projects.\n\n\n\nIn biological and ecological sciences, statistical methods play a crucial role in analysing and interpreting data. Some of the basic statistical methods used include:\n\nDescriptive statistics These methods are used to summarise and describe the basic features of a dataset, such as the mean, median, and standard deviation.\nInferential statistics These allow you, the scientist, to make predictions and inferences about a population based on a sample of data. Common inferential statistical techniques include t-tests, ANOVA, and regression analysis.\nNon-parametric statistics Non-parametric methods are called for when the data do not meet the assumptions of parametric statistics. Examples of non-parametric techniques include Wilcoxon rank-sum test and Kruskal-Wallis test.\n\n\n\n\n\n\n\nCore Skills\nGraduate Attributes\n\n\n\nBy the end of this module, you will be able to:\n\nUnderstand and use use R within the RStudio IDE\nKnow and understand the the tidyverse suite of functions and approach to data analysis and graphics\nUnderstand the principles underlying tidy data\n\nUnderstand the types of data and data distributions that biologists and ecologists will frequently encounter\nUnderstand and be able to execute the most frequently used inferential statistics\nUse the R software and associated packages to undertake these analyses\nInterpret the outcomes of these analyses and use it to probabilistically make inferences about the scientific enquiries\nCommunicate the findings by written and oral means\n\n\n\nThe graduate attributes resulting from completion of this modules alignment with the expectations of the workspace across diverse organisations and institutions where graduates typically find employment.\n\n\n\nData Used\nAll the data required for BCB744 may be downloaded here. After you have downloaded the archived (.zip) data, unzip it in a folder named data placed at the root of your R project. This will ensure that all the data are easily accessible to you.\nR also gives you access to many built-in datasets that are useful for practicing our R skills. To find out which datasets are available to you on your system, execute the following command. Help files for each of the datasets are also available:\n\n# load the data like this:\ndata()\n\n# find help, for example:\n?datasets::ChickWeight\n\nIt is important to use these (or any) datasets to practice your R skills on. Actively engaging with my comprehensive and detailed web pages, and practising on the included and additional other datasets will make to difference between a 60% average mark for the module, and a mark in excess of 80%.\nPrerequisites\nYou should have a moderate numerical literacy, but prior programming experience is not required. In all sciences, practical problem solving skills and a tenacity for challenges are crucial for success. Scientific disciplines constantly evolve and present new and complex problems that require creative and innovative solutions. You will have to demonstrate agile and adaptive approaches to solving challenges, and you must have the ability to break down complex problems into smaller parts and approach them systematically. You must also be able to identify and overcome roadblocks, and be persistent in your efforts to find a solution. These attributes will allow you to be effective in this module.\nMethod of Instruction\nThe workshop is designed to be as interactive as possible, so while you are working on exercises the tutor and I will circulate among you and engage with you to help you understand any material and the associated code you are uncomfortable with. Often this will result in discussions of novel applications and alternative approaches to the data analysis challenges you are required to solve. More challenging concepts might emerge during the Tasks and Assignments (typically these will be submitted the following day), and any such challenges will be dealt with in class prior to learning new concepts.\nAlthough the module ultimately supports the application of biologically-oriented statistics, a large part of it is also about programming. It is up to you to take your coding skills to the next level and move beyond what I teach in class. Coding is a bit like learning a language, and as such programming is a skill that is best learned by doing.\nLearning\n\n\nCollaboration\nFound Code\nAI tools\n\n\n\n\n\n\n\n\n\nAlso read: How to learn\n\n\n\nPlease refer to my advice about how to learn.\n\n\nCollaborative learning provides an opportunity for you to work together and learn from each other. In this way, you will develop a deeper understanding of the subject matter. Collaborating with your friends and peers allows you to explore different perspectives and ideas, which can broaden your understanding and help you to see the subject matter from new angles. This type of learning environment also fosters the development of important skills such as communication, teamwork, and leadership, which are essential for success in academic and professional careers. Collaborative learning can create a sense of community and support among your group of peers. In the end, it will enhance your university experience, drive your love for learning, and prepare you for success beyond the university.\nDiscuss the BCB744 Workshop activities with your peers as you work on them. Use the WhatsApp group set up for the module for discussion purposes (I might assist via this medium if necessary if your questions/comments have relevance to the whole class). A better option is to use GitHub Issues. You will learn more in this module if you work with your friends than if you do not. Ask questions, answer questions, and share ideas liberally. Please identify your work partners by name on all assignments (if you decide to work in pairs).\nCollaborative learning does not give you permission to reuse someone else’ code or text. Plagiarism is a serious offence and will be dealt with concisely. Consequences of cheating are severe—they range from a 0% for the assignment or exam up to dismissal from the course for a second offense.\n\n\nA huge volume of code is available on the web and it can be adapted to solve your own problems. You may make use of any online resources (e.g. form StackOverflow, a thoroughly-used source of discussion about R code)—but you MUST clearly indicate (cite) that your solution relies on found code, regardless to what extent you have modified it to your own needs. Reused code that is discovered via a web search and which is not explicitly cited is plagiarism and it will be treated as such. On assignments you may not directly share code with your peers in this workshop.\n\n\nThe 2025 BSc (Hons) cohort will be the first to experience the use of AI tools in the BCB744 module. The use of AI tools is a new and exciting development and it is important that you are exposed to these tools. The use of AI tools will be limited to the use of the OpenAI ChatGPT tool, which may be used to generate ‘proto-code’ that will assist you in becoming familiar with the R langauge. We will explore ideas together, and the mark allocation to tasks and assignments will be adjusted accoringly.\n\n\n\nSoftware\nIn this course you will rely entirely on R running within the RStudio IDE. The use of R is covered extensively in the BCB744 module where the installation process is discussed.\nAdditionally, the very basics—i.e. about R, RStudio, packages, their installation, etc.—can also be found on the ModernDive website. A slightly longer and more detailed account of the installation process and the very basics is provided on the datacamp platform.\nModernDive also provides a nice overview of using R for data science.\nFor more in-depth coverage of the R language, refer to R Master Hadley Wickham’s pages. There you will find everything you need to know in a well thought through presentation. Thoroughly working through this material, page by page, will quickly make you a R Master yourself (well, almost).\nComputers\nYou are encouraged to provide your own laptops and to install the necessary software before the module starts. Limited support can be provided if required, but in the end, the onus is on you to understand how your computer works (from the filesystem through to dealing with software installation issues). There are also computers with R and RStudio (and the necessary add-on libraries) available in the 5th floor lab in the BCB Department.\nAttendance\nThis workshop-based, hands on course can only deliver acceptible outcomes if you attend all classes. The schedule is set and cannot be changed. Sometimes an occasional absence cannot be avoided. Please be curtious and notify myself or the tutor in advance of any absence. If you work with a partner in class, notify them too. Keep up with the reading assignments while you are away and we will all work with you to get you back up to speed on what you miss. If you do miss a class, however, the assignments must still be submitted on time (also see Late submission of CA).\nSince you may decide to work in collaboration with a peer on tasks and assignments, please keep this person informed at all times in case some emergency makes you unavailable for a period of time. Someone might depend on your input and contributions—do not leave someone in the lurch so that they cannot complete a task in your absence.\nAssessment Policy\nContinuous Assessment (CA) and a Final Assessment will provide a Final Mark for the module. These modes of assessment meet our needs as far as formative and summative assessments are concerned. The weighting of the CA and the Final Assessment is 0.6 and 0.4, respectively. All assessments are open book, so consult your code and reading material if and when you need to.\n\n\nAssessment Component\nWeight\nContribution (%)\n\n\n\nCONTINUOUS ASSESSMENT\n(0.6)\n\n\n\nIntroduction to R\n\n\n\n\nPresentations\n\n10\n\n\nSelf-Assessment Tasks A–D (Random penalty)1\n\n\n(max. -10).\n\n\nIntro R Test\n\n40\n\n\nBiostatistics\n\n\n\n\nPresentations\n\n10\n\n\nSelf-Assessment Tasks E–H (Random penalty)\n\n(max. -10).\n\n\nBiostatistics Test\n\n40\n\n\nTotal\n\n100\n\n\nFINAL ASSESSMENT\n(0.4)\n\n\n\nExam (Intro R + Biostatistics)\n\n100\n\n\n\n1 A maximum of 10% may be deducted from your presentation marks should you be found to be dishonest in your self assessments.Care must be taken that the tests and exams are submitted as instructed, i.e. paying attention to naming conventions and the format of the files submitted – typically this will be in a Quarto document (.qmd) and the knitted output (I prefer .html).\nRandom quizzes will not form part of the CA for BCB744.\n\n\nPresentations\nSelf-Assessment Tasks\nTests\nExam\nSubmission of Assignments\n\n\n\nThe presentations are a critical part of the CA. They are designed to help you develop your communication around topics tangentially to the broad field of knowledge generation. The presentations will cover topics such as the the nature of knowledge and belief, the nature of science, the scientific method, the limits to sciencde, and other broader societal topics.\nFor more detail, see these links:\n\nPresentations\nAssessment Sheet\n\n\n\nBCB744 (Introduction to R and Biostatistics) relies on the expectation that you will engage in regular, honest self-reflection about your grasp of each day’s lecture content. After every lecture, time should be devoted to completing the Daily Self-Assessment Tasks, which are designed to help you gauge your understanding of the covered material. Answers to these tasks will be provided the following day, before introducing new content. The honesty of these reflections cannot be overstated: each task should be rated on a personal scale from 1 (no real comprehension) to 10 (complete mastery). These self-assessment marks will be kept on record and serve as an indicator of progress. We will not permit the submission of these tasks, but they will be checked randomly. We will also discourage students from undertaking the Intro R Test and the BioStats Test if their self-assessment scores are consistently low.\nStudents who realise they are struggling are strongly advised to seek assistance from the lecturer or teaching assistant well before the gap in understanding becomes too large to bridge (i.e. on the day). The correlation between consistent, candid, and honest self-assessment and later performance in the Intro R Test, the Biostatistics Test, and the combined Exam (Intro R + Biostatistics) is high. By admitting the need for help early, you can align your learning strategies with course expectations and reinforce your command of the subject matter. Being the judge of personal preparedness demands self-reflection and honesty about your own strengths and weaknesses so as to develop a strong foundation for success.\nFor the daily self-assessment tasks to be effective, you must work alone on all of them.\nBe responsible for your own learning. The lecturer and teaching assistant are here to help you, but you must take the initiative to seek assistance when needed. The more you engage with the material, the more you will learn and the better you will perform in the assessments.\nFor more detail, see these links:\n\n\nSelf-Aassessments.\nRubric (All Tasks)\n\n\n\nAt the conclusion of Intro R, and Biostatistics, you will take the more rigorous Intro R Test and Biostatistics Test. As indicated in the table above, these assessments carry significant weight. The tests will be conducted over several days, and you may complete them both at home and on campus. They constitute a key component of Continuous Assessment (CA) and are designed to prepare you for the final exam.\nEach test consists of two parts:\n\n\nTheory Test (30%) – This is a written, closed-book assessment where you will be tested on theoretical concepts. The only resource available during this test is the R help system.\n\nPractical Test (70%) – In this open-book coding assessment, you will apply your theoretical knowledge to real data problems. While you may reference online materials (including ChatGPT), collaboration with peers is strictly prohibited.\n\nThe practical component of the tests will be graded as follows:\n\nContent (20%):\n\nQuestions answered in order\nA written explanation of approach included for each question\nAppropriate formatting of text, for example, fonts not larger than necessary, headings used properly, etc. Be sensible and tasteful.\n\n\nCode formatting, structure, and correctness (50%):\n\nUse Tidyverse code\nNo more than ~80 characters of code per line (pay particular attention to the comments)\nApplication of R code conventions, e.g. spaces around &lt;-, after #, after ,, etc.\nNew line for each dplyr function (lines end in %&gt;%) or ggplot layer (lines end in +)\nProper indentation of pipes and ggplot() layers\nAll chunks labelled without spaces\nNo unwanted / commented out code left behind in the document\n\n\nFigures (30%):\n\nSensible use of themes / colours\nPublication quality\nInformative and complete titles, axes labels, legends, etc.\nNo redundant features or aesthetics\n\n\n\n\n\nThe Exam is the final assessment. As such, it will test your skills broadly across both Intro R and Biostatistics. The Exam may be up to five days in duration. It will involve the analysis of real world data. Some of the questions might expect that you write 1) statements of aims, objectives, and hypotheses; 2) the full and detailed methods followed by analyses together with all code, 3) full reporting of results in a manner suited for peer reviewed publications; 4) graphical support highlighting the patterns observed (again with the code), and 5) a discussion if and when required. The weighting of marks to these various sections is:\n\nAims, objectives, and hypotheses: 5%\nMethods and analyses: 45%\nResults: 20%\nGraphs: 15%\nDiscussion: 15%\n\nOther questions might be shorter in nature, designed to specifically test important aspects of BCB744. Such questions might be worth anything from 10 to 50 marks.\nThe Exam is also open book. Go home. Look at the questions. Answer them at home. Submit them by the deadline.\n\n\nA statement such as the one below accompanies every assignment—pay attention, as failing to observe this instruction may result in a loss of marks (i.e. if an assignment remains ungraded because the owner of the material cannot be identified):\nSubmit the outpt of your Quarto script wherein you provide answers to the task questions by no later than 8:30 the following data (or the Monday in cases when assignments were given on Fridays). Label the script as follows (e.g.): BCB744_Smit_Task_A.html.\nLate Submissions\nLate assignments will be penalised 10% per day and will not be accepted more than 48 hours late, unless evidence such as a doctor’s note, a death certificate, or another documented emergency can be provided. If you know in advance that a submission will be late, please discuss this and seek prior approval. This policy is based on the idea that in order to learn how to translate your human thoughts into computer language (coding) you should be working with them at multiple times each week—ideally daily. Time has been allocated in class for working on assignments and students are expected to continue to work on the assignments outside of class. Successfully completing (and passing) this module requires that you finish assignments based on what we have covered in class by the following class period. Work diligently from the onset so that even if something unexpected happens at the last minute you should already be close to done. This approach also allows rapid feedback to be provided to you, which can only be accomplished by returning assignments quickly and punctually.\n\n\n\nSupport\nIt’s expected that some tricky aspects of the module will take time to master, and the best way to master problematic material is to practice, practice some more, and then to ask questions. Trying for 10 minutes and then giving up is not good enough. I’ll be more sympathetic to your cause if you can demonstrate having tried for a full day before giving up and asking me. When you ask questions about some challenge, this is the way to do it—explain to me your numerous attempts at trying to solve the problem, and explain how these various attempts have failed. I will not help you if you have not tried to help yourself first (maybe with advice from friends). There will be time in class to do this, typically before we embark on a new topic. You are also encouraged to bring up related questions that arise in your own B.Sc. (Hons.) research project.\nShould you require more time with me, find out when I am ‘free’ and set an appointment by sending me a calendar invitation. I am happy to have a personal meeting with you via Zoom, but I prefer face-to-face in my office.\nGuidelines for asking questions:\n\nFirst search existing issues (open or closed) for answers. If the question has already been answered, you’re done! If there is an open issue, feel free to contribute to it. Or feel free to open a closed issue if you believe the answer is not satisfactory.\nGive your issue an informative title.\n\nGood: “Error: could not find function”ggplot””\nBad: “My code does not work!” Note that you can edit an issue’s title after it’s been posted.\n\n\nFormat your questions nicely using markdown and code formatting. Preview your issue prior to posting.\nAs I explained above, your peers and I will more sympathetic to your cause if you can show all the things you have tried as you, yourself, tried to fix the issue first.\nInclude code and example data so the person trying to help you have something to work with (and which results in the error, perhaps)\nWhere appropriate, provide links to specific files, or even lines within them, in the body of your issue. This will help your peers understand your question. Note that only the teaching team will have access to private repos.\n(Optional) Tag someone or some group of people. Start by typing their GitHub username prefixed with the @ symbol. Of course this supposes that each of you have a GitHub account and username.\nHit Submit new issue when you’re ready to post.\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2025,\n  author = {Smit, A. J.,},\n  title = {BCB744: {Introduction} to {R,} \\& {Biostatistics}},\n  date = {2025-02-03},\n  url = {http://tangledbank.netlify.app/BCB744/BCB744_index.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2025) BCB744: Introduction to R, & Biostatistics. http://tangledbank.netlify.app/BCB744/BCB744_index.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "**About**"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/13-confidence.html",
    "href": "BCB744/basic_stats/13-confidence.html",
    "title": "13. Confidence Intervals",
    "section": "",
    "text": "Introduction\nA confidence interval (CI) tells us within what range we may be certain to find the true mean from which any sample has been taken. If we were to repeatedly sample the same population over and over and calculated a mean every time, the 95% CI indicates the range that 95% of those means would fall into.\nCalculating confidence intervals\n\nInput &lt;- (\"\nStudent  Grade   Teacher   Score  Rating\na        Gr_1    Vladimir  80     7\nb        Gr_1    Vladimir  90    10\nc        Gr_1    Vladimir 100     9\nd        Gr_1    Vladimir  70     5\ne        Gr_1    Vladimir  60     4\nf        Gr_1    Vladimir  80     8\ng        Gr_10   Vladimir  70     6\nh        Gr_10   Vladimir  50     5\ni        Gr_10   Vladimir  90    10\nj        Gr_10   Vladimir  70     8\nk        Gr_1    Sadam     80     7\nl        Gr_1    Sadam     90     8\nm        Gr_1    Sadam     90     8\nn        Gr_1    Sadam     80     9\no        Gr_10   Sadam     60     5\np        Gr_10   Sadam     80     9\nq        Gr_10   Sadam     70     6\nr        Gr_1    Donald   100    10\ns        Gr_1    Donald    90    10\nt        Gr_1    Donald    80     8\nu        Gr_1    Donald    80     7\nv        Gr_1    Donald    60     7\nw        Gr_10   Donald    60     8\nx        Gr_10   Donald    80    10\ny        Gr_10   Donald    70     7\nz        Gr_10   Donald    70     7\n\")\n\ndata &lt;- read.table(textConnection(Input), header = TRUE)\nsummary(data)\n\n   Student             Grade             Teacher              Score       \n Length:26          Length:26          Length:26          Min.   : 50.00  \n Class :character   Class :character   Class :character   1st Qu.: 70.00  \n Mode  :character   Mode  :character   Mode  :character   Median : 80.00  \n                                                          Mean   : 76.92  \n                                                          3rd Qu.: 87.50  \n                                                          Max.   :100.00  \n     Rating      \n Min.   : 4.000  \n 1st Qu.: 7.000  \n Median : 8.000  \n Mean   : 7.615  \n 3rd Qu.: 9.000  \n Max.   :10.000  \n\n\nThe package rcompanion has a convenient function for estimating the confidence intervals for our sample data. The function is called groupwiseMean() and it has a few options (methods) for estimating the confidence intervals, e.g. the ‘traditional’ way using the t-distribution, and a bootstrapping procedure.\nLet us produce the confidence intervals using the traditional method for the group means:\n\nlibrary(rcompanion)\n# Ungrouped data are indicated with a 1 on the right side of the formula,\n# or the group = NULL argument; so, this produces the overall mean\ngroupwiseMean(Score ~ 1, data = data, conf = 0.95, digits = 3)\n\n   .id  n Mean Conf.level Trad.lower Trad.upper\n1 &lt;NA&gt; 26 76.9       0.95       71.7       82.1\n\n# One-way data\ngroupwiseMean(Score ~ Grade, data = data, conf = 0.95, digits = 3)\n\n  Grade  n Mean Conf.level Trad.lower Trad.upper\n1  Gr_1 15   82       0.95       75.3       88.7\n2 Gr_10 11   70       0.95       62.6       77.4\n\n# Two-way data\ngroupwiseMean(Score ~ Teacher + Grade, data = data, conf = 0.95, digits = 3)\n\n   Teacher Grade n Mean Conf.level Trad.lower Trad.upper\n1   Donald  Gr_1 5   82       0.95       63.6      100.0\n2   Donald Gr_10 4   70       0.95       57.0       83.0\n3    Sadam  Gr_1 4   85       0.95       75.8       94.2\n4    Sadam Gr_10 3   70       0.95       45.2       94.8\n5 Vladimir  Gr_1 6   80       0.95       65.2       94.8\n6 Vladimir Gr_10 4   70       0.95       44.0       96.0\n\n\nNow let us do it through bootstrapping:\n\ngroupwiseMean(Score ~ Grade,\n              data = data,\n              conf = 0.95,\n              digits = 3,\n              R = 10000,\n              boot = TRUE,\n              traditional = FALSE,\n              normal = FALSE,\n              basic = FALSE,\n              percentile = FALSE,\n              bca = TRUE)\n\n  Grade  n Mean Boot.mean Conf.level Bca.lower Bca.upper\n1  Gr_1 15   82        82       0.95      74.7      86.7\n2 Gr_10 11   70        70       0.95      62.7      75.5\n\ngroupwiseMean(Score ~ Teacher + Grade,\n              data = data,\n              conf = 0.95,\n              digits = 3,\n              R = 10000,\n              boot = TRUE,\n              traditional = FALSE,\n              normal = FALSE,\n              basic = FALSE,\n              percentile = FALSE,\n              bca = TRUE)\n\n   Teacher Grade n Mean Boot.mean Conf.level Bca.lower Bca.upper\n1   Donald  Gr_1 5   82        82       0.95      68.0      90.0\n2   Donald Gr_10 4   70        70       0.95      62.5      75.0\n3    Sadam  Gr_1 4   85        85       0.95      80.0      87.5\n4    Sadam Gr_10 3   70        70       0.95      60.0      76.7\n5 Vladimir  Gr_1 6   80        80       0.95      68.3      88.3\n6 Vladimir Gr_10 4   70        70       0.95      55.0      80.0\n\n\nThese upper and lower limits may then be used easily within a figure.\n\n# Load libraries\nlibrary(tidyverse)\n\n# Create dummy data\nr_dat &lt;- data.frame(value = rnorm(n = 20, mean = 10, sd = 2),\n                    sample = rep(\"A\", 20))\n\n# Create basic plot\nggplot(data = r_dat, aes(x = sample, y = value)) +\n  geom_errorbar(aes(ymin = mean(value) - sd(value), ymax = mean(value) + sd(value))) +\n  geom_jitter(colour = \"firebrick1\")\n\n\n\nA very basic figure showing confidence intervals (CI) for a random normal distribution.\n\n\n\nCI of compared means\nAS stated above, we may also use CI to investigate the difference in means between two or more sample sets of data. We have already seen this in the ANOVA Chapter, but we shall look at it again here with our now expanded understanding of the concept.\n\n# First calculate ANOVA of seapl length of different iris species\niris_aov &lt;- aov(Sepal.Length ~ Species, data = iris)\n\n# Then run a Tukey test\niris_Tukey &lt;- TukeyHSD(iris_aov)\n\n# Lastly use base R to quickly plot the results\nplot(iris_Tukey)\n\n\n\nResults of a post-hoc Tukey test showing the confidence interval for the effect size between each group.\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {13. {Confidence} {Intervals}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/basic_stats/13-confidence.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 13. Confidence Intervals. http://tangledbank.netlify.app/BCB744/basic_stats/13-confidence.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "13. Confidence Intervals"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html",
    "href": "BCB744/basic_stats/03-visualise.html",
    "title": "3. Statistical Figures",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nThe diversity of graphs used to communicate statistical results\nHow to select the right graph for any particular dataset\nAdditional packages available to extend ggplot’s functionality",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-histograms",
    "href": "BCB744/basic_stats/03-visualise.html#sec-histograms",
    "title": "3. Statistical Figures",
    "section": "Frequency distributions",
    "text": "Frequency distributions\nFrequency distributions are typically displayed as histograms. Histograms are a type of graph that displays the frequency of occurrences of observations forming a continuous variable. To construct a histogram, the data are divided into intervals, or bins, and the number of occurrences of observations within each bin is tallied. The height of each bar (y-axis) in the histogram represents the number of observations falling within that bin. The x-axis displays the bins, arranged such that the intervals they represent go from small to large on an ordinal scale. The intervals should be chosen such that they best represent the distribution of the data without being too narrow or too wide. Histograms can be used to quickly assess the distribution of the data, identify any skewness or outliers, and provide a visual representation of the central tendency and variation of the data.\nWe have a choice of absolute (Figure 1 A) and relative (Figure 1 B-C) frequency histograms. In absolute frequency distributions, the sum of all the counts per bin will add up to the total number of obervations. In relative frequency distributions the the frequency of each category is expressed as a proportion or percentage of the total number of observations, and hence the sum of the relative counts per bin is 1. This is useful if two populations being compared have different numbers of observations. There’s also the empirical cumulative distribution function (ECDF) (Figure 1 D) that shows the cumulative proportion of observations that fall below or equal to a certain value. See the Old Faithful data, for example. The eruptions last between 1.6 and 5.1 minutes. So, we create intervals of time spanning these times, and within each count the number of times an event lasts as long as denoted by the intervals. Here we might choose intervals of 1-2 minutes, 2-3 minutes, 3-4 minutes, 4-5 minutes, and 5-6 minutes. The ggplot2 geom_histogram() function automatically creates the bins, but we may specify our own. It is best to explain these principles by example (Figure 1 A-D).\n\n# a normal frequency histogram, with count along y\nhist1 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"'Vanilla' histogram\",\n       x = \"Eruption duration (min)\",\n       y = \"Count\") + theme_pubclean()\n\n# when the binwidth is 1, the density histogram *is* the relative\n# frequency histogram\nhist2 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..density..),\n                 position = 'identity', binwidth = 1,\n                 colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Relative frequency\",\n       x = \"Eruption duration (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\n\n# if binwidth is something other than 1, the relative frequency in\n# a histogram is ..density.. * binwidth\nhist3 &lt;- ggplot(data = faithful, aes(x = waiting)) +\n  geom_histogram(aes(y = 0.5 * ..density..),\n                 position = 'identity', binwidth = 0.5,\n                 colour = \"salmon\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Relative frequency\",\n       x = \"Waiting time (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\n# ECDF\nhist4 &lt;- ggplot(data = faithful, aes(x = eruptions)) + \n  stat_ecdf() +\n  labs(title = \"ECDF\",\n       x = \"Eruption duration (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\nggarrange(hist1, hist2, hist3, hist4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 1: Example histograms for the Old Faithful data. A) A default frequency histogram with the count of eruption times falling within the specified bins. B) A relative frequency histogram with bins adjusted to a width of 1 minute intervals; here, the sum of counts within each of the four bins is 1. C) Another relative frequency histogram, but with the bins adjusted to each be 0.5 minute increments; again the sum of counts represented by each bin is equal to 1.\n\n\n\n\nAs we see above, ggplot2 can automatically construct a frequency histogram with the geom_histogram() function. We can also manually create a frequency distribution with the cut() function.\n\n\n\n\n\n\nDo it now!\n\n\n\nStarting with the cut() function, recreate Figure 1 A-C manually.\n\n\nWhat if we have continuous data belonging with multiple categories? The iris dataset provides a nice collection of measurements that we may use to demonstrate a grouped frequency histogram. These data are size measurements (cm) of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of three species of Iris. The species are Iris setosa, I. versicolor, and I. virginica. The figures are shown in Figure 2.\n\n# first we make long data\niris.long &lt;- iris %&gt;% \n  gather(key = \"variable\", value = \"size\", -Species)\n\nggplot(data = iris.long, aes(x = size)) +\n  geom_histogram(position = \"dodge\", # ommitting this creates a stacked histogram\n                 colour = NA, bins = 20,\n                 aes(fill = Species)) +\n  facet_wrap(~variable) +\n  labs(title = \"Iris data\",\n       subtitle = \"Grouped frequency histogram\",\n       x = \"Size (cm)\",\n       y = \"Count\") +\n  theme_pubclean()\n\n\n\n\n\n\nFigure 2: Grouped histograms for the four Iris variables.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-bargraphs",
    "href": "BCB744/basic_stats/03-visualise.html#sec-bargraphs",
    "title": "3. Statistical Figures",
    "section": "Bar graphs",
    "text": "Bar graphs\nBar graphs are popular among biologists and ecologists. Often used to represent discrete categories or groups, bar graphs provide a visual representation of mean values for each category, thus allowing us to identify trends, patterns, and differences across data sets or experimental treatments. In complex biological systems, such as population dynamics, species abundance, or ecological niches, bar graphs offer a clear and concise way to depict the interactions and variations among different elements. Importantly, bar graphs may also include some indication of variation, such as error bars (a term that also applies when the variation statistic used is the standard deviation) or other visual cues to denote the range of variation within the data, such as confidence intervals. This additional layer of information not only highlights the variability inherent in biological and ecological data but also aids in the interpretation of results and the overall understanding of the phenomena under investigation. Note that it is not incorrect to plot the median in bar graphs, but bat graphs is typically reserved for displaying the mean. For plotting the median, see Section 2.3, below.\nA naïve application of bar graphs is to indicate the number of observations within several groups. Although this can be presented numerically in tabular form, sometimes one might want to create a bar or pie graph of the number of occurrences in a collection of non-overlapping classes or categories. Both the data and graphical displays will be demonstrated here.\nThe first case is of a variation of frequency distribution histograms, but here showing the raw counts per each of the categories that are represented in the data—unlike ‘true’ frequency histograms in Section 2.1 that divide data into bins, this one takes a cruder approach. The count within each of the categories sums to the sample size, \\(n\\). In the second case, we may want to report those data as proportions. Here we show the frequency proportion in a collection of non-overlapping categories. For example, we have a sample size of 12 (\\(n=12\\)). In this sample, two are coloured blue, six red, and five purple. The relative proportions are \\(2/12=0.1666667\\) blue, \\(6/12=0.5\\) red, and \\(5/12=0.4166667\\) purple. The important thing to note here is that the relative proportions sum to 1, i.e. \\(0.1666667+0.5+0.4166667=1\\). These data may be presented as a table or as a graph.\nIn Figure 3 I demonstrate the numerical and graphical summaries using the built-in iris data (I’d not do this in real life, it’s silly; just write it out in the text of the Methods section):\n\n# the numerical summary produced by a piped series of functions;\n# create a summary of the data (i.e. number of replicates per species)\n# used for (A), (B) and (C), below\niris.cnt &lt;- iris %&gt;%\n  count(Species) %&gt;% # automagically creates a column, n, with the counts\n  mutate(prop = n / sum(n)) # creates the relative proportion of each species\n\n\n# a stacked bar graph with the cumulative sum of observations\nplt1 &lt;- ggplot(data = iris.cnt, aes(x = \"\", y = n, fill = Species)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  labs(title = \"Stacked bar graph\", subtitle = \"cumulative sum\",\n       x = NULL, y = \"Count\") +\n  theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\n# a stacked bar graph with the relative proportions of observations\nplt2 &lt;- ggplot(data = iris.cnt, aes(x = \"\", y = prop, fill = Species)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  scale_y_continuous(breaks = c(0.00, 0.33, 0.66, 1.00)) +\n  labs(title = \"Stacked bar graph\", subtitle = \"relative proportions\",\n       x = NULL, y = \"Proportion\") +\n  theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\n# a basic pie chart\nplt3 &lt;- plt1 + coord_polar(\"y\", start = 0) +\n  labs(title = \"Friends don't let...\", subtitle = \"...friends make pie charts\",\n       x = NULL, y = NULL) +\n  scale_fill_brewer(palette = \"Blues\") +\n  theme_minimal()\n# if you seriously want a pie chart, rather use the base R function, `pie()`\n\n# here now a bar graph...\n# the default mapping of `geom_bar` is `stat = count`, which is a\n# bar for each fo the categories (`Species`), with `count` along y\nplt4 &lt;- ggplot(data = iris, aes(x = Species, fill = Species)) +\n  geom_bar(show.legend = FALSE) +\n  labs(title = \"Side-by-side bars\", subtitle = \"n per species\", y = \"Count\") +\n theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\nggarrange(plt1, plt2, plt3, plt4, nrow = 2, ncol = 2, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 3: Examples of histograms for the built-in Iris data. A) A default frequency histogram showing the count of samples for each of the three species. B) A relative frequency histogram of the same data; here, the sum of counts of samples available for each of the three species is 1. C) A boring pie chart. D) A frequency histogram of raw data counts shown as a series of side-by-side bars.\n\n\n\n\nNow I’ll demonstrate more realistic bar graphs. We stay with the iris data (Figure 4):\n\niris |&gt;\n  pivot_longer(cols = Sepal.Length:Petal.Width,\n               names_to = \"variable\",\n               values_to = \"size\") |&gt; \n  group_by(Species, variable) |&gt; \n  summarise(mean = round(mean(size), 1),\n            sd = round(sd(size), 1), .groups = \"drop\") |&gt; \n  ggplot(aes(x = Species, y = mean)) +\n  geom_bar(position = position_dodge(), stat = \"identity\", \n           col = \"black\", fill = \"salmon\", alpha = 0.4) +\n  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),\n                width = .2) +\n  facet_wrap(~variable,\n             scales = \"free\") +\n  ylab(\"Size (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 4: Bar graphs indicating the mean size (± SD) for various flower features of three species of Iris.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-boxplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-boxplots",
    "title": "3. Statistical Figures",
    "section": "Box plots",
    "text": "Box plots\nA box plot provides a graphical summary of the distribution of data. They allow us to compare the medians, quartiles, and ranges of the data for multiple groups, and identify any differences or similarities in the distributions. For example, box plots can be used to compare the body size distributions of different species, or to compare the reproductive output of different populations. Additionally, box plots can be used to identify outliers and other anomalies in the data, which may be indicative of underlying ecological processes or environmental factors.\nBox plots plots are traditionally used to display data that are not normally distributed, but I like to use them for any old data, even normal data. I prefer these over the old-fashioned bar graphs (seen in Section 2.2). As a variation of the basic box-and-whisker plot, I also quite like to superimpose a jittered scatter plot of the raw data on each bar.\nI create a simple example using the msleep dataset (Figure 5). Additional examples are provided in Chapter 2.\n\nmsleep |&gt; \n  ggplot(aes(x = vore, y = sleep_total)) + \n  geom_boxplot(colour = \"black\", fill = \"salmon\", alpha = 0.4,\n               outlier.color = \"red3\", outlier.fill = \"red\",\n               outlier.alpha = 1.0, outlier.size = 2.2) +\n  geom_jitter(width = 0.10, fill = \"blue\", alpha = 0.5,\n              col = \"navy\", shape = 21, size = 2.2) +\n  labs(x = \"'-vore'\",\n       y = \"Sleep duration (hr)\") +\n  theme_pubclean()\n\n\n\n\n\n\nFigure 5: Box-plot summarising the amount of sleep required by different ‘vores’.\n\n\n\n\nBox plots are sometimes called box-and-whisker plots. The keen eye can glance the ‘shape’ of the data distribution; they provide an alternative view to that given by the frequency distribution. There is a lot of information in these graphs, so let’s see what’s there. From the geom_boxplot documentation, which says it best (type ?geom_boxplot):\n\n“The lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles).”\n“The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called ‘outlying’ points and are plotted individually.”\n“In a notched box plot, the notches extend 1.58 * IQR / sqrt(n). This gives a roughly 95% confidence interval for comparing medians.”\n\nHere be more examples (Figure 6), this time of notched box plots:\n\nlibrary(ggsci) # for nice colours\n\nggplot(data = iris.long, aes(x = Species, y = size)) +\n  geom_boxplot(alpha = 0.4, notch = TRUE) +\n  geom_jitter(width = 0.1, shape = 21, fill = NA,\n              alpha = 0.4, aes(colour = as.factor(Species))) +\n  facet_wrap(~variable, nrow = 2) +\n  scale_color_npg() +\n  labs(y = \"Size (cm)\") +\n  guides(colour = FALSE) +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\n\n\n\n\n\nFigure 6: A panelled collection of box plots, one for each of the four variables, with a scatterplot to indicate the spread of the raw data points.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-densityplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-densityplots",
    "title": "3. Statistical Figures",
    "section": "Density plots",
    "text": "Density plots\nOften when we are displaying a distribution of data we are interested in the ‘shape’ of the data more than the actual count of values in a specific category, as shown by a standard histogram. When one wishes to more organically visualise the frequency of values in a sample set a density graphs is used. These may also be thought of as smooth histograms. These work well with histograms and rug plots, as we may see in the figure below. It is important to note with density plots that they show the relative density of the distribution along the \\(y\\)-axis, and not the counts of the data. This can of course be changed, as seen below, but is not the default setting. Sometimes it can be informative to see how different the count and density distributions appear.\nFigure 7 shows examples af density plots:\n\n# a normal density plot\ndens1 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"A vanilla density plot\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a density and rug plot combo\ndens2 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  geom_rug(colour = \"red\") +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"A density and rug plot\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a relative frequency histogram overlayed with a density plot\ndens3 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..density..),\n                 position = 'identity', binwidth = 1,\n                 colour = \"black\", fill = \"turquoise\", alpha = 0.6) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"Relative frequency with density\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a normal frequency histogram with density overlayed\n# note that the density curve must be adjusted by\n# the number of data points times the bin width\ndens4 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..count..),\n                 binwidth = 0.2, colour = \"black\", fill = \"turquoise\", alpha = 0.6) +\n  geom_density(aes(y = ..density.. * nrow(datasets::faithful) * 0.2), position = \"identity\",\n               colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"Frequency with density\",\n       x = \"Eruption duration (min)\",\n       y = \"Count\") + theme_pubr()\n\nggarrange(dens1, dens2, dens3, dens4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 7: A bevy of density graphs option based on the iris data. A) A lone density graph. B) A density graph accompanied by a rug plot. C) A histogram with a density graph overlay. D) A ridge plot.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-violinplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-violinplots",
    "title": "3. Statistical Figures",
    "section": "Violin plots",
    "text": "Violin plots\nWe may combine the box plot and density graph concepts into a new figure type. They can become quite snooty and display more information in more informative ways than vanilla box plots. These are known as violin plots and are very useful when we want to show the distribution of multiple categories of a continuous variate alongside one another.\nViolin plots show the same information as box plots but take things one step further by allowing the shape of the box plot to also show the distribution of the continuous data within the sample sets. They show not only central tendencies (like median) but also the full distribution, including possible multimodal or skewed characteristics.\nOne needs to install additional packages to make then, such as the package ggstatplot. This package offers many non-traditional options for graphical statistical summaries. Here, the violin plot includes the following features:\n\nViolins The vertical, symmetrical, and mirrored shapes represent the estimated probability density of the data at different values. The wider the violin at a given point, the higher the density of data at that value.\nBox plot A box plot can be embedded within the violin plot to show the median, quartiles, and the possible outliers.\nStatistical annotations The violin plots offered by ggstatplot accommodate various statistical annotations such as mean, median, confidence intervals, or p-values, depending on the your needs.\n\nWe will use the iris data below to highlight the different types of violin plots one may use (Figure 8):\n\nlibrary(ggstatsplot)\nset.seed(123) # for reproducibility\n\n# plot\nggstatsplot::ggbetweenstats(\n  data = iris,\n  x = Species,\n  y = Sepal.Length,\n  ylab = \"Sepal length (cm)\",\n  title = \"Distribution of sepal length across the three *Iris* species\"\n)\n\n\n\n\n\n\nFigure 8: Examples of violin plots made for the Iris data.\n\n\n\n\nHere’s another verson of the iris data analysed with violin plots (Figure 9):\n\nvio1 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_violin() + \n  labs(title = \"Iris data\",\n       subtitle = \"Basic violin plot\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# A violin plot showing the quartiles as lines\nvio2 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_violin(show.legend = FALSE, draw_quantiles = c(0.25, 0.5, 0.75)) + \n  labs(title = \"Iris data\",\n       subtitle = \"Violin plot with quartiles\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# Box plots nested within violin plots\nvio3 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, colour = Species)) +\n  geom_violin(fill = \"grey70\") + \n  geom_boxplot(width = 0.1, colour = \"grey30\", fill = \"white\") +\n  labs(title = \"Iris data\",\n       subtitle = \"Box plots nested within violin plots\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# Boxes in violins with the raw data jittered about\nvio4 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, colour = Species)) +\n  geom_violin(fill = \"grey70\") + \n  geom_boxplot(width = 0.1, colour = \"black\", fill = \"white\") +\n  geom_jitter(shape = 1, width = 0.1, colour = \"red\", alpha = 0.7, fill = NA) +\n  labs(title = \"Iris data\",\n       subtitle = \"Violins, boxes, and jittered data\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\nggarrange(vio1, vio2, vio3, vio4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 9: More variations of violin plots applied to the Iris data.\n\n\n\n\nThe ggpubr package also provides many convenience functions for the drawing of publication quality graphs, including violin plots.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-scatterplots",
    "href": "BCB744/basic_stats/03-visualise.html#sec-scatterplots",
    "title": "3. Statistical Figures",
    "section": "Scatter plots",
    "text": "Scatter plots\nThe relationship between two continuous variables is typically displayed with scatter plots. In a scatter plot, each data point is represented by a dot or other symbol plotted on a Cartesian coordinate system, with one variable mapped to the \\(x\\)-axis and the other to the \\(y\\)-axis. One may choose to fit a best fit line through these points, but displaying the scatter of points is typically enough. In scatter plots, the points are not connected by lines, and the use of discrete points causes us to not assume a specific order or continuity in the data between ‘consecutive’ points on the graph. Also, a scatter plot typically does not require that the \\(x\\)-axis is independent.\nThe most basic use of scatter plots is the following:\n\nExploratory data analysis Scatter plots are useful in the initial exploration of data sets. They help us identify patterns and relationships that might warrant further investigation using more advanced statistical techniques.\nIdentifying trends One can identify whether there is a positive, negative, or no apparent trend between the two variables by observing the overall pattern (slope) of an imaginary or real line fitted to the data points. The detection of trends is something we will encounter in Chapter 9 on Simple linear regressions.\nIdentifying correlations Scatter plots can be used to visually assess the correlation between two variables. A strong positive correlation will result in data points forming a line or curve sloping upward, while a strong negative correlation will result in data points forming a line or curve sloping downward. A weak or no correlation will result in a more scattered and less structured pattern. We will discover more about this in Chapter 10 on Correlation.\nAssessing clustering Scatter plots can reveal natural groupings or clusters of data points, which can be helpful in understanding the structure of the data or identifying potential subgroups for further analysis.\n\nAll of these applications of scatter plots are shown in Figure 10. In Figure 10 I show the relationship between two (matched) continuous variables. The statistical strength of the relationship can be indicated by a correlation (no causal relationship implied as is the case here) or a regression (when a causal link of \\(x\\) on \\(y\\) is demonstrated), and the grouping structure is clearly indicated with colour.\n\nplt1 &lt;- ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +\n  geom_point() +\n  labs(x = \"Petal length (cm)\", y = \"Petal width (cm)\") +\n  theme(legend.position = c(0.22, 0.75)) +\n  scale_color_fivethirtyeight() +\n  scale_fill_fivethirtyeight() +\n  theme_minimal()\n\nplt2 &lt;- ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, show.legend = FALSE) +\n  scale_color_fivethirtyeight() +\n  scale_fill_fivethirtyeight() +\n  labs(x = \"Petal length (cm)\", y = \"Petal width (cm)\") +\n  theme_minimal()\n\nggarrange(plt1, plt2, ncol = 2, nrow = 1, labels = \"AUTO\",\n          common.legend = TRUE)\n\n\n\n\n\n\nFigure 10: Examples of scatterplots made for the Iris data. A) A default scatter plot showing the relationship between petal length and width. B) The same as (A) but with a correlation line added.\n\n\n\n\nScatter plots may also indicate some of the following properties of our datasets, which make them useful as a diagnostic tool in inferential data analysis, specifically when it comes to assessing assumptions about our data:\n\nDetecting outliers Outliers are data points that deviate significantly from the overall pattern of the data. Scatter plots can help identify such points that might warrant further investigation or indicate problems in data collection.\nAssessing linearity Scatter plots can reveal whether the relationship between two variables is linear or nonlinear. A linear relationship will result in data points forming a straight line, while a nonlinear relationship will result in data points forming a curve or more complex pattern.\n\nWe will encounter these uses in later Chapters dealing with inferential statistics.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-linegraph",
    "href": "BCB744/basic_stats/03-visualise.html#sec-linegraph",
    "title": "3. Statistical Figures",
    "section": "Line graphs",
    "text": "Line graphs\nA line graph connects data points with lines, typically emphasising a continuous relationship or a sequence over time or some other continuous scale. The \\(x\\)-axis often represents time (or another independent variable), while the \\(y\\)-axis represents the other variable (usually the dependent variable). Line graphs are particularly useful for tracking changes, trends, or patterns over time and for comparing multiple data series. They suggest a more explicit connection between data points, making it easier to observe fluctuations and the overall direction of the data.\nWe typically encounter line graphs in visual displays of time-series. One might include a point for each observation in time, but it may be omitted. The important thing to note is that a line connects each consecutive observation to the next, indicating the continuity of time. It is a useful tool for exploring trends, patterns, and seasonality in data. For example, a time-series plot can be used to visualise the seasonal trends in temperature over an annual cycle (Figure 11). In this example, points are not used at all, and I opt instead for a stepped line that suggests continuity and yet maintain a ‘discrete’ measure per month (i.e. ignoring the higher frequency daily and finer scale variations within a month).\n\nlibrary(lubridate)\nread_csv(\"../../data/SACTN_SAWS.csv\") |&gt; \n  mutate(month = month(date)) |&gt; \n  group_by(site, month) |&gt; \n  dplyr::summarise(temp = mean(temp, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = month, y = temp)) +\n  geom_step(colour = \"red4\") +\n  scale_x_continuous(breaks = c(1, 3, 5, 7, 9, 11)) +\n  xlab(\"Month\") + ylab(\"Temperature (°C)\") +\n  facet_wrap(~site, ncol = 5) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 11: A time series plot showing the monthly climatology for several sites around South Africa. The specific kind of line drawn here forms a stepped graph.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/03-visualise.html#sec-heatmaps",
    "href": "BCB744/basic_stats/03-visualise.html#sec-heatmaps",
    "title": "3. Statistical Figures",
    "section": "Heatmaps and Hovmöller Diagrams",
    "text": "Heatmaps and Hovmöller Diagrams\nWe can extend the time series line graph to two dimensions. A heatmap is a raster representation of data where the values in a matrix are represented as colours. We will see some heatmaps in Chapter 10 on Correlations. A special kind of heatmap is a calendar heatmap, which is a visualisation technique that uses a calendar layout to show patterns in data over time. For example, a calendar heatmap can be used to show the daily time series or climatologies of temperature or some other environmental variable that varies seasonally (Figure 12).\n\n# Load the function to the local through Paul Bleicher's GitHub page\nsource(\"https://raw.githubusercontent.com/iascchen/VisHealth/master/R/calendarHeat.R\")\n\ntemps &lt;- heatwaveR::sst_WA |&gt; \n  filter(t &gt;= \"2010-01-01\" & t &lt;= \"2019-12-31\") |&gt; \n  mutate(weekday = wday(t),\n         weekday_f = wday(t, label = TRUE),\n         week = week(t),\n         month = month(t, label = TRUE),\n         year = year(t)) |&gt; \n  group_by(year, month) |&gt; \n  mutate(monthweek = 1 + week - min(week))\n\nggplot(temps, aes(monthweek, weekday_f, fill = temp)) + \n  geom_tile(colour = \"white\") +\n  facet_grid(year(t) ~ month) +\n  scale_x_continuous(breaks = c(1, 3, 5)) +\n  scale_y_discrete(breaks = c(\"Sun\", \"Wed\", \"Sat\")) +\n  scale_fill_viridis_c() +\n  xlab(\"Week of Month\") +\n  ylab(\"\") +\n  ggtitle(\"Time-Series Calendar Heatmap: Western Australia SST\") +\n  labs(fill = \"[°C]\")\n\n\n\n\n\n\nFigure 12: A calendar heatmap showing a timeseries of SST for Western Australia. The infamous marine heatwave that resulted in a new field of study on extreme temperatures can be seen in the summer of 2011.\n\n\n\n\nA special kind of heatmap is used in Ocean and Atmospheric Science is the Hovmöller Diagram (see Figure 13), where we have one continuous spatial covariate along one axis (e.g. latitude or longitude) and time along the other axis on a two-dimensional graph. These diagrams were originally developed by Swedish meteorologist Ernest Hovmöller. By mapping oceanographic variables such as sea surface temperature, salinity, or ocean currents, Hovmöller Diagrams allow us to track the progression of phenomena like El Niño and La Niña events, or to examine the migration of ocean eddies and gyres.\nA variation of Hovmöller Diagrams is the horizon plot (Figure 14), which shows the same kind of information (and more) but in a more visually impactful format, in my opinion. I provide more information on horizon plots in my vignette, where I also demonstrate their application to the visualisation of extreme temperature events.\n\nlibrary(data.table)\nlibrary(colorspace)\n\nNWA &lt;- fread(\"../../data/NWA_Hovmoller.csv\")\n\n# calculate anomalies\nNWA |&gt; \n  mutate(anom = zonal_sst - mean(zonal_sst)) |&gt; \n  ggplot(aes(x = t, y = lat, fill = anom)) +\n  geom_tile(colour = \"transparent\") +\n  scale_fill_binned_diverging(palette = \"Blue-Red 3\", n_interp = 21) +\n  # scale_fill_viridis_c() +\n  xlab(\"\") + ylab(\"Latitude [°N]\") + labs(fill = \"[°C]\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 13: A Hovmöller Diagram of zonally averaged SST for a region off Northwest Africa in the Canary upwelling system. A variation of this figure appears in the vignette and shows the timeline of marine heatwaves and cold spells in the region.\n\n\n\n\n\nlibrary(ggHoriPlot)\n\ncutpoints &lt;- NWA  %&gt;% \n  mutate(\n    outlier = between(\n      zonal_sst, \n      quantile(zonal_sst, 0.25, na.rm = TRUE)-\n        1.5*IQR(zonal_sst, na.rm = TRUE),\n      quantile(zonal_sst, 0.75, na.rm = TRUE)+\n        1.5*IQR(zonal_sst, na.rm=TRUE))) %&gt;% \n  filter(outlier)\n\n# The origin\nori &lt;- round(sum(range(cutpoints$zonal_sst))/2, 2)\n\n# The horizon scale cutpoints\nsca &lt;- round(seq(range(cutpoints$zonal_sst)[1], \n                 range(cutpoints$zonal_sst)[2], \n                 length.out = 7)[-4], 2)\n\nNWA %&gt;% ggplot() +\n  geom_horizon(aes(t,\n                   zonal_sst,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = TRUE) +\n  facet_grid(lat~.) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand = c(0,0),\n               date_breaks = \"1 year\",\n               date_labels = \"%Y\") +\n  xlab('Year') +\n  ggtitle('Canary current system zonal SST')\n\n\n\n\n\n\nFigure 14: Zonally average time series of SST in the Canary current system displayed as a horizon plot.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html",
    "href": "BCB744/basic_stats/09-regressions.html",
    "title": "9. Simple Linear Regressions",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nThe simple linear regression\nThe model coefficients\nGraphing linear regressions\nConfidence intervals\nPrediction intervals\nModel fit diagnostics",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#the-intercept",
    "href": "BCB744/basic_stats/09-regressions.html#the-intercept",
    "title": "9. Simple Linear Regressions",
    "section": "The intercept",
    "text": "The intercept\nThe intercept (more precisely, the \\(y\\)-intercept, \\(\\alpha\\)) is the best estimate of the starting point of the fitted line on the left hand side of the graph where it crosses the \\(y\\)-axis. You will notice that there is also an estimate for the standard error of the estimate for the intercept.\nThere are several hypothesis tests associated with a simple linear regression. All of them assume that the residual error, \\(\\epsilon\\), in the linear regression model is independent of \\(X\\) (i.e. nothing about the structure of the error term can be inferred based on a knowledge of \\(X\\)), is normally distributed, with zero mean and constant variance. We say the residuals are i.i.d. (independent and identically distributed, which is a fancy way of saying they are random).\nOne of the tests looks at the significance of the intercept, i.e. it tests the H0 that \\(\\alpha=0\\). Is the value of the \\(y\\)-intercept zero? Rejecting this H0 causes the alternate hypothesis of \\(\\alpha \\neq 0\\) to be accepted. This test is automatically performed when fitting a linear model in R and asking for a summary of the regression object, but it is insightful and important to know that the test is simply a one-sample t-test. In the sparrows data, this statistic is in the Coefficients table in the row indicated by (Intercept) under the Pr(&gt;|t|) column.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#the-regression-coefficient",
    "href": "BCB744/basic_stats/09-regressions.html#the-regression-coefficient",
    "title": "9. Simple Linear Regressions",
    "section": "The regression coefficient",
    "text": "The regression coefficient\nThe interpretation of the regression coefficient, \\(\\beta\\), is simple. For every one unit of change in the independent variable (here waiting time) there is a corresponding average change in the dependent variable (here the duration of the eruption). This is the slope or gradient, and it may be positive or negative. In the example the slope of the line is denoted by the value 0.27 \\(cm.day^{-1}\\) in the column termed Estimate and in the row called age (the latter name will of course depend on the name of the response column in your dataset). The coefficient of determination (\\(r^2\\), see Section 7.2) multiplies the response variable to produce a prediction of the response based on the slope of the relationship between the response and the predictor. It tells us how much one unit in change of the independent variable determines the corresponding change in the response variable. There is also a standard error for the estimate.\nThe second hypothesis test performed when fitting a linear regression model concerns the regression coefficient. It looks for whether there is a significant relationship (slope) of \\(Y\\) on \\(X\\) by testing the H0 that \\(\\beta=0\\). As before, this is also simply a one-sample t-test. In the regression summary the probability associated with this test is given in the Coefficients table in the column called Pr(&gt;|t|) in the row age. In the sparrows data, the p-value associated with wing is less than 0.05 and we therefore reject the H0 that \\(\\beta=0\\). So, there is a significant linear relationship of eruption duration on the waiting time between eruptions.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#residual-standard-error-rse-and-root-mean-square-error-rmse",
    "href": "BCB744/basic_stats/09-regressions.html#residual-standard-error-rse-and-root-mean-square-error-rmse",
    "title": "9. Simple Linear Regressions",
    "section": "Residual standard error (RSE) and root mean square error (RMSE)",
    "text": "Residual standard error (RSE) and root mean square error (RMSE)\nThe residual standard error (RSE) is a measure of the average amount that the response variable deviates from the regression line. It is calculated as the square root of the residual sum of squares divided by the degrees of freedom (Equation 3).\n\n\nThe RSE: \\[RSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2}{n-2}} \\tag{3}\\]\nwhere \\(y_i\\) represents the observed value of the dependent variable for the \\(i\\)-th observation, \\(\\hat{y}_i\\) represents the predicted value of the dependent variable for the \\(i\\)-th observation, and n is the number of observations in the sample.\nThe root mean square error (RMSE) is a similar measure, but it is calculated as the square root of the mean of the squared residuals. It is a measure of the standard deviation of the residuals (Equation 4).\n\n\nThe RMSE: \\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2} \\tag{4}\\] where the model components are as in Equation 3.\nRSE and RMSE are similar but different. There is a small difference in how they are calculated. The RSE takes into account the degrees of freedom which becomes important when models with different numbers of variables are compared. The RMSE is more commonly used in machine learning and data mining, where the focus is on prediction accuracy rather than statistical inference.\nBoth the RSE and RMSE provide information about the amount of error in the model predictions, with smaller values indicating a better fit. However, both may be influenced by outliers or other sources of variability in the data. Use a variety of means to assess the model fit diagnostics.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#sec-coef-det",
    "href": "BCB744/basic_stats/09-regressions.html#sec-coef-det",
    "title": "9. Simple Linear Regressions",
    "section": "\nR-squared (R2)",
    "text": "R-squared (R2)\nThe coefficient of determination, the \\(R^{2}\\), of a linear model is the quotient of the variances of the fitted values, \\(\\hat{y_{i}}\\), and observed values, \\(y_{i}\\), of the dependent variable. If the mean of the dependent variable is \\(\\bar y\\), then the \\(R^{2}\\) is as shown in Equation 5.\n\n\n\n\n\n\nThe R2: \\[R^{2}=\\frac{\\sum(\\hat{Y_{i}} - \\bar{Y})^{2}}{\\sum(Y_{i} - \\bar{Y})^{2}} \\tag{5}\\]\n\n\n\n\n\n\n\nFigure 3: A linear regression through random normal data.\n\n\n\n\nSimply put, the \\(R^{2}\\) is a measure of the proportion of the variation in the dependent variable that is explained (can be predicted) by the independent variable(s) in the model. It ranges from 0 to 1, with a value of 1 indicating a perfect fit (i.e. a scatter of points to denote the \\(Y\\) vs. \\(X\\) relationship will all fall perfectly on a straight line). It gives us an indication of how well the observed outcome variable is predicted by the observed influential variable, and in the case of a simple linear regression, that the geometric relationship of \\(Y\\) on \\(X\\) is a straight line. For example, in Figure 3 there is absolutely no relationship of \\(y\\) on \\(x\\). Here, the slope is 0.001 and the \\(R^{2}\\) is 0.\nNote, however, that a high \\(R^{2}\\) does not necessarily mean that the model is a good fit; it may also suggest that the model is unduly influenced by outliers or the inclusion of irrelevant variables. Expert knowledge will help with the interpretation of the \\(R^{2}\\).\n\n\nRegressions may take on any relationship, not only a linear one. For example, there are parabolic, hyperbolic, logistic, exponential, etc. relationships of \\(Y\\) on \\(X\\), and here, too, does \\(R^{2}\\) tell us the same thing. If we assume that the samples were representatively drawn from a population (i.e. the sample fully captures the relationship of \\(Y\\) on \\(X\\) that is present in the entire population), the \\(R^{2}\\) will represent the relationship in the population too.\n\nIn the case of our sparrows data, the \\(R^{2}\\) is 0.973, meaning that the proportion of variance explained is 97.3%; the remaining 2.7% is not (yet) accounted for by the linear relationship. Adding more predictors into the regression (i.e. a multiple regression) might consume some of the unexplained variance and increase the overall \\(R^{2}\\).\nSometimes you will also see something called the adjusted \\(R^{2}\\). This is a modified version of \\(R^{2}\\) that takes into account the number of independent variables in the model. It penalises models that include too many variables that do not improve the fit. Generally this is not something to be too concerned with in linear models that have only one independent variable, such as the models seen in this Chapter.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#f-statistic",
    "href": "BCB744/basic_stats/09-regressions.html#f-statistic",
    "title": "9. Simple Linear Regressions",
    "section": "\nF-statistic",
    "text": "F-statistic\nThe F-statistic (or F-value) is another measure of the overall significance of the model. It is used to test whether at least one of the independent variables in the model has a non-zero coefficient, indicating that it has a significant effect on the dependent variable.\nIt is calculated by taking the ratio of the mean square regression (MSR) to the mean square error (MSE) (Equation 6). The MSR measures the variation in the dependent variable that is explained by the independent variables in the model, while the MSE measures the variation in the dependent variable that is not explained by the independent variables.\n\n\nCalculating the F-statistic: \\[MSR = \\frac{\\sum_{i=1}^{n}(\\hat{Y}_i - \\bar{Y})^2}{1}\\] \\[MSE = \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{n-2}\n\\] \\[F = \\frac{MSR}{MSE} \\tag{6}\\]\nwhere the model components are as in Equation 3.\nIf the F-statistic is large and the associated p-value is small (typically less than 0.05), it indicates that at least one of the independent variables in the model has a significant effect on the dependent variable. In other words, the H0 that all the independent variables have zero coefficients can be rejected in favour of the Ha that at least one independent variable has a non-zero coefficient.\nNote that a significant F-statistic does not necessarily mean that all the independent variables in the model are significant. Additional diagnostic tools, such as individual t-tests and residual plots, should be used to determine which independent variables are significant and whether the model is a good fit for the data.\nFortunately, in this Chapter we will encounter linear regressions with only one independent variable. The situation where we deal with multiple independent variables is called multiple regression. We will encounter some multiple regression type models in Quantitative Ecology.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#plot-of-residuals-vs.-fitted-values",
    "href": "BCB744/basic_stats/09-regressions.html#plot-of-residuals-vs.-fitted-values",
    "title": "9. Simple Linear Regressions",
    "section": "Plot of residuals vs. fitted values",
    "text": "Plot of residuals vs. fitted values\nA residual plot shows the residuals (values predicted by the linear model, \\(\\hat{Y}\\), minus the observed values, \\(Y\\), on the y-axis and the independent (\\(X\\)) variable on the x-axis. Points in a residual plot that are randomly dispersed around the horizontal axis indicates a linear regression model that is appropriate for the data. If this simple ‘test’ fails, a non-linear model might be more appropriate, or one might transform the data to normality (assuming that the non-normality of the data is responsible for the non-random dispersal above and below the horizontal line).",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#plot-of-standardised-residuals",
    "href": "BCB744/basic_stats/09-regressions.html#plot-of-standardised-residuals",
    "title": "9. Simple Linear Regressions",
    "section": "Plot of standardised residuals",
    "text": "Plot of standardised residuals\nWe may use a plot of the residuals vs. the fitted values, which is helpful for detecting heteroscedasticity—e.g. a systematic change in the spread of residuals over a range of predicted values.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/09-regressions.html#normal-probability-plot-of-residuals-normal-q-q-plot",
    "href": "BCB744/basic_stats/09-regressions.html#normal-probability-plot-of-residuals-normal-q-q-plot",
    "title": "9. Simple Linear Regressions",
    "section": "Normal probability plot of residuals (Normal Q-Q plot)",
    "text": "Normal probability plot of residuals (Normal Q-Q plot)\nLet see all these plots in action for the sparrows data. The package ggfortify has a convenient function to automagically make all of these graphs:\n\nlibrary(ggfortify)\nautoplot(lm(wing ~ age, data = sparrows), label.size = 3,\n         col = \"red3\", shape = 10, smooth.colour = 'blue3')\n\n\n\n\n\n\nFigure 4: Four diagnostic plots testing the assumptions to be met for linear regressions.\n\n\n\n\nOne might also use the package gg_diagnose to create all the various (above plus some!) diagnostic plots available for fitted linear models.\nDiagnostic plots will be further explored in the exercises (see below).",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#remember-the-t-test",
    "href": "BCB744/basic_stats/08-anova.html#remember-the-t-test",
    "title": "8. ANOVA",
    "section": "\n2.1 Remember the t-test",
    "text": "2.1 Remember the t-test\nAs you already know, a t-test is used when we want to compare two different sample sets against one another. This is also known as a two-factor or two level test. When one wants to compare multiple (more than two) sample sets against one another an ANOVA is required (I’ll get there shortly). Remember how to perform a t-test in R: we will revisit this test using the chicks data, but only for Diets 1 and 2 from day 21.\n\n# First grab the data\nchicks &lt;- as_tibble(ChickWeight)\n\n# Then subset out only the sample sets to be compared\nchicks_sub &lt;- chicks %&gt;% \n  filter(Diet %in% c(1, 2), Time == 21)\n\nOnce we have filtered our data we may now perform the t-test.\n\nt.test(weight ~ Diet, data = chicks_sub)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by Diet\nt = -1.2857, df = 15.325, p-value = 0.2176\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -98.09263  24.19263\nsample estimates:\nmean in group 1 mean in group 2 \n         177.75          214.70 \n\n\nAs one may recall from Chapter 7, whenever we want to give a formula to a function in R, we use the ~. The formula used above, weight ~ Diet, reads in plain English as “weight as a function of diet”. This is perhaps easier to understand as “Y as a function of X.” This means that we are assuming whatever is to the left of the ~ is the dependant variable, and whatever is to the right is the independent variable. Did the Diet 1 and 2 produce significantly fatter birds?\nOne could also supplement the output by producing a graph (Figure 1).\n\nlibrary(ggstatsplot)\n\n## since the confidence intervals for the effect sizes are computed using\n## bootstrapping, important to set a seed for reproducibility\nset.seed(13)\n\n## parametric t-test and box plot\nggbetweenstats(\n  data = chicks_sub,\n  x = Diet,\n  y = weight,\n  xlab = \"Diet\",\n  ylab = \"Chick mass (g)\",\n  plot.type = \"box\",\n  p.adjust.method = \"bonferroni\",\n  pairwise.display = \"ns\",\n  type = \"p\",\n  results.subtitle = FALSE,\n  conf.level = 0.95,\n  title = \"t-test\",\n  ggtheme = ggthemes::theme_fivethirtyeight(),\n  package = \"basetheme\",\n  palette = \"ink\"\n)\n\n\n\n\n\n\nFigure 1: Box-and-whisker plot showing the differences in means between chicks reared to 21 days old and fed Diets 1 and 2\n\n\n\n\nNotice above that we did not need to specify to use a t-test. The ggbetweenstats() function automatically determines if an independent samples t-test or a 1-way ANOVA is required based on whether there are two groups or three or more groups within the grouping (factor) variable.\nThat was a nice revision. But applied to the chicks data it seemed a bit silly, because you may ask, “What if I wanted to know if there are differences among the means computed at Day 1, Day 6, Day 10, and Day 21?” We should not use t-tests to do this (although we can). So now we can move on to the ANOVA.\n\n\n\n\n\n\nTask G.1: Do it now!\n\n\n\n\nWhy should we not just apply t-tests once per each of the pairs of comparisons we want to make?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#why-not-do-multiple-t-tests",
    "href": "BCB744/basic_stats/08-anova.html#why-not-do-multiple-t-tests",
    "title": "8. ANOVA",
    "section": "\n2.2 Why not do multiple t-tests?",
    "text": "2.2 Why not do multiple t-tests?\nIn the chicks data we have four diets, not only two as in the t-test example just performed. Why not then simply do a t-test multiple times, once for each pair of diets given to the chickens? Multiple t-tests would be written as:\n\n\\(H_{0}: \\mu_1 = \\mu_2\\)\n\\(H_{0}: \\mu_1 = \\mu_3\\)\n\\(H_{0}: \\mu_1 = \\mu_4\\)\n\\(H_{0}: \\mu_2 = \\mu_3\\)\n\\(H_{0}: \\mu_2 = \\mu_4\\)\n\\(H_{0}: \\mu_3 = \\mu_4\\)\n\nThis would be invalid. The problem is that the chance of committing a Type I error increases as more multiple comparisons are done. So, the overall chance of rejecting the H0 increases. Why? If one sets \\(\\alpha=0.05\\) (the significance level below which the H0 is no longer accepted), one will still reject the H0 5% of the time when it is in fact true (i.e. when there is no difference between the groups). When many pairwise comparisons are made, the probability of rejecting the H0 at least once is higher because we take this 5% risk each time we repeat a t-test. In the case of the chicken diets, we would have to perform six t-tests, and the error rate would increase to slightly less than \\(6\\times5\\%\\). See Table 1. ::: {.cell layout-align=“center”}\n:::\n\n\n\n\n\n\nK\n0.2\n0.1\n0.05\n0.02\n0.01\n0.001\n\n\n\n2\n0.20\n0.10\n0.05\n0.02\n0.01\n0.00\n\n\n3\n0.49\n0.27\n0.14\n0.06\n0.03\n0.00\n\n\n4\n0.74\n0.47\n0.26\n0.11\n0.06\n0.01\n\n\n5\n0.89\n0.65\n0.40\n0.18\n0.10\n0.01\n\n\n10\n1.00\n0.99\n0.90\n0.60\n0.36\n0.04\n\n\n20\n1.00\n1.00\n1.00\n0.98\n0.85\n0.17\n\n\n100\n1.00\n1.00\n1.00\n1.00\n1.00\n0.99\n\n\n\n\nTable 1. Probability of committing a Type I error due to applying multiple t-tests to test for differences between K means. α from 0.2 to 0.0001 are shown.\n\n\nIf you insist in creating more work for yourself and do t-tests many times, one way to overcome the problem of committing Type I errors that stem from multiple comparisons is to apply a Bonferroni correction.\n\n\n\n\n\n\nBonferonni correction\n\n\n\nThe Bonferroni correction is used to adjust the significance level of multiple hypothesis tests, such as multiple paired t-tests among many groups, in order to reduce the risk of false positives or Type I errors. It is named after the Italian mathematician Carlo Emilio Bonferroni.\nThe Bonferroni correction is based on the principle that when multiple hypothesis tests are performed, the probability of observing at least one significant result due to random chance increases. To correct for this, the significance level (usually 0.05) is divided by the number of tests being performed. This results in a more stringent significance level for each individual test and it so reduces the risk of committing a Type I error.\nFor example, if we conduct ten hypothesis tests, the significance level for each test after Bonferonni correction would become 0.05/10 = 0.005. The implication is that each individual test would need to have a p-value less than 0.005 to be considered significant at the overall significance level of 0.05.\nOn the downside, this method can be overly conservative and we may then increase the risk of Type II errors, which are false negatives. If you really cannot avoid multiple tests, then also assess one of the alternatives to Bonferonni’s method, viz: the false discovery rate (FDR) correction, the Holm-Bonferroni correction, Benjamini-Hochberg’s procedure, the Sidak correction, or some of the Bayesian approaches.\n\n\nOr better still, we do an ANOVA that controls for these Type I errors so that it remains at 5%.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#single-factor",
    "href": "BCB744/basic_stats/08-anova.html#single-factor",
    "title": "8. ANOVA",
    "section": "\n3.1 Single factor",
    "text": "3.1 Single factor\nWe continue with the chicken data. The t-test showed that Diets 1 and 2 resulted in the same chicken mass at Day 21. What about the other two diets? Our H0 is that, at Day 21, \\(\\mu_{1}=\\mu_{2}=\\mu_{3}=\\mu_{4}\\). Is there a statistical difference between chickens fed these four diets, or do we retain the H0? The R function for an ANOVA is aov(). To look for significant differences between all four diets on the last day of sampling we use this one line of code:\n\nchicks.aov1 &lt;- aov(weight ~ Diet, data = filter(chicks, Time == 21))\nsummary(chicks.aov1)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  57164   19055   4.655 0.00686 **\nResiduals   41 167839    4094                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask G.2: Do it now!\n\n\n\n\nWhat does the outcome say about the chicken masses? Which ones are different from each other?\nDevise a graphical display of this outcome.\n\n\n\nIf this seems too easy to be true, it’s because we aren’t quite done yet. You could use your graphical display to eyeball where the significant differences are, or we can turn to a more ‘precise’ approach. The next step one could take is to run a Tukey HSD test on the results of the ANOVA by wrapping tukeyHSD() around aov():\n\nTukeyHSD(chicks.aov1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ Diet, data = filter(chicks, Time == 21))\n\n$Diet\n         diff        lwr       upr     p adj\n2-1  36.95000  -32.11064 106.01064 0.4868095\n3-1  92.55000   23.48936 161.61064 0.0046959\n4-1  60.80556  -10.57710 132.18821 0.1192661\n3-2  55.60000  -21.01591 132.21591 0.2263918\n4-2  23.85556  -54.85981 102.57092 0.8486781\n4-3 -31.74444 -110.45981  46.97092 0.7036249\n\n\nThe output of tukeyHSD() shows us that pairwise comparisons of all of the groups we are comparing. We can also display this as a very rough figure (Figure 2):\n\nplot(TukeyHSD(chicks.aov1))\n\n\n\n\n\n\nFigure 2: A plot of the Tukey-HSD test showing the differences in means between chicks reared to 21 days old and fed four diets.\n\n\n\n\nWe may also produce a nicer looking graphical summary in the form of a box-and-whisker plot and/or a violin plot. Here I combine both (Figure 3):\n\nset.seed(666)\n\n## parametric t-test and box plot\nggbetweenstats(\n  data = filter(chicks, Time == 21),\n  x = Diet,\n  y = weight,\n  xlab = \"Diet\",\n  ylab = \"Chick mass (g)\",\n  plot.type = \"box\",\n  boxplot.args = list(notch = TRUE),\n  type = \"parametric\",\n  results.subtitle = FALSE,\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"s\",\n  p.adjust.method = \"bonferroni\",\n  conf.level = 0.95,\n  title = \"ANOVA\",\n  ggtheme = ggthemes::theme_fivethirtyeight(),\n  package = \"basetheme\",\n  palette = \"ink\"\n)\n\n\n\n\n\n\nFigure 3: Box-and-whisker plot showing the differences in means between chicks reared to 21 days old and fed four diets. Shown is a notched box plot where the extent of the notches is 1.58 * IQR / sqrt(n). This is approximately equivalent to a 95% confidence interval andf may be used for comparing medians.\n\n\n\n\n\n\n\n\n\n\nTask G.3: Do it now!\n\n\n\nLook at the help file for the TukeyHSD() function to better understand what the output means.\n\nHow does one interpret the results? What does this tell us about the effect that that different diets has on the chicken weights at Day 21?\nFigure out a way to plot the Tukey HSD outcomes in ggplot.\nWhy does the ANOVA return a significant result, but the Tukey test shows that not all of the groups are significantly different from one another?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#multiple-factors",
    "href": "BCB744/basic_stats/08-anova.html#multiple-factors",
    "title": "8. ANOVA",
    "section": "\n3.2 Multiple factors",
    "text": "3.2 Multiple factors\nWhat if we have multiple grouping variables, and not just one? We would encounter this kind of situation in factorial designs. In the case of the chicken data, there is also time that seems to be having an effect.\n\n\n\n\n\n\nTask G.4: Do it now!\n\n\n\n\nHow is time having an effect? (/3)\n\nWhat hypotheses can we construct around time? (/2)\n\n\n\n\nLet us look at some variations around questions concerning time. We might ask, at a particular time step, are there differences amongst the effect due to diet on chicken mass? Let’s see when diets are starting to have an effect by examining the outcomes at times 0, 2, 10, and 21:\n\n# effect at time = 0\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 0)))\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nDiet         3   4.32   1.440   1.132  0.346\nResiduals   46  58.50   1.272               \n\n# effect at time = 2\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 2)))\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  158.4   52.81   4.781 0.00555 **\nResiduals   46  508.1   11.05                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# effect at time = 10\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 10)))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nDiet         3   8314    2771    6.46 0.000989 ***\nResiduals   45  19304     429                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# effect at time = 21\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 21)))\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  57164   19055   4.655 0.00686 **\nResiduals   41 167839    4094                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask G.5: Do it now!\n\n\n\n\nWhat do you conclude from the above series of ANOVAs? (/3)\n\nWhat problem is associated with running multiple tests in the way that we have done here? (/2)\n\n\n\n\nOr we may ask, regardless of diet (i.e. disregarding the effect of diet by clumping all chickens together), is time having an effect?\n\nchicks.aov2 &lt;- aov(weight ~ as.factor(Time),\n                   data = filter(chicks, Time %in% c(0, 2, 10, 21)))\nsummary(chicks.aov2)\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)    \nas.factor(Time)   3 939259  313086   234.8 &lt;2e-16 ***\nResiduals       190 253352    1333                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask G.6: Do it now!\n\n\n\n\nWrite out the hypotheses for this ANOVA. (/2)\n\nWhat do you conclude from the above ANOVA? (/3)\n\n\n\n\nOr, to save ourselves a lot of time and reduce the coding effort, we may simply run a two-way ANOVA and look at the effects of Diet and Time simultaneously. To specify the different factors we put them in our formula and separate them with a +:\n\nsummary(aov(weight ~ Diet + as.factor(Time),\n            data = filter(chicks, Time %in% c(0, 21))))\n\n                Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nDiet             3  39595   13198   5.987 0.00091 ***\nas.factor(Time)  1 734353  734353 333.120 &lt; 2e-16 ***\nResiduals       90 198402    2204                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask G.7: Do it now!\n\n\n\n\nWhat question are we asking with the above line of code? (/3)\n\nWhat is the answer? (/2)\n\nWhy did we wrap Time in as.factor()? (/2)\n\n\n\n\nIt is also possible to look at what the interaction effect between grouping variables (i.e. in this case the effect of time on diet—does the effect of time depend on which diet we are looking at?), and not just within the individual grouping variables. To do this we replace the + in our formula with *:\n\nsummary(aov(weight ~ Diet * as.factor(Time),\n            data = filter(chicks, Time %in% c(4, 21))))\n\n                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nDiet                  3  40914   13638   6.968 0.000298 ***\nas.factor(Time)       1 582221  582221 297.472  &lt; 2e-16 ***\nDiet:as.factor(Time)  3  25530    8510   4.348 0.006684 ** \nResiduals            86 168322    1957                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTask G.8: Do it now!\n\n\n\nHow do these results differ from the previous set? (/3)\n\n\nOne may also run a post-hoc Tukey test on these results the same as for a single factor ANOVA:\n\nTukeyHSD(aov(weight ~ Diet * as.factor(Time),\n             data = filter(chicks, Time %in% c(20, 21))))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21)))\n\n$Diet\n         diff        lwr       upr     p adj\n2-1  36.18030  -9.301330  81.66194 0.1663037\n3-1  90.63030  45.148670 136.11194 0.0000075\n4-1  62.25253  15.223937 109.28111 0.0045092\n3-2  54.45000   3.696023 105.20398 0.0305957\n4-2  26.07222 -26.072532  78.21698 0.5586643\n4-3 -28.37778 -80.522532  23.76698 0.4863940\n\n$`as.factor(Time)`\n          diff       lwr      upr     p adj\n21-20 8.088223 -17.44017 33.61661 0.5303164\n\n$`Diet:as.factor(Time)`\n                diff        lwr        upr     p adj\n2:20-1:20  35.188235  -40.67378 111.050253 0.8347209\n3:20-1:20  88.488235   12.62622 164.350253 0.0111136\n4:20-1:20  63.477124  -14.99365 141.947897 0.2035951\n1:21-1:20   7.338235  -58.96573  73.642198 0.9999703\n2:21-1:20  44.288235  -31.57378 120.150253 0.6116081\n3:21-1:20  99.888235   24.02622 175.750253 0.0023872\n4:21-1:20  68.143791  -10.32698 146.614563 0.1371181\n3:20-2:20  53.300000  -31.82987 138.429869 0.5234263\n4:20-2:20  28.288889  -59.17374 115.751515 0.9723470\n1:21-2:20 -27.850000 -104.58503  48.885027 0.9486212\n2:21-2:20   9.100000  -76.02987  94.229869 0.9999766\n3:21-2:20  64.700000  -20.42987 149.829869 0.2732059\n4:21-2:20  32.955556  -54.50707 120.418182 0.9377007\n4:20-3:20 -25.011111 -112.47374  62.451515 0.9862822\n1:21-3:20 -81.150000 -157.88503  -4.414973 0.0305283\n2:21-3:20 -44.200000 -129.32987  40.929869 0.7402877\n3:21-3:20  11.400000  -73.72987  96.529869 0.9998919\n4:21-3:20 -20.344444 -107.80707  67.118182 0.9960548\n1:21-4:20 -56.138889 -135.45396  23.176184 0.3619622\n2:21-4:20 -19.188889 -106.65152  68.273738 0.9972631\n3:21-4:20  36.411111  -51.05152 123.873738 0.8984019\n4:21-4:20   4.666667  -85.06809  94.401428 0.9999998\n2:21-1:21  36.950000  -39.78503 113.685027 0.8067041\n3:21-1:21  92.550000   15.81497 169.285027 0.0075185\n4:21-1:21  60.805556  -18.50952 140.120628 0.2629945\n3:21-2:21  55.600000  -29.52987 140.729869 0.4679025\n4:21-2:21  23.855556  -63.60707 111.318182 0.9896157\n4:21-3:21 -31.744444 -119.20707  55.718182 0.9486128\n\n\n\n\n\n\n\n\nTask G.9: Do it now!\n\n\n\nYikes! That’s a massive amount of results. What does all of this mean, and why is it so verbose? (/5)\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nTo summarise t-tests, single-factor (1-way) and multifactor (2- or 3-way, etc.) ANOVAs:\n\nA t-test is applied to situations where one wants to compare the means of only two groups of a response variable within one categorical independent variable (we say a factor with two levels).\nA 1-way ANOVA also looks at the means of a response variable belonging to one categorical independent variable, but the categorical response variable has more than two levels in it.\nFollowing on from there, a 2-way ANOVA compares the means of response variables belonging to all the levels within two categorical independent variables (e.g. Factor 1 might have three levels, and Factor 2 five levels). In the simplest formulaton, it does so by looking at the main effects, which is the group differences between the three levels of Factor 1 and disregarding the contribution due to the group membership to Factor 2, and also the group differences amongst the levels of Factor 2 but disregarding the group membership of Factor 1. In addition to looking at the main effects, a 2-way ANOVA can also consider the interaction (or combined effect) of Factors 1 and 2 in influencing the means.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#wilcoxon-rank-sum-test",
    "href": "BCB744/basic_stats/08-anova.html#wilcoxon-rank-sum-test",
    "title": "8. ANOVA",
    "section": "\n4.1 Wilcoxon rank sum test",
    "text": "4.1 Wilcoxon rank sum test\nThe non-parametric version of a t-test is a Wilcox rank sum test. To perform this test in R we may again use compare_means() and specify the test we want:\n\ncompare_means(weight ~ Diet,\n              data = filter(chicks, Time == 0,\n                            Diet %in% c(1, 2)),\n              method = \"wilcox.test\")\n\n# A tibble: 1 × 8\n  .y.    group1 group2     p p.adj p.format p.signif method  \n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n1 weight 1      2      0.235  0.23 0.23     ns       Wilcoxon\n\n\nWhat do our results show?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#kruskall-wallis-rank-sum-test",
    "href": "BCB744/basic_stats/08-anova.html#kruskall-wallis-rank-sum-test",
    "title": "8. ANOVA",
    "section": "\n4.2 Kruskall-Wallis rank sum test",
    "text": "4.2 Kruskall-Wallis rank sum test\n\n4.2.1 Single factor\nThe non-parametric version of an ANOVA is a Kruskall-Wallis rank sum test. As you may have by now surmised, this may be done with compare_means() as seen below:\n\ncompare_means(weight ~ Diet,\n              data = filter(chicks, Time == 0),\n              method = \"kruskal.test\")\n\n# A tibble: 1 × 6\n  .y.        p p.adj p.format p.signif method        \n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         \n1 weight 0.475  0.48 0.48     ns       Kruskal-Wallis\n\n\nAs with the ANOVA, this first step with the Kruskall-Wallis test is not the last. We must again run a post-hoc test on our results. This time we will need to use pgirmess::kruskalmc(), which means we will need to load a new library.\n\nlibrary(pgirmess)\n\nkruskalmc(weight ~ Diet, data = filter(chicks, Time == 0))\n\nMultiple comparison test after Kruskal-Wallis \nalpha: 0.05 \nComparisons\n    obs.dif critical.dif stat.signif\n1-2    6.95     14.89506       FALSE\n1-3    6.90     14.89506       FALSE\n1-4    4.15     14.89506       FALSE\n2-3    0.05     17.19933       FALSE\n2-4    2.80     17.19933       FALSE\n3-4    2.75     17.19933       FALSE\n\n\nLet’s consult the help file for kruskalmc() to understand what this print-out means.\n\n4.2.2 Multiple factors\nThe water becomes murky quickly when one wants to perform multiple factor non-parametric comparison of means tests. To that end, we will not cover the few existing methods here. Rather, one should avoid the necessity for these types of tests when designing an experiment.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#the-sa-time-data",
    "href": "BCB744/basic_stats/08-anova.html#the-sa-time-data",
    "title": "8. ANOVA",
    "section": "\n4.3 The SA time data",
    "text": "4.3 The SA time data\n\nsa_time &lt;- as_tibble(read_csv(\"../../data/snakes.csv\",\n                              col_types = list(col_double(),\n                                               col_double(),\n                                               col_double())))\nsa_time_long &lt;- sa_time %&gt;% \n  gather(key = \"term\", value = \"minutes\") %&gt;% \n  filter(minutes &lt; 300) %&gt;% \n  mutate(term = as.factor(term))\n\nmy_comparisons &lt;- list( c(\"now\", \"now_now\"),\n                        c(\"now_now\", \"just_now\"),\n                        c(\"now\", \"just_now\") )\n\nggboxplot(sa_time_long, x = \"term\", y = \"minutes\",\n          color = \"term\", palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n          add = \"jitter\", shape = \"term\")\n\n\n\n\n\n\nFigure 4: Time is not a limited resource in South Africa.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/08-anova.html#snakes",
    "href": "BCB744/basic_stats/08-anova.html#snakes",
    "title": "8. ANOVA",
    "section": "\n5.1 Snakes!",
    "text": "5.1 Snakes!\nThese data could be analysed by a two-way ANOVA without replication, or a repeated measures ANOVA. Here I will analyse it by using a two-way ANOVA without replication.\nPlace and Abramson (2008) placed diamondback rattlesnakes (Crotalus atrox) in a ‘rattlebox,’ a box with a lid that would slide open and shut every 5 minutes. At first, the snake would rattle its tail each time the box opened. After a while, the snake would become habituated to the box opening and stop rattling its tail. They counted the number of box openings until a snake stopped rattling; fewer box openings means the snake was more quickly habituated. They repeated this experiment on each snake on four successive days, which is treated as an influential variable here. Place and Abramson (2008) used 10 snakes, but some of them never became habituated; to simplify this example, data from the six snakes that did become habituated on each day are used.\nFirst, we read in the data, making sure to convert the column named day to a factor. Why? Because ANOVAs work with factor independent variables, while day as it is encoded by default is in fact a continuous variable.\n\nsnakes &lt;- read_csv(\"../../data/snakes.csv\")\nsnakes$day = as.factor(snakes$day)\n\nThe first thing we do is to create some summaries of the data. Refer to the summary statistics Chapter.\n\nsnakes.summary &lt;- snakes %&gt;% \n  group_by(day, snake) %&gt;% \n  summarise(mean_openings = mean(openings),\n            sd_openings = sd(openings)) %&gt;% \n  ungroup()\nsnakes.summary\n\n# A tibble: 24 × 4\n   day   snake mean_openings sd_openings\n   &lt;fct&gt; &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 1     D1               85          NA\n 2 1     D11              40          NA\n 3 1     D12              65          NA\n 4 1     D3              107          NA\n 5 1     D5               61          NA\n 6 1     D8               22          NA\n 7 2     D1               58          NA\n 8 2     D11              45          NA\n 9 2     D12              27          NA\n10 2     D3               51          NA\n# ℹ 14 more rows\n\n\n\n\n\n\n\n\nTask G.9: Do it now!\n\n\n\n\nSomething seems… off. What’s going on here? Please explain this outcome.\n\n\n\nTo fix this problem, let us ignore the grouping by both snake and day.\n\nsnakes.summary &lt;- snakes %&gt;% \n  group_by(day) %&gt;% \n  summarise(mean_openings = mean(openings),\n            sd_openings = sd(openings)) %&gt;% \n  ungroup()\nsnakes.summary\n\n# A tibble: 4 × 3\n  day   mean_openings sd_openings\n  &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 1              63.3        30.5\n2 2              47          12.2\n3 3              34.5        26.0\n4 4              25.3        18.1\n\n\nRmisc::summarySE() offers a convenience function if your feeling less frisky about calculating the summary statistics yourself:\n\nlibrary(Rmisc)\nsnakes.summary2 &lt;- summarySE(data = snakes,\n                             measurevar = \"openings\",\n                             groupvars = c(\"day\"))\nsnakes.summary2\n\n  day N openings       sd        se       ci\n1   1 6 63.33333 30.45434 12.432931 31.95987\n2   2 6 47.00000 12.21475  4.986649 12.81859\n3   3 6 34.50000 25.95958 10.597956 27.24291\n4   4 6 25.33333 18.08498  7.383164 18.97903\n\n\nNow we turn to some visual data summaries (Figure 5).\n\nggplot(data = snakes, aes(x = day, y = openings)) +\n  geom_segment(data = snakes.summary2, aes(x = day, xend = day,\n                                           y = openings - ci,\n                                           yend = openings + ci,\n                                           colour = day),\n              size = 2.0, linetype = \"solid\", show.legend = FALSE) +\n  geom_boxplot(aes(fill = day), alpha = 0.3, show.legend = FALSE) + \n  geom_jitter(width = 0.05) +\n  theme_pubclean()\n\n\n\n\n\n\nFigure 5: Boxplots showing the change in the snakes’ habituation to box opening over time.\n\n\n\n\nWhat are our null hypotheses?\n\n\nH0 There is no difference between snakes with respect to the number of openings at which they habituate.\n\nH0 There is no difference between days in terms of the number of openings at which the snakes habituate.\n\nFit the ANOVA model to test these hypotheses:\n\nsnakes.aov &lt;- aov(openings ~ day + snake, data = snakes)\nsummary(snakes.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nday          3   4878  1625.9   3.320 0.0487 *\nsnake        5   3042   608.4   1.242 0.3382  \nResiduals   15   7346   489.7                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow we need to test of the assumptions hold true (i.e. erros are normally distributed and heteroscedastic) (Figure 6). Also, where are the differences (Figure 7)?\n\npar(mfrow = c(1, 2))\n# Checking assumptions...\n# make a histogram of the residuals;\n# they must be normal\nsnakes.res &lt;- residuals(snakes.aov)\nhist(snakes.res, col = \"red\")\n\n# make a plot of residuals and the fitted values;\n# # they must be normal and homoscedastic\nplot(fitted(snakes.aov), residuals(snakes.aov), col = \"red\")\n\n\n\n\n\n\nFigure 6: Exploring the assumptions visually.\n\n\n\n\n\nsnakes.tukey &lt;- TukeyHSD(snakes.aov, which = \"day\", conf.level = 0.90)\nplot(snakes.tukey, las = 1, col = \"red\")\n\n\n\n\n\n\nFigure 7: Exploring the differences between days.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/11-decision_guide.html",
    "href": "BCB744/basic_stats/11-decision_guide.html",
    "title": "11. Parametric Tests",
    "section": "",
    "text": "In this Chapter\n\n\n\nNew users sometimes find it challenging to select the right statistical test for their data. Here, I provide guides that might help you make the right choice.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "11. Parametric Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/11-decision_guide.html#the-importance-of-selecting-the-correct-test",
    "href": "BCB744/basic_stats/11-decision_guide.html#the-importance-of-selecting-the-correct-test",
    "title": "11. Parametric Tests",
    "section": "The importance of selecting the correct test",
    "text": "The importance of selecting the correct test\nSelecting the appropriate inferential statistical method is important for correctly and accurately analysing the outcome of our sampling campaign or experimental treatment. The decision typically hinges on the type and distribution of our data, our research question or hypothesis, and the assumptions each test requires.\nThe main decision-making process starts with the following considerations:\n\nResearch Question/Hypothesis: Start by clearly defining what we’re trying to investigate or determine. Are we comparing group means? Investigating relationships between variables? Or assessing associations between categorical variables?\nType and Distribution of Data: Identify the types of variables we have (e.g., continuous, ordinal, nominal) and check the distribution of our data (e.g., normal vs. non-normal).\n\nThe foundation of the scientific process is hypotheses. These are the propositions or expectations that we set out to test. A hypothesis provides a direction to our research and guides us towards what we aim to prove.\nThe next step is to anticipate the nature of the data that our research will generate. This involves understanding not just the type of data (e.g., continuous, categorical), but also its potential distribution and variability. Such foresight stems from a clear understanding of the research design, the instruments we use, and the population we study. This might seem daunting to a novice, but experienced scientists should be able to do this with ease.\nOnce we have a firm grip on our hypotheses and a clear anticipation of the nature of our forthcoming data, we are in a position to choose the most suitable statistical inference test. Different tests are designed to handle different types of data and answer varied research questions. For instance, a t-test might be appropriate for comparing the means of two groups, while a linear model might shed insight into cause-effect relationships.\nWell-defined scientific enquiry should offer clarity. With this clarity, we can predict the statistical tests to use, even before the actual data are available. This is not just an academic exercise; it reflects thorough planning and a deep understanding of the research process. Knowing which tests to employ ahead of time also helps one to design the research methodology and ensure the data collected will indeed serve the purpose of the study.\nA robust scientific approach requires us to anticipate the nature of our data and understand our hypotheses thoroughly. This ensures that, even before our data are available, we’re prepared with the appropriate statistical tools to analyse it and draw meaningful conclusions.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "11. Parametric Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/11-decision_guide.html#a-detailed-breakdown-of-inferential-statistical-tests",
    "href": "BCB744/basic_stats/11-decision_guide.html#a-detailed-breakdown-of-inferential-statistical-tests",
    "title": "11. Parametric Tests",
    "section": "A detailed breakdown of inferential statistical tests",
    "text": "A detailed breakdown of inferential statistical tests\nHere is a moderately detailed breakdown of the tests you’ll encounter in this module. Also included are tests that I have not (yet) covered, including Generalised Linear Models (GLMs), Generalised Additive Models (GAMs), and non-Linear Regressions.\n\nt-tests:\n\nUsed to compare means between two groups.\nAssumes independent samples, normally distributed data, and homogeneity of variance.\nIf the data are paired (e.g., before and after scores from the same group), then a paired t-test is used.\nIf assumptions are not valid, use the Wilcoxon rank-sum (in lieu of a paired sample t-test) test or Mann-Whitney U test (in lieu of a Student or Welch’s t-test).\n\nANOVA (Analysis of Variance):\n\nUsed to compare means of three or more independent groups.\nAssumes independence, normal distribution, and homogeneity of variance across groups.\nIf assumptions are violated, consider a non-parametric equivalent (e.g., Kruskal-Wallis).\n\nANCOVA (Analysis of Covariance):\n\nExtends ANOVA by including one or more continuous covariates that might account for variability in the dependent variable.\nUsed to compare means of independent groups while statistically controlling for the effects of other continuous variables (covariates).\nIf assumptions are violated, consider a non-parametric equivalent (e.g., Kruskal-Wallis).\n\nChi-square Analysis:\n\nUsed for testing relationships between categorical variables.\nAssumes that observations are independent and that there are adequate expected frequencies in each cell of a contingency table.\n\nLinear Regression:\n\nExamines the linear relationship between a continuous dependent variable and one or more independent variables.\nCausality is typically implied (independent variable influences the outcome or measurement).\nAssumes linearity, independence of observations, homoscedasticity, and normally distributed residuals.\n\nGeneralised Linear Model (GLM):\n\nAn extension of linear regression that allows for response variables with error distribution models other than a normal distribution (e.g., Poisson, binomial).\nUseful when dealing with non-normally distributed dependent variables.\n\nnon-Linear Regression:\n\nUsed to model non-linear relationships which are described by cause-effect responses that are underpinned by well-defined mechanistic models or responses, often with parameter estimates that relate to components of the mechanistic model.\nAssumes independence of observations, homoscedasticity, and normally distributed residuals.\n\nGeneralised Additive Models (GAM):\n\nUsed to model non-linear relationships. It’s an extension of GLM but doesn’t restrict the relationship to be linear.\nAllows for flexible curves to be fit to data.\n\nCorrelations:\n\nUsed to examine the strength and direction of the linear relationship between two continuous variables.\n\nPearson’s: Assumes a linear relationship and that both variables are normally distributed.\nSpearman’s: Used when the relationship is monotonic but not necessarily linear, or when one/both of the variables are ordinal.\nKendall’s: Similar to Spearman’s but based on the concordant and discordant pairs. Useful for smaller sample sizes or when there are many tied ranks.\n\n\n\nRemember to always visualise your data and examine it thoroughly before selecting a test. If unsure, consider consulting with a statistician who can guide the decision-making process.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "11. Parametric Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/11-decision_guide.html#a-tabulated-view",
    "href": "BCB744/basic_stats/11-decision_guide.html#a-tabulated-view",
    "title": "11. Parametric Tests",
    "section": "A tabulated view",
    "text": "A tabulated view\nA tabulated summary of these tests is included below. Refer to 12. Non-parametric statistical tests at a glance for information about non-parametric tests to use when assumptions fail.\n\n\n\nStatistic\nApplication\nData Requirements\nAssumptions\n\n\n\n\nt-tests\nCompare means between two groups.\nContinuous dependent, categorical independent (2 groups).\nIndependent samples, normal distribution, homogeneity of variance.\n\n\nANOVA\nCompare means of three or more independent groups.\nContinuous dependent, categorical independent (3+ groups).\nIndependence, normal distribution, homogeneity of variance across groups.\n\n\nANCOVA\nCompare means while controlling for other continuous variables.\nContinuous dependent, categorical and continuous independents.\nSame as ANOVA plus linearity and homogeneity of regression slopes.\n\n\nChi-square Analysis\nTest relationships between categorical variables.\nCategorical variables.\nIndependent observations, adequate expected frequencies in each cell.\n\n\nLinear Regression\nExamine linear relationship between continuous variables.\nContinuous dependent and independent(s).\nLinearity, independence, homoscedasticity, normally distributed residuals.\n\n\nNon-linear Regression\nModel relationships that follow a specific non-linear equation.\nContinuous dependent and independent(s).\nSpecific to the equation/form used, residuals should be random and normally distributed around zero.\n\n\nGeneralised Linear Model (GLM)\nModel relationships for non-normally distributed dependent variables.\nDepending on link function (e.g., continuous, binary).\nDepending on family (e.g., binomial: binary dependent; Poisson: count dependent).\n\n\nGeneralised Additive Models (GAM)\nModel non-linear relationships flexibly.\nContinuous dependent, continuous/categorical independents.\nDepending on response distribution but more flexible regarding the form of the predictors.\n\n\nPearson’s Correlation\nMeasure linear association between two continuous variables.\nTwo continuous variables.\nBoth variables should be normally distributed, linear relationship.\n\n\nSpearman’s Correlation\nMeasure monotonic relationship between two ordinal/continuous variables.\nTwo ordinal/continuous variables.\nMonotonic relationship. Doesn’t assume normality.\n\n\nKendall’s Tau\nMeasure association between two ordinal variables.\nTwo ordinal variables.\nNo specific distributional assumptions. Measures strength of association based on concordant/discordant pairs.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "11. Parametric Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/12-glance.html",
    "href": "BCB744/basic_stats/12-glance.html",
    "title": "12. Non-Parametric Tests",
    "section": "",
    "text": "In Chapters 7, 8, 9, and 10 we have seen t-tests, ANOVAs, simple linear regressions, and correlations. These tests may be substituted with non-parametric tests if our assumptions about our data fail us. The most commonly encountered non-parametric methods include the following:\n\nWilcoxon rank-sum test The test is used when the two samples being compared are related, meaning that each observation in one sample is paired with a corresponding observation in the other sample. The test is designed to detect whether there is a difference between the paired observations. Specifically, the Wilcoxon signed-rank test ranks the absolute differences between the pairs of observations, and then compares the sum of the ranks for positive differences to the sum of the ranks for negative differences. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the null hypothesis that there is no difference between the paired observations. Use the Wilcoxon test as a non-parametric substitute for a paired sample t-test. See wilcox.test().\nMann-Whitney \\(U\\) test This test is used when the two samples being compared are independent, meaning that there is no pairing between observations in the two samples. The test is designed to detect whether there is a difference between the two groups based on the ranks of the observations. Specifically, the Mann-Whitney \\(U\\) test ranks all observations from both samples, combines the ranks across the two samples, and calculates a test statistic (\\(U\\)) that indicates whether one sample tends to have higher ranks than the other sample. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference between the two groups. Use this test in stead of a one- or two-sample t-test when assumptions of normality or homoscedasticity are not met. See wilcox.test().\nKruskal-Wallis test The Kruskal-Wallis test is a non-parametric statistical test used to compare three or more independent groups on a continuous outcome variable. The test is designed to detect whether there is a difference in the medians of the groups. The Kruskal-Wallis test works by ranking all the observations from all the groups, then calculating a test statistic (\\(H\\)) that measures the degree of difference in the ranked values between the groups. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference in the medians of the groups. The Kruskal-Wallis test is often used as a non-parametric alternative to the one-way ANOVA. See kruskal.test().\nFriedman test This test is a non-parametric statistical test used to compare three or more related (i.e. not-independent) groups on a continuous outcome variable. The test is designed to detect whether there is a difference in the medians of the groups. The Friedman test works by ranking all the observations within each group, then calculating a test statistic (\\(\\chi^2\\)) that measures the degree of difference in the ranked values between the groups. The test produces a \\(p\\)-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference in the medians of the groups. The Friedman test is often used as a non-parametric alternative to the repeated measures ANOVA. You can use the friedman_._test() in the rstatix package or the friedman.test() in Base R.\n\nTables 1 and 2 summarise common parametric and non-parametric statistical tests, along with a brief explanation of each test and the most common R function used to perform the test. Non-parametric tests are robust alternatives to parametric tests when the assumptions of the parametric test are not met. Also provided is additional information on the nature of the independent (IV) and dependent variables (DV) for each test.\n\nTable 1: When our data are normal with equal variances across groups, choose the suitable parametric test\n\n\n\nStatistical Test\nExplanation\nVariables\nR Function\nNon-Parametric Substitute\n\n\n\n\nParametric Tests\n\n\n\n\n\n\nPaired-sample t-test\nTests if the difference in means between paired samples is significantly different from zero. Assumes normality and equal variances.\nContinuous (DV)\nt.test(..., var.equal = TRUE)\nWilcoxon signed-rank test\n\n\nStudent’s t-test\nTests if the means of two independent groups are significantly different. Assumes normality and equal variances.\nContinuous (DV) and categorical (IV)\nt.test(..., var.equal = TRUE)\nMann-Whitney U test\n\n\nWelch’s t-test (unequal variances)\nUse this test when data are normal but variances differ between the two groups. It can be used for paired- and two-sample t-tests.\nContinuous (DV) and categorical (IV)\nt.test()\nMann-Whitney U test or Wilcoxon signed-rank test\n\n\nANOVA (one-way ANOVA only; ANOVAs with interactions do not have non-parametric tests)\nTests if the means of three or more independent groups are significantly different. Assumes normality, equal variances, and independence.\nContinuous (DV) and categorical (IV)\naov()\nKruskal-Wallis test\n\n\nANOVA with Welch’s approximation of variances\nTests if the means of three or more independent groups are significantly different. Assumes normality but variances may differ.\nContinuous (DV) and categorical (IV)\noneway.test()\nKruskal-Wallis test\n\n\nRegression Analysis\nModels the relationship between two continuous variables. Assumes linearity, normality, and equal variances of errors.\nContinuous (DV) and continuous (IV)\nlm()\nGeneralised Linear Models\n\n\nPearson Correlation\nMeasures the strength and direction of the linear relationship between two continuous variables. Assumes normality and linearity.\nContinuous (DV) and continuous (IV)\ncor.test()\nSpearman’s \\(\\rho\\) or Kendall’s \\(\\tau\\) rank correlation\n\n\n\n\nTable 2: Should the data not be normal and/or are heteroscedastic, substitute the parametric test with a non-parametric option.\n\n\n\n\nStatistical Test\nExplanation\nVariables\nR Function\nParametric Equivalent\n\n\n\n\nNon-Parametric Tests\n\n\n\n\n\n\nWilcoxon signed-rank test\nTests if the medians of two related samples are significantly different. Does not assume normality.\nContinuous (DV)\nwilcox.test()\nPaired-sample t-test\n\n\nMann-Whitney U test\nTests if the medians of two independent groups are significantly different. Does not assume normality or equal variances.\nContinuous (DV) and categorical (IV)\nwilcox.test()\nStudent’s t-test\n\n\nKruskal-Wallis test\nTests if the medians of three or more independent groups are significantly different. Does not assume normality or equal variances.\nContinuous (DV) and categorical (IV)\nkruskal.test()\nANOVA, or ANOVA with Welch’s approximation of variances\n\n\nFriedman test\nTests if the medians of three or more related samples are significantly different. Use when assumption of independence of data cannot be accepted and data might therefore be non-normal (such as repeated measures or unreplicated full-block design).\nContinuous (DV) and categorical (IV)\nfriedman.test()\nRepeated measures ANOVA\n\n\nSpearman’s rank correlation\nMeasures the strength and direction of the monotonic relationship between two continuous variables. Does not assume normality or linearity.\nContinuous (DV) and continuous (IV)\ncor.test(method = \"spearman\")\nPearson correlation\n\n\n\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {12. {Non-Parametric} {Tests}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/basic_stats/12-glance.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 12. Non-Parametric Tests. http://tangledbank.netlify.app/BCB744/basic_stats/12-glance.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "12. Non-Parametric Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/05-inference.html",
    "href": "BCB744/basic_stats/05-inference.html",
    "title": "5. Statistical Inference",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nThe concept of inferential statistics\nHypothesis testing\nProbabilities\nAssumptions and parametric statistics\n\nNormality and the Shapiro-Wilk test\nHomoscedasticity\n\n\n\n\n\n\n\n\n\n\n\nTasks to complete in this Chapter\n\n\n\n\nNone\n\n\n\nIntroduction\nWe have seen in Chapter 2 and Chapter 3 how to summarise, describe, and visualise our data—these processes form part of descriptive statistics. The next step is the process of conducting inferential statistics.\nInferential statistics is a branch of statistics that focuses on drawing conclusions and making generalisations about a larger population based on the analysis of a smaller, representative sample. This is particularly valuable in research situations where it is impractical or impossible to collect data from every member of a population—i.e. all of biology and ecology. By employing probabilistic reasoning, inferential statistics enable us to estimate population parameters, make predictions, and test hypotheses with a certain level of confidence.\nOne of the key aspects of inferential statistics is the concept of sampling variability. Since samples are only a subset of the population, they imperfectly represent whole populations, leading to variations in the estimates of population parameters (repeatedly drawing samples at random from a population will result in slightly different values for key statistical parameters, such as the sample mean and variance). Inferential statistics accounts for this variability by providing measures of uncertainty, such as confidence intervals and margins of error, which convey the range within which the true population parameter is likely to fall.\nParametric statistics form the foundation of inferential statistics, and they are used to make inferences about population parameters based on sample data. These statistics assume that the data are generated from a specific probability distribution—the normal distribution. An alternative to parametric tests is non-parametric statistics, and we shall hear more about it in Chapter 6.\nThe most common parametric statistics used in inferential statistics include:\n\nt-tests (Chapter 7) used to determine if there is a significant difference between the means of two groups of continuous dependent (response) variables.\nANOVA (Chapter 8) used to determine if there is a significant difference between the means of three or more groups of continuous variables.\nRegression analysis (Chapter 9) used to model the relationship between one or more continuous predictor variables and a continuous response variable.\nPearson correlation (Chapter 10) used to measure the linear association or relationships between two continuous variables.\nChi-squared tests used to determine if there is a significant association between two categorical variables.\n\nThese tests typically involve the calculation of a test statistic and the comparison of this value with a critical value and then establishing a p-value to determine whether the results are statistically significant or likely due to chance. These methods are included within a subset of inferential statistics called probablilistic statistics.\n\n\n\n\n\n\nProbabilistic and Bayesian statistics\n\n\n\nProbabilistic and Bayesian statistics are two related but distinct branches of statistics that offer tools for modelling, analysing, and drawing inferences from complex data sets. At their core, both approaches rely on the use of probability theory to quantify uncertainty and variability in data, but they differ in their assumptions about the nature of this uncertainty and how it should be modelled.\nProbabilistic statistics is a classical approach that assumes that all sources of variability in a data set can be described by a fixed set of probability distributions, such as the normal distribution or the Poisson distribution. These distributions are characterised by a set of parameters, such as the mean and standard deviation, that can be estimated from the data. Probabilistic statistics is widely used in fields such as biology, physics, and economics, where the data are often assumed to be generated by a deterministic process with some random noise present. In contrast, Bayesian statistics takes a more flexible approach to modelling uncertainty, allowing for uncertainty in both the parameters of the model and the underlying distribution itself. Bayesian methods are useful when dealing with complex and high-dimensional data sets, with lots of unknowns and assumptions, and have become increasingly popular in fields such as ecology and machine learningin recent years.\n\n\nHypothesis testing\nHypothesis testing is a fundamental aspect of the scientific method and is used to evaluate the validity of scientific hypotheses. A hypothesis is a proposed explanation for a phenomenon or observation that can be tested through experimentation or observation. To test a hypothesis, we design experiments or collect data, which we analyse using inferential statistical methods to determine whether the data support or refute the hypothesis.\nTwo competing hypotheses about the data are set up at the onset of hypothesis testing: a null hypothesis (H0) and an alternative hypothesis (Ha). The null hypothesis typically represents the status quo or a default assumption (a statement of no difference), while the alternative hypothesis represents a new or alternative explanation for the data.\nThe goal is to make objective and evidence-based conclusions about the validity of the hypothesis, and to determine whether it can be accepted or rejected based on the available evidence. Hypothesis testing is a critical tool for advancing scientific knowledge and understanding, as it allows us to identify the most promising hypotheses and develop more accurate models of the natural world. Effectively, scientific progress can only be made if the null hypothesis is rejected and the alternative hypothesis accepted.\n\n\n\n\n\n\nHypotheses and theories\n\n\n\nHypotheses and theories are both important components of the scientific process, but they serve different functions and represent distinct levels of understanding.\nA hypothesis is a tentative explanation or proposition for a specific phenomenon, often based on observations and grounded in existing knowledge. It is a testable statement that can be either supported or refuted through further observation, experimentation, and hypothesis testing through the application of inferential statistics. Hypotheses are typically formulated at the beginning of a research study. They guide the design of experiments and the collection of data. Hypotheses help us make predictions and answer specific questions about the phenomena under investigation. If a hypothesis is repeatedly tested and confirmed through various experiments, it may gain credibility and contribute to the development of a theory.\nA theory is a well-substantiated explanation for a broad range of observed phenomena that has been consistently supported by a large body of evidence. Theories are more comprehensive and mature than hypotheses, as they integrate and generalise multiple related hypotheses and empirical findings to explain complex phenomena. They are built upon a solid foundation of tested hypotheses and provide a coherent framework that enables us to make accurate predictions, generate new hypotheses, and further advance our understanding of the natural world.\n\n\nAt the heart of many basic scientific inquiries, and hence hypotheses, is the simple question “Is A different from B?” The scientific notation for this question is:\n\n\nH0: Group A is not different from Group B\n\nHa: Group A is different from Group B\n\nMore formally, one would say:\n\n\n\\(H_{0}: \\bar{A} = \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} \\neq \\bar{B}\\)\n\n\n\\(H_{0}: \\bar{A} \\leq \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} &gt; \\bar{B}\\)\n\n\n\\(H_{0}: \\bar{A} \\geq \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} &lt; \\bar{B}\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nHypothesis 1 is a two-sided t-test and hypotheses 2 and 3 are one-sided tests. This will make sense once you have studied the material in Chapter 7 about t-tests.\n\n\nProbabilities\nThe p-value (the significance level, \\(\\alpha\\)) is the probability of finding the observed (or measured) outcome to be more extreme (i.e. very different) than that suggested by the null hypothesis (\\(H_{0}\\)). Typically, biologists set the p-value at \\(\\alpha \\leq 0.05\\)—in other words, the measured outcome of our experiment only has a 1 in 20 chance of being the same as that of the reference (or control) group. So, when the p-value is \\(\\leq\\) 0.05, for example, we say that there is a very good probability that our experimental treatment resulted in an outcome that is very different (we say statistically significantly different) from the measurement obtained from the group to which the treatment had not been applied—in this case we do not accept \\(H_{0}\\) and by necessity \\(H_{a}\\) becomes true.\n\n\n\n\n\n\nThe \\(H_{0}\\)\n\n\n\nIn inferential statistics, when conducting hypothesis testing, we don’t “accept” or “prove” the null hypothesis. Instead, we either “reject” or “fail to reject” the null hypothesis based on the evidence provided by our sample data. So, it doesn’t mean the null hypothesis is true, just that there isn’t enough evidence in your sample to reject it.\n\n\nThe choice of p-value at which we reject \\(H_{0}\\) is arbitrary and exists by convention only. Traditionally, the 5% cut-off (i.e. less than 1 in 20 chance of being wrong or \\(p \\leq 0.05\\)) is used in biology, but sometimes the threshold is set at 1% or 0.1% (0.01 or 0.001, respectively), particularly in the medical sciences where avoiding false positives or negatives could be a public health concern. However, more and more biologists shy away from the p-value as they argue that it can give a false sense of security.\n\n\nStatistical tests indicate a statistically significant outcome (the \\(p \\leq 0.05\\)) and we accept the \\(H_{a}\\), or it does not (\\(p \\gt 0.05\\)) and we do not reject the \\(H_{0}\\). There’s no “almost significant”. It is, or it is not. \nWe generally refer to \\(p \\leq 0.05\\) as being statistically significant. Statistically highly significant is seen at as \\(p \\leq 0.001\\). In the first instance there is a less than 1 in 20 chance that our experimental sample is not different from the reference group, and in the second instance there is a less than 1 in a 1000 chance tat they are the same. This says something about the acceptable error rates: there is a better chance the \\(H_{0}\\) may in fact be falsely accepted or rejected when the p-value is set at 0.05 than at 0.001.\n\n\n\n\n\n\nType I and Type II errors\n\n\n\nA Type I error is the false rejection of the \\(H_{0}\\) hypothesis (i.e. in reality we should not be rejecting it, but the p-value suggests that we must). A Type II error, on the other hand, is the false acceptance of the \\(H_{0}\\) hypothesis (i.e. the p-value suggests we should not reject the \\(H_{0}\\), but in fact we must). When a statistical test results in a p-value of, say, \\(p \\leq 0.05\\) we would conclude that our experimental sample is statistically different from the reference group, but probabilistically there is a 1 in 20 change that this outcome is incorrect (i.e. the difference was arrived at by random chance only).\nThe choice of p-value threshold depends on several factors, including the nature of the data, the research question, and the desired level of statistical significance. In medical sciences, where the consequences of false positive or false negative results can have significant implications for patient health, a more stringent threshold is often used. A p-value of 0.001 is commonly used in medical research to minimise the risk of Type I errors (rejecting the null hypothesis when it is actually true) and to ensure a high level of statistical confidence in the results.\nIn biological sciences, the consequences of false positive or false negative results may be less severe, and a p-value of 0.05 is often considered an appropriate threshold for statistical significance. However, it is important to note that the choice of p-value threshold is ultimately subjective and should be based on a careful consideration of the research question, the nature of the data, and the potential consequences of false positive or false negative results.\n\n\nTo conclude, when \\(p \\gt 0.05\\) there is a lack of compelling evidence to suggest that our experiment has had an influential effect of the hypothesised outcome—even if a graphs hints at differences between groups. When \\(p \\leq 0.05\\), however, there is a good probability that the experiment (etc.) has had an effect, and that the effect is likely not due to random chance. In this case we have a statistically significant finding.\nAssumptions\nIrrespective of the kind of statistical test we wish to perform, we have to make a couple of important assumptions that are not guaranteed to be true. In fact, these assumptions are often violated because real data, especially biological data, are messy.\nThe issue of assumption is an important one, and one that we need to understand well. This is will be the purpose of Chapter 6, where we will learn about how to test the assumptions, and discover what to do when it does.\nConclusion\nWe use inferential statistics to draw conclusions about a population based on a sample of data. By using probability theory and statistical inference, we can make inferences about the characteristics of a larger population with a certain level of confidence. We must always keep the assumptions behind inferential statistics in mind so that we can apply the right statistical test and answer our research question within the limits of what our data can tell us.\nIn practice, the process works like this:\n\n\nSetting the significance level (\\(\\alpha\\)):\n\nBefore conducting the test, you decide on a significance level, \\(\\alpha\\), which is the probability of rejecting the null hypothesis when it’s actually true (Type I error). Common choices for \\(\\alpha\\) are 0.05, 0.01, and 0.10, though the choice is context-dependent.\n\n\n\nConducting the test:\n\nYou then compute the test statistic (like a t-statistic, F-statistic, etc.) based on your sample data.\nThis test statistic is then compared to a distribution (like the t-distribution for the t-test) to find the p-value.\n\n\n\nInterpreting the p-value:\n\nThe p-value is the probability of observing a test statistic as extreme as, or more extreme than, the statistic computed from the sample, assuming that the null hypothesis is true.\nIf the p-value is less than \\(\\alpha\\) (i.e., below the critical value), then the evidence suggests that the null hypothesis can be rejected in favour of the alternative hypothesis.\nIf the p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis. This doesn’t mean the null hypothesis is true, just that there isn’t enough evidence in your sample to reject it.\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {5. {Statistical} {Inference}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/basic_stats/05-inference.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 5. Statistical Inference. http://tangledbank.netlify.app/BCB744/basic_stats/05-inference.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "5. Statistical Inference"
    ]
  },
  {
    "objectID": "BCB743/randomisation.html",
    "href": "BCB743/randomisation.html",
    "title": "Randomisation of Rows in a Data Frame",
    "section": "",
    "text": "Randomising the order of observations is effectively a permutation of the data.frame’s (row) index vector. Because R stores a data.frame as a list of equal-length vectors, shuffling the rows amounts to reordering each vector simultaneously according to a single, randomly drawn permutation."
  },
  {
    "objectID": "BCB743/randomisation.html#base-r",
    "href": "BCB743/randomisation.html#base-r",
    "title": "Randomisation of Rows in a Data Frame",
    "section": "Base-R",
    "text": "Base-R\n\n1set.seed(42)\n2df_perm &lt;- df[sample(nrow(df)), ]\n3row.names(df_perm) &lt;- NULL\n\n\n1\n\nFix the RNG for reproducibility when needed\n\n2\n\nsample() without the replace argument returns a permutation\n\n3\n\n(Optional) drop the original row indices\n\n\n\n\nsample(nrow(df)) results in a vector 1:n rearranged into a fresh random order, so subsetting the data.frame with that vector coerces every column to follow suit. The row names stay attached unless one removes them."
  },
  {
    "objectID": "BCB743/randomisation.html#tidyverse",
    "href": "BCB743/randomisation.html#tidyverse",
    "title": "Randomisation of Rows in a Data Frame",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nlibrary(dplyr)\n\ndf_perm &lt;- df %&gt;%\n  slice_sample(n = nrow(.))\n\n# or, predating slice_sample():\ndf_perm &lt;- df %&gt;%\n  sample_frac(1)\n\nslice_sample() accepts an optional weight_by argument, permitting unequal permutation weights should the null hypothesis require them. By contrast, sample_frac(1) asks to “take a 100 % sample”, and, being fraction-based, it sidesteps the need to compute nrow(df)."
  },
  {
    "objectID": "BCB743/randomisation.html#statistical-context",
    "href": "BCB743/randomisation.html#statistical-context",
    "title": "Randomisation of Rows in a Data Frame",
    "section": "Statistical context",
    "text": "Statistical context\nWithin permutation-based inference, randomising rows removes any systematic linkage between predictor and response variables but it conserves the empirical marginal distributions. Recomputing a test statistic over many such reshufflings results in distributions that approximate the data’s sampling distribution under the null hypothesis of no association. One call to sample() is sufficient to create one permutation. By placing that call inside replicate() or purrr::rerun() supplies the Monte-Carlo ensemble.\nRandomisation of rows presupposes exchangeability. If one’s data carry temporal, spatial, or hierarchical strata, a naïve global shuffle may void the null model’s statistical validity. In such cases one would confine sample() to blocks delineated by the relevant grouping variable, for example via dplyr::group_modify(~ .x[sample(nrow(.x)), ]), and so restrict permutations to contextually relevant partitions."
  },
  {
    "objectID": "BCB743/PCA.html",
    "href": "BCB743/PCA.html",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\n\nType\nName\nLink\n\n\n\n\nTheory\nNumerical Ecology in R\nSee pages 117-132\n\n\nSlides\nPCA lecture slides\n💾 BCB743_08_PCA.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\n\n\nR function\nA function for ordination plots\n💾 cleanplot.pca.R\nOrdination refers to a suite of multivariate techniques that reduce a high-dimensional dataset into a lower-dimensional space, typically 2D or 3D, in such a way that any intrinsic structure in the data forms visually-discernible patterns (Pielou, 1984). In ecology, ordination techniques are used to describe relationships between community structure patterns and underlying environmental gradients. They allow us to determine the relative importance of different gradients and visualise species-environment relationships.\nPrincipal Component Analysis (PCA) is one of the commonly used ordination techniques in ecology. It is a dimensionality reduction technique that transforms the original set of variables into a new set of uncorrelated variables called principal components. PCA is performed on a data matrix containing species abundances or environmental variables across multiple sites or samples.\nThe PCA process involves calculating the eigenvectors and eigenvalues of the covariance or correlation matrix of the data. The eigenvectors represent the directions of maximum variance in the data, and the corresponding eigenvalues represent the amount of variance explained by each eigenvector. The new axes, called principal components, are linear combinations of the original variables, ordered by the amount of variance they explain.\nPCA preserves the Euclidean distances between samples in the original high-dimensional space when projecting them onto the lower-dimensional ordination space. This property makes PCA more suitable for analysing environmental data, where Euclidean distances between samples are meaningful and interpretable. However, for species data, which is often in the form of counts or frequencies, Euclidean distances may not be an appropriate measure of dissimilarity between samples.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#sec-horseshoe-effect",
    "href": "BCB743/PCA.html#sec-horseshoe-effect",
    "title": "Principal Component Analysis (PCA)",
    "section": "The Horseshoe Effect",
    "text": "The Horseshoe Effect\nThe ‘horseshoe effect’ (sometimes called the Guttman effect) is an artefact often seen with PCA when applied to species data, especially when using species abundance data for communities along environmental gradients. It distorts the data points in the ordination space. A less severe version of the horseshoe effect is called the ‘arch effect’ and is seen in Correspondence Analysis (CA).\nThe horseshoe effect occurs because PCA assumes linear relationships between variables, while species data often exhibit unimodal responses to environmental gradients. The unimodal model was discussed in BDC334. When species have unimodal distributions along a gradient, PCA tends to fold the ends of the gradient towards each other. This is visible as a horseshoe-shaped pattern in the ordination diagram.\nThis distortion can lead to several issues:\n\nThe horseshoe shape can make it appear that sites at opposite ends of the gradient are more similar than they really are.\nThe folding of the gradient ends compresses the data and potentially obscures important ecological patterns.\nThe second PCA axis—the most affected axis—often doesn’t represent a meaningful ecological gradient, making interpretation challenging.\nUnlike the arch effect in CA, the horseshoe effect in PCA can lead to incorrect ordering of samples along the gradient.\n\nTo address these issues, we prefer to use non-metric Multidimensional Scaling (nMDS) or a distance-based method (like Principal Coordinates Analysis, PCoA) that is less susceptible to this artefact. Alternatively, we may use techniques specifically designed to handle unimodal species responses, such as Correspondence Analysis (CA), which uses \\(\\chi^2\\)-distance and not Euclidean distance, or its detrended version (Detrended Correspondence Analysis, DCA), but these come with their own set of considerations.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#set-up-the-analysis-environment",
    "href": "BCB743/PCA.html#set-up-the-analysis-environment",
    "title": "Principal Component Analysis (PCA)",
    "section": "Set-up the Analysis Environment",
    "text": "Set-up the Analysis Environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(ggcorrplot) # for the correlations\nlibrary(ggpubr)",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#the-doubs-river-data",
    "href": "BCB743/PCA.html#the-doubs-river-data",
    "title": "Principal Component Analysis (PCA)",
    "section": "The Doubs River Data",
    "text": "The Doubs River Data\n\nload(\"../data/NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\")\nhead(env)\n\n   dfs ele  slo  dis  pH har  pho  nit  amm  oxy bod\n1  0.3 934 48.0 0.84 7.9  45 0.01 0.20 0.00 12.2 2.7\n2  2.2 932  3.0 1.00 8.0  40 0.02 0.20 0.10 10.3 1.9\n3 10.2 914  3.7 1.80 8.3  52 0.05 0.22 0.05 10.5 3.5\n4 18.5 854  3.2 2.53 8.0  72 0.10 0.21 0.00 11.0 1.3\n5 21.5 849  2.3 2.64 8.1  84 0.38 0.52 0.20  8.0 6.2\n6 32.4 846  3.2 2.86 7.9  60 0.20 0.15 0.00 10.2 5.3",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#first-do-a-correlation",
    "href": "BCB743/PCA.html#first-do-a-correlation",
    "title": "Principal Component Analysis (PCA)",
    "section": "First Do a Correlation",
    "text": "First Do a Correlation\n\n# computing a correlation matrix\ncorr &lt;- round(cor(env), 1)\n\n# visualisation of the correlation matrix\nggcorrplot(corr, type = 'upper', outline.col = \"white\",\n           colors = c(\"#1679a1\", \"white\", \"#f8766d\"),\n           lab = TRUE)\n\n\n\n\n\n\n\nFigure 1: Pairwise correlations among the environmental variables included with the Doubs River study.\n\n\n\n\n\nSome variables are very correlated, and they might be omitted from the subsequent analyses. We say that these variables are ‘collinear.’ Collinear variables cannot be teased apart in terms of finding out which one is most influential in structuring the community. There are more advanced ways to search for collinear variables (e.g. Variance Inflation Factors, VIF) and in this way we can systematically exclude them from the PCA. See Graham (2003) for a discussion on collinearity. Here we will proceed with all the variables.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#see-the-spatial-context",
    "href": "BCB743/PCA.html#see-the-spatial-context",
    "title": "Principal Component Analysis (PCA)",
    "section": "See the Spatial Context",
    "text": "See the Spatial Context\nThe patterns in the data and the correlations between them will make more sense if we can visualise a spatial context. Thankfully spatial data are available:\n\nhead(spa)\n\n       X      Y\n1 85.678 20.000\n2 84.955 20.100\n3 92.301 23.796\n4 91.280 26.431\n5 92.005 29.163\n6 95.954 36.315\n\nggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1) +\n  geom_label(vjust = 0, nudge_y = 0.5, check_overlap = TRUE)\n\n\n\n\n\n\n\nFigure 2: The spatial configuration of the Doubs River sites.\n\n\n\n\n\nThese site numbers correspond approximately to the ones in Verneaux (1973) but some of the numbers may have been shifted slightly in the example Doubs dataset used here compared to how they were originally numbered in Verneaux’s thesis and subsequent publication. This should not affect the interpretation. We can also scale the symbol size by the magnitude of the environmental variables. Lets look at two pairs of variables that are strongly correlated with one-another:\n\n# We scale the data first so as to better represent the full\n# magnitude of all variables with a common symbol size\nenv_std &lt;- decostand(env, method = \"standardize\")\n\n# positive correlations\nplt1 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"red\", aes(size = env_std$amm, shape = 3)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(size = \"Magnitude\", title = \"Ammonium concentration\")\n\nplt2 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"red\", aes(size = env_std$bod)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(title = \"Biological oxygen demand\")\n\n# inverse correlations\nplt3 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"blue\", aes(size = env_std$alt)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(title = \"Altitude\")\n\nplt4 &lt;- ggplot(spa, aes(x = X, y = Y, label = rownames(spa))) +\n  geom_point(shape = 1, col = \"blue\", aes(size = env_std$flo)) +\n  geom_text(vjust = -0.5, nudge_y = 0.5, check_overlap = TRUE) +\n  labs(title = \"Flow rate\")\n\nggarrange(plt1, plt2, plt3, plt4, nrow = 2, ncol = 2,\n          common.legend = TRUE, labels = \"AUTO\")\n\n\n\n\n\n\n\nFigure 3: Four different representations of the site configuration (spatial context) of the Doubs River sampling layout. Symbols are scaled relative to A) ammonium concentration, B) BOD, C) Altitude, and D) Flow rate.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#do-the-pca",
    "href": "BCB743/PCA.html#do-the-pca",
    "title": "Principal Component Analysis (PCA)",
    "section": "Do the PCA",
    "text": "Do the PCA\nWe use the function rda() to do the PCA, but it can also be performed in base R with the functions prcomp() and princomp(). rda() is the same function that we will use later for a Redundancy Analysis, but when used without specifying constraints (as we do here) it amounts to simply doing a PCA. Typically we standardise environmental data to unit variance, but the PCA done by the rda() function accomplishes this step automagically when scale = TRUE. When applied to environmental data (as we typically do with a PCA) it works with correlations amongst the scaled variables. PCA preserves Euclidean distance and the relationships detected are linear, and for this reason it is not typically applied to species data without suitable transformations. In fact, in this module we will seldom apply a PCA to species data at all.\n\nenv_pca &lt;- rda(env, scale = TRUE)\nenv_pca\n\nCall: rda(X = env, scale = TRUE)\n\n-- Model Summary --\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\n\nInertia is correlations\n\n-- Eigenvalues --\n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n5.969 2.164 1.065 0.739 0.400 0.336 0.173 0.108 0.024 0.017 0.006 \n\n# same ...\n# env_std &lt;- scale(env)\n# env_pca &lt;- rda(env_std, scale = FALSE)\n# env_pca\n\nIn ordination we use the term inertia as a synonym for ‘variation’, but some PCA software (such as R’s prcomp() and princomp()) simply uses the term sdev for standard deviations. In PCA, when we use a correlation matrix (as we do here), the inertia is the sum of the diagonal values of the correlation matrix, which is simply the number of variables (11 in this example). When a PCA uses a covariance matrix the inertia is the sum of the variances of the variables.\nYou will also see in the output the mention of the term ‘unconstrained’. In a PCA the analysis is always unconstrained (i.e. not influenced by some a priori defined variables we hypothesise to explain the between site patterns in the multivariate data).\nThe section headed Eigenvalues for unconstrained axes shows the relative importance of the resultant reduced axes, and they can be used to determine the proportion of the total inertia (sum of the eigenvalues) captured by any one of the axes. They can be accessed with the function eigenvals() (the preferred function; see ?rda for help), but an alternative method is given below. The first eigenvalue (the one associated with PC1) always explains the most variation (the largest fraction), and each subsequent one explains the largest proportion of the remaining variance. We say the axes are orthogonal and ranked in decreasing order of importance. The sum of all eigenvalues is the total inertia, so collectively they theoretically can explain all of the variation in the dataset (but clearly they should not be used to explain all the variance). To extract the first eigenvalue we can do:\n\nround(eigenvals(env_pca)[1], 3)\n\n  PC1 \n5.969 \n\n# or\n\nround(env_pca$CA$eig[1], 3)\n\n  PC1 \n5.969 \n\n\nThe total inertia is:\n\nsum(eigenvals(env_pca))\n\n[1] 11\n\n# or\n\nsum(env_pca$CA$eig)\n\n[1] 11\n\n\nSo the proportion of variation explained by the first PC is:\n\nround(env_pca$CA$eig[1] / sum(env_pca$CA$eig) * 100, 1) # result in %\n\n PC1 \n54.3 \n\n\nWe can show the same information as part of a more verbose summary. Here we see the pre-calculated Proportion Explained and Cumulative Proportion (it should be obvious what this is). There is also an assortment of other information, viz. Scaling 2 for species and site scores, Species scores, and Site scores.\n\nsummary(env_pca)\n\n\nCall:\nrda(X = env, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal              11          1\nUnconstrained      11          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                         PC1    PC2     PC3     PC4     PC5     PC6     PC7\nEigenvalue            5.9687 2.1639 1.06517 0.73875 0.40019 0.33563 0.17263\nProportion Explained  0.5426 0.1967 0.09683 0.06716 0.03638 0.03051 0.01569\nCumulative Proportion 0.5426 0.7393 0.83616 0.90332 0.93970 0.97022 0.98591\n                           PC8      PC9     PC10     PC11\nEigenvalue            0.108228 0.023701 0.017083 0.005983\nProportion Explained  0.009839 0.002155 0.001553 0.000544\nCumulative Proportion 0.995748 0.997903 0.999456 1.000000\n\n\nNewer versions of the ordinations’ summary() method no longer shows the species and site scores. To access these scores, we use the scores() function:\nThe species scores can be extracted from the ordination object by:\n\n# extract species scores for first three PCA axes:\nscores(env_pca, display = \"species\", choices = 1:3)\n\n            PC1        PC2         PC3\ndfs  1.08657429  0.5342463 -0.27333233\nele -1.04396094 -0.6147742  0.20711738\nslo -0.57702931 -0.4892811 -0.63490306\ndis  0.95843365  0.6608150 -0.32456247\npH  -0.06364143  0.4629373  1.01316786\nhar  0.90118054  0.5849982  0.06448776\npho  1.05820606 -0.6013982  0.13865996\nnit  1.15013343 -0.1004908 -0.05166769\namm  1.00678966 -0.6969288  0.14076555\noxy -0.97458581  0.4990886 -0.09017454\nbod  0.97315389 -0.7148470  0.15145231\nattr(,\"const\")\n[1] 4.226177\n\n\nSimilarly, for the site scores, do:\n\n# extract the site scores for axes 1 and 2\nscores(env_pca, display = \"site\", choices = 1:3) |&gt; \n  head() # truncate to save space in this example\n\n         PC1        PC2         PC3\n1 -1.4127388 -1.4009788 -2.03483870\n2 -1.0372473 -0.7795535  0.24400009\n3 -0.9450700 -0.4676536  1.25042488\n4 -0.8737116 -0.2698849  0.19304045\n5 -0.4208759 -0.6694396  0.83190665\n6 -0.7722358 -0.7206662 -0.07357441\n\n\nThe scores() method also applies to other ordination methods.\n\n\n\n\n\n\nQuestion\n\n\n\nWe are dealing with environmental variables here, so why do we call",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#terminology",
    "href": "BCB743/PCA.html#terminology",
    "title": "Principal Component Analysis (PCA)",
    "section": "Terminology",
    "text": "Terminology\nOrdination methods in vegan such as rda() for a PCA/RDA or cca() for a CA/CCA is performed with a singular-value decomposition (SVD). Once the data matrix \\(X\\) has been centred (and, where necessary, scaled or \\(\\chi^2\\)-transformed), the SVD computes\n\\[X = U \\,\\Sigma \\,V^{\\mathsf T},\\]\nwith orthonormal left-singular vectors \\(U\\), orthonormal right-singular vectors \\(V\\), and positive singular values on the diagonal of \\(\\Sigma\\). From here, all the components of our ordination can be extracted. How do these relate to our data?\n\nEigenvectors are the directions in the original variable space along which the variance in the data is maximised. They relate to our original data in two ways:\n\n\\(V\\) is the matrix of right singular vectors; they are the eigenvectors of the columns (species or variables). The quantities can be accessed by env_pca$CA$v.\n\\(U\\) is the matrix of left singular vectors; i.e., the eigenvectors of the rows (sites). The left singular vectors can be accessed by env_pca$CA$u\n\n\\(\\Sigma\\) is a diagonal matrix of singular values \\(d\\), and the eigenvalues are the square of these singular values, \\(eig=d^ 2\\). Eigenvalues represent the amount of variance explained by each corresponding eigenvector (or principal component), represented in PCA (and other unconstrained ordinations such as CA) by env_pca$CA$eig. Higher eigenvalues indicate that the corresponding principal component explains a larger portion of the total variance in the data. The sum of all eigenvalues is equal to the total variance in the data.\n\nLet’s map these directly to our env_pca object, which I’ll assume was created with the rda() function. Looking inside the env_pca object, we’ll see a structure with a component named \\(CA\\) or \\(CCA\\). This holds the results for the constrained/canonical analysis. For a simple PCA, all the results are in \\(CA\\). The eigenvectors (env_pca$CA$eig) define the direction of the new axes, they aren’t the final coordinates for ordination plots. Projection is the process of mapping the original data points onto the new reduced axes (principal components). What we plot are actually these scores.\n\nScores are the coordinates of the original sites and species projected onto the new ordination axes. They are calculated by scaling the eigenvectors. These scores are what we actually plot to create a biplot or triplot.\n\nThe Site Scores tell us where each site (sample) is located along the new ordination axes. Sites that are close together in the ordination plot have similar variable (or species) compositions. We obtain the site scores as scores(env_pca, display = \"site\", ...).\nThe Species Scores (or Variable Scores) inform us about how the original variables (whatever is in the columns, i.e., species or environmental variables) relate to the ordination axes. Sometimes we call the species scores loadings. In a PCA biplot, these are represented by arrows (but in CA not, as that reduced space is no longer linear). An arrow pointing strongly towards PCA1 means that species or variable is a major contributor to that axis of variation. So, species scores represent the contributions of the original variables to the principal components. They indicate the strength and direction of the correlation between the original variables and the new principal components. Species scores are obtained with scores(env_pca, display = \"species\", ...). Larger positive values indicate a stronger positive correlation, while larger negative values indicate a stronger negative correlation. Even though the term “species scores” is used in the software, it refers to the loadings of the original variables (environmental variables in this case).\n\n\nHere’s the direct mapping:\n\n\n\nMathematical Concept\nvegan Term\nWhere to Find It in the rda Object\nDescription\n\n\n\n\nEigenvalues\nEigenvalues\nenv_pca$CA$eig\nThe amount of variance explained by each axis (component). summary(env_pca) shows this clearly.\n\n\nEigenvectors of Variables\nSpecies Scores\nenv_pca$CA$v\nThe raw eigenvectors for the columns of the data (the variables). These are scaled to become the “species scores” that you plot as arrows.\n\n\nEigenvectors of Sites\nSite Scores\nenv_pca$CA$u\nThe raw eigenvectors for the rows of the data (the sites). These are scaled to become the “site scores” that you plot as points.\n\n\nPlottable Coordinates\nScores\nscores(env_pca)\nThe function scores() extracts and properly scales the raw eigenvectors ($u and $v) to give the coordinates ready for plotting.\n\n\n\nWhen interpreting a PCA biplot (see below), the species scores (loadings) indicate the contributions of the original variables to the principal components, while the site scores represent the positions of the sites in the reduced-dimensional space defined by those principal components. The direction and length of the species score vectors (arrows) provide information about the relationships between the original variables and the principal components, while the positions of the site points reflect the similarities or differences between sites based on the environmental gradients represented by the principal components.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#how-many-axes-to-retain",
    "href": "BCB743/PCA.html#how-many-axes-to-retain",
    "title": "Principal Component Analysis (PCA)",
    "section": "How Many Axes to Retain?",
    "text": "How Many Axes to Retain?\nThe number of axes to retain is a difficult question to answer. The first few axes will always explain the most variation, but how do we know how many reduced axes are influential and should be kept? Commonly recommended is the broken stick method—keep the principal components whose eigenvalues are higher than corresponding random broken stick components:\n\n# make a scree plot using the vegan function:\nscreeplot(env_pca, bstick = TRUE, type = \"lines\")\n\n\n\n\n\n\n\nFigure 4: Scree plot of the Doubs River environmental data PCA.\n\n\n\n\n\nOr I can make a scree plot using ggplot2, which is more flexible:\n\nscree_dat &lt;- data.frame(eigenvalue = as.vector(eigenvals(env_pca)),\n                        bstick = bstick(env_pca))\nscree_dat$axis &lt;- rownames(scree_dat)\nrownames(scree_dat) &lt;- NULL\nscree_dat &lt;- scree_dat |&gt; \n  mutate(axis = factor(axis, levels = paste0(rep(\"PC\", 11), seq(1:11))))\n\nggplot(data = scree_dat, aes(x = axis, y = eigenvalue)) +\n  geom_point() +\n  geom_line(aes(group = 1)) +\n  geom_point(aes(y = bstick), colour = \"red\") +\n  geom_line(aes(y = bstick, group = 1), colour = \"red\") +\n  labs(x = \"Principal component\", y = \"Inertia\")\n\n\n\n\n\n\n\nFigure 5: Scree plot of the Doubs River environmental data PCA made in ggplot2.\n\n\n\n\n\nIn the plot, above, the red line is the broken stick components and the black line the eigenvalues for the different PCs. The plot suggests keeping the first two PC axes, which explain approximately 74% of the total inertia. See Numerical Ecology with R pp. 121-122 for more information about how to decide how many PCs to retain.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#ordination-diagrams",
    "href": "BCB743/PCA.html#ordination-diagrams",
    "title": "Principal Component Analysis (PCA)",
    "section": "Ordination Diagrams",
    "text": "Ordination Diagrams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI provide some examples of ordination diagrams scattered throughout the course content (e.g. here), but you may also refer to the step-by-step walk throughs provided by Roeland Kindt. Also see David Zelený’s excellent writing on the topic.\nLet us look at examples. In a PCA ordination diagram, following the tradition of scatter diagrams in Cartesian coordinate systems, objects are represented as points and variables are displayed as arrows. We first use the standard vegan biplot() function:\n\nopar &lt;- par(no.readonly = TRUE)\npar(mfrow = c(1, 2))\nbiplot(env_pca, scaling = 1, main = \"PCA scaling 1\", choices = c(1, 2))\nbiplot(env_pca, scaling = 2, main = \"PCA scaling 2\", choices = c(1, 2))\npar(opar)\n\n\n\n\n\n\n\nFigure 6: Ordination plot of the Doubs River environmental data showing site scaling (left) and species scaling (right).",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#scaling",
    "href": "BCB743/PCA.html#scaling",
    "title": "Principal Component Analysis (PCA)",
    "section": "Scaling",
    "text": "Scaling\nScaling 1: This scaling emphasises relationships between rows accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are approximations of their Euclidian distances in multidimensional space. Objects positioned further apart show a greater degree of environmental dissimilarity. The angles among descriptor vectors should not be interpreted as indicating the degree of correlation between the variables.\nScaling 2: This scaling emphasises relationships between columns accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are not approximations of their Euclidian distances in multidimensional space. The angles among descriptor vectors can be interpreted as indicating the degree of correlation between the variables.\nNow we create biplots using the cleanplot.pca() function that comes with the Numerical Ecology in R book. The figures are more or less the same, except the plot showing the Site scores with Scaling 1 adds a ‘circle of equilibrium contribution’ (see Numerical Ecolology with R, p. 125). The circle of equilibrium contribution is a visual aid drawn on a biplot to help assess the relative importance of species (or variables) in the ordination space. It’s most useful in PCA biplots using scaling 1, where the focus is on species relationships. The circle is not a formal statistical test. It helps us to quickly identify the most important variables or species, but it doesn’t directly indicate statistical significance.\nWe only assign importance to the arrows that extend beyond the radius of the circle (Figure 7):\n\n# we need to load the function first from its R file:\nsource(\"../data/NEwR-2ed_code_data/NEwR2-Functions/cleanplot.pca.R\")\ncleanplot.pca(env_pca, scaling = 1)\n\n\n\n\n\n\n\nFigure 7: Ordination plot of the Doubs River environmental data made with the cleanplot.pca() function.\n\n\n\n\n\nAt this point it is essential that you refer to Numerical Ecology in R (pp. 118 to 126) for help with interpreting the ordination diagrams.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA.html#fitting-environmental-response-surfaces",
    "href": "BCB743/PCA.html#fitting-environmental-response-surfaces",
    "title": "Principal Component Analysis (PCA)",
    "section": "Fitting Environmental Response Surfaces",
    "text": "Fitting Environmental Response Surfaces\nThe ordisurf() function in vegan is used to visualise underlying environmental gradients on an ordination plot. This function fits a smooth surface (usually a generalised additive model GAM fitted to the site scores of PC axes of interest) to the ordination plot based on a specified environmental variable. It highlights how that variable changes across the ordination space so that we may interpret the spatial structure of the data in relation to the environmental gradient more easily.\nFor more about ordisurf(), see Gavin Simpson’s blog post What is ordifurf() doing?.\nWe plot the response surfaces for elevation and biological oxygen demand:\n\nbiplot(env_pca, type = c(\"text\", \"points\"), col = c(\"black\", \"black\"))\n1invisible(ordisurf(env_pca ~ bod, env, add = TRUE, col = \"turquoise\", knots = 1))\ninvisible(ordisurf(env_pca ~ ele, env, add = TRUE, col = \"salmon\", knots = 1))\n\n\n1\n\ninvisible() is used to suppress the output of the ordisurf() function; only the figure is returned.\n\n\n\n\n\n\n\n\n\n\nFigure 8: Ordination plot of the Doubs River environmental data fitted with a smooth response surface for elevation and biological oxygen demand.\n\n\n\n\n\nPCA does well as simpifying multidimensional data, but it has important limitations when applied to ecological community data due to its linear assumptions. In PCA biplots, environmental gradient contours form linear trend surfaces perpendicular to their vectors (Figure 8), reflecting the method’s inherent linearity. However, ecological data and environmental gradients are typically non-linear, with species exhibiting complex, unimodal responses to environmental factors. This mismatch can lead to oversimplified and misleading interpretations. Consequently, PCA is generally not recommended for community data analysis. Instead, alternative ordination methods like Correspondence Analysis (CA), Canonical Correspondence Analysis (CCA), and non-metric Multidimensional Scaling (nMDS) are preferred, as they better capture the non-linear relationships and provide more ecologically meaningful insights.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8a: Principal Components Analysis"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html",
    "href": "BCB743/nMDS_diatoms.html",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nReading\nSerge Mayobo’s diatom paper\n💾 Mayombo_et_al_2019.pdf\n\n\nData\nAbbreviated diatom data matrix\n💾 PB_data_matrix_abrev.csv\n\n\n\nDiatoms data matrix\n💾 PB_data_matrix.csv\n\n\n\nDiatom environmental data\n💾 PB_diat_env.csv\nKelp forests are known to host a large biomass of epiphytic fauna and flora, including diatoms, which constitute the base of aquatic food webs and play an important role in the transfer of energy to higher trophic levels. Epiphytic diatom assemblages associated with two common species of South African kelps, Ecklonia maxima and Laminaria pallida, were investigated in this study. Primary blades of adult and juvenile thalli of both kelp species were sampled at False Bay in July 2017 and analysed using scanning electron microscopy. The diatom community data are here subjected to a suit of multivariate methods in order to show the structure of the diatom flora as a function of i) kelp species, and ii) kelp size. Read Mayombo et al. (2019) for more details and the findings of the research.\nSome feedback was received by anonymous reviewers as part of the peer review process, and it together with my response is repeated below.\nReviewer 1\nThe design of the observational study includes 2 treatments - age (young versus old) and host species (Laminaria versus Ecklonia), 4 replicates (4 primary blades from each combination of host algae and age), and 3 subsamples from each blade (pseudoreplicates, if treated incorrectly as replicates). The experimental design is analogous to a 2-way ANOVA, but with community data instead of a single individual response variable. This design can evaluate interactive effects between the two treatments (age and species). The authors’ experimental design is most suited to analyses using PERMANOVA, which is the community statistics version of the ANOVA.\nPlease indicate for the readers why the data were transformed and standardised using the stated procedures. Definitely a good idea to transform data, but the readers need to understand why particular procedures were employed. Please describe the Wisconsin double standardisation:\nWhy a double standardisation + square-root transformation, as opposed to a single row/column standardisation by row/column total + square-root transformation?\nAJS: About ANOSIM and PERMANOVA\nOverall, Analysis of Similarities (ANOSIM) and the Mantel test were very sensitive to heterogeneity in dispersions, with ANOSIM generally being more sensitive than the Mantel test. In contrast, PERMANOVA and Pillai’s trace were largely unaffected by heterogeneity for balanced designs. […]. PERMANOVA was also unaffected by differences in correlation structure. […] PERMANOVA was generally, but not always, more powerful than the others to detect changes in community structure.\nAJS: About data transformation\nUseful when the range of data values is very large. Data are square root transformed, and then submitted to Wisconsin double standardisation, or species divided by their maxima, and stands standardised to equal totals. These two standardisations often improve the quality of ordinations.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html#set-up-the-analysis-environment",
    "href": "BCB743/nMDS_diatoms.html#set-up-the-analysis-environment",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "Set-Up the Analysis Environment",
    "text": "Set-Up the Analysis Environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(plyr)\n# library(BiodiversityR)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/diatoms/\"",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html#load-and-prepare-the-data",
    "href": "BCB743/nMDS_diatoms.html#load-and-prepare-the-data",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "Load and Prepare the Data",
    "text": "Load and Prepare the Data\nThe species data\nThe diatom species data include the following:\n\ncolumns: diatom genera\nrows: samples (samples taken from two species of kelp; equivalent to sites in other species x sites tables)\nrow names correspond to combinations of the factors in the columns inside PB_diat_env.csv\n\n\nwhere host_size is A for adult kelp plant (host), J for juvenile kelp plant (host), host_spp is Lp for kelp species Laminaria pallida (host), Em for kelp plant Ecklonia maxima (host), plant is the unique number identifying a specific kelp plant, and rep is the replicate tissue sample from each kelp host plant from which the diatoms were extracted.\n\n# with shortened name to fix nMDS overplotting\nspp &lt;- read.csv(paste0(root, \"PB_data_matrix_abrev.csv\"),\n                row.names = \"Replicate\", sep = \",\", header = TRUE)\nspp[1:6, 1:6]\n\n        Amphora.spp Asteromphalus.spp Cocconeis.spp Craspedostauros.spp\nAPB1LP1           0                 0             0                   0\nAPB1LP2           0                 0             0                   0\nAPB1LP3           0                 0             0                   0\nAPB2LP1           0                 0             0                   0\nAPB2LP2           0                 0             0                   0\nAPB2LP3           0                 0             0                   0\n        Cylindrotheca.spp Diploneis.spp\nAPB1LP1                 0             0\nAPB1LP2                 0             0\nAPB1LP3                 0             0\nAPB2LP1                 0             0\nAPB2LP2                 0             0\nAPB2LP3                 0             0\n\n# with full names\nspp2 &lt;- read.csv(paste0(root, \"PB_data_matrix.csv\"),\n                 row.names = \"Replicate\", sep = \",\", header = TRUE)\nspp2[1:6, 1:6]\n\n        Amphora.spp Asteromphalus.spp Cocconeis.spp Craspedostauros.spp\nAPB1LP1           0                 0             0                   0\nAPB1LP2           0                 0             0                   0\nAPB1LP3           0                 0             0                   0\nAPB2LP1           0                 0             0                   0\nAPB2LP2           0                 0             0                   0\nAPB2LP3           0                 0             0                   0\n        Cylindrotheca.spp Diploneis.spp\nAPB1LP1                 0             0\nAPB1LP2                 0             0\nAPB1LP3                 0             0\nAPB2LP1                 0             0\nAPB2LP2                 0             0\nAPB2LP3                 0             0\n\n# remove \".spp\" from column header name\ncolnames(spp) &lt;- str_replace(colnames(spp), \"\\\\.spp\", \"\")\ncolnames(spp2) &lt;- str_replace(colnames(spp2), \"\\\\.spp\", \"\")\n\nLogarithmic transformation as suggested by Anderson (2006): \\(log_{b}(x) + 1\\) for \\(x &gt; 0\\), where \\(b\\) is the base of the logarithm; zeros are left as zeros. Higher bases give less weight to quantities and more to presences.\n\nspp.log &lt;- decostand(spp, method = \"log\")\nspp.log.dis &lt;- vegdist(spp.log, method = \"bray\")\n\nThe Predictors\nThe content is described above; these variables are categorical vars – they are not actually ‘environmental’ data, but their purpose in the analysis is analogous to true environmental data; it’s simply data that describe where the samples were taken from.\n\nenv &lt;- tibble(read.csv(paste0(root, \"PB_diat_env.csv\")),\n              sep = \",\", header = TRUE)\nenv$plant &lt;- as.factor(env$plant)\nenv$rep &lt;- as.factor(env$rep)\nhead(env)\n\n# A tibble: 6 × 7\n  replicate host_size host_spp plant rep   sep   header\n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;lgl&gt; \n1 APB1LP1   A         Lp       1     1     ,     TRUE  \n2 APB1LP2   A         Lp       1     2     ,     TRUE  \n3 APB1LP3   A         Lp       1     3     ,     TRUE  \n4 APB2LP1   A         Lp       2     1     ,     TRUE  \n5 APB2LP2   A         Lp       2     2     ,     TRUE  \n6 APB2LP3   A         Lp       2     3     ,     TRUE  \n\n\nWith the environmental data (factors), the following analyses can be done:\n\n✘ Discriminant Analysis (DA)\n✘ Analysis of Similarities (ANOSIM)\n✔︎ Permutational Analysis of Variance (PERMANOVA)\n✘ Mantel test\n\nWe will do an nMDS and PERMANOVA.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html#multivariate-homogeneity-of-group-dispersions-variances",
    "href": "BCB743/nMDS_diatoms.html#multivariate-homogeneity-of-group-dispersions-variances",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "Multivariate Homogeneity of Group Dispersions (Variances)",
    "text": "Multivariate Homogeneity of Group Dispersions (Variances)\nBefore doing the PERMANOVA (testing differences between means), first check to see if the dispersion is the same. See ?adonis2 for more on this.\nHomogeneity of groups betadisper() evaluates the differences in group homogeneities. We can view it as being analogous to Levene’s test of the equality of variances. The null hypothesis evaluated is that the population variances are equal. Unfortunately we can only use one factor as an independent variable so it is not yet possible to look for interactions (species × size).\nSo, we test the \\(H_{0}\\) that the dispersion (variance) in diatom community structure does not differ between the two host species:\n\n(mod.spp &lt;- with(env, betadisper(spp.log.dis, host_spp)))\n\n\n    Homogeneity of multivariate dispersions\n\nCall: betadisper(d = spp.log.dis, group = host_spp)\n\nNo. of Positive Eigenvalues: 20\nNo. of Negative Eigenvalues: 21\n\nAverage distance to median:\n    Em     Lp \n0.3640 0.4391 \n\nEigenvalues for PCoA axes:\n(Showing 8 of 41 eigenvalues)\n PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n1.9619 1.7968 1.3888 1.0040 0.8491 0.6366 0.3132 0.3008 \n\nanova(mod.spp)\n\nAnalysis of Variance Table\n\nResponse: Distances\n          Df  Sum Sq  Mean Sq F value Pr(&gt;F)\nGroups     1 0.05876 0.058761  2.6087 0.1141\nResiduals 40 0.90101 0.022525               \n\n\nThere is no difference in dispersion between the diatom communities on the two host species. Apply the same procedure to see if host size has an effect:\n\n(mod.size &lt;- with(env, betadisper(spp.log.dis, host_size)))\n\n\n    Homogeneity of multivariate dispersions\n\nCall: betadisper(d = spp.log.dis, group = host_size)\n\nNo. of Positive Eigenvalues: 20\nNo. of Negative Eigenvalues: 21\n\nAverage distance to median:\n     A      J \n0.4005 0.3889 \n\nEigenvalues for PCoA axes:\n(Showing 8 of 41 eigenvalues)\n PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n1.9619 1.7968 1.3888 1.0040 0.8491 0.6366 0.3132 0.3008 \n\nanova(mod.size)\n\nAnalysis of Variance Table\n\nResponse: Distances\n          Df  Sum Sq   Mean Sq F value Pr(&gt;F)\nGroups     1 0.00141 0.0014134  0.0604 0.8071\nResiduals 40 0.93615 0.0234038               \n\n\nNo, it does not have an effect either. Make some plots to visualise the patterns:\n\npar(mfrow = c(2, 2))\nplot(mod.spp, sub = NULL)\nboxplot(mod.spp)\n\nplot(mod.size)\nboxplot(mod.size)\n\n\n\n\n\n\n\nOptionally, we can confirm the above analysis with the permutest() function. permutest() is a permutational ANOVA-like test that tests the \\(H_{0}\\) that there is no difference in the multivariate dispersion of diatom community structure between Ecklonia maxima and Laminaria pallida, and between adult and juvenile plants:\n\npermutest(mod.spp) # there is in fact no difference\n\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 999\n\nResponse: Distances\n          Df  Sum Sq  Mean Sq      F N.Perm Pr(&gt;F)\nGroups     1 0.05876 0.058761 2.6087    999  0.121\nResiduals 40 0.90101 0.022525                     \n\npermutest(mod.size) # nope...\n\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 999\n\nResponse: Distances\n          Df  Sum Sq   Mean Sq      F N.Perm Pr(&gt;F)\nGroups     1 0.00141 0.0014134 0.0604    999  0.827\nResiduals 40 0.93615 0.0234038                     \n\n\nIt should be sufficient to do the anova(), above, though. You can safely ignore the permutest().",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html#permanova",
    "href": "BCB743/nMDS_diatoms.html#permanova",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "PERMANOVA",
    "text": "PERMANOVA\nPermutational Multivariate Analysis of Variance (PERMANOVA; Anderson and Walsh (2013)) uses distance matrices (Bray-Curtis similarities by default), whereas ANOSIM uses only ranks of Bray-Curtis. The former therefore preserves more information and it is the recommended approach to test for differences between multivariate means. PERMANOVA also allows for variation partitioning and permits for more complex designs (multiple factors, nested factors, interactions, covariates, etc.). To this end, we use adonis2() to evaluate the differences in the group means, which makes it analogous to multivariate analysis of variance.\nNote that nestedness should be stated in the blocks (plants): “If you have a nested error structure, so that you do not want your data be shuffled over classes (blocks), you should define blocks in your permutation” – Jari Oksannen\n\n# the permutational structure captures the nesting of replicates within plant\nperm &lt;- how(nperm = 1000)\nsetBlocks(perm) &lt;- with(env, plant)\n\n(perm.1 &lt;- adonis2(spp.log.dis ~ host_spp * host_size,\n                   method = p, data = env,\n                   permutations = perm))\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nBlocks:  with(env, plant) \nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = spp.log.dis ~ host_spp * host_size, data = env, permutations = perm, method = p)\n                   Df SumOfSqs      R2      F Pr(&gt;F)\nhost_spp            1   0.2991 0.03815 1.7234      1\nhost_size           1   0.3726 0.04754 2.1475      1\nhost_spp:host_size  1   0.5727 0.07306 3.3003      1\nResidual           38   6.5938 0.84124              \nTotal              41   7.8381 1.00000              \n\n\nThere is no effect resulting from host species, host size, or interactions between the two.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html#nmds",
    "href": "BCB743/nMDS_diatoms.html#nmds",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "nMDS",
    "text": "nMDS\nDo the nMDS and assemble the figures:\n\nspp.nmds &lt;- metaMDS(spp.log, k = 2,trymax = 100, trace = 0,\n                    distance = \"bray\", wascores = TRUE)\n\n# not printed as it is too long...\n# scores(spp.nmds, display = \"species\")\n# scores(spp.nmds, display = \"sites\")\n\n\ncol &lt;- c(\"indianred3\", \"steelblue4\")\npch &lt;- c(17, 19)\nopar &lt;- par()\nplt1 &lt;- layout(rbind(c(1, 1, 2, 2, 3, 3),\n                     c(4, 4, 4, 5, 5, 5)),\n               heights = c(2, 3),\n               respect = TRUE)\n\n# layout.show(plt1)\n\npar(mar = c(3,3,1,1))\n\n# plot 1\nplot(mod.spp, main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0), col = col, pch = pch,\n     sub = NULL)\n\n# plot 2\nplot(mod.size, main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0), col = col, pch = pch,\n     sub = NULL)\n\n# plot 3\nstressplot(spp.nmds, p.col = \"steelblue4\", l.col = \"indianred3\",\n           tck = .05, mgp = c(1.8, 0.5, 0))\n\n# plot 4\npar(mar = c(3,3,2,1))\nplot(spp.nmds, display = \"sites\", type = \"n\",\n     main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0),\n     xlim = c(-2, 2), ylim = c(-1, 2))\nwith(env,\n     points(spp.nmds, display = \"sites\", col = col[host_spp],\n            pch = pch[host_spp]))\nwith(env,\n     ordispider(spp.nmds, groups = host_spp,\n                label = TRUE,\n                col = col))\nwith(env, ordiellipse(spp.nmds, groups = host_spp,\n                      col = col, label = FALSE))\npoints(spp.nmds, display = \"species\", pch = 1, col = \"seagreen\")\norditorp(spp.nmds, display = \"species\", cex = 0.8,\n         col = \"black\", air = 0.01)\n\n# plot 5\npar(mar = c(3, 3, 2, 1))\nplot(spp.nmds, display = \"sites\", type = \"n\",\n     main = NULL,\n     tck = .05, mgp = c(1.8, 0.5, 0),\n     xlim = c(-2, 2), ylim = c(-1, 2))\nwith(env,\n     points(spp.nmds, display = \"sites\", col = col[host_size],\n            pch = pch[host_size]))\nwith(env,\n     ordispider(spp.nmds, groups = host_size,\n                label = TRUE,\n                col = col))\nwith(env, ordiellipse(spp.nmds, groups = host_size,\n                      col = col, label = FALSE))\npoints(spp.nmds, display = \"species\", pch = 1, col = \"seagreen\")\norditorp(spp.nmds, display = \"species\", cex = 0.8,\n         col = \"black\", air = 0.01)\n\n\n\n\n\n\n# dev.off()\npar(opar)",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html#multivariate-abundance-using-generalised-linear-models",
    "href": "BCB743/nMDS_diatoms.html#multivariate-abundance-using-generalised-linear-models",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "Multivariate Abundance Using Generalised Linear Models",
    "text": "Multivariate Abundance Using Generalised Linear Models\nWhat follows is an example of ‘Model-based Multivariate Analyses.’ I’ll not discuss this method here, but merely repeat the code as used in the Mayombo et al. (2019) paper. For background to the Multivariate abundance using Generalised Linear Models approach, refer to Wang et al. (2012) and Wang et al. (2017).\n\nlibrary(mvabund)\ndiat_spp &lt;- mvabund(spp2)\n\nLook at the spread of the data using the boxplot function. The figure is not used in paper:\n\npar(mar = c(2, 10, 2, 2)) # adjusts the margins\nboxplot(spp, horizontal = TRUE, las = 2, main = \"Abundance\", col = \"indianred\")\n\n\n\n\n\n\n\nCheck the mean-variance relationship:\n\nmeanvar.plot(diat_spp)\n\n\n\n\n\n\n\nThe above plot shows that spp with a high mean also have a high variance.\n\nAre there differences in the species composition of the diatom spp. sampled? This has already been addressed above, but we can apply an lternative approach below.\nDo some of them specialise on particular spp of kelp, while others are more generalised? Addressed below.\nDo some occur more on juveniles, while some are on adults, and which ones indiscriminately live across age classes? Addressed below.\nWhich species? Addressed below.\n\nScale manually for ggplot2() custom plot. Create a scale function:\n\nlog_fun &lt;- function(x) {\n  min_x &lt;- min(x[x != 0], na.rm = TRUE)\n  a &lt;- log(x) / min_x\n  a[which(!is.finite(a))] &lt;- 0\n  return(a)\n}\n\nMake a plot that shows which diatoms species are responsible for differences between adult and juvenile kelps:\n\nspp2 %&gt;%\n  mutate(host_size = env$host_size) %&gt;%\n  gather(key = species, value = abund, -host_size) %&gt;%\n  as_tibble() %&gt;%\n  group_by(species) %&gt;%\n  mutate(log.abund = log_fun(abund)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = fct_reorder(species, abund, .fun = mean), y = log.abund)) +\n  geom_boxplot(aes(colour = host_size), size = 0.4, outlier.size = 0,\n               fill = \"grey90\") +\n  geom_point(aes(colour = host_size, shape = host_size),\n             position = position_dodge2(width = 0.8),\n             alpha = 0.6, size = 2.5) +\n  scale_colour_manual(name = \"Age\", values = c(\"indianred3\", \"steelblue4\")) +\n  scale_shape_manual(name = \"Age\", values = c(17, 19)) +\n  annotate(\"text\", x = 15, y = 3, size = 4.5,\n           label = expression(paste(italic(\"p\"), \"=0.017\"))) +\n  annotate(\"text\", x = 14, y = 3, size = 4.5,\n           label = expression(paste(italic(\"p\"), \"=0.004\"))) +\n  scale_y_continuous(name = \"Log abundance\") +\n  coord_flip() + theme_bw() +\n  theme(panel.grid.major = element_line(linetype = \"dashed\",\n                                        colour = \"seagreen3\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_text(size = 13, color = \"black\",\n                                   margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")),\n        axis.text.y = element_text(size = 13, color = \"black\", face = \"italic\",\n                                   margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")),\n        axis.title.x = element_text(size = 14, vjust = 5.75, color = \"black\"),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(-0.25, \"cm\"),\n        axis.ticks = element_line(color = \"black\", size = 0.5))\n\n\n\n\n\n\n\nI settle on a negative binomial distribution for the species data. This will be provided to the manyglm() function:\n\nsize_mod2 &lt;- manyglm(diat_spp ~ (env$host_spp * env$host_size) / env$plant,\n                     family = \"negative binomial\")\nplot(size_mod2) # better residuals...\n\n\n\n\n\n\n\n\n# anova(size_mod2, test = \"wald\")\nout &lt;- anova(size_mod2, p.uni = \"adjusted\", test = \"wald\")\n\nTime elapsed: 0 hr 0 min 15 sec\n\nout$table\n\n                                     Res.Df Df.diff     wald Pr(&gt;wald)\n(Intercept)                              41      NA       NA        NA\nenv$host_spp                             40       1 5.227772     0.298\nenv$host_size                            39       1 7.799205     0.004\nenv$host_spp:env$host_size               38       1 5.434128     0.010\nenv$host_spp:env$host_size:env$plant     26      16      NaN     0.001\n\n\nWhat is the proportional contribution of some important species to juvenile and adult plants?\n\nprop.contrib &lt;- data.frame(spp = colnames(out$uni.test),\n                           prop = out$uni.test[3, ],\n                           row.names = NULL)\nprop.contrib %&gt;%\n  mutate(perc = round((prop / sum(prop)) * 100, 1)) %&gt;%\n  arrange(desc(perc)) %&gt;%\n  mutate(cum = cumsum(perc))\n\n               spp       prop perc   cum\n1    Rhoicosphenia 4.05979831 16.7  16.7\n2         Navicula 3.65789127 15.1  31.8\n3        Nitzschia 2.65842186 10.9  42.7\n4          Amphora 2.39874550  9.9  52.6\n5        Cocconeis 2.10936669  8.7  61.3\n6         Nagumoea 1.92435482  7.9  69.2\n7   Gomphoseptatum 1.86301773  7.7  76.9\n8    Cylindrotheca 1.79282759  7.4  84.3\n9      Parlibellus 1.40898477  5.8  90.1\n10      Licmophora 0.83203299  3.4  93.5\n11       Tabularia 0.64240853  2.6  96.1\n12 Craspedostauros 0.40412954  1.7  97.8\n13   Grammatophora 0.14035115  0.6  98.4\n14   Thalassionema 0.09310783  0.4  98.8\n15   Asteromphalus 0.08434341  0.3  99.1\n16       Diploneis 0.07915274  0.3  99.4\n17          Haslea 0.07915274  0.3  99.7\n18      Trachyneis 0.07149347  0.3 100.0",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/nMDS_diatoms.html#references",
    "href": "BCB743/nMDS_diatoms.html#references",
    "title": "nMDS: Mayombo’s Diatom Data",
    "section": "References",
    "text": "References\n\n\nAnderson MJ (2006) Distance-based tests for homogeneity of multivariate dispersions. Biometrics 62:245–253.\n\n\nAnderson MJ, Walsh DC (2013) PERMANOVA, ANOSIM, and the mantel test in the face of heterogeneous dispersions: What null hypothesis are you testing? Ecological monographs 83:557–574.\n\n\nMayombo N, Majewska R, Smit A (2019) Diatoms associated with two south african kelp species: Ecklonia maxima and laminaria pallida. African Journal of Marine Science 41:221–229.\n\n\nWang Y, Naumann U, Wright ST, Warton DI (2012) Mvabund–an r package for model-based analysis of multivariate abundance data. Methods in Ecology and Evolution 3:471–474.\n\n\nWang Y, Naumann U, Eddelbuettel D, Wilshire J, Warton D, Byrnes J, Santos Silva R dos, Niku J, Renner I, Wright S (2017) Mvabund: Statistical methods for analysing multivariate abundance data.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11b: nMDS: PERMANOVA (Diatoms) Example"
    ]
  },
  {
    "objectID": "BCB743/CA.html",
    "href": "BCB743/CA.html",
    "title": "Correspondence Analysis (CA)",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nTheory\nNumerical Ecology in R\nSee pages 132-140\n\n\nSlides\nCA lecture slides\n💾 BCB743_09_CA.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\nCorrespondence Analysis (CA) is an eigenvector-based ordination method that handles nonlinear species responses more effectively than Principal Component Analysis (PCA). PCA relies on linear relationships and maximises variance explained using a covariance or correlation matrix, but CA employs similar regression techniques based on \\(\\chi^2\\)-standardised data and weights. This makes it more appropriate for species count and presence/absence data.\nCA maximises the correspondence between species scores and sample scores by preserving \\(\\chi^2\\) distances between sites in a species-by-site matrix instead of Euclidean distances. The \\(\\chi^2\\) distance metric is not influenced by double zeros, making it suitable for situation when many species might be absent from several sites. The process involves performing a Singular Value Decomposition (SVD) or eigenvalue decomposition (two different approaches in linear algebra applied to the analysis of matrices) on the standardised data matrix and reporting the eigenvalues and eigenvectors.\nIn CA ordination diagrams, species and sites are presented as points within a reduced-dimensional space. Their relative positions encode the strength and structure of their associations. The distances between these points approximate the \\(\\chi^2\\) distances calculated between the rows (sites) or columns (species) of the original contingency or abundance matrix, and preserve a measure of compositional dissimilarity that is sensitive to the distributional asymmetries characteristic of ecological data. The ordination thus provides a geometric framework for addressing inferential questions of the type: Which sites have compositional affinities with particular species assemblages? or Which species distributions align with which site characteristics?\nThe species scores are derived as weighted averages of site scores or equivalently as linear combinations of the original species data. As such, they are constructed to maximise the dispersion of species configurations along successive ordination axes. So, they capture dominant gradients and patterns of variation that may reflect underlying ecological processes. Whereas PCAs provide a linear mapping of species onto environmental gradients, CAs better approximate species’ nonlinear, often unimodal or skewed, responses to latent environmental factors. Because of this nonlinear structure, species points in CA biplots are not represented as vectors radiating from the origin (as they are in PCA, where linear monotonic gradients predominate). Instead, CA are better suited to visualisations involving curved response surfaces, which indicate the fact that species’ occurrence or abundance peaks at intermediate positions along gradients rather than increasing or decreasing uniformly across the ordination space.\nOne potential downside of CA is that it assumes the total abundance or presence of species across sites to be constant, which may not always hold true. Additionally, some ecologists argue that CA might be overly influenced by rare species, as their contributions to the \\(\\chi^2\\) statistic can be disproportionately large. This issue can be mitigated by applying appropriate transformations or down weighting rare species in the analysis.\nAnother problem with CA is the ‘arch effect.’ This is similar to the horseshoe effect in PCA, but less severe. The arch effect can be mitigated by using a Detrended Correspondence Analysis (DCA), which is a variation of CA that detrends the arch effect by removing the linear trend from the eigenvalues.\nCA produces one axis fewer than the minimum of the number of sites (n) or the number of species (p). Like PCA, CA produces orthogonal axes ranked in decreasing order of importance. However, the variation represented is the total inertia, which is the sum of squares of all values in the \\(\\chi^2\\) matrix, rather than the sum of eigenvalues along the diagonal as in PCA. Individual eigenvalues in CA can be greater than 1, indicating that the corresponding axis captures a significant portion of the total variance in the data.\nThe scaling of ordination plots in CA is similar to that in PCA. Scaling 1 (site scaling) means that sites close together in the plot have similar species relative frequencies, and any site near a species point will have a relatively large abundance of that species. Scaling 2 (species scaling) means that species points close together will have similar abundance patterns across sites, and any species close to a site point is more likely to have a high abundance at that site.\nAs with all ordination techniques, interpreting CA results should be done with caution and in conjunction with additional ecological knowledge and statistical tests, as the ordination axes may not always have a clear ecological interpretation. Please supplement your reading by referring to GUSTA ME) and David Zelený’s writing on the topic in Analysis of community ecology data in R.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "9a: Correspondence Analysis"
    ]
  },
  {
    "objectID": "BCB743/CA.html#set-up-the-analysis-environment",
    "href": "BCB743/CA.html#set-up-the-analysis-environment",
    "title": "Correspondence Analysis (CA)",
    "section": "Set-up the Analysis Environment",
    "text": "Set-up the Analysis Environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(viridis)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/\"",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "9a: Correspondence Analysis"
    ]
  },
  {
    "objectID": "BCB743/CA.html#the-doubs-river-data",
    "href": "BCB743/CA.html#the-doubs-river-data",
    "title": "Correspondence Analysis (CA)",
    "section": "The Doubs River Data",
    "text": "The Doubs River Data\nIn the PCA chapter we analysed the environmental data. This time we work with the species data.\n\nload(paste0(root, \"NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\"))\nhead(spe, 8)\n\n  Cogo Satr Phph Babl Thth Teso Chna Pato Lele Sqce Baba Albi Gogo Eslu Pefl\n1    0    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0    0    1    0\n4    0    4    5    5    0    0    0    0    0    1    0    0    1    2    2\n5    0    2    3    2    0    0    0    0    5    2    0    0    2    4    4\n6    0    3    4    5    0    0    0    0    1    2    0    0    1    1    1\n7    0    5    4    5    0    0    0    0    1    1    0    0    0    0    0\n8    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n  Rham Legi Scer Cyca Titi Abbr Icme Gyce Ruru Blbj Alal Anan\n1    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    0    0    0    0    0    0    0    0    0    0    0\n3    0    0    0    0    0    0    0    0    0    0    0    0\n4    0    0    0    0    1    0    0    0    0    0    0    0\n5    0    0    2    0    3    0    0    0    5    0    0    0\n6    0    0    0    0    2    0    0    0    1    0    0    0\n7    0    0    0    0    0    0    0    0    0    0    0    0\n8    0    0    0    0    0    0    0    0    0    0    0    0",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "9a: Correspondence Analysis"
    ]
  },
  {
    "objectID": "BCB743/CA.html#do-the-ca",
    "href": "BCB743/CA.html#do-the-ca",
    "title": "Correspondence Analysis (CA)",
    "section": "Do the CA",
    "text": "Do the CA\nThe vegan function cca() can be used for CA and Constrained Correspondence Analysis (CCA). When we do not specify constraints, as we do here, we will do a simple CA:\n\nspe_ca &lt;- cca(spe)\n\nError in cca.default(spe): all row sums must be &gt;0 in the community data matrix\n\n\nOkay, so there’s a problem. The error message says that at least one of the rows sums to 0. Which one?\n\napply(spe, 1, sum)\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 3 12 16 21 34 21 16  0 14 14 11 18 19 28 33 40 44 42 46 56 62 72  4 15 11 43 \n27 28 29 30 \n63 70 87 89 \n\n\nWe see that the offending row is row 8, so we can omit it. This function will omit any row that sums to zero (or less):\n\nspe &lt;- spe[rowSums(spe) &gt; 0, ]\nhead(spe, 8)\n\n  Cogo Satr Phph Babl Thth Teso Chna Pato Lele Sqce Baba Albi Gogo Eslu Pefl\n1    0    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0    0    1    0\n4    0    4    5    5    0    0    0    0    0    1    0    0    1    2    2\n5    0    2    3    2    0    0    0    0    5    2    0    0    2    4    4\n6    0    3    4    5    0    0    0    0    1    2    0    0    1    1    1\n7    0    5    4    5    0    0    0    0    1    1    0    0    0    0    0\n9    0    0    1    3    0    0    0    0    0    5    0    0    0    0    0\n  Rham Legi Scer Cyca Titi Abbr Icme Gyce Ruru Blbj Alal Anan\n1    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    0    0    0    0    0    0    0    0    0    0    0\n3    0    0    0    0    0    0    0    0    0    0    0    0\n4    0    0    0    0    1    0    0    0    0    0    0    0\n5    0    0    2    0    3    0    0    0    5    0    0    0\n6    0    0    0    0    2    0    0    0    1    0    0    0\n7    0    0    0    0    0    0    0    0    0    0    0    0\n9    0    0    0    0    1    0    0    0    4    0    0    0\n\n\nNow we are ready for the CA:\n\nspe_ca &lt;- cca(spe)\nspe_ca\n\nCall: cca(X = spe)\n\n-- Model Summary --\n\n              Inertia Rank\nTotal           1.167     \nUnconstrained   1.167   26\n\nInertia is scaled Chi-square\n\n-- Eigenvalues --\n\nEigenvalues for unconstrained axes:\n   CA1    CA2    CA3    CA4    CA5    CA6    CA7    CA8 \n0.6010 0.1444 0.1073 0.0834 0.0516 0.0418 0.0339 0.0288 \n(Showing 8 of 26 unconstrained eigenvalues)\n\n\nThe more verbose summary() output:\n\nsummary(spe_ca)\n\n\nCall:\ncca(X = spe) \n\nPartitioning of scaled Chi-square:\n              Inertia Proportion\nTotal           1.167          1\nUnconstrained   1.167          1\n\nEigenvalues, and their contribution to the scaled Chi-square \n\nImportance of components:\n                        CA1    CA2     CA3     CA4     CA5     CA6     CA7\nEigenvalue            0.601 0.1444 0.10729 0.08337 0.05158 0.04185 0.03389\nProportion Explained  0.515 0.1237 0.09195 0.07145 0.04420 0.03586 0.02904\nCumulative Proportion 0.515 0.6387 0.73069 0.80214 0.84634 0.88220 0.91124\n                          CA8     CA9     CA10     CA11     CA12     CA13\nEigenvalue            0.02883 0.01684 0.010826 0.010142 0.007886 0.006123\nProportion Explained  0.02470 0.01443 0.009278 0.008691 0.006758 0.005247\nCumulative Proportion 0.93594 0.95038 0.959655 0.968346 0.975104 0.980351\n                          CA14     CA15     CA16     CA17     CA18     CA19\nEigenvalue            0.004867 0.004606 0.003844 0.003067 0.001823 0.001642\nProportion Explained  0.004171 0.003948 0.003294 0.002629 0.001562 0.001407\nCumulative Proportion 0.984522 0.988470 0.991764 0.994393 0.995955 0.997362\n                          CA20      CA21      CA22      CA23      CA24\nEigenvalue            0.001295 0.0008775 0.0004217 0.0002149 0.0001528\nProportion Explained  0.001110 0.0007520 0.0003614 0.0001841 0.0001309\nCumulative Proportion 0.998472 0.9992238 0.9995852 0.9997693 0.9999002\n                           CA25      CA26\nEigenvalue            8.949e-05 2.695e-05\nProportion Explained  7.669e-05 2.310e-05\nCumulative Proportion 1.000e+00 1.000e+00\n\n\nThe output looks similar to that of a PCA. The important things to note are the inertia (unconstrained and total inertia are the same), the Eigenvalues for the unconstrained axes, the Species scores, and the Site scores. Their interpretation is the same as before, but we can reiterate. Let us calculate the total inertia:\n\nround(sum(spe_ca$CA$eig), 5)\n\n[1] 1.16691\n\n\nThe inertia for the first axis (CA1) is:\n\nround(spe_ca$CA$eig[1], 5)\n\n    CA1 \n0.60099 \n\n\nThe inertia of CA1 and CA2 is:\n\nround(sum(spe_ca$CA$eig[1:2]), 5)\n\n[1] 0.74536\n\n\nThe fraction of the variance explained by CA1 and CA2 is:\n\nround(sum(spe_ca$CA$eig[1:2]) / sum(spe_ca$CA$eig) * 100, 2) # result in %\n\n[1] 63.87\n\n\nAbove, the value is the same one as in Cumulative Proportion in the summary(spe_ca) output under the CA2 column.\n\n# make a scree plot using the vegan function:\nscreeplot(spe_ca, bstick = TRUE, type = \"lines\")\n\n\n\n\n\n\nFigure 1: Scree plot of the Doubs River environmental data PCA.\n\n\n\n\nThe scree plot (Figure 1) shows the eigenvalues of the CA axes which helps us decide how many axes to retain in the analysis. In this case, we will retain the first two axes, as they explain the most variance in the data.\nSpecies scores are actual species scores as they now relate to species data (in the PCA, the environmental variables were in the columns and so the species scores referred instead to the environment). The most positive and most negative eigenvectors (or loadings) indicate those species that dominate in their influence along particular CA axes. For example, CA1 will be most heavily loaded by the species Cogo and Satr (eigenvectors of 1.50075 and 1.66167, respectively). If there is an environmental gradient, it will be these species that show the strongest response. At the very least, we can say that the contributions of these species are having an overriding influence on the community differences seen between sites.\nSite scores are also as seen earlier in PCA. The highest positive or negative loadings indicate sites that are dispersed far apart on the biplot (in ordination space). They will have large differences in fish community composition.\nPlease see Numerical Ecology in R (pp. 133 to 140). There you will find explanations for how to interpret the ordinations and the ordination diagrams shown below.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "9a: Correspondence Analysis"
    ]
  },
  {
    "objectID": "BCB743/CA.html#ordination-diagrams",
    "href": "BCB743/CA.html#ordination-diagrams",
    "title": "Correspondence Analysis (CA)",
    "section": "Ordination Diagrams",
    "text": "Ordination Diagrams\nThe biplots for the above ordination are given in Figure 2.\n\nopar &lt;- par(no.readonly = TRUE)\npar(mfrow = c(1, 2))\nplot(spe_ca, scaling = 1, main = \"CA fish abundances - biplot scaling 1\")\nplot(spe_ca, scaling = 2, main = \"CA fish abundances - biplot scaling 2\")\npar(opar)\n\n\n\n\n\n\nFigure 2: CA ordination plot of the Doubs River species data showing site scaling (left) and species scaling (right).\n\n\n\n\nScaling 1: This is site scaling, which emphasises relationships between rows accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are approximations of their \\(\\chi^{2}\\) distances in multidimensional space. Objects found near a point representing a species are likely to contain a high contribution of that species. Site scaling means that sites close together in the plot have similar species relative frequencies, and any site near a species point will have a relatively large abundance of that species.\nScaling 2: Species scaling. This emphasises relationships between columns accurately in low-dimensional ordination space. Distances among objects (samples or sites) in the biplot are not approximations of their \\(\\chi^{2}\\) distances in multidimensional space, but the distances among species are. Species scaling means that species points close together will have similar abundance patterns across sites, and any species close to a site point is more likely to have a high abundance at that site.\nBelow I provide biplots with site and species scores for four selected species (Figure 3). The point size of the site scores scales with species scores: the larger the point, the greater the species score. Here, the species score is seen as a centre of abundance; it represents the species’ maximum abundance, which decreases in every direction from the centroid. The plots are augmented with response surfaces created using the ordisurf() function. This function fits models to predict the abundance of the species Salmo trutta fario (Brown Trout), Scardinius erythrophthalmus (Rudd), Telestes souffia (Souffia or Western Vairone), and Cottus gobio (Bullhead) using a Generalised Additive Model (GAM) of the Correspondence Analysis (CA) site scores on axes 1 and 2 as the predictor variables. The response surfaces illustrate where the species are most abundant and the direction of their response.\nAdditionally, I used the envfit() function to project biplot arrows for the continuous environmental variables into the ordination space. Each arrow points in the direction of the maximum increase of the variable. The length of the arrow is proportional to the correlation between the variable and the ordination axes. The significance of the correlation is tested by permutation, with significant vectors shown in red. The environmental variables are the same as those used in the PCA.\n\npalette(viridis(8))\nopar &lt;- par(no.readonly = TRUE)\npar(mar = c(4, 4, 0.9, 0.5) + .1, mfrow = c(2, 2))\n\ninvisible(ordisurf(spe_ca ~ Satr, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Salmo trutta fario\"))\nabline(h = 0, v = 0, lty = 3)\n\ninvisible(ordisurf(spe_ca ~ Scer, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Scardinius erythrophthalmus\"))\nabline(h = 0, v = 0, lty = 3)\n\ninvisible(ordisurf(spe_ca ~ Teso, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Telestes souffia\"))\nabline(h = 0, v = 0, lty = 3)\n\ninvisible(ordisurf(spe_ca ~ Cogo, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Cottus gobio\"))\nabline(h = 0, v = 0, lty = 3)\n\nenv &lt;- env[-8, ] # because we removed the eighth site in the spp data\n\n# A posteriori projection of environmental variables in a CA\n# The last plot produced (CA scaling 2) must be active\nspe_ca_env &lt;- envfit(spe_ca, env, scaling = 2) # Scaling 2 is default\nplot(spe_ca_env)\n\n# Plot significant variables with a different colour\nplot(spe_ca_env, p.max = 0.05, col = \"red\")\npar(opar)\n\n\n\n\n\n\nFigure 3: CA ordination plots with species response surfaces of the Doubs River species data emphasising four species of fish: A) Satr, B) Scer, C) Teso, and D) Cogo. D) additionally has the environmental vectors projected on the plot, with the significant vectors shown in red.\n\n\n\n\nThe species response surfaces in Figure 3 show the change of species abundance across the ordination space and the vectors indicate how the species distribution and abundance relate to the predominant environmental gradients. Seen in this way, it quickly becomes evident that the biplot is a simplification of coenospaces.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "9a: Correspondence Analysis"
    ]
  },
  {
    "objectID": "BCB743/CA.html#references",
    "href": "BCB743/CA.html#references",
    "title": "Correspondence Analysis (CA)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "9a: Correspondence Analysis"
    ]
  },
  {
    "objectID": "BCB743/PCA_examples.html",
    "href": "BCB743/PCA_examples.html",
    "title": "PCA: Additional Examples",
    "section": "",
    "text": "Below I offer a simple example of how to perform a Principal Component Analysis (PCA) on the Iris dataset. This is not an ecological dataset, but it nevertheless works as a nice example of how to perform a PCA.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8b: PCA: Additional Examples"
    ]
  },
  {
    "objectID": "BCB743/PCA_examples.html#the-iris-data",
    "href": "BCB743/PCA_examples.html#the-iris-data",
    "title": "PCA: Additional Examples",
    "section": "The Iris Data",
    "text": "The Iris Data\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(ggcorrplot) # for the correlations\nlibrary(ggpubr)\ndata(\"iris\")\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThe Iris dataset is a well-known collection of data that represent the morphological characteristics of three species of Iris, viz. I. setosa, I. versicolor, and I. virginica. The morphological characteristics measured include sepal length and width and petal length and width.\nThe question we can address using a PCA is, “which of these variables (sepal length and width, petal length and width) is most responsible for causing visual morphological differences between the three species?”",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8b: PCA: Additional Examples"
    ]
  },
  {
    "objectID": "BCB743/PCA_examples.html#visualise-the-raw-data",
    "href": "BCB743/PCA_examples.html#visualise-the-raw-data",
    "title": "PCA: Additional Examples",
    "section": "Visualise the Raw Data",
    "text": "Visualise the Raw Data\nThe first thing to do after having loaded the data is to see how the variables are correlated with one-another, and we can do so with a simple pairwise correlation. I’ll demonstrate five ways of doing so.\nMethod 1\n\ncorr &lt;- cor(iris[, 1:4])\n\nggcorrplot(corr, type = 'upper', outline.col = \"white\",\n           colors = c(\"#00AFBB\", \"white\", \"#FC4E07\"),\n           lab = TRUE)\n\n\n\n\n\n\n\nMethod 2\n\ncols &lt;- c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\npairs(iris[, 1:4], pch = 19,  cex = 0.5,\n      col = cols[iris$Species],\n      lower.panel = NULL)\n\n\n\n\n\n\n\nMethod 3\n\nlibrary(GGally)\nggpairs(iris, aes(colour = Species, alpha = 0.4)) +\n  scale_color_discrete(type = cols) +\n  scale_fill_discrete(type = cols)\n\n\n\n\n\n\n\nMethod 4\n\nlibrary(scatterPlotMatrix)\nscatterPlotMatrix(iris, zAxisDim = \"Species\")\n\n\n\n\n\nMethod 5\n\niris |&gt; \n  pivot_longer(cols = Sepal.Length:Petal.Width,\n               values_to = \"mm\",\n               names_to = \"structure\") |&gt; \n  ggplot(aes(x = structure, y = mm)) +\n  geom_jitter(aes(colour = Species), shape = 9, width = 0.3, alpha = 0.6) +\n  scale_color_discrete(type = cols) +\n  coord_flip() +\n  theme_bw() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"grey60\", linetype = \"dashed\")\n  )\n\n\n\n\n\n\n\nBy examining all the plots, above (but particularly the simplest one in Method 5), what can we conclude about which morphological variable is most responsible for the visual differences among species? The petal dimensions seem to be the most telling by virtue of their being less overlap of point representing the three species, particularly that of its length. The dimensions of the sepals seem to be less important as offering a way to distinguish the species.\nA PCA should be able to reduce the complexity of measurements and tell us which of the four variables is most able to tell the species apart. It should reduce the four dimensions (sepal width and length, and petal width and length) into the most influential one or two rotated and scaled orthogonal dimensions (axes).",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8b: PCA: Additional Examples"
    ]
  },
  {
    "objectID": "BCB743/PCA_examples.html#do-the-pca",
    "href": "BCB743/PCA_examples.html#do-the-pca",
    "title": "PCA: Additional Examples",
    "section": "Do the PCA",
    "text": "Do the PCA\n\niris_pca &lt;- rda(iris[, 1:4], scale = FALSE)\niris_pca\n\nCall: rda(X = iris[, 1:4], scale = FALSE)\n\n              Inertia Rank\nTotal           4.573     \nUnconstrained   4.573    4\nInertia is variance \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4 \n4.228 0.243 0.078 0.024 \n\n\n\nsummary(iris_pca, display = \"sp\") # omit display of site scores\n\n\nCall:\nrda(X = iris[, 1:4], scale = FALSE) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal           4.573          1\nUnconstrained   4.573          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                         PC1     PC2     PC3      PC4\nEigenvalue            4.2282 0.24267 0.07821 0.023835\nProportion Explained  0.9246 0.05307 0.01710 0.005212\nCumulative Proportion 0.9246 0.97769 0.99479 1.000000",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8b: PCA: Additional Examples"
    ]
  },
  {
    "objectID": "BCB743/PCA_examples.html#plot-the-pc-scores-as-a-normal-panel-of-points",
    "href": "BCB743/PCA_examples.html#plot-the-pc-scores-as-a-normal-panel-of-points",
    "title": "PCA: Additional Examples",
    "section": "Plot the PC scores as a normal panel of points",
    "text": "Plot the PC scores as a normal panel of points\n\nPC1_scores &lt;- as.data.frame(scores(iris_pca, choices = c(1, 2, 3, 4), display = \"sites\"))\nPC1_scores$Species &lt;- iris$Species\n\nPC1_scores |&gt; \n  pivot_longer(cols = PC1:PC4,\n               values_to = \"score\",\n               names_to = \"PC\") |&gt; \n  ggplot(aes(x = PC, y = score)) +\n  geom_jitter(aes(colour = Species), shape = 9, width = 0.3, alpha = 0.6) +\n  scale_color_discrete(type = cols) +\n  coord_flip() +\n  theme_bw() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"pink\", linetype = \"dashed\")\n  )",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8b: PCA: Additional Examples"
    ]
  },
  {
    "objectID": "BCB743/PCA_examples.html#make-biplots",
    "href": "BCB743/PCA_examples.html#make-biplots",
    "title": "PCA: Additional Examples",
    "section": "Make Biplots",
    "text": "Make Biplots\nA default biplot\n\nbiplot(iris_pca, type = c(\"text\", \"points\"))\n\n\n\n\n\n\n\nA ggplot() biplot\nAssemble a biplot from scratch in ggplot2. This requires that we extract from the iris_pca object all the necessary components and layer them one-by-one using ggplot():\n\nlibrary(ggforce) # for geom_circle\n\n# species scores (actually morph properties here) for biplot arrows:\niris_spp_scores &lt;- data.frame(scores(iris_pca, display = \"species\"))\n\n# add center point for arrows to start at:\niris_spp_scores$xy_start &lt;- rep(0, 4)\n\n# add the rownames as a column for plotting at the arrow heads:\niris_spp_scores$morph &lt;- rownames(iris_spp_scores)\nrownames(iris_spp_scores) &lt;- NULL\n\n# var explained along PC1 used for labeling the x-axis:\nPC1_var &lt;- round(iris_pca$CA$eig[1] / sum(iris_pca$CA$eig) * 100, 1)\n\n# var explained along PC2 used for labeling the y-axis:\nPC2_var &lt;- round(iris_pca$CA$eig[2] / sum(iris_pca$CA$eig) * 100, 1)\n\n# calculate the radius of the circle of equilibrium contribution\n# (Num Ecol with R, p. 125):\nr &lt;- sqrt(2/4)\n\n# species scores (actually indiv measurements here) for biplot points:\niris_site_scores &lt;- data.frame(scores(iris_pca, display = \"sites\"))\niris_site_scores$Species &lt;- iris$Species\n\nggplot(iris_site_scores, aes(x = PC1, y = PC2)) +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 0), linetype = \"dashed\") +\n  geom_point(aes(colour = Species), shape = 9) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = r), # not yet correctly scaled!!\n              linetype = 'dashed',\n              lwd = 0.6, inherit.aes = FALSE) +\n  geom_segment(data = iris_spp_scores, aes(x = xy_start, y = xy_start,\n                                           xend = PC1, yend = PC2),\n               lineend = \"butt\",\n               arrow = arrow(length = unit(3, \"mm\"),\n                             type = \"closed\",\n                             angle = 20),\n               alpha = 0.7, colour = \"dodgerblue\") +\n  geom_label(data = iris_spp_scores, aes(x = PC1, y = PC2, label = morph),\n             nudge_y = -0.12,\n             colour = \"dodgerblue\") +\n  scale_color_discrete(type = cols) +\n  coord_equal() +\n  scale_x_continuous(limits = c(-1, 4.6)) +\n  labs(x = paste0(\"PC1 (\", PC1_var, \"% variance explained)\"),\n       y = paste0(\"PC2 (\", PC2_var, \"% variance explained)\")) +\n  theme_bw() +\n  theme(\n    panel.grid.major.x = element_line(colour = \"pink\", linetype = \"dashed\"),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"pink\", linetype = \"dashed\"),\n    panel.grid.minor.y = element_blank(),\n    legend.position = c(0.9, 0.2),\n    legend.box.background = element_rect(colour = \"black\")\n  )\n\n\n\n\n\n\n\nWhat do we see in the biplot? We see that most of the variation in morphology between the three Iris species is explained by PC1 (obviously), which accounts for 92.5% of the total inertia. Very little is added along PC2 (only an additional 5.3% variance explained), so we may safely ignore it. Looking at the ‘Species scores’ associated with PC1 (see summary(iris_pca)), we see that the heaviest loading is with petal length, which causes the long arrow in the positive PC1 direction; it has virtually no loading along PC2, and this is confirmed by the fact that the arrow is positioned almost parallel along PC1 and does not deviate up or down in the PC2 direction. We can also see that the biplot arrow for petal width sits completely on top of the petal length arrow. This means that petal length and width are almost perfectly correlated (we can also see this in the pairwise correlations where the r-value is 0.96).",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "8b: PCA: Additional Examples"
    ]
  },
  {
    "objectID": "BCB743/nMDS.html",
    "href": "BCB743/nMDS.html",
    "title": "non-Metric Multidimensional Scaling (nMDS)",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\n\nType\nName\nLink\n\n\n\n\nTheory\nNumerical Ecology in R\nSee pages 145-151\n\n\nSlides\nnMDS lecture slides\n💾 BCB743_11_nMDS.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\nNon-Metric Multidimensional Scaling (nMDS) is a rank-based indirect gradient analysis that uses a distance or dissimilarity matrix as its input (either pre-calculated using vegdist() or constructed internal to the metaMDS() function via the dist argument). Should one supply a ‘raw’ species × site table, the default dissimilarity matrix is Bray-Curtis dissimilarity, but any other dissimilarity index in vegdist() can be specified. Unlike other ordination methods such as Principal Component Analysis (PCA) and Correspondence Analysis (CA), which aim to maximise variance or correspondence between sites, nMDS focuses on representing the pairwise dissimilarities between sites in an ordination space. It does not use the raw distances or dissimilarities directly; instead, these values are replaced with their ranks, which is why the method is termed “non-metric.”\nnMDS is the non-metric equivalent of Principal Coordinates Analysis (PCoA); in fact, PCoA is sometimes referred to as metric multidimensional scaling. PCoA and nMDS can both produce ordinations of objects from any distance or dissimilarity matrix. However, nMDS does not preserve the exact distances among objects in an ordination plot. Instead, it tries to represent the ordering (rank) relationships among objects as accurately as possible on a specified number of axes. This nonlinear mapping of dissimilarities onto a low-dimensional ordination space means that the Euclidean distances of points in the ordination space are rank-order similar to the original community dissimilarities.\nThe ordination space in nMDS is metric, but the regression used to fit the dissimilarities to this space is non-metric. This makes nMDS more robust than the eigen-value methods, especially when the data are not well-represented by a specific distribution as may sometimes be the case for ecological data. As with PCoA, it can handle quantitative, semi-quantitative, qualitative, or mixed variables, so we can flexibly apply it to many ecological problems.\nA new concept, not seen in the eigen-approaches, is the idea of ‘stress.’ Stress quantifies the discrepancy between the observed dissimilarities and the distances in the ordination space. Stress is visually presented as the scatter of observed dissimilarities against the expected monotone regression. Lower stress values indicate a better fit of the data to the ordination space. Because rank orders of dissimilarities cannot be exactly preserved by rank-orders of ordination distances in low-dimensional space, some stress cannot be avoided.\nnMDS does have some limitations. The rank-based approach means that information about the magnitude of differences between site pairs is lost and this can be a disadvantage when the actual distances are important for interpretation. Also, nMDS can be computationally intensive with large datasets or when trying to minimise stress through numerous iterations.\nAfter performing nMDS, environmental interpretation can be facilitated using vegan’s envfit() and ordisurf() functions, as we have already seen in PCA, CA, and PCoA. As before, they allow for the fitting of environmental variables onto the ordination. This aids in visualising and understanding how the environmental variables influence the species ordination. Again, I must emphasis that this is not the same as doing a formal constrained ordination, which will be discussed in the next section (RDA and CCA). Additionally, if we require a statistical framework to assess the influence of categorical factors on the observed dissimilarities, we can use PERMANOVA (Permutational Multivariate Analysis of Variance) to test for differences that might be attributed to group effects. These ideas will be demonstrated below.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11a: Non-metric Multidimensional Scaling"
    ]
  },
  {
    "objectID": "BCB743/nMDS.html#set-up-the-analysis-environment",
    "href": "BCB743/nMDS.html#set-up-the-analysis-environment",
    "title": "non-Metric Multidimensional Scaling (nMDS)",
    "section": "Set-Up the Analysis Environment",
    "text": "Set-Up the Analysis Environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(viridis)\n\n# setting up a 'root' file path so I don't have to keep doing it later...\nroot &lt;- \"../data/\"",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11a: Non-metric Multidimensional Scaling"
    ]
  },
  {
    "objectID": "BCB743/nMDS.html#the-doubs-river-data",
    "href": "BCB743/nMDS.html#the-doubs-river-data",
    "title": "non-Metric Multidimensional Scaling (nMDS)",
    "section": "The Doubs River Data",
    "text": "The Doubs River Data\nWe continue to use the species data:\n\nload(paste0(root, \"NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\"))\nspe &lt;- dplyr::slice(spe, -8)",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11a: Non-metric Multidimensional Scaling"
    ]
  },
  {
    "objectID": "BCB743/nMDS.html#do-the-nmds",
    "href": "BCB743/nMDS.html#do-the-nmds",
    "title": "non-Metric Multidimensional Scaling (nMDS)",
    "section": "Do the nMDS",
    "text": "Do the nMDS\n\n1spe_nmds &lt;- metaMDS(spe, distance = \"bray\", trace = 0)\nspe_nmds\n\n\n1\n\nI use trace = 0 to suppress the output of the iterations.\n\n\n\n\n\nCall:\nmetaMDS(comm = spe, distance = \"bray\", trace = 0) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     spe \nDistance: bray \n\nDimensions: 2 \nStress:     0.07383663 \nStress type 1, weak ties\nBest solution was not repeated after 20 tries\nThe best solution was from try 10 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'spe' \n\n\nAs always, reading the help file (accessible as ?vegan::metaMDS) is invaluable, as it is for all other ordination techniques.\nThere’s a summary method available, but it is not particularly useful and I don’t display the output here:\n\nsummary(spe_nmds)\n\nAlthough summary(spe_nmds) does not return anything interesting, the species and site scores are nevertheless available directly through the scores() command, and they can be plotted as layer in ggplot2 if need be:\n\nscores(spe_nmds)\n\n$sites\n         NMDS1       NMDS2\n1  -1.79030700  0.81627940\n2  -1.14283792 -0.16029572\n3  -1.00160702 -0.14778221\n4  -0.62112404 -0.08255030\n5   0.07295609  0.45828960\n6  -0.42762076 -0.15598943\n7  -0.87429643 -0.23120352\n8  -0.01506354 -0.86128619\n9  -0.52497940 -0.40525880\n10 -1.00839414 -0.37979671\n11 -0.97805856 -0.08559725\n12 -1.15922594  0.10409874\n13 -0.80520832  0.12745554\n14 -0.49408674  0.18096462\n15 -0.18710477  0.28001633\n16  0.08426766  0.12223470\n17  0.29752301  0.11348497\n18  0.44599916  0.14733905\n19  0.77078678  0.28186685\n20  0.86581233  0.37461844\n21  0.95565447  0.44335648\n22  0.75234556 -1.44653979\n23  1.13039673 -0.63131939\n24  0.85989408 -0.87168749\n25  0.93317091  0.12297337\n26  0.97418656  0.36676972\n27  1.02422882  0.37679560\n28  0.79548581  0.55876891\n29  1.06720661  0.58399447\n\n$species\n          NMDS1       NMDS2\nCogo -0.9130076 -0.07653571\nSatr -1.1128954 -0.22563057\nPhph -0.7889489 -0.32476577\nBabl -0.5342073 -0.29329558\nThth -0.9390579 -0.06898398\nTeso -0.5243956  0.16916607\nChna  0.9023733  0.36442990\nPato  0.5180442  0.38552811\nLele  0.3307598  0.28576947\nSqce  0.3584800 -0.15225009\nBaba  0.7042294  0.47527741\nAlbi  0.7284594  0.46988216\nGogo  0.6856238  0.30371673\nEslu  0.6097670  0.41713674\nPefl  0.6174938  0.50243339\nRham  0.9667597  0.58308133\nLegi  0.9563545  0.51021506\nScer  0.9660788  0.55152417\nCyca  0.9585354  0.62960316\nTiti  0.7245647  0.41753246\nAbbr  1.0823537  0.68035414\nIcme  1.1273075  0.78456940\nGyce  1.0742532  0.40811571\nRuru  0.7597479  0.15062704\nBlbj  1.0951530  0.58135119\nAlal  0.9938664  0.02649176\nAnan  1.0089923  0.61329546\n\n\nSee Numerical Ecology in R (pp. 145 to 149) for information about the interpretation of a nMDSand the ordination diagrams shown below.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11a: Non-metric Multidimensional Scaling"
    ]
  },
  {
    "objectID": "BCB743/nMDS.html#ordination-diagrams",
    "href": "BCB743/nMDS.html#ordination-diagrams",
    "title": "non-Metric Multidimensional Scaling (nMDS)",
    "section": "Ordination Diagrams",
    "text": "Ordination Diagrams\nWe create the ordination diagrammes as before, but new concepts introduced here are stress, Shepard plots, and goodness of fit (Figure 1). The stress indicates the scatter of observed dissimilarities against an expected monotone regression, while a Shepard diagram plots ordination distances against original dissimilarities, and adds a monotone or linear fit line to highlight this relationship. The stressplot() function also produces two fit statistics. The goodness-of-fit of the ordination is measured as the \\(R^{2}\\) of either a linear or a non-linear regression of the nMDS distances on the original ones.\n\nopar &lt;- par(no.readonly = TRUE)\npar(mfrow = c(2, 2))\nstressplot(spe_nmds, main = \"Shepard plot\")\nordiplot(spe_nmds, type = \"t\", cex = 1.2,\n         main = paste0(\"nMDS stress = \", round(spe_nmds$stress, 2)))\ngof &lt;- goodness(spe_nmds)\nplot(spe_nmds, type = \"t\", cex = 1.2, main = \"Goodness of fit\")\npoints(spe_nmds, display = \"sites\", cex = gof * 200)\n# ...bigger bubbles indicate a worse fit\npar(opar)\n\n\n\n\n\n\n\nFigure 1: nMDS ordination plots of the Doubs River species data showing a Shepard plot (top, left), the ordination diagram (top, right), and goodness of fit (bottom, right).\n\n\n\n\n\nA good rule of thumb: stress &lt;0.05 provides an excellent representation in reduced dimensions, &lt;0.1 is great, &lt;0.2 is so-so, and stress &lt;0.3 provides a poor representation.\nWe can also build ordination plots from scratch to suit specific needs:\n\npl &lt;- ordiplot(spe_nmds, type = \"none\", main = \"nMDS fish abundances \")\npoints(pl, \"sites\", pch = 21, cex = 1.75, col = \"grey80\", bg = \"grey80\")\npoints(pl, \"species\", pch = 21, col = \"turquoise\", arrows = TRUE)\ntext(pl, \"species\", col = \"blue4\", cex = 0.9)\ntext(pl, \"sites\", col = \"red4\", cex = 0.9)\n\n\n\n\n\n\n\nFigure 2: nMDS ordination plot of the Doubs River species data assembled from scratch.\n\n\n\n\n\nOr we can fit response surfaces using ordisurf() and project environmental drivers (Figure 3):\n\npalette(viridis(8))\nopar &lt;- par(no.readonly = TRUE)\npar(mar = c(4, 4, 0.9, 0.5) + .1, mfrow = c(2, 2))\n\ninvisible(ordisurf(spe_nmds ~ Satr, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Salmo trutta fario\"))\nabline(h = 0, v = 0, lty = 3)\n\ninvisible(ordisurf(spe_nmds ~ Scer, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Scardinius erythrophthalmus\"))\nabline(h = 0, v = 0, lty = 3)\n\ninvisible(ordisurf(spe_nmds ~ Teso, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Telestes souffia\"))\nabline(h = 0, v = 0, lty = 3)\n\ninvisible(ordisurf(spe_nmds ~ Cogo, data = spe, bubble = 3,\n                   family = quasipoisson, knots = 2, col = 6,\n                   display = \"sites\", main = \"Cottus gobio\"))\nabline(h = 0, v = 0, lty = 3)\n\nenv &lt;- env[-8, ] # because we removed the eighth site in the spp data\n\n# A posteriori projection of environmental variables in a CA\n# The last plot produced (CA scaling 2) must be active\nspe_nmds_env &lt;- envfit(spe_nmds, env, scaling = 2) # Scaling 2 is default\nplot(spe_nmds_env)\n\n# Plot significant variables with a different colour\nplot(spe_nmds_env, p.max = 0.05, col = \"red\")\npar(opar)\n\n\n\n\n\n\n\nFigure 3: nMDS ordination plots with species response surfaces of the Doubs River species data emphasising four species of fish: A) Satr, B) Scer, C) Teso, and D) Cogo. D) additionally has the environmental vectors projected on the plot, with the significant vectors shown in red.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11a: Non-metric Multidimensional Scaling"
    ]
  },
  {
    "objectID": "BCB743/nMDS.html#references",
    "href": "BCB743/nMDS.html#references",
    "title": "non-Metric Multidimensional Scaling (nMDS)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "11a: Non-metric Multidimensional Scaling"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html",
    "href": "BCB743/unconstrained-summary.html",
    "title": "Summary: Unconstrained Ordinations",
    "section": "",
    "text": "In all the ordination techniques we have seen thus far, the primary goal is to represent high-dimensional data in a lower-dimensional space (usually 2D or 3D) while preserving as much of the original structure as possible. Points that are close together in the ordination plot are generally more similar in the original high-dimensional space.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#principal-component-analysis-pca",
    "href": "BCB743/unconstrained-summary.html#principal-component-analysis-pca",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nData Type: Continuous data (e.g., environmental variables)\nInterpretation: PCA identifies axes (principal components) that explain the maximum variation in the data. The axes represent linear combinations of the original variables that explain the most variance. The distance between points reflects their Euclidean dissimilarity. The loading values of variables on the axes indicate their contribution to the variation. Angles between variable arrows in the biplot represent correlations.\nIn vegan: rda() without constraining variables is used for PCA. Biplots can be created using the biplot() function, showing both sample scores and variable loadings.\nAssumption: PCA assumes linear relationships between variables.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#correspondence-analysis-ca",
    "href": "BCB743/unconstrained-summary.html#correspondence-analysis-ca",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Correspondence Analysis (CA)",
    "text": "Correspondence Analysis (CA)\n\nData Type: Categorical or count data (e.g., species abundance)\nInterpretation: CA explores the relationship between rows (e.g., sites) and columns (e.g., species), i.e. the biplot shows the relationships between rows and columns; but pay attention to the scaling. The distances between points in the ordination space reflect their \\(\\chi^2\\) dissimilarity. Preserves original distances as well as possible in low-dimensional space.\nIn vegan: The cca() function with no constraining variables, specified with the formula = ~., is used for CA. Similar to PCA, biplots can be created.\nAssumption: Assumes unimodal species responses and weighted averaging.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#principal-coordinate-analysis-pcoa",
    "href": "BCB743/unconstrained-summary.html#principal-coordinate-analysis-pcoa",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Principal Coordinate Analysis (PCoA)",
    "text": "Principal Coordinate Analysis (PCoA)\n\nData Type: Distance or dissimilarity matrices (any type of distance) in vegan’s vegdist()\nInterpretation: PCoA aims to represent the distances between objects in a low-dimensional space while preserving the original dissimilarities as much as possible. Interpretation depends on the chosen distance measure.\nIn vegan: The capscale() function with a distance matrix as input performs PCoA. Biplots are not directly applicable to PCoA but figures can be constructed in layers using ordiplot(), etc.\nAssumption: PCoA does not assume linear relationships between variables.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#non-metric-multidimensional-scaling-nmds",
    "href": "BCB743/unconstrained-summary.html#non-metric-multidimensional-scaling-nmds",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Non-Metric Multidimensional Scaling (NMDS)",
    "text": "Non-Metric Multidimensional Scaling (NMDS)\n\nData Type: Distance or dissimilarity matrices (any type of distance)\nInterpretation: NMDS is an iterative method that tries to arrange objects in low-dimensional space so that the rank order of distances in the ordination matches the rank order of the original dissimilarities. The stress value indicates how well the ordination represents the original distances. Like PCoA, interpretation depends on the chosen distance measure. nMDS is considered more robust as it doesn’t assume linearity, but it can be sensitive to outliers and tied ranks.\nIn vegan: The metaMDS() function is used for NMDS. You can use the envfit() function to add environmental variables or species scores to the plot, but it’s an indirect fitting process.\nAssumption: nMDS does not assume linear relationships between variables.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#which-method-to-choose",
    "href": "BCB743/unconstrained-summary.html#which-method-to-choose",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Which Method to Choose?",
    "text": "Which Method to Choose?\nThis depends on your data type, research question, and the type of dissimilarities you want to analyse:\n\nData Type: CA is more suitable for abundance data with many zeros, while PCA is better for continuous environmental variables. PCoA and NMDS for dissimilarities (use Gower distances for categorical, ordinal, or binary data types).\nDistance/Dissimilarity: If you have a specific distance measure in mind (e.g., Bray-Curtis), use PCoA or NMDS.\nLinear vs. Non-linear: PCA and CA assume linear relationships, while NMDS can capture non-linear patterns.\nFocus: If you want to emphasise species composition, CA or NMDS might be suitable. If the focus is on the underlying gradients explaining the variation, PCA or PCoA could be preferred.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#considerations-in-vegan",
    "href": "BCB743/unconstrained-summary.html#considerations-in-vegan",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Considerations in vegan",
    "text": "Considerations in vegan\n\nStandardisation: Pay attention to data standardisation or transformation before analysis—this is especially the case for environmental variables measured along different units. vegan provides various options for standardisation, or use functions in base R. Species data typically do not require transformation, unless some special considerations are needed, for instance when working with overly dominant or rare species.\nEcological Interpretation: Use vegan’s envfit() and ordistep() to facilitate the interpretation of the relationship between community composition and environmental variables in ordination plots.\nDimensionality: Typically we visualise the relationships in 2D plots, but higher dimensions may be important. We can use vegan’s screeplot() function to help determine how many influential axes to retain.\nScaling: The scaling of ordination biplots can affect interpretation. scaling = 1 emphasises relationships among samples and scaling = 2 emphasises relationships among variables.\nProportion of Variance Explained: The vegan functions provide information on the proportion of variation explained by the reduced axes for PCA, CA, and PCoA.\nPlotting: The ordiplot() function provides a consistent interface for plotting different ordination results. There are also various ways to enhance ordination plots, such as ordihull(), ordiellipse() for grouping; envfit() for fitting environmental variables; ordisurf() for response surfaces. You can also access the various components of the ordination results (e.g., scores, loadings) for custom plotting with ggplot2, which might be necessary to create more insightful and less cluttered figures for publication.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#applying-ordination-techniques-to-environmental-data",
    "href": "BCB743/unconstrained-summary.html#applying-ordination-techniques-to-environmental-data",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Applying Ordination Techniques to Environmental Data",
    "text": "Applying Ordination Techniques to Environmental Data\nTypically, ordinations are applied to species data. Sometimes, however, we may want to apply ordinations to the environmental data itself. In this way, we allow the ‘environment’ to speak for itself, revealing some patterns that we may then use to inform subsequent analyses. For example:\n\nIt can reveal the presence of correlated variables, which can be problematic in subsequent analyses. For example, if two variables are highly correlated, they may both appear to be important in explaining the species data, but in reality, only one of them is driving the patterns. Such correlated variable can be seen on the ordination plots as vectors pointing in the same direction.\nIt can help identify major gradients in the environmental variables, and this can then be related to the species composition. The lengths of the environmental vectors on ordination plots can be used to infer the importance of the variables in structuring the data. Strong gradients can be hypothesised to influence species composition. So, once we have set up hypotheses about the presumed influential environmental gradients, we can explore how these gradients correlate with the species data. Even without directly analysing the species data, we can infer potential influences of environmental factors on species distributions and community composition.\nAnother way to identify influential variables that might not form strong gradients is by plotting the ordination results and identifying clusters of similar environmental conditions. As before, we can use these clusters to hypothesise about potential similarities in community structure in these areas.\nIt allows us to develop a solid understanding of the environmental variation across the landscape and sets a baseline for interpreting any patterns observed in the species data. These ordination results can be plotted on maps for supplementary visualisations of the environmental gradients across the study area.\nWe can use functions like envfit() (see below) to fit species data to the environmental ordination space, which will facilitate our understanding of how well the environmental variables explain the species composition. If the environmental variables explain a large proportion of the variation in the species data, this suggests a strong relationship between the environment and the species composition.\nAn analysis of the environmental data can also lead us to further analyses, such as some of the constrained ordinations or multiple linear regression, which directly relate environmental variables to species data.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/unconstrained-summary.html#linking-environmental-properties-to-species-data",
    "href": "BCB743/unconstrained-summary.html#linking-environmental-properties-to-species-data",
    "title": "Summary: Unconstrained Ordinations",
    "section": "Linking Environmental Properties to Species Data",
    "text": "Linking Environmental Properties to Species Data\nTypically, one is interested in understanding the relationship between species composition and environmental variables. This can be achieved by fitting environmental variables to the ordination space using envfit(), ordisurf(), ordiellipse(), ordispider(), and ordihull(). The ordistep() function can help identify the most important variables.\nenvfit() involves performing an unconstrained ordination on the species data alone and afterwards fitting environmental vectors onto the ordination plot. The environmental vectors are projected onto the ordination space, and their direction and length indicate the correlation and strength of each environmental variable with the ordination axes. The envfit() function can also be used to test the significance of the environmental variables in structuring the species data. We use envfit() to explore species patterns first and then see how these patterns are related to the environment—so, our primary interest is in understanding the intrinsic patterns of species composition without initially imposing any constraints from environmental data.\nordisuf() and ordiellipse() are used to visualise the response of species composition to environmental gradients or factors. ordisurf() fits a response surface to the ordination plot, showing how species composition changes along the environmental gradients. ordiellipse() draws ellipses around groups of samples, which can be defined by environmental variables or other factors. ordispider() and ordihull() are used to draw lines or polygons around groups of samples, respectively. These functions, therefore, show us how gradients vary across the landscape, and how species or sites are related to some categorical influential variables.\nConstrained ordination (also known as canonical ordination) directly incorporates environmental variables into the ordination process. The ordination axes are linear combinations of environmental variables, meaning that the ordination is directly constrained by the environmental data. To do this, we do a constrained ordination (such as db-RDA or CCA), where the species data are directly related to the environmental variables. This allows us to to explicitly model the variation in species data that can be explained by the environmental variables, and it helps us understand the direct influence of environmental factors on species composition. Typically, we would choose constrained ordinations when our primary interest is in understanding how much of the variation in species composition can be explained by environmental variables. It is also useful when we have some hypotheses about the influence of a priori selected environmental variables on species distribution and want to test them formally. Lastly, constrained ordination also lets us partition the variance in species data into components explained by different kinds of environmental variables, and in so doing revealing also the residual (unexplained) components. Use the capscale() function in vegan to perform constrained ordination (see Distance-Based Redundancy Analysis). This allows us to explore how environmental variables structure the data and how they relate to each other.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "12: Unconstrained Ordination Summary"
    ]
  },
  {
    "objectID": "BCB743/correlations.html",
    "href": "BCB743/correlations.html",
    "title": "Correlations and Associations",
    "section": "",
    "text": "Material required for this chapter\n\n\n\n\n\nType\nName\nLink\n\n\n\nSlides\nCorrelation lecture slides\n💾 BCB743_06_correlations.pdf\n\n\nData\nThe Doubs River data\n💾 Doubs.RData\nYou were introduced to correlations in BCB744, and you will now revisit this concept in the context of environmental data.",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "5: Correlations & Associations"
    ]
  },
  {
    "objectID": "BCB743/correlations.html#set-up-the-analysis-environment",
    "href": "BCB743/correlations.html#set-up-the-analysis-environment",
    "title": "Correlations and Associations",
    "section": "Set-Up the Analysis Environment",
    "text": "Set-Up the Analysis Environment\n\nlibrary(tidyverse)\nlibrary(vegan)\nlibrary(Hmisc) # for rcorr()",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "5: Correlations & Associations"
    ]
  },
  {
    "objectID": "BCB743/correlations.html#the-doubs-river-data",
    "href": "BCB743/correlations.html#the-doubs-river-data",
    "title": "Correlations and Associations",
    "section": "The Doubs River Data",
    "text": "The Doubs River Data\nThe background to the data is described by David Zelený on his excellent website and in the book Numerical Ecology with R by Borcard et al. (2011). These data are a beautiful example of how gradients structure biodiversity. It will be in your own interest to fully understand how the various environmental factors used as explanatory variables vary along a riverine gradient from the source to the terminus of the river.\nCorrelations between environmental variables\nCorrelation refers to the statistical (non-causal) relationship between two continuous variables. It measures the extent to which changes in one variable correspond to changes in another variable. Correlations are quantified into values ranging from -1 and +1, with -1 indicating a perfect negative correlation, +1 indicating a perfect positive correlation, and 0 indicating no correlation. A positive correlation implies that as one variable increases, the other variable also increases. Conversely, a negative correlation implies that as one variable increases, the other decreases. Correlation can be calculated using several methods, the most common one being the Pearson correlation coefficient. Non-parametric correlations can be applied to ordinal or non-normal data.\n\nload(\"../data/NEwR-2ed_code_data/NEwR2-Data/Doubs.RData\")\n\nhead(env, 5)\n\n   dfs ele  slo  dis  pH har  pho  nit  amm  oxy bod\n1  0.3 934 48.0 0.84 7.9  45 0.01 0.20 0.00 12.2 2.7\n2  2.2 932  3.0 1.00 8.0  40 0.02 0.20 0.10 10.3 1.9\n3 10.2 914  3.7 1.80 8.3  52 0.05 0.22 0.05 10.5 3.5\n4 18.5 854  3.2 2.53 8.0  72 0.10 0.21 0.00 11.0 1.3\n5 21.5 849  2.3 2.64 8.1  84 0.38 0.52 0.20  8.0 6.2\n\n\nWe use correlations to establish how the environmental variables relate to one another across the sample sites. We do not need to standardise as one would do for the calculation of Euclidian distances, but in some instances data transformations might be necessary:\n\nenv_cor &lt;- round(cor(env), 2)\nenv_cor\n\n      dfs   ele   slo   dis    pH   har   pho   nit   amm   oxy   bod\ndfs  1.00 -0.94 -0.38  0.95  0.01  0.70  0.48  0.75  0.41 -0.51  0.39\nele -0.94  1.00  0.44 -0.87 -0.04 -0.74 -0.44 -0.76 -0.38  0.36 -0.34\nslo -0.38  0.44  1.00 -0.34 -0.22 -0.53 -0.19 -0.31 -0.17  0.31 -0.18\ndis  0.95 -0.87 -0.34  1.00  0.02  0.70  0.39  0.61  0.29 -0.36  0.25\npH   0.01 -0.04 -0.22  0.02  1.00  0.09 -0.08 -0.05 -0.12  0.18 -0.15\nhar  0.70 -0.74 -0.53  0.70  0.09  1.00  0.36  0.51  0.29 -0.38  0.34\npho  0.48 -0.44 -0.19  0.39 -0.08  0.36  1.00  0.80  0.97 -0.72  0.89\nnit  0.75 -0.76 -0.31  0.61 -0.05  0.51  0.80  1.00  0.80 -0.63  0.64\namm  0.41 -0.38 -0.17  0.29 -0.12  0.29  0.97  0.80  1.00 -0.72  0.89\noxy -0.51  0.36  0.31 -0.36  0.18 -0.38 -0.72 -0.63 -0.72  1.00 -0.84\nbod  0.39 -0.34 -0.18  0.25 -0.15  0.34  0.89  0.64  0.89 -0.84  1.00\n\n\nOr if we want to see the associated p-values to establish a statistical significance:\n\nrcorr(as.matrix(env))\n\n      dfs   ele   slo   dis    pH   har   pho   nit   amm   oxy   bod\ndfs  1.00 -0.94 -0.38  0.95  0.01  0.70  0.48  0.75  0.41 -0.51  0.39\nele -0.94  1.00  0.44 -0.87 -0.04 -0.74 -0.44 -0.76 -0.38  0.36 -0.34\nslo -0.38  0.44  1.00 -0.34 -0.22 -0.53 -0.19 -0.31 -0.17  0.31 -0.18\ndis  0.95 -0.87 -0.34  1.00  0.02  0.70  0.39  0.61  0.29 -0.36  0.25\npH   0.01 -0.04 -0.22  0.02  1.00  0.09 -0.08 -0.05 -0.12  0.18 -0.15\nhar  0.70 -0.74 -0.53  0.70  0.09  1.00  0.36  0.51  0.29 -0.38  0.34\npho  0.48 -0.44 -0.19  0.39 -0.08  0.36  1.00  0.80  0.97 -0.72  0.89\nnit  0.75 -0.76 -0.31  0.61 -0.05  0.51  0.80  1.00  0.80 -0.63  0.64\namm  0.41 -0.38 -0.17  0.29 -0.12  0.29  0.97  0.80  1.00 -0.72  0.89\noxy -0.51  0.36  0.31 -0.36  0.18 -0.38 -0.72 -0.63 -0.72  1.00 -0.84\nbod  0.39 -0.34 -0.18  0.25 -0.15  0.34  0.89  0.64  0.89 -0.84  1.00\n\nn= 30 \n\n\nP\n    dfs    ele    slo    dis    pH     har    pho    nit    amm    oxy   \ndfs        0.0000 0.0365 0.0000 0.9771 0.0000 0.0076 0.0000 0.0251 0.0040\nele 0.0000        0.0146 0.0000 0.8447 0.0000 0.0144 0.0000 0.0376 0.0493\nslo 0.0365 0.0146        0.0625 0.2362 0.0028 0.3067 0.0997 0.3593 0.1006\ndis 0.0000 0.0000 0.0625        0.9147 0.0000 0.0355 0.0004 0.1136 0.0522\npH  0.9771 0.8447 0.2362 0.9147        0.6405 0.6619 0.7976 0.5134 0.3494\nhar 0.0000 0.0000 0.0028 0.0000 0.6405        0.0481 0.0039 0.1191 0.0370\npho 0.0076 0.0144 0.3067 0.0355 0.6619 0.0481        0.0000 0.0000 0.0000\nnit 0.0000 0.0000 0.0997 0.0004 0.7976 0.0039 0.0000        0.0000 0.0002\namm 0.0251 0.0376 0.3593 0.1136 0.5134 0.1191 0.0000 0.0000        0.0000\noxy 0.0040 0.0493 0.1006 0.0522 0.3494 0.0370 0.0000 0.0002 0.0000       \nbod 0.0309 0.0677 0.3546 0.1770 0.4232 0.0619 0.0000 0.0001 0.0000 0.0000\n    bod   \ndfs 0.0309\nele 0.0677\nslo 0.3546\ndis 0.1770\npH  0.4232\nhar 0.0619\npho 0.0000\nnit 0.0001\namm 0.0000\noxy 0.0000\nbod       \n\n\nWe can also do a visual exploration (see Question 1, below).\n\n\n\n\n\n\nAssociation between species\nSpecies associations refer to the relationships or interactions between different species within an ecosystem or community. The term can be used to describe the outcome of a wide range of relationships, including competition, predation, symbiosis (mutualism, commensalism, parasitism), or simply the tendency for different species to occur in the same habitats or microhabitats.\nWhen two or more species are frequently found in the same area or under the same conditions, they are positively associated. This could be due to similar environmental preferences, mutualistic relationships, or one species depending on the presence of another. For example, bees and flowering plants have a mutualistic relationship where the bees gather nectar for food, and in the process, they pollinate the flowers. In this sense, bees would be positively associated with some flowering plants.\nConversely, if two species are rarely found in the same area or under the same conditions, they are negatively associated. This can be due to competition for resources, predation, or differing environmental preferences.\nAnalyses of species associations can help us understand the complex dynamics of ecological communities, including how species interact with each other and their environment, the roles they play in their ecosystems, and the effects of environmental changes on species distributions and community composition. A first glance insight into the existence of some of these types of interactions can be found by examining tables of association among species.\nThe Doubs River fish species dataset is an example of abundance data and it will serve well to examine the properties of an association matrix:\n\nhead(spe)\n\n  Cogo Satr Phph Babl Thth Teso Chna Pato Lele Sqce Baba Albi Gogo Eslu Pefl\n1    0    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    5    4    3    0    0    0    0    0    0    0    0    0    0    0\n3    0    5    5    5    0    0    0    0    0    0    0    0    0    1    0\n4    0    4    5    5    0    0    0    0    0    1    0    0    1    2    2\n5    0    2    3    2    0    0    0    0    5    2    0    0    2    4    4\n6    0    3    4    5    0    0    0    0    1    2    0    0    1    1    1\n  Rham Legi Scer Cyca Titi Abbr Icme Gyce Ruru Blbj Alal Anan\n1    0    0    0    0    0    0    0    0    0    0    0    0\n2    0    0    0    0    0    0    0    0    0    0    0    0\n3    0    0    0    0    0    0    0    0    0    0    0    0\n4    0    0    0    0    1    0    0    0    0    0    0    0\n5    0    0    2    0    3    0    0    0    5    0    0    0\n6    0    0    0    0    2    0    0    0    1    0    0    0\n\n\nIn order to calculate an association matrix for the fish species we first need to transpose the data:\n\nspe_t &lt;- t(spe)\n\nNow we can calculate the association matrix:\n\nspe_assoc1 &lt;- vegdist(spe_t, method = \"jaccard\")\n # display only a portion of the data...\nas.matrix((spe_assoc1))[1:10, 1:10]\n\n          Cogo      Satr      Phph      Babl      Thth      Teso      Chna\nCogo 0.0000000 0.7368421 0.7794118 0.7945205 0.3333333 0.4545455 0.9354839\nSatr 0.7368421 0.0000000 0.3108108 0.4705882 0.7368421 0.7333333 0.9583333\nPhph 0.7794118 0.3108108 0.0000000 0.2804878 0.7794118 0.7571429 0.9113924\nBabl 0.7945205 0.4705882 0.2804878 0.0000000 0.8108108 0.7397260 0.8481013\nThth 0.3333333 0.7368421 0.7794118 0.8108108 0.0000000 0.5833333 0.9000000\nTeso 0.4545455 0.7333333 0.7571429 0.7397260 0.5833333 0.0000000 0.8787879\nChna 0.9354839 0.9583333 0.9113924 0.8481013 0.9000000 0.8787879 0.0000000\nPato 0.8918919 0.9078947 0.7948718 0.7307692 0.9210526 0.7500000 0.4827586\nLele 0.8627451 0.8235294 0.7386364 0.6666667 0.9056604 0.7346939 0.6136364\nSqce 0.8360656 0.7978723 0.7346939 0.6562500 0.8730159 0.8281250 0.7017544\n          Pato      Lele      Sqce\nCogo 0.8918919 0.8627451 0.8360656\nSatr 0.9078947 0.8235294 0.7978723\nPhph 0.7948718 0.7386364 0.7346939\nBabl 0.7307692 0.6666667 0.6562500\nThth 0.9210526 0.9056604 0.8730159\nTeso 0.7500000 0.7346939 0.8281250\nChna 0.4827586 0.6136364 0.7017544\nPato 0.0000000 0.5000000 0.6774194\nLele 0.5000000 0.0000000 0.4531250\nSqce 0.6774194 0.4531250 0.0000000\n\n\n\nspe_assoc2 &lt;- vegdist(spe_t, method = \"jaccard\", binary = TRUE)\nas.matrix((spe_assoc2))[1:10, 1:10]\n\n          Cogo      Satr      Phph      Babl      Thth      Teso      Chna\nCogo 0.0000000 0.5294118 0.6000000 0.6666667 0.2222222 0.4000000 0.8888889\nSatr 0.5294118 0.0000000 0.2380952 0.3600000 0.5294118 0.6111111 0.8846154\nPhph 0.6000000 0.2380952 0.0000000 0.1666667 0.6000000 0.6000000 0.7692308\nBabl 0.6666667 0.3600000 0.1666667 0.0000000 0.6666667 0.6666667 0.6153846\nThth 0.2222222 0.5294118 0.6000000 0.6666667 0.0000000 0.4000000 0.8235294\nTeso 0.4000000 0.6111111 0.6000000 0.6666667 0.4000000 0.0000000 0.7500000\nChna 0.8888889 0.8846154 0.7692308 0.6153846 0.8235294 0.7500000 0.0000000\nPato 0.8125000 0.8333333 0.7083333 0.6000000 0.8125000 0.6428571 0.2307692\nLele 0.8181818 0.6538462 0.5384615 0.3846154 0.8181818 0.7000000 0.4210526\nSqce 0.7307692 0.5517241 0.3928571 0.2500000 0.7307692 0.7307692 0.5200000\n          Pato      Lele      Sqce\nCogo 0.8125000 0.8181818 0.7307692\nSatr 0.8333333 0.6538462 0.5517241\nPhph 0.7083333 0.5384615 0.3928571\nBabl 0.6000000 0.3846154 0.2500000\nThth 0.8125000 0.8181818 0.7307692\nTeso 0.6428571 0.7000000 0.7307692\nChna 0.2307692 0.4210526 0.5200000\nPato 0.0000000 0.3888889 0.5600000\nLele 0.3888889 0.0000000 0.2800000\nSqce 0.5600000 0.2800000 0.0000000",
    "crumbs": [
      "Home",
      "BCB743: Quantitative Ecology",
      "5: Correlations & Associations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/10-correlations.html",
    "href": "BCB744/basic_stats/10-correlations.html",
    "title": "10. Correlations",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nCorrelations\nPearson’s product moment correlation\nPaired correlations\nSpearman rank correlation\nKendal rank correlation",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/10-correlations.html#at-a-glance",
    "href": "BCB744/basic_stats/10-correlations.html#at-a-glance",
    "title": "10. Correlations",
    "section": "At a glance",
    "text": "At a glance\nCorrelation analysis is used to quantify the strength and direction of the linear relationship between two continuous variables. The expectations about the data needed for a correlation analysis are:\n\nContinuous variables Both variables should be measured on a continuous scale (e.g., height, depth, income). Note that we do not have dependent and independent variables as no dependency of one variable upon the other is implied.\nBivariate relationship Correlation analysis is used to assess the relationship between two variables at a time. If you are interested in the relationship between multiple variables, you may need to consider pairwise correlations, or other multivariate techniques such as multiple regression or canonical correlation.\nLinear relationship The relationship between the two variables should be linear. This can be visually assessed using scatter plots. If the relationship is not linear, you may need to consider non-linear correlation measures, such as Spearman’s \\(\\rho\\) correlation or Kendall’s \\(\\tau\\).\nNo outliers Outliers can have a strong influence on the correlation coefficient, potentially leading to misleading conclusions. It’s important to visually inspect the data using scatter plots and address any outliers before performing correlation analysis.\nNormality While not strictly required for correlation analysis, the assumption of bivariate normality can be important when making inferences about the population correlation coefficient. If the variables are not normally distributed or have a non-linear relationship, consider using non-parametric correlation measures like Spearman’s \\(\\rho\\) correlation or Kendall’s \\(\\tau\\).\nIndependence of observations The observations should be independent of each other. In the case of time series data or clustered data, this assumption may be violated, requiring specific techniques to account for the dependence (e.g., autocorrelation, cross-correlation).\nRandom sampling The data should be obtained through random sampling, ensuring that each observation has an equal chance of being included in the sample.\n\nKeep in mind that correlation does not imply causation; it only describes the association between variables without establishing a cause-and-effect relationship. When the intention is to model causation you’ll need to apply a regression.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/10-correlations.html#introduction-to-correlation",
    "href": "BCB744/basic_stats/10-correlations.html#introduction-to-correlation",
    "title": "10. Correlations",
    "section": "Introduction to correlation",
    "text": "Introduction to correlation\nA correlation is performed when we want to investigate the potential association between two continuous quantitative variables, or between some ordinal variables. We assume that the association is linear, like in a linear regression, and that one variable increases or decreases by a constant amount for a corresponding unit increase or decrease in the other variable. This does not suggest that one variable explains the other—that is the purpose of regression, as seen in Chapter 9. Like all statistical tests, correlation requires a series of assumptions:\n\npair-wise data\nabsence of outliers\nlinearity\nnormality of distribution\nhomoscedasticity\nlevel (type) of measurement\ncontinuous data (Pearson \\(r\\))\nnon-parametric correlations (Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\))",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/10-correlations.html#pearson-correlation",
    "href": "BCB744/basic_stats/10-correlations.html#pearson-correlation",
    "title": "10. Correlations",
    "section": "Pearson correlation",
    "text": "Pearson correlation\n\n\nPearson’s \\(r\\):\n\\[r_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} \\tag{1}\\]\nwhere \\(r_{xy}\\) is the Pearson correlation coefficient, \\(x_i\\) and \\(y_i\\) are the observed values of the two variables for each observation \\(i\\), \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means of the two variables, and \\(n\\) is the sample size.\nPearson’s \\(r\\) is a measure of the linear relationship between two variables. It assumes that the relationship between the variables is linear, and is calculated as the ratio of the covariance between the variables to the product of their standard deviations (Equation 1).\nThe degree of association is measured by a correlation coefficient, denoted by \\(r\\) (note, in a regression we use the \\(r^{2}\\), or \\(R^{2}\\)). The \\(r\\) statistic is a measure of linear association. The value for \\(r\\) varies from -1 to 1, with 0 indicating that there is absolutely no association, 1 showing a perfect positive association, and -1 a perfect inverse correlation.\nIn order to investigate correlations in biological data lets load the ecklonia dataset.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(corrplot)\nlibrary(kableExtra)\n\n# Load data\necklonia &lt;- read.csv(\"../../data/ecklonia.csv\")\n\nWe will also create a subsetted version of our data by removing all of the categorical variables. If we have a dataframe where each column represents pair-wise continuous/ordinal measurements with all of the other columns we may very quickly and easily perform a much wider range of correlation analyses.\n\necklonia_sub &lt;- ecklonia %&gt;%\n  select(-species, - site, - ID)\n\n# order the columns alphabetically\necklonia_sub &lt;- ecklonia_sub[,order(colnames(ecklonia_sub))]\n\nWhen the values we are comparing are continuous, we may use a Pearson test. This is the default and so requires little work on our end. The resulting statistic from this test is known as the Pearson correlation coefficient:\n\n# Perform correlation analysis on two specific variables\n# Note that we do not need the final two arguments in this function to be stated\n# as they are the defaut settings.\n# They are only shown here to illustrate that they exist.\ncor.test(x = ecklonia$stipe_length, ecklonia$frond_length,\n         use = \"everything\", method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  ecklonia$stipe_length and ecklonia$frond_length\nt = 4.2182, df = 24, p-value = 0.0003032\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3548169 0.8300525\nsample estimates:\n      cor \n0.6524911 \n\n\nAbove we have tested the correlation between the length of Ecklonia maxima stipes and the length of their fronds. A perfect positive (negative) relationship would produce a value of 1 (-1), whereas no relationship would produce a value of 0. The result above, cor = 0.65 is relatively strong.\nAs is the case with everything else we have learned thus far, a good visualisation can go a long way to help us understand what the statistics are doing. Below we visualise the stipe length to frond length relationship.\n\n# Calculate Pearson r beforehand for plotting\nr_print &lt;- paste0(\"r = \",\n                  round(cor(x = ecklonia$stipe_length, ecklonia$frond_length),2))\n\n# Then create a single panel showing one correlation\nggplot(data = ecklonia, aes(x = stipe_length, y = frond_length)) +\n  geom_smooth(method = \"lm\", colour = \"blue3\", se = FALSE, size = 1.2) +\n  geom_point(size = 3, col = \"red3\", shape = 16) +\n  geom_label(x = 300, y = 240, label = r_print) +\n  labs(x = \"Stipe length (cm)\", y = \"Frond length (cm)\") +\n  theme_pubclean()\n\n\n\n\n\n\nFigure 1: Scatterplot showing relationship between Ecklonia maxima stipe length (cm) and frond length (cm). The correlation coefficient (Pearson r) is shown in the top left corner. Note that the best fit blue line was produced by a linear model and that it is not responsible for generating the correlation coefficient; rather it is included to help visually demonstrate the strength of the relationship.\n\n\n\n\nJust by eye-balling this scatterplot it should be clear that these data tend to increase at a roughly similar rate. Our Pearson r value is an indication of what that is.\nShould our dataset contain multiple variables, as ecklonia does, we may investigate all of the correlations simultaneously. Remember that in order to do so we want to ensure that we may perform the same test on each of our paired variables. In this case we will use ecklonia_sub as we know that it contains only continuous data and so are appropriate for use with a Pearson test. By default R will use all of the data we give it and perform a Pearson test so we do not need to specify any further arguments. Note however that this will only output the correlation coefficients, and does not produce a full test of each correlation. This will however be useful for us to have just now.\n\necklonia_pearson &lt;- round(cor(ecklonia_sub), 2)\necklonia_pearson |&gt; \n  kbl(caption = \"A pairwise matrix of the *Ecklonia* dataset.\") %&gt;%\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndigits\nepiphyte_length\nfrond_length\nfrond_mass\nprimary_blade_length\nprimary_blade_width\nstipe_diameter\nstipe_length\nstipe_mass\n\n\n\ndigits\n1.00\n0.05\n0.36\n0.28\n0.10\n0.14\n0.24\n0.24\n0.07\n\n\nepiphyte_length\n0.05\n1.00\n0.61\n0.44\n0.26\n0.41\n0.54\n0.61\n0.51\n\n\nfrond_length\n0.36\n0.61\n1.00\n0.57\n-0.02\n0.28\n0.39\n0.65\n0.39\n\n\nfrond_mass\n0.28\n0.44\n0.57\n1.00\n0.15\n0.36\n0.51\n0.51\n0.47\n\n\nprimary_blade_length\n0.10\n0.26\n-0.02\n0.15\n1.00\n0.34\n0.32\n0.13\n0.16\n\n\nprimary_blade_width\n0.14\n0.41\n0.28\n0.36\n0.34\n1.00\n0.83\n0.34\n0.83\n\n\nstipe_diameter\n0.24\n0.54\n0.39\n0.51\n0.32\n0.83\n1.00\n0.59\n0.82\n\n\nstipe_length\n0.24\n0.61\n0.65\n0.51\n0.13\n0.34\n0.59\n1.00\n0.58\n\n\nstipe_mass\n0.07\n0.51\n0.39\n0.47\n0.16\n0.83\n0.82\n0.58\n1.00\n\n\n\nA pairwise matrix of the *Ecklonia* dataset.\n\nHow would we visualise this matrix of correlations? It is relatively straightforward to quickly plot correlation results for all of our variables in one go. In order to show which variables associate most with which other variables all at once, without creating chaos, we will create what is known as a pairwise correlation plot. This visualisation uses a range of colours, usually blue to red, to demonstrate where more of something is. In this case, we use it to show where more correlation is occurring between morphometric properties of the kelp Ecklonia maxima.\n\n# extract the lower triangle and plot\necklonia_pearson[upper.tri(ecklonia_pearson)] &lt;- NA\ncorrplot(ecklonia_pearson, method = \"circle\", na.label.col = \"white\")\n\n\n\n\n\n\nFigure 2: Plot of pairwise correlations showing the strength of all correlations between all variables as a scale from red (negative) to blue (positive).\n\n\n\n\nLet’s do it is ggplot2 (Figure 3). Here I use the geom_tile() function. However, before I can use the data in ggplot2, I need to create a long dataframe from the correlation matrix, and I can do this with the pivot_longer() function. There are several other methods for plotting pairwise correlations available—please feel free to scratch around the internet for options you like. This graph is called a heatmap, which is not dissimilar to the heatmaps and Hovmöller diagrams created in Chapter 2.\nPairwise correlations are useful for identifying patterns and relationships between variables that may be hidden in the overall correlation structure of the dataset. This is particularly useful in a large dataset with many variables, where this type of analysis—especially when coupled with a suitable visualisation—can help identify subsets of variables that are strongly related to each other, which can then point the path to further analysis or modelling.\n\necklonia_pearson |&gt; \n  as.data.frame() |&gt; \n  mutate(x = rownames(ecklonia_pearson)) |&gt; \n  pivot_longer(cols = stipe_length:epiphyte_length,\n               names_to = \"y\",\n               values_to = \"r\") |&gt; \n  filter(x != \"digits\") |&gt; \n  ggplot(aes(x, y, fill = r)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1),\n                          na.value = \"grey95\",, space = \"Lab\",\n                         name = \"r\") +\n    xlab(NULL) + ylab(NULL) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1,\n                                     hjust = 1)) +\n    coord_fixed() \n\n\n\n\n\n\nFigure 3: Pairwise of the Ecklonia dataset correlations created in ggplot2.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/10-correlations.html#spearman-rank-correlation",
    "href": "BCB744/basic_stats/10-correlations.html#spearman-rank-correlation",
    "title": "10. Correlations",
    "section": "Spearman rank correlation",
    "text": "Spearman rank correlation\nSpearman correlation is used to measure the strength and direction of the relationship between two variables, based on their rank order. Unlike Pearson correlation, which assumes that the relationship between two variables is linear, Spearman correlation can be used to measure the strength of any monotonic relationship, whether it is linear or not. Additionally, this correlation is useful even when the data are not normally distributed, or contain outliers.\nTo calculate the Spearman correlation coefficient, \\(\\rho\\), the values of both variables are first ranked from lowest to highest and each value is assigned a numerical rank based on its position in the ordered list. Then, the difference between the ranks of the two variables is calculated for each observation, and the squared differences are summed across all observations. The Spearman correlation coefficient is then calculated as the ratio of the sum of the squared differences to the total number of observations, adjusted for ties (Equation 2). Like the Pearson correlation coefficient, \\(\\rho\\) can also range from -1 to +1.\n\n\nSpearman’s \\(\\rho\\): \\[\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2-1)} \\tag{2}\\]\nwhere \\(\\rho\\) is the Spearman correlation, \\(d_i\\) is the difference between the ranks of the two variables for the \\(i^{th}\\) observation, and \\(n\\) is the sample size. The factor of 6 in the equation is a normalisation constant that adjusts the range of possible values of the correlation coefficient to be between -1 and +1.\nIn the code below we will add a column of ordinal data to our ecklonia data to so that we may look at this test.\n\n# Create ordinal data\necklonia$length &lt;- as.numeric(cut((ecklonia$stipe_length + ecklonia$frond_length), breaks = 3))\n\n# What does this new column look like?\nhead(select(ecklonia, c(species, site, stipe_length, frond_length, length)), 10)\n\n   species           site stipe_length frond_length length\n1   maxima Boulders Beach          456          116      1\n2   maxima Boulders Beach          477          141      2\n3   maxima Boulders Beach          427          144      1\n4   maxima Boulders Beach          347          127      1\n5   maxima Boulders Beach          470          160      2\n6   maxima Boulders Beach          478          181      2\n7   maxima Boulders Beach          472          174      2\n8   maxima Boulders Beach          459           95      1\n9   maxima Boulders Beach          397           87      1\n10  maxima Boulders Beach          541          127      2\n\n\nNow let us correlate the new length variable with any one of the other variables:\n\ncor.test(ecklonia$length, ecklonia$digits, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  ecklonia$length and ecklonia$digits\nS = 1930, p-value = 0.08906\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.3401765",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/10-correlations.html#kendall-rank-correlation",
    "href": "BCB744/basic_stats/10-correlations.html#kendall-rank-correlation",
    "title": "10. Correlations",
    "section": "Kendall rank correlation",
    "text": "Kendall rank correlation\nKendall’s correlation, also known as Kendall’s \\(\\tau\\), is a non-parametric correlation method for assessing the strength and direction of the relationship between two variables. It is similar to Spearman’s rank correlation, but it is calculated differently.\nKendall’s \\(\\tau\\) is calculated based on the number of concordant and discordant pairs of observations between the two variables being correlated. A concordant pair is one in which the values of both variables have the same order, meaning that if the value of one variable is higher than the other for one observation, it is also higher for the other observation. A discordant pair is one in which the values of the two variables have different order, meaning that if one variable is higher than the other for one observation, it is lower for the other observation.\n\\(\\tau\\) is calculated as the difference between the number of concordant and discordant pairs of observations, divided by the total number of possible pairs (Equation 3). As in Pearson’s and Spearman’s correlations, the result also ranges from -1 and +1.\n\n\nKendal’s \\(\\tau\\): \\[\\tau = \\frac{n_c - n_d}{\\binom{n}{2}} \\tag{3}\\]\nwhere \\(\\tau\\) is Kendall’s \\(\\tau\\) correlation coefficient, \\(n\\) is the sample size, \\(n_c\\) is the number of concordant pairs of observations, \\(n_d\\) is the number of discordant pairs of observations, and \\(\\binom{n}{2}\\) is the number of possible pairs of observations in the sample.\nKendall’s \\(\\tau\\) is a useful correlation statistic for non-parametric data, such as ordinal or categorical data, and is robust to outliers and non-normal distributions.\nLet’s look at the normality of our ecklonia variables and pull out those that are not normal in order to see how the results of this test may differ from our Pearson tests.\n\necklonia_norm &lt;- ecklonia_sub %&gt;%\n  gather(key = \"variable\") %&gt;%\n  group_by(variable) %&gt;%\n  summarise(variable_norm = as.numeric(shapiro.test(value)[2]))\necklonia_norm\n\n# A tibble: 9 × 2\n  variable             variable_norm\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 digits                     0.0671 \n2 epiphyte_length            0.626  \n3 frond_length               0.202  \n4 frond_mass                 0.277  \n5 primary_blade_length       0.00393\n6 primary_blade_width        0.314  \n7 stipe_diameter             0.170  \n8 stipe_length               0.213  \n9 stipe_mass                 0.817  \n\n\nFrom this analysis we may see that the values for primary blade length are not normally distributed. In order to make up for this violation of our assumption of normality we may use the Kendall test.\n\ncor.test(ecklonia$primary_blade_length, ecklonia$primary_blade_width, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  ecklonia$primary_blade_length and ecklonia$primary_blade_width\nz = 2.3601, p-value = 0.01827\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.3426171 \n\n\nHere the correlation coefficient is called Kendall’s \\(\\tau\\) but it is interpreted as we would Pearson’s.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html",
    "href": "BCB744/basic_stats/14-transformations.html",
    "title": "14. Data Transformations",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nDiscuss the reasons for transforming data\nCover some of the most common data transformations\nDiscuss the importance of checking the assumptions of the transformed data",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html#log-transformation",
    "href": "BCB744/basic_stats/14-transformations.html#log-transformation",
    "title": "14. Data Transformations",
    "section": "Log transformation",
    "text": "Log transformation\nLog transformation is often applied to positively skewed data. It consists of taking the log of each observation. You can use either base-10 logs (log10(x)) or base-\\(e\\) logs, also known as natural logs (log(x)). It makes no difference for a statistical test whether you use base-10 logs or natural logs, because they differ by a constant factor; the base- 10 log of a number is just 2.303…× the natural log of the number. You should specify which log you’re using when you write up the results, as it will affect things like the slope and intercept in a regression. I prefer base-10 logs, because it’s possible to look at them and see the magnitude of the original number: \\(log(1) = 0\\), \\(log(10) = 1\\), \\(log(100) = 2\\), etc.\nThe back transformation is to raise 10 or \\(e\\) to the power of the number; if the mean of your base-10 log-transformed data is 1.43, the back transformed mean is \\(10^{1.43} = 26.9\\) (in R, 10^1.43). If the mean of your base-\\(e\\) log-transformed data is 3.65, the back transformed mean is \\(e^{3.65} = 38.5\\) (in R, exp(3.65)). If you have zeros or negative numbers, you can’t take the log; you should add a constant to each number to make them positive and non-zero (i.e. log10(x + 1)). If you have count data, and some of the counts are zero, the convention is to add 0.5 to each number.\nMany variables in biology have log-normal distributions, meaning that after log-transformation, the values are normally distributed. This is because if you take a bunch of independent factors and multiply them together, the resulting product is log-normal. For example, let’s say you’ve planted a bunch of weed seeds, then 10 years later you see how tall the trees are. The height of an individual tree would be affected by the nitrogen in the soil, the amount of water, amount of sunlight, amount of insect damage, etc. Having more nitrogen might make a tree 10% larger than one with less nitrogen; the right amount of water might make it 30% larger than one with too much or too little water; more sunlight might make it 20% larger; less insect damage might make it 15% larger, etc. Thus the final size of a tree would be a function of nitrogen × water × sunlight × insects, and mathematically, this kind of function turns out to be log-normal.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html#arcsine-transformation",
    "href": "BCB744/basic_stats/14-transformations.html#arcsine-transformation",
    "title": "14. Data Transformations",
    "section": "Arcsine transformation",
    "text": "Arcsine transformation\nArcsine transformation is commonly used for proportions, which range from 0 to 1, or percentages that go from 0 to 100. Specifically, this transformation is quite useful when the data follow a binomial distribution and have extreme proportions close to 0 or 1.\nA biological example of the type of data suitable for arcsine transformation is the proportion of offspring that survives or the proportion of plants that succumbs to a disease; such data often follow a binomial distribution.\nThis transformation involves of taking the arcsine of the square root of a number (in R, arcsin(sqrt(x))). (The result is given in radians, not degrees, and can range from −π/2 to π/2). The numbers to be arcsine transformed must be in the range 0 to 1. […] the back-transformation is to square the sine of the number (in R, sin(x)^2).",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html#square-root-transformation",
    "href": "BCB744/basic_stats/14-transformations.html#square-root-transformation",
    "title": "14. Data Transformations",
    "section": "Square root transformation",
    "text": "Square root transformation\nThe square root transformation (in R, sqrt(x)) is often used to stabilise the variance of data that have a non-linear relationship between the mean and variance (heteroscedasticity). It is effective for reducing right-skewness (positively skewed). Taking the square root of each observation has the effect of compressing the data towards zero and reducing the impact of extreme values. It is a monotonic transformation, which means that it preserves the order of the data and does not change the relative rankings of the observations.\nThe square root transformation does not work with negative values, but one could add a constant to each number to make them positive.\nA square root transformation is most frequently applied where the data are counts or frequencies, such as the number of individuals in a population or the number of events in a certain time period. Count data are prone to the variance increasing with the mean due to the discrete nature of the data. In these cases, the data tend to follow a Poisson distribution, which is characterised by a variance that is equal to the mean. The same applies to some environmental data, such as rainfall or wind; these may also exhibit heteroscedasticity due to extreme weather phenomena.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html#square-transformation",
    "href": "BCB744/basic_stats/14-transformations.html#square-transformation",
    "title": "14. Data Transformations",
    "section": "Square transformation",
    "text": "Square transformation\nAnother transformation available for dealing with heteroscedasticity is the square transformation. As the name suggests, it involves taking the square of each observation in a dataset (x^2). The effect sought is to reduce left skewness.\nThis transformation has the effect of magnifying the differences between values and so increasing the influence of extreme values. However, this can make outliers more prominent and can make it more challenging to interpret the results of statistical analysis.\nThe square transformation is often used in situations where the data are related to areas or volumes, such as the size of cells or the volume of an organ, where the data may follow a nonlinear relationship between the mean and variance.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html#cube-transformation",
    "href": "BCB744/basic_stats/14-transformations.html#cube-transformation",
    "title": "14. Data Transformations",
    "section": "Cube transformation",
    "text": "Cube transformation\nThis transformation also applies to heteroscedastic data. It is sometimes used with moderately left skewed data. This transformation is more drastic than a square transformation, and the drawback are more severe.\nThe cube transformation is less commonly used than other data transformations such as square-root or log transformation. Use with caution.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html#reciprocal-transformation",
    "href": "BCB744/basic_stats/14-transformations.html#reciprocal-transformation",
    "title": "14. Data Transformations",
    "section": "Reciprocal transformation",
    "text": "Reciprocal transformation\nIt involves taking the reciprocal or inverse of each observation in a dataset (1/x). It is another variance stabilising transformation and is used with severely positively skewed data.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/14-transformations.html#anscombe-transformation",
    "href": "BCB744/basic_stats/14-transformations.html#anscombe-transformation",
    "title": "14. Data Transformations",
    "section": "Anscombe transformation",
    "text": "Anscombe transformation\nAnother variance stabilising transformation is the Anscombe transformation, sqrt(max(x+1)-x). It is applied to negatively skewed data. This transformation can be used to shift the data and compress it towards zero, and remove the influence of extreme values. It is a monotonic transformation, which means that it preserves the order of the data and does not change the relative rankings of the observations.\nThe Anscombe transformation is useful when dealing with count or frequency data that have a non-linear relationship between the mean and variance; such data are characteristic of Poisson-distributed count data.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html",
    "title": "2. Data Summaries & Descriptions",
    "section": "",
    "text": "“I think it is much more interesting to live with uncertainty than to live with answers that might be wrong.”\n—- Richard Feynman",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-central-tendency",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-central-tendency",
    "title": "2. Data Summaries & Descriptions",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\n\nStatistic\nFunction\nPackage\n\n\n\nMean\nmean()\nbase\n\n\nMedian\nmedian()\nbase\n\n\nMode\nDo it!\n\n\n\nSkewness\nskewness()\ne1071\n\n\nKurtosis\nkurtosis()\ne1071\n\n\n\nCentral tendency is a fundamental concept in statistics, referring to the central or typical value of a dataset that best represents its overall distribution. The measures of central tendency are also sometimes called ‘location’ statistics. As a key component of descriptive statistics, central tendency is essential for summarising and simplifying complex data. It provides a single representative value that captures the data’s general behaviour and which might tell us something about the bigger population from which the random samples were drawn.\nA thorough assessment of the central tendency in EDA serves several purposes:\n\nSummary of data Measures of central tendency, such as the mean, median, and mode, provide a single value that represents the center or typical value of a dataset. They help summarise the data and allow us to gain an early insight into the dataset’s general properties and behaviour.\nComparing groups or distributions Central tendency measures allow us to compare different datasets or groups within a dataset. They can help identify differences or similarities in the data. This may be useful for hypothesis testing and inferential statistics.\nData transformation decisions Understanding the central tendency of our data can inform decisions on whether to apply transformations to the data to better meet the assumptions of certain statistical tests or improve the interpretability of the results.\nIdentifying potential issues Examining the central tendency can help reveal issues with the data, such as outliers or data entry errors, that could influence the results of inferential statistics. Outliers, for example, can greatly impact the mean, making the median a more robust measure of central tendency in such cases.\n\nUnderstanding the central tendency informs the choice of inferential statistics in the following ways:\n\nAssumptions of statistical tests Many inferential statistical tests have assumptions about the distribution of the data, such as normality, linearity, or homoscedasticity. Analysing the central tendency helps assess whether these assumptions are met and informs the choice of an appropriate test.\nChoice of statistical models The central tendency can influence the choice of statistical models or the selection of dependent and independent variables in regression analyses—certain models or relationships may be more appropriate depending on the data’s distribution and central tendencies.\nChoice of estimators Central tendency measures can influence our choice of estimators for further inferential statistics, depending on the data’s distribution and presence of outliers (e.g., mean vs. median).\n\nBefore I discuss each central tendency statistic, I’ll generate some random data to represent normal and skewed distributions. I’ll use these data in my discussions, below.\n\n# Generate random data from a normal distribution\nset.seed(666)\nn &lt;- 5000 # Number of data points\nmean &lt;- 0\nsd &lt;- 1\nnormal_data &lt;- rnorm(n, mean, sd)\n\n# Generate random data from a slightly\n# right-skewed beta distribution\nalpha &lt;- 2\nbeta &lt;- 5\nright_skewed_data &lt;- rbeta(n, alpha, beta)\n\n# Generate random data from a slightly\n# left-skewed beta distribution\nalpha &lt;- 5\nbeta &lt;- 2\nleft_skewed_data &lt;- rbeta(n, alpha, beta)\n\n# Generate random data with a bimodal distribution\nmean1 &lt;- 0\nmean2 &lt;- 10\nsd1 &lt;- 3\nsd2 &lt;- 4\n\n# Generate data from two normal distributions\ndata1 &lt;- rnorm(n, mean1, sd1)\ndata2 &lt;- rnorm(n, mean2, sd2)\n\n# Combine the data from both distributions to\n# create a bimodal distribution\nbimodal_data &lt;- c(data1, data2)\n\n\n# Set up a three-panel plot layout\npar(mfrow = c(2, 2))\n\n# Plot the histogram of the normal distribution\nhist(normal_data, main = \"Normal Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightblue\", border = \"black\")\n\n# Plot the histogram of the right-skewed distribution\nhist(right_skewed_data, main = \"Right-Skewed Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightgreen\", border = \"black\")\n\n# Plot the histogram of the left-skewed distribution\nhist(left_skewed_data, main = \"Left-Skewed Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightcoral\", border = \"black\")\n\n# Plot the histogram of the left-skewed distribution\nhist(bimodal_data, main = \"Bimodal Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"khaki2\", border = \"black\")\n\n# Reset the plot layout to default\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nFigure 2: A series of plots with histograms for the previously generated normal, right-skewed, and left-skewed distributions.\n\n\n\n\nThe sample mean\nThe mean is the arithmetic average of the data, and it is calculated by summing all the data and dividing it by the sample size, n (Equation 1).\n\n\nThe mean, \\(\\bar{x}\\), is calculated thus: \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} \\tag{1}\\] where \\(x_{1} + x_{2} + \\cdots + x_{n}\\) are the observations and \\(n\\) is the number of observations in the sample.\nWe can calculate the mean of a sample using the … wait for it … mean() function:\n\nround(mean(normal_data), 3)\n\n[1] 0.009\n\n\n\n\n\n\n\n\nDo it now!\n\n\n\nHow would you manually calculate the mean value for the normal_data?\n\n\nThe mean is quite sensitive to the presence of outliers or extreme values in the data, and it is advised that its use be reserved for normally distributed data from which the extremes/outliers have been removed. When extreme values are indeed part of our data and not simply ‘noise,’ then we have to resort to a different measure of central tendency: the median.\nThe median\nThe median indicates the center value in our dataset. The simplest way to explain what is is is to describe how it is determined. It can be calculated by ‘hand’ (if you have a small enough amount of data) by arranging all the numbers in sequence from low to high, and then finding the middle value. If there are five numbers, say 5, 2, 6, 13, 1, then you would arrange them from low to high, i.e. 1, 2, 5, 6, 13. The middle number is 5. This is the median. But there is no middle if we have an even number of values. What now? Take this example sequence of six integers (they may also be floating point numbers), which has already been ordered for your pleasure: 1, 2, 5, 6, 9, 13. Find the middle two numbers (i.e. 5, 6) and take the mean. It is 5.5. That is the median.\nThe median is therefore the value that separates the lower half of the sample data from the upper half. In normally distributed continuous data the median is equal to the mean. Comparable concepts to the median are the 1st and 3rd quartiles, which, respectively, separate the first quarter of the data from the last quarter—see the later in the section on ‘Measures of variance and dispersal’ in this Chapter. The advantage of the median over the mean is that it is unaffected by extreme values or outliers. The median is also used to provide a robust description of non-parametric data (see Chapter 4 for a discussion on normal data and other data distributions).\nWhat is the median of the normal_data dataset? We use the median() function for this:\n\nround(median(normal_data), 3)\n\n[1] 0.017\n\n\nIt is easier to see what the median is by looking at a much smaller dataset. Let’s take 11 random data points:\n\nsmall_normal_data &lt;- round(rnorm(11, 13, 3), 1)\nsort(small_normal_data)\n\n [1]  8.9  9.1  9.2 10.3 10.7 11.8 11.9 12.8 13.8 14.5 19.7\n\nmedian(small_normal_data)\n\n[1] 11.8\n\n\nThe mean and median together provide a comprehensive understanding of the data’s central tendency and underlying distribution.\n\n\n\n\n\n\nWhat is the relationship between the median and quantiles?\n\n\n\nThe relation between the median and quantiles lies in their roles as measures that describe the relative position of data points within a dataset. Quantiles are values that partition a dataset into equal intervals, with each interval containing the same proportion of the data. The most common types of quantiles are quartiles, quintiles, deciles, and percentiles.\nThe median is a special case of a quantile, specifically the 50th percentile or the second quartile (Q2). It divides the dataset into two equal halves, with 50% of the data points falling below the median and 50% of the data points falling above the median. In this sense, the median is a central quantile that represents the middle value of the dataset.\nBoth the median and quantiles help describe the distribution and spread of a dataset, with the median providing information about the center and other quantiles (such as quartiles) offering insights into the overall shape, skewness, and dispersion of the data.\n\n\nThe mode\nThe mode is a measure that represents the value or values that occur most frequently in a dataset. Unlike the mean and median, the mode can be used with both numerical and categorical data, making it quite versatile. For a dataset with a single value that appears most often, the distribution is considered unimodal. However, datasets can also be bimodal (having two modes) or multimodal (having multiple modes) when there are multiple values that occur with the same highest frequency.\nWhile the mode may not always be a good representative of the dataset’s center, especially in the presence of extreme values or skewed distributions, it can still offer valuable information about the data’s characteristics when used alongside the other measures of central tendency.\nThere is no built-in function to calculate the mode of a numeric vector, but you can make one if you need it. There are some examples on the internet that you will be able to adapt to your needs, but my cursory evaluation of them does not suggest they are particularly useful. The easiest way to see the data’s mode(s) is to examine a histogram of your data. All the data we have explored above are examples of unimodal distributions, but a bimodal distribution can also be seen in Figure 2.\nSkewness\nSkewness is a measure of symmetry (or asymmetry) of the data distribution, and it is best understood by understanding the location of the median relative to the mean. A distribution with a skewness of zero is considered symmetric, with both tails extending equally on either side of the mean. Here, the mean will be the same as the median. A negative skewness indicates that the mean of the data is less than their median—the data distribution is left-skewed; that is, there is a longer or heavier tail to the left of the mean. A positive skewness results from data that have a mean that is larger than their median; these data have a right-skewed distribution; so there will be a longer or heavier tail to the right of the mean. Base R does not have a built-in skewness function, but we can use the one included with the e1071 package:\n\nlibrary(e1071)\n# Positive skewness\nskewness(right_skewed_data)\n\n[1] 0.5453162\n\n# Is the mean larger than the median?\nmean(right_skewed_data) &gt; median(right_skewed_data)\n\n[1] TRUE\n\n# Negative skewness\nskewness(left_skewed_data)\n\n[1] -0.5790834\n\n# Is the mean less than the median?\nmean(left_skewed_data) &lt; median(left_skewed_data)\n\n[1] TRUE\n\n\nKurtosis\nKurtosis describes the tail shape of the data’s distribution. Kurtosis is effectively a measure of the ‘tailedness’ or the concentration of data in the tails of a distribution, relative to a normal distribution. A normal distribution has zero kurtosis (or close to) and thus the standard tail shape (mesokurtic). Negative kurtosis indicates data with a thin-tailed (platykurtic) distribution. Positive kurtosis indicates a fat-tailed distribution (leptokurtic).\nSimilarly as to skewness, we use the e1071 package for a kurtosis function. All the output shown below suggests a tendency towards thin-tailedness, but it is subtle.\n\nkurtosis(normal_data)\n\n[1] -0.01646261\n\nkurtosis(right_skewed_data)\n\n[1] -0.1898941\n\nkurtosis(left_skewed_data)\n\n[1] -0.1805365\n\n\nI have seldom used the concepts of the skewness or kurtosis in any EDA, but it is worth being aware of them. The overall purpose of examining data using the range of central tendency statistics is to get an idea of whether our data are normally distributed—a normal distribution is a key requirement for all parametric inferential statistics. See Chapter 4 for a discourse of data distributions. These central tendency statistics will serve you well as a first glance, but formal tests for normality do exist and I encourage their use before embarking on the rest of the journey. We will explore these formal tests in Chapter 7.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-variance-or-dispersion-around-the-center",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#measures-of-variance-or-dispersion-around-the-center",
    "title": "2. Data Summaries & Descriptions",
    "section": "Measures of variance or dispersion around the center",
    "text": "Measures of variance or dispersion around the center\n\n\nStatistic\nFunction\n\n\n\nVariance\nvar()\n\n\nStandard deviation\nsd()\n\n\nMinimum\nmin()\n\n\nMaximum\nmax()\n\n\nRange\nrange()\n\n\nQuantile\nquantile()\n\n\nInter Quartile Range\nIQR()\n\n\n\nA good understanding of variability, or variation around the central point, is crucial in EDA for several reasons:\n\nSignal vs. noise Variability helps distinguish between the signal (true underlying pattern) and noise (random fluctuations that might arise from stochastic processes, measurement or experimental error, or other unaccounted for influences) in the data. High variability can make it difficult to detect meaningful patterns or relationships in the data, while low variability may indicate a strong underlying pattern.\nPrecision and reliability Variability is related to the precision and reliability of measurements. Smaller variability indicates more consistent and precise measurements, whereas larger variability suggests inconsistency and potential issues with the data collection process.\nComparing groups Understanding variability is essential when comparing different groups or datasets. Even if two groups have similar central tendencies, their variability may differ significantly, leading to different interpretations of the data.\nAssumptions of statistical tests Many inferential statistical tests have assumptions about the variability of the data, such as homoscedasticity (equal variances across groups) or independence of observations. Assessing variability helps determine whether these assumptions are met and informs the choice of appropriate tests.\nEffect sizes and statistical power Variability plays a role in determining the effect size (magnitude of the difference between groups or strength of relationships) and the statistical power (ability to detect a true effect) of a study. High variability can make it harder to detect significant effects, requiring larger sample sizes to achieve adequate statistical power.\n\nUnderstanding variability informs the choice of inferential statistics:\n\nParametric vs non-parametric tests If the data exhibit normality and homoscedasticity, parametric tests may be appropriate (see Chapter 7). However, if the data have high variability or violates the assumptions of parametric tests, non-parametric alternatives may be more suitable.\nChoice of estimators Variability can influence the choice of estimators (e.g., mean vs. median) for central tendency, depending on the data’s distribution and presence of outliers.\nSample size calculations Variability informs sample size calculations for inferential statistics. Higher variability typically requires larger sample sizes to achieve sufficient statistical power.\nModel selection Variability can influence the choice of statistical models, as certain models may better accommodate the variability in the data than others (e.g., linear vs. non-linear models, fixed vs. random effects).\n\nLet us now look at the estimators of variance.\nVariance and standard deviation\nVariance and standard deviation (SD) are examples of interval estimates. The sample variance, \\(S^{2}\\), may be calculated according to the following formula (Equation 2). If we cannot be bothered to calculate the variance and SD by hand, we may use the built-in functions var() and sd():\n\n\nThe sample variance: \\[S^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\tag{2}\\]\nThis reads, “the sum of the squared differences from the mean, divided by the sample size minus 1.” To get the standard deviation, \\(S\\), we take the square root of the variance, i.e. \\(S = \\sqrt{S^{2}}\\).\n\nvar(normal_data)\n\n[1] 1.002459\n\nsd(normal_data)\n\n[1] 1.001229\n\n\n\n\n\n\n\n\nDo it now!\n\n\n\nManually calculate the variance and SD for the normal_data. Make sure your answer is the same as those reported above.\n\n\nThe interpretation of the concepts of mean and median are fairly straight forward and intuitive. Not so for the measures of variance. What does \\(S\\) represent? Firstly, the unit of measurement of \\(S\\) is the same as that of \\(\\bar{x}\\) (but the variance doesn’t share this characteristic). If temperature is measured in °C, then \\(S\\) also takes a unit of °C. Since \\(S\\) measures the dispersion around the mean, we write it as \\(\\bar{x} \\pm S\\) (note that often the mean and standard deviation are written with the letters mu and sigma, respectively; i.e. \\(\\mu \\pm \\sigma\\)). The smaller \\(S\\) the closer the sample data are to \\(\\bar{x}\\), and the larger the value is the further away they will spread out from \\(\\bar{x}\\). So, it tells us about the proportion of observations above and below \\(\\bar{x}\\). But what proportion? We invoke the the 68-95-99.7 rule: ~68% of the population (as represented by a random sample of \\(n\\) observations taken from the population) falls within 1\\(S\\) of \\(\\bar{x}\\) (i.e. ~34% below \\(\\bar{x}\\) and ~34% above \\(\\bar{x}\\)); ~95% of the population falls within 2\\(S\\); and ~99.7% falls within 3\\(S\\) (Figure 3).\n\n\n\n\n\n\n\nFigure 3: The idealised Normal distribution showing the proportion of data within 1, 2, and 3SD from the mean.\n\n\nLike the mean, \\(S\\) is affected by extreme values and outliers, so before we attach \\(S\\) as a summary statistic to describe some data, we need to ensure that the data are in fact normally distributed. We will talk about how to do this in Chapter 7, where we will go over the numerous ways to check the assumption of normality. When the data are found to be non-normal, we need to find appropriate ways to express the spread of the data. Enter the quartiles.\nThe minimum, maximum, and range\nA description of the extremes (edges of the distribution) of the data can also be provided by the functions min(), max() and range(). These concepts are straight forward and do not require elaboration. They apply to data of any distribution, and not only to normal data. These statistics are often the first places you want to start when looking at the data for the first time. Note that range() does something different from min() and max():\n\nmin(normal_data)\n\n[1] -3.400137\n\nmax(normal_data)\n\n[1] 3.235566\n\nrange(normal_data)\n\n[1] -3.400137  3.235566\n\n\nrange() actually gives us the minimum and maximum values, and not the difference between them. To find the range value properly we must be a bit more clever:\n\nrange(normal_data)[2] - range(normal_data)[1]\n\n[1] 6.635703\n\n\nQuartiles and the interquartile range\nA more forgiving approach (forgiving of the extremes, often called ‘robust’) is to divide the distribution of ordered data into quarters and finding the points below which 25% (0.25, the first quartile; Q1), 50% (0.50, the median; Q2) and 75% (0.75, the third quartile; Q3) of the data are distributed. These are called quartiles (for ‘quarter;’ not to be confused with quantile, which is a more general concept that divides the distribution into any arbitrary proportion from 0 to 1).\nThe interquartile range (IQR) is a measure of statistical dispersion that provides information about the spread of the middle 50% of a dataset. It is calculated by subtracting the first quartile (25th percentile) from the third quartile (75th percentile).\nThe quartiles and IQR have several important uses:\n\nIdentifying central tendency As I have shown earlier, the second quartile, or median, is a measure of central tendency that is less sensitive to outliers than the mean. It offers a more robust estimate of the typical value in skewed distributions or those with extreme values.\nMeasure of variability The IQR is a robust measure of variability that is less sensitive to outliers and extreme values compared to other measures like the range or standard deviation. It gives a better understanding of the data spread in the middle part of the distribution.\nIdentifying outliers The IQR can be used to identify potential outliers in the data. A common method is to define outliers as data points falling below the first quartile minus 1.5 times the IQR or above the third quartile plus 1.5 times the IQR.\nDescribing skewed data For skewed distributions, the quartiles and IQR provide a better description of the data spread than the standard deviation, as it is not influenced by the skewness of the data. It can help reveal the degree of asymmetry in the distribution and the concentration of values in the middle portion.\nComparing distributions The IQR can be used to compare the variability or spread of two or more distributions. It provides a more robust comparison than the standard deviation or range when the distributions have outliers or are not symmetric, and the median reveals departures from normality.\nBox plots The quartiles and IQR are key components of box plots, which are graphical representations of the distribution of a dataset. Box plots display the median, first quartile, third quartile, and potential outliers, providing a visual representation of the data’s central tendency, spread, and potential outliers.\n\nIn R we use the quantile() function to provide the quartiles. Here is a demonstration:\n\n# Look at the normal data\nquantile(normal_data, p = 0.25)\n\n       25% \n-0.6597937 \n\nquantile(normal_data, p = 0.75)\n\n      75% \n0.6840946 \n\n# Look at skewed data\nquantile(left_skewed_data, p = 0.25)\n\n      25% \n0.6133139 \n\nquantile(left_skewed_data, p = 0.75)\n\n      75% \n0.8390202 \n\n\nWe calculate the interquartile range using the IQR() function:\n\nIQR(normal_data)\n\n[1] 1.343888",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#summary",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#summary",
    "title": "2. Data Summaries & Descriptions",
    "section": "summary()",
    "text": "summary()\nThe first method is a generic function that can be applied to a range of R data structures, and whose output depends on the class of the structure. It is called summary(). This function can be applied to the dataset itself (here a tibble) and also to the output of some models fitted to the data (later we will see, for instance, how it is applied to t-tests, ANOVAs, correlations, and regressions). When applied to a dataframe or tibble, we will be presented with something quite useful. Let us return to the Palmer penguin dataset, and you’ll see many familiar descriptive statistics:\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#psychdescribe",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#psychdescribe",
    "title": "2. Data Summaries & Descriptions",
    "section": "psych::describe()",
    "text": "psych::describe()\nThe psych package has the describe() function, which provides a somewhat more verbose output containing many of the descriptive statistics I introduced earlier in this Chapter:\n\npsych::describe(penguins)\n\n                  vars   n    mean     sd  median trimmed    mad    min    max\nspecies*             1 344    1.92   0.89    2.00    1.90   1.48    1.0    3.0\nisland*              2 344    1.66   0.73    2.00    1.58   1.48    1.0    3.0\nbill_length_mm       3 342   43.92   5.46   44.45   43.91   7.04   32.1   59.6\nbill_depth_mm        4 342   17.15   1.97   17.30   17.17   2.22   13.1   21.5\nflipper_length_mm    5 342  200.92  14.06  197.00  200.34  16.31  172.0  231.0\nbody_mass_g          6 342 4201.75 801.95 4050.00 4154.01 889.56 2700.0 6300.0\nsex*                 7 333    1.50   0.50    2.00    1.51   0.00    1.0    2.0\nyear                 8 344 2008.03   0.82 2008.00 2008.04   1.48 2007.0 2009.0\n                   range  skew kurtosis    se\nspecies*             2.0  0.16    -1.73  0.05\nisland*              2.0  0.61    -0.91  0.04\nbill_length_mm      27.5  0.05    -0.89  0.30\nbill_depth_mm        8.4 -0.14    -0.92  0.11\nflipper_length_mm   59.0  0.34    -1.00  0.76\nbody_mass_g       3600.0  0.47    -0.74 43.36\nsex*                 1.0 -0.02    -2.01  0.03\nyear                 2.0 -0.05    -1.51  0.04",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#skimrskim",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#skimrskim",
    "title": "2. Data Summaries & Descriptions",
    "section": "skimr::skim()",
    "text": "skimr::skim()\nThe skimr package offers something similar, but different. The skim() function returns:\n\nlibrary(skimr)\nskim(penguins)\n\n\n\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: factor\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#jmvdescriptives",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#jmvdescriptives",
    "title": "2. Data Summaries & Descriptions",
    "section": "jmv::descriptives()",
    "text": "jmv::descriptives()\nHere’s yet another view into our data, this time courtesy of the jmv package:\n\nlibrary(jmv)\ndescriptives(penguins, freq = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                                                                           \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                         species    island    bill_length_mm    bill_depth_mm    flipper_length_mm    body_mass_g    sex    year        \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   N                         344       344               342              342                  342            342    333          344   \n   Missing                     0         0                 2                2                    2              2     11            0   \n   Mean                                             43.92193         17.15117             200.9152       4201.754            2008.029   \n   Median                                           44.45000         17.30000             197.0000       4050.000            2008.000   \n   Standard deviation                               5.459584         1.974793             14.06171       801.9545           0.8183559   \n   Minimum                                          32.10000         13.10000                  172           2700                2007   \n   Maximum                                          59.60000         21.50000                  231           6300                2009   \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n FREQUENCIES\n\n Frequencies of species                                \n ───────────────────────────────────────────────────── \n   species      Counts    % of Total    Cumulative %   \n ───────────────────────────────────────────────────── \n   Adelie          152      44.18605        44.18605   \n   Chinstrap        68      19.76744        63.95349   \n   Gentoo          124      36.04651       100.00000   \n ───────────────────────────────────────────────────── \n\n\n Frequencies of island                                 \n ───────────────────────────────────────────────────── \n   island       Counts    % of Total    Cumulative %   \n ───────────────────────────────────────────────────── \n   Biscoe          168      48.83721        48.83721   \n   Dream           124      36.04651        84.88372   \n   Torgersen        52      15.11628       100.00000   \n ───────────────────────────────────────────────────── \n\n\n Frequencies of sex                                 \n ────────────────────────────────────────────────── \n   sex       Counts    % of Total    Cumulative %   \n ────────────────────────────────────────────────── \n   female       165      49.54955        49.54955   \n   male         168      50.45045       100.00000   \n ──────────────────────────────────────────────────",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/02-summarise-and-describe.html#summarytoolsdfsummary",
    "href": "BCB744/basic_stats/02-summarise-and-describe.html#summarytoolsdfsummary",
    "title": "2. Data Summaries & Descriptions",
    "section": "summarytools::dfSummary()",
    "text": "summarytools::dfSummary()\nAnd lastly, there is the summarytools package and the dfSummary() function within:\n\nlibrary(summarytools)\nprint(dfSummary(penguins, \n                varnumbers   = FALSE, \n                valid.col    = FALSE, \n                graph.magnif = 0.76),\n      method = 'render')\n\n\nData Frame Summary\npenguins\nDimensions: 344 x 8\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nMissing\n\n\n\nspecies [factor]\n\n\n1. Adelie\n\n\n2. Chinstrap\n\n\n3. Gentoo\n\n\n\n\n152\n(\n44.2%\n)\n\n\n68\n(\n19.8%\n)\n\n\n124\n(\n36.0%\n)\n\n\n\n0 (0.0%)\n\n\nisland [factor]\n\n\n1. Biscoe\n\n\n2. Dream\n\n\n3. Torgersen\n\n\n\n\n168\n(\n48.8%\n)\n\n\n124\n(\n36.0%\n)\n\n\n52\n(\n15.1%\n)\n\n\n\n0 (0.0%)\n\n\nbill_length_mm [numeric]\n\n\nMean (sd) : 43.9 (5.5)\n\n\nmin ≤ med ≤ max:\n\n\n32.1 ≤ 44.5 ≤ 59.6\n\n\nIQR (CV) : 9.3 (0.1)\n\n\n164 distinct values\n\n2 (0.6%)\n\n\nbill_depth_mm [numeric]\n\n\nMean (sd) : 17.2 (2)\n\n\nmin ≤ med ≤ max:\n\n\n13.1 ≤ 17.3 ≤ 21.5\n\n\nIQR (CV) : 3.1 (0.1)\n\n\n80 distinct values\n\n2 (0.6%)\n\n\nflipper_length_mm [integer]\n\n\nMean (sd) : 200.9 (14.1)\n\n\nmin ≤ med ≤ max:\n\n\n172 ≤ 197 ≤ 231\n\n\nIQR (CV) : 23 (0.1)\n\n\n55 distinct values\n\n2 (0.6%)\n\n\nbody_mass_g [integer]\n\n\nMean (sd) : 4201.8 (802)\n\n\nmin ≤ med ≤ max:\n\n\n2700 ≤ 4050 ≤ 6300\n\n\nIQR (CV) : 1200 (0.2)\n\n\n94 distinct values\n\n2 (0.6%)\n\n\nsex [factor]\n\n\n1. female\n\n\n2. male\n\n\n\n\n165\n(\n49.5%\n)\n\n\n168\n(\n50.5%\n)\n\n\n\n11 (3.2%)\n\n\nyear [integer]\n\n\nMean (sd) : 2008 (0.8)\n\n\nmin ≤ med ≤ max:\n\n\n2007 ≤ 2008 ≤ 2009\n\n\nIQR (CV) : 2 (0)\n\n\n\n\n2007\n:\n110\n(\n32.0%\n)\n\n\n2008\n:\n114\n(\n33.1%\n)\n\n\n2009\n:\n120\n(\n34.9%\n)\n\n\n\n0 (0.0%)\n\n\n\nGenerated by summarytools 1.1.2 (R version 4.4.3)2025-03-30\n\n\n\nAs you can see, there are many option and you may use the one you least dislike. I’ll not be prescriptive or openly opinionated about it.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html",
    "href": "BCB744/basic_stats/04-distributions.html",
    "title": "4. Data Distributions",
    "section": "",
    "text": "In this chapter\n\n\n\n\nThe concept of data distributions",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#bernoulli-and-binomial-distributions",
    "href": "BCB744/basic_stats/04-distributions.html#bernoulli-and-binomial-distributions",
    "title": "4. Data Distributions",
    "section": "Bernoulli and Binomial distributions",
    "text": "Bernoulli and Binomial distributions\nThe Bernoulli and Binomial distributions belong to what might be termed the “trial-based” subfamily of discrete distributions: they directly model outcomes of repeated experiments. Trial-based distributions require specifying both the number of trials and success probability.\nBernoulli and Binomial distributions are both discrete probability distributions that describe the outcomes of binary events. They are similar but there are also some key differences between the two. In real life examples encountered in ecology and biology we will probably mostly encounter the Binomial distributions. Let us consider each is more detail.\nBernoulli distribution The Bernoulli distribution represents a single binary trial or experiment with only two possible outcomes: ‘success’ (usually represented as 1) and ‘failure’ (usually represented as 0). The probability of success is denoted by \\(p\\), while the probability of failure is \\(1 - p\\). A Bernoulli distribution is characterised by only one parameter, \\(p\\), which represents the probability of success for the single trial (Equation 1) (Figure 1, Figure 2).\n\n\nThe Bernoulli distribution: \\[\nP(X=k) = \\begin{cases}\n  p, & \\text{if } k=1 \\\\\n  1-p, & \\text{if } k=0\n\\end{cases}\n\\tag{1}\\]\nwhere \\(X\\) is a random variable, \\(k\\) is the outcome (1 for success and 0 for failure), and \\(p\\) is the probability of success.\n\n\n\n\n\n\n\nFigure 1: Bernoulli distribution with 10 trials and 20 simulations set at a probability of p = 0.2. This could be a heavily loaded coin that has a 20% chance of landing heads.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bernoulli distribution with 10 trials and 20 simulations set at a probability of p = 0.5. This represents an unbiased coin that has an equal chance of landing on head or tail.\n\n\n\n\nBinomial distribution The Binomial distribution represents the sum of outcomes in a fixed number of independent Bernoulli trials with the same probability of success, \\(p\\). It is characterised by two parameters, \\(n\\) (the number of trials) and \\(p\\) (the probability of success in each trial). The Binomial distribution describes the probability of obtaining a specific number of successes (\\(k\\)) in \\(n\\) trials (Equation 2) (Figure 3).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Binomial distribution: \\[P(X=k) = C(n,k) \\cdot p^k \\cdot (1-p)^{(n-k)} \\tag{2}\\]\nwhere \\(X\\) is a random variable, \\(k\\) is the number of successes, \\(n\\) is the total number of trials, \\(p\\) is the probability of success in each trial, and \\(C(n,k)\\) represents combinations.\nIn practice, determine \\(n\\) (number of trials), \\(p\\) (success probability), and \\(k\\) (desired number of successes). Calculate the binomial coefficient \\(C(n,k) = \\frac{n!}{k!(n-k)!}\\), then apply Equation 2. If flipping a coin 10 times (\\(n=10\\), \\(p=0.5\\)) and seeking exactly 3 heads (\\(k=3\\)): \\(C(10,3) = 120\\), so \\(P(X=3) = 120 \\cdot (0.5)^3 \\cdot (0.5)^7 = 120 \\cdot (0.5)^{10} = 0.117\\).\n\n\n\n\n\n\n\nFigure 3: Ten simulations of a binomial distribution with 40 randomly generated points for each of 100 trials at an a priori set probability of p = 0.75.\n\n\n\n\nThere are several examples of Binomial distributions in ecological and biological contexts. The Binomial distribution is relevant when studying the number of successes in a fixed number of independent trials, each with the same probability of success. A few examples of the Bernoulli distribution:\n\nSeed germination Suppose we plant 100 seeds of a particular plant species and wants to know the probability of a certain number of seeds germinating. If the probability of germination for each seed is constant then we can model the number of germinated seeds by a Binomial distribution.\nDisease prevalence An epidemiologist studies the prevalence of a disease within a population. For a random sample of 500 individuals, and with a fixed probability of an individual having the disease, the number of infected individuals in the sample can be modeled using a Binomial distribution.\nSpecies occupancy We do an ecological assessment to determine the occupancy of bird species across 50 habitat patches. If the probability of the species occupying a patch is the same across all patches, the number of patches occupied by the species will follow a Binomial distribution.\nAllele inheritance We want to examine the inheritance of a specific trait following Mendelian inheritance patterns. If the probability of inheriting the dominant allele for a given gene is constant, the number of offspring with the dominant trait in a fixed number of offspring follows the Binomial distribution.\n\nNote that in these examples we assume a fixed probability and independence between trials and this is not always be true in real-world situations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#negative-binomial-and-geometric-distributions",
    "href": "BCB744/basic_stats/04-distributions.html#negative-binomial-and-geometric-distributions",
    "title": "4. Data Distributions",
    "section": "Negative Binomial and Geometric distributions",
    "text": "Negative Binomial and Geometric distributions\nThe Geometric and Negative Binomial distributions form a “waiting time” subfamily, focusing on the number of trials preceding specified success patterns. These waiting-time distributions focus on success probability and target achievement levels.\nNegative Binomial distribution A Negative Binomial random variable, \\(X\\), counts the number of successes in a sequence of independent Bernoulli trials with probability \\(p\\) before \\(r\\) failures occur. This distribution could for example be used to predict the number of heads that result from a series of coin tosses before three tails are observed (Equation 3) (Figure 4).\n\n\nThe Negative Binomial distribution: \\[P(X=k) = \\binom{k+r-1}{k} p^r (1-p)^k \\tag{3}\\]\nThe equation describes the probability mass function (PMF) of a Negative Binomial distribution, where \\(X\\) is a random variable, \\(k\\) is the number of failures, \\(r\\) is the number of successes, and \\(p\\) is the probability of success in each trial. The binomial coefficient is denoted by \\(\\binom{k+r-1}{k}\\), which calculates the number of ways to arrange \\(k\\) failures and \\(r\\) successes such that the last trial is a success.\n\n\n\n\n\n\n\nFigure 4: A negative binomial distribution with 50 trials and 10 simulations at an a priori expectation of p = 0.75.\n\n\n\n\nGeometric distribution A geometric random variable, \\(X\\), represents the number of trials that are required to observe a single success. Each trial is independent and has success probability \\(p\\). As an example, the geometric distribution is useful to model the number of times a die must be tossed in order for a six to be observed (Equation 4) (Figure 5).\n\n\nThe Geometric distribution: \\[P(X=k) = p (1-p)^k \\tag{4}\\]\nThe equation represents the PMF of a Geometric distribution, where \\(X\\) is a random variable, \\(k\\) is the number of failures before the first success, and \\(p\\) is the probability of success in each trial. The Geometric distribution can be thought of as a special case of the Negative Binomial distribution with \\(r = 1\\), which models the number of failures before achieving a single success.\n\n\n\n\n\n\n\nFigure 5: A geometric distribution with 50 trials and 10 simulations at an a priori expectation of p = 0.75.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#poisson-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#poisson-distribution",
    "title": "4. Data Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\n\nThe Poisson distribution: \\[P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\tag{5}\\]\nThe function represents the PMF of a Poisson distribution, where \\(X\\) is a random variable, \\(k\\) is the number of events or occurrences, and \\(\\lambda\\) (lambda) is the average rate of occurrences (events per unit of time or space). The constant \\(e\\) is the base of the natural logarithm, and \\(k!\\) is the factorial of \\(k\\). The Poisson distribution is commonly used to model the number of events occurring within a fixed interval of time or space when events occur independently and at a constant average rate.\nFor practical application, identify \\(\\lambda\\) (average rate) and \\(k\\) (observed count). If a gerbil enters a nest 4 times per hour (\\(\\lambda=4\\)) and you want the probability of exactly 2 entries in one hour (\\(k=2\\)): \\(P(X=2) = \\frac{4^2 \\cdot e^{-4}}{2!} = \\frac{16 \\cdot 0.0183}{2} = 0.147\\).\nThe Poisson distribution represents a “rate-based” subfamily, which is used to model count phenomena without explicit trial structure. They involve the average occurrence rates over specified intervals.\nA Poisson random variable, \\(X\\), tallies the number of events occurring in a fixed interval of time or space, given that these events occur with an average rate \\(\\lambda\\). Poisson distributions can be used to model events such as meteor showers and or number of people entering a shopping mall (Equation 5) (Figure 6).\n\n\n\n\n\n\n\nFigure 6: A Poisson distribution with 50 trials and 10 simulations at an a priori expectation of p = 0.75.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#hypergeometric-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#hypergeometric-distribution",
    "title": "4. Data Distributions",
    "section": "Hypergeometric distribution",
    "text": "Hypergeometric distribution\n\n\nThe Hypergeometricd distribution: \\[P(X=k) = \\frac{C(K,k) \\cdot C(N-K,n-k)}{C(N,n)} \\tag{6}\\] In the probability mass function, above, \\(N\\) is population size, \\(K\\) is number of success items in population, \\(n\\) is sample size, and \\(k\\) is observed successes in sample.\nConsider drawing 5 cards from a deck without replacement, seeking exactly 2 hearts. Here \\(N=52\\), \\(K=13\\) (hearts), \\(n=5\\), \\(k=2\\): \\(P(X=2) = \\frac{C(13,2) \\cdot C(39,3)}{C(52,5)} = \\frac{78 \\cdot 9139}{2,598,960} = 0.274\\).\nThe hypergeometric distribution occupies a distinct position within the discrete distribution taxonomy: we might term the “finite population sampling” subfamily.\nThe hypergeometric distribution models sampling without replacement from finite populations containing two types of items. Unlike binomial sampling, each draw changes the composition of remaining items.\nTo do: insert figures.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#sec-normal",
    "href": "BCB744/basic_stats/04-distributions.html#sec-normal",
    "title": "4. Data Distributions",
    "section": "Normal distribution",
    "text": "Normal distribution\n\n\nThe Normal distribution: \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\tag{7}\\]\nwhere \\(x\\) is a continuous random variable, \\(\\mu\\) (mu) is the mean, and \\(\\sigma\\) (sigma) is the standard deviation. The constant factor \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\) ensures that the Probability Density Function (PDF) integrates to 1, and the exponential term is responsible for the characteristic bell-shaped curve of the Normal distribution.\n\n\n\n\n\n\nFigure 7: The idealised Normal distribution showing the proportion of data within 1, 2, and 3SD from the mean.\n\n\n\nAnother name for this kind of distribution is a Gaussian distribution. A random sample with a Gaussian distribution is normally distributed. These values are independent and identically distributed random variables (i.i.d.), and they have an expected mean given by \\(\\mu\\) (or \\(\\hat{x}\\) in Chapter 3.2.1) and a finite variance given by \\(\\sigma^{2}\\) (or \\(S^{2}\\) in Chapter 3.3.1); if the number of samples drawn from a population is sufficiently large, the estimated mean and SD will be indistinguishable from the population (as per the central limit theorem). It is represented by Equation 7 (Figure 8).\n\n\n\n\n\n\n\nFigure 8: Normal distribution with 40 trials and 5 simulations.\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThe Central Limit Theorem (CLT) is a fundamental result in probability theory and statistics, which states that the distribution of the sum (or average) of a large number of independent, identically distributed (IID) random variables approaches a Normal distribution regardless of the shape of the original distribution. So, the CLT asserts that the Normal distribution is the limiting distribution for the sum or average of many random variables, as long as certain conditions are met.\nThe CLT provides a basis for making inferences about population parameters using sample statistics. For example, when dealing with large sample sizes, the sampling distribution of the sample mean is approximately normally distributed, even if the underlying population distribution is not normal. This allows us to apply inferential techniques based on the Normal distribution, such as hypothesis testing and constructing confidence intervals, to estimate population parameters using sample data.\nSome conditions must be met for the CLT to be true:\n\n\nThe random variables must be independent The observations should not be influenced by one another.\n\nThe random variables must be identically distributed They must come from the same population with the same mean and variance.\n\nThe number of random variables (sample size) must be sufficiently large Although there is no strict rule for the sample size, a common rule of thumb is that the sample size should be at least 30 for the CLT to be a reasonable approximation.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#uniform-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#uniform-distribution",
    "title": "4. Data Distributions",
    "section": "Uniform distribution",
    "text": "Uniform distribution\nThe continuous uniform distribution is sometime called a rectangular distribution. Simply, it states that all measurements of the same magnitude included with this distribution are equally probable. This is basically random numbers (Figure 9).\n\n\n\n\n\n\n\nFigure 9: Uniform distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#student-t-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#student-t-distribution",
    "title": "4. Data Distributions",
    "section": "Student T distribution",
    "text": "Student T distribution\nThis is a continuous probability distribution that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. It is used in the statistical significance testing between the means of different sets of samples, and not much so in the modelling of many kinds of experiments or observations (Figure 10).\n\n\n\n\n\n\n\nFigure 10: Uniform distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#chi-squared-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#chi-squared-distribution",
    "title": "4. Data Distributions",
    "section": "Chi-squared distribution",
    "text": "Chi-squared distribution\nMostly used in hypothesis testing, but not to encapsulate the distribution of data drawn to represent natural phenomena (Figure 11).\n\n\n\n\n\n\n\nFigure 11: Chi distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#exponential-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#exponential-distribution",
    "title": "4. Data Distributions",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nThis is a probability distribution that describes the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate (Figure 12).\n\n\n\n\n\n\n\nFigure 12: An exponential distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#f-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#f-distribution",
    "title": "4. Data Distributions",
    "section": "F distribution",
    "text": "F distribution\nThis is a probability distribution that arises in the context of the analysis of variance (ANOVA) and regression analysis. It is used to compare the variances of two populations (Figure 13).\n\n\n\n\n\n\n\nFigure 13: An F distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#gamma-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#gamma-distribution",
    "title": "4. Data Distributions",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nThis is a two-parameter family of continuous probability distributions. It is used to model the time until an event occurs. It is a generalisation of the exponential distribution (Figure 14).\n\n\n\n\n\n\n\nFigure 14: A Gamma distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/04-distributions.html#beta-distribution",
    "href": "BCB744/basic_stats/04-distributions.html#beta-distribution",
    "title": "4. Data Distributions",
    "section": "Beta distribution",
    "text": "Beta distribution\nThis is a family of continuous probability distributions defined on the interval [0, 1] parameterised by two positive shape parameters, typically denoted by α and β. It is used to model the behaviour of random variables limited to intervals of finite length in a wide variety of disciplines (Figure 15).\n\n\n\n\n\n\n\nFigure 15: A Beta distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html",
    "href": "BCB744/basic_stats/06-assumptions.html",
    "title": "6. Assumptions",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nRevisiting assumptions\nNormality and homogeneity of variance tests\nRevisiting the non-parametric tests\nLog transformation\nSquare-root transformation\nArcsine transformation\nPower transformation\nLesser-used transformation",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Assumptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#tests-for-normality",
    "href": "BCB744/basic_stats/06-assumptions.html#tests-for-normality",
    "title": "6. Assumptions",
    "section": "Tests for normality",
    "text": "Tests for normality\n\n\n\n\n\n\n\nFigure 1: Histograms showing two randomly generated normal distributions.\n\n\n\n\nRemember from Chapter 4 what a normal distribution is/looks like? Let’s have a peek below to remind ourselves (Figure 1).\nWhereas histograms may be a pretty way to check the normality of our data, there are actual statistical tests for this, which is preferable to a visual inspection alone. But remember that you should always visualise your data before performing any statistics on them.\n\n\n\n\n\n\nHypothesis for normailty\n\n\n\n\\(H_{0}\\): The distribution of our data is not different from normal (or, the variable is normally distributed).\n\n\nThe Shapiro-Wilk test is frequently used to assess the normality of a dataset. It is known to have good power and accuracy for detecting departures from normality, even for small sample sizes, and it is also robust to outliers, making it useful for analysing data that may contain extreme values.\nIt tests the H0 that the population from which the sample, \\(x_{1},..., x_{n}\\), was drawn is not significantly different from normal. The test does so by sorting the data from lowest to highest, and a test statistic, \\(W\\), is calculated based on the deviations of the observed values from the expected values under a normal distribution (Equation 1). \\(W\\) is compared to a critical value, based on the sample size and significance level, to determine whether to reject or fail to reject the H0.\n\n\nThe Shapiro-Wilk test: \\[W = \\frac{(\\sum_{i=1}^n a_i x_{(i)})^2}{\\sum_{i=1}^n (x_i - \\overline{x})^2} \\tag{1}\\]\nHere, \\(W\\) represents the Shapiro-Wilk test statistic, \\(a_{i}\\) are coefficients that depend on the sample size and distribution of the data, \\(x_{(i)}\\) represents the \\(i\\)-th order statistic, or the \\(i\\)-th smallest value in the sample, and \\(\\overline{x}\\) represents the sample mean.\nThe Shapiro-Wilk test is available within base R as the function shapiro.test(). If the p-value is above 0.05 we may assume the data to be normally distributed. In order to demonstrate what the output of shapiro.test() looks like we will run it on all of the random data we generated.\n\nshapiro.test(r_dat$dat)\n\n\n    Shapiro-Wilk normality test\n\ndata:  r_dat$dat\nW = 0.70346, p-value &lt; 2.2e-16\n\n\nNote that this shows that the data are not normally distributed. This is because we have incorrectly run this function simultaneously on two different samples of data. To perform this test correctly, and in the tidy way, we need to recognise the grouping structure (chickens and giraffes) and select only the second piece of information from the shapiro.test() output and ensure that it is presented as a numeric value:\n\n# we use the square bracket notation to select only the *-value;\n# had we used `[1]` we'd have gotten W\nr_dat %&gt;% \n  group_by(sample) %&gt;% \n  summarise(norm_dat = as.numeric(shapiro.test(dat)[2]))\n\n# A tibble: 2 × 2\n  sample   norm_dat\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Chickens    0.461\n2 Giraffes    0.375\n\n\nNow we see that our two sample sets are indeed normally distributed.\nSeveral other tests are available to test whether our data are consistent with a normal distribution:\n\nKolmogorov-Smirnov test This test is a non-parametric test that compares the empirical distribution of a sample with a hypothesised normal distribution. It is based on the maximum absolute difference between the cumulative distribution function of the sample and the theoretical normal distribution function. This test can also be used to see if one’s own data are consistent with other kinds of data distributions. In R the Kolmogorov-Smirnov test is available as ks.test().\nAnderson-Darling test Similar to the Shapiro-Wilk test, the Anderson-Darling test is used to test the hypothesis that a sample comes from a normal (or any other) distribution. It is based on the squared differences between the empirical distribution function of the sample and the theoretical normal distribution function. This function is not natively available in base R but the function ad.test() is made available in two packages (that I know of), namely, nortest and kSamples. Read the help files—even though the name of the function is the same in the two packages, they are implemented differently.\nLilliefors test This test is a modification of the Kolmogorov-Smirnov test that is specifically designed for small sample sizes. It is based on the maximum difference between the empirical distribution function of the sample and the normal distribution function. Some R packages such as nortest and descTools seem to use Lilliefors synonymously with Kolmogorov-Smirnov. These functions are called lillie.test() and LillieTest(), respectively.\nJarque-Bera test This test is based on the skewness and kurtosis of a sample and tests whether the sample has the skewness and kurtosis expected from a normal distribution. Find it in R as jarque.bera.test() in the DescTools and tseries packages. Again, read the help files as a function with the same name appears in two independent packages and I cannot give assurance that it implemented consistently.\nCramer-Von Mises test The Cramer-Von Mises test is used to assess the goodness of fit of a distribution to a sample of data. The test is based on the cumulative distribution function (CDF) of the sample and the theoretical distribution being tested. See the cvm.test() function in the goftest package.\n\nTake your pick. The Shapiro-Wilk and Kolmogorov-Smirnov tests are the most frequently used normality tests in my experience but be adventurous and use the Cramer-Von Mises test and surprise your supervisor in an interesting way—more than likely, they will not have heard of it before. When you decide, however, do your homework and read about these pros and cons of the tests as they are not all equally robust to all the surprises data can throw at them.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Assumptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/06-assumptions.html#sec-homogeneity",
    "href": "BCB744/basic_stats/06-assumptions.html#sec-homogeneity",
    "title": "6. Assumptions",
    "section": "Tests for homogeneity of variances",
    "text": "Tests for homogeneity of variances\nBesides requiring that our data are normally distributed, we must also ensure that they are homoscedastic. This word means that the scedasticity (variance) of our samples is homogeneous (similar). In practical terms this means that the variance of the samples we are comparing should not be more than two to four times greater than one another. In R, we use the function var() to check the variance in a sample:\n\nr_dat %&gt;% \n  group_by(sample) %&gt;% \n  summarise(sample_var = var(dat))\n\n# A tibble: 2 × 2\n  sample   sample_var\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Chickens    0.00994\n2 Giraffes    0.0872 \n\n\nAbove we see that the variance of our two samples is heteroscedastic because the variance of one more than two to four times greater than the other. However, there are formal tests to establish the equality of variances, as we can see in the following hypothesis tests:\n\n\n\n\n\n\nHypotheses for equality of variances\n\n\n\nThe two-sided and one-sided formulations are:\n\\(H_{0}: \\sigma^{2}_{A} = \\sigma^{2}_{B}\\) and \\(H_{a}: \\sigma^{2}_{A} \\ne \\sigma^{2}_{B}\\)\n\\(H_{0}: \\sigma^{2}_{A} \\le \\sigma^{2}_{B}\\) and \\(H_{a}: \\sigma^{2}_{A} \\gt \\sigma^{2}_{B}\\)\n\\(H_{0}: \\sigma^{2}_{A} \\ge \\sigma^{2}_{B}\\) and \\(H_{a}: \\sigma^{2}_{A} \\lt \\sigma^{2}_{B}\\)\nwhere \\(\\sigma^{2}_{A}\\) and \\(\\sigma^{2}_{B}\\) are the variances for samples \\(A\\) and \\(B\\), respectively.\n\n\nThe most commonly used test for equality of variances is Levene’s test, car::leveneTest(). Levene’s test assess the equality of variances between two or more groups in a dataset. The H0 is that the variances of the groups are equal. It is a non-parametric test that does not assume anything about the data’s normality and as such it is more robust than the F-test.\nThe test is commonly used in t-tests and ANOVA to check that the variances of the dependent variable are the same across all levels of the independent variable. Violating this assumption can lead to incorrect conclusions made from the test outcome, such as those resulting from Type I and Type II errors.\nIn Levene’s test, the absolute deviations of the observations from their group medians are calculated, and the test statistic is computed as the ratio of the sum of the deviations to the degrees of freedom (Equation 2). The test statistic follows an F distribution under the H0, and a significant result indicates that the variances of the groups are significantly different.\n\n\nLevene’s test:\n\\[W = \\frac{(N-k)}{(k-1)} \\cdot \\frac{\\sum_{i=1}^k n_i (\\bar{z}_i - \\bar{z})^2}{\\sum_{i=1}^k \\sum_{j=1}^{n_i} (z_{ij} - \\bar{z}_i)^2} \\tag{2}\\]\nwhere \\(W\\) represents the Levene’s test statistic, \\(N\\) is the total sample size, \\(k\\) is the number of groups being compared, \\(n_i\\) is the sample size of the \\(i\\)-th group, \\(z_{ij}\\) is the \\(j\\)-th observation in the \\(i\\)-th group, \\(z_{i}\\) is the mean of the ith group, and \\(\\bar{z}\\) is the overall mean of all groups combined.\nThe test statistic is calculated by comparing the deviations of the observations within each group from their group mean (\\(\\bar{z}_i\\)) to the deviations of the group means from the overall mean (\\(\\bar{z}\\)).\nLevene’s test is considered robust to non-normality and outliers, making it a useful tool for analysing data that do not meet the assumptions of normality, but it can be sensitive to unequal sample sizes and may not be appropriate for very small sample sizes.\n\ncar::leveneTest(dat ~ sample, data = r_dat)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup    1  702.15 &lt; 2.2e-16 ***\n      1998                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAbove, we see that p &lt; 0.05, causing us to accept the alternative hypothesis that the variances are unequal between the sample of giraffes and the sample of chickens.\nSeveral other statistical tests are available to assess the homogeneity of variances in a dataset:\n\nF-test This test is also known as the variance ratio test. Use the var.test() function in R. It assumes that the underlying data follows a normal distribution and is designed to test the H0 that the variances of two populations are equal. The test statistic is the ratio of the variances of the two populations. You will often see this test used in the context of an ANOVA to test for homogeneity of variance across groups.\nBartlett’s test This test is similar to Levene’s test and is used to assess the equality of variances across multiple groups. The test is based on the \\(\\chi\\)-squared distribution and assumes that the data are normally distributed. Base R has the bartlett.test() function.\nBrown-Forsythe test This test is a modification of the Levene’s test that uses the absolute deviations of the observations from their respective group medians instead of means. This makes the test more robust to outliers and non-normality. It is available in onewaytests as the function bf.test().\nFligner-Killeen test This is another non-parametric test that uses the medians of the groups instead of the means. It is based on the \\(\\chi\\)-squared distribution and is also robust to non-normality and outliers. The Fligner test is available in Base R as fligner.test().\n\nAs always, supplement your analysis with these checks: i) perform any of the diagnostic plots we covered in the earlier Chapters, or ii) compare the variances and see if they differ by more than a factor of four.\nSee this discussion if you would like to know about some more advanced options when faced with heteroscedastic data.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "6. Assumptions"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html",
    "title": "1. Philosophy of Science",
    "section": "",
    "text": "“Most people use statistics like a drunk man uses a lamppost; more for support than illumination.”\n— Andrew Lang\nToday, we’re going to delve into the concept of hypothesis testing. We’ll look at its foundations, its uses in various disciplines, and the ongoing debates about its role in the greater scheme of scientific knowledge creation. To start, let’s go back to a key figure in the philosophy of science…",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#parametric-statistics",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#parametric-statistics",
    "title": "1. Philosophy of Science",
    "section": "3.1 Parametric Statistics",
    "text": "3.1 Parametric Statistics\nParametric statistics rely on certain distributional assumptions—most commonly, the normal distribution—and serve as a foundation in quantitative analysis across many scientific disciplines. They facilitate the extraction of precise statistical properties and parameter estimates, making them particularly powerful for testing hypotheses and deriving inferences in controlled experimental setups or data derived from systematic, structured sampling campaigns. In cases where data do not conform to the assumed distributions, we must employ data transformations or leverage non-parametric methods that do not require specific distributional assumptions.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#non-parametric-statistics",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#non-parametric-statistics",
    "title": "1. Philosophy of Science",
    "section": "3.2 Non-Parametric Statistics",
    "text": "3.2 Non-Parametric Statistics\nWhile parametric statistics offer significant advantages in terms of precision and power, their applicability across various scientific disciplines necessitates a thorough understanding of their assumptions and limitations. Non-parametric statistics are inherently more flexible and do not depend on restrictive assumptions about the nature of our data—as such, they can accommodate non-normal data, skewed data, or data measured on an ordinal scale. They focus instead on ranks or medians rather than mean values, and provide a means to conduct robust set of statistical inference tests on a far wider range of data types.\nThere are a few trade-offs we need to know about when opting for non-rarametric approaches. This includes the potential loss in statistical power and the nuances of interpreting rank-based or median-based results as opposed to mean values. Nevertheless, non-parametric statistics are a critical component of our toolbox across disciplines such as taxonomy, systematics, organismal biology, ecology, socio-ecological studies, and Earth sciences.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#multivariate-analyses",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#multivariate-analyses",
    "title": "1. Philosophy of Science",
    "section": "3.3 Multivariate Analyses",
    "text": "3.3 Multivariate Analyses\nEcologists consider questions about the complex interactions between the biotic and abiotic world. Often they work across multiple spatial and temporal scales. Multivariate analyses such as cluster analysis and ordination are powerful exploratory tools. They untangle ecological datasets to discern patterns and relationships among multiple variables—be it species abundance across different habitats, environmental gradients, or the dynamical properties of ecosystems. Cluster analysis groups similar entities based on their characteristics and reveals natural groupings within the data. Ordination, on the other hand, reduces multidimensional space, making it easier to visualise and interpret complex ecological relationships. In contrast to the more traditional parametric and non-parametric statistics, which often focus on testing hypotheses about the relationships between variables, multivariate analyses provide a more overarching view. They allow us to uncover hidden structures and gradients in the data without a priori hypotheses. While parametric methods hinge on assumptions about data distribution and non-parametric methods offer flexibility in handling data that don’t meet these assumptions, multivariate analyses surpass these by focusing on the ecosystem’s interconnectedness and the patterns emerging from these connections.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#bayesian-methods",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#bayesian-methods",
    "title": "1. Philosophy of Science",
    "section": "3.4 Bayesian Methods",
    "text": "3.4 Bayesian Methods\nBayesian methods offer a distinct perspective within the statistical toolbox, allowing us to formally incorporate prior knowledge into our data analysis. Unlike traditional frequentist statistics, which focus solely on the observed data, Bayesian approaches let us blend in existing beliefs or information and then update those beliefs as new evidence comes in. This emphasis on continuously refining our understanding, rather than just finding a single best-fit hypothesis given the data, makes Bayesian methods powerful in scientific fields where we have substantial background knowledge but still need to carefully quantify uncertainty. Bayesian methods are particularly useful in fields like ecology or phyologenetics, where prior knowledge about species interactions or relatiosnhips, environmental conditions, or ecosystem dynamics can be leveraged to make more informed inferences. They also provide a natural framework for decision-making under uncertainty, allowing us to quantify the risks and benefits of different courses of action.\nThe downside of Bayesian analyses is that they can be computationally intensive, especially when dealing with complex models or large datasets. They also require careful consideration of the prior distributions, which can introduce subjectivity into the analysis. But as computational resources continue to expand and methodologies evolve, Bayesian approaches are likely to play an increasingly prominent role in advancing our understanding of the natural world.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#taxonomy-systematics-and-phylogenetics",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#taxonomy-systematics-and-phylogenetics",
    "title": "1. Philosophy of Science",
    "section": "3.5 Taxonomy, Systematics, and Phylogenetics",
    "text": "3.5 Taxonomy, Systematics, and Phylogenetics\nPhylogenetic analysis, while grounded in data and statistical principles, operates within a distinct framework from traditional parametric, non-parametric, or multivariate methods. Its primary goal is to infer evolutionary relationships and patterns of change, rather than classical hypothesis testing. Phylogenetics explicitly models the interconnectedness of evolutionary lineages, a stark contrast to the assumption of independent data points often found in other statistical approaches. While multivariate analyses help examine complex interactions among multiple variables, phylogenetics focuses on how those variables (traits or genes) have evolved across a branching, tree-like structure. Bayesian statistics offer a powerful tool within phylogenetics, aiding in the estimation of probabilities for different evolutionary histories. Yet, the core of phylogenetics lies in specialised algorithms and models designed to reconstruct these evolutionary narratives.\nPhylogenetic analysis is deeply intertwined with systematics and taxonomy, disciplines that seek to understand and classify the diversity of life. Systematics broadly encompasses the study of organismal relationships, while taxonomy focuses on the practice of naming and classification. Phylogenetics serves as a powerful tool within the systematics toolbox, using data to infer evolutionary patterns and inform classification decisions. While statistical methods like parametric and non-parametric tests are used in systematics and taxonomy (e.g., for comparing morphological traits), much of the analytical toolkit centers on techniques specifically designed for evolutionary data. These techniques include methods for building phylogenetic trees, assessing congruence between different data sources (like genes and morphology), and interpreting patterns of diversification over time.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#artificial-intelligence-and-machine-learning",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#artificial-intelligence-and-machine-learning",
    "title": "1. Philosophy of Science",
    "section": "3.6 Artificial Intelligence and Machine Learning",
    "text": "3.6 Artificial Intelligence and Machine Learning",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#models-and-simulations",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#models-and-simulations",
    "title": "1. Philosophy of Science",
    "section": "3.7 Models and Simulations",
    "text": "3.7 Models and Simulations",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#qualitative-analysis",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#qualitative-analysis",
    "title": "1. Philosophy of Science",
    "section": "3.8 Qualitative Analysis",
    "text": "3.8 Qualitative Analysis",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/01-scientific-philosophy.html#phylogenetic-analysis",
    "href": "BCB744/basic_stats/01-scientific-philosophy.html#phylogenetic-analysis",
    "title": "1. Philosophy of Science",
    "section": "3.9 Phylogenetic Analysis",
    "text": "3.9 Phylogenetic Analysis\n\nNon-Parametric Statistics:Free us from strict distributional assumptions, great in areas like taxonomy where data might violate parametric rules.\nMultivariate Analyses: Let us tackle ecological complexity where multiple factors interweave with messy, non-linear outcomes.\nBayesian Statistics: Update our beliefs based on evidence, valuable where prior knowledge exists and data is uncertain.\nAI and Machine Learning: Data-driven patterns and prediction,a powerful addition to the hypothesis-testing arsenal.\nModels and Simulations: Allow us to explore complex systems and make predictions, vital in fields like oceanography.\nQualitative Analysis: Socio-ecological studies benefit from in-depth exploration of human attitudes and actions, where quantification may not tell the full story.\nPhylogenetic Analysis: Data-driven exploration of evolutionary relationships, less about statistical tests and more about algorithms and inference.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "1. Philosophy of Science"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html",
    "href": "BCB744/basic_stats/07-t_tests.html",
    "title": "7. t-Tests",
    "section": "",
    "text": "In this Chapter\n\n\n\n\nOne-sample t-tests\nTwo-sample s t-tests\nPaired t-tests\nComparison of proportions",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#two-sided-one-sample-t-test",
    "href": "BCB744/basic_stats/07-t_tests.html#two-sided-one-sample-t-test",
    "title": "7. t-Tests",
    "section": "Two-sided one-sample t-test",
    "text": "Two-sided one-sample t-test\n\n\n\n\n\n\nHypothesis for two-sided one-sample t-test\n\n\n\n\\(H_{0}: \\bar{x} = \\mu_{0}\\) and \\(H_{a}: \\bar{x} \\ne \\mu_{0}\\)\nThis is the same as:\n\\(H_{0}: \\bar{x} - \\mu_{0} = 0\\) and \\(H_{a}: \\bar{x} - \\mu_{0} \\ne 0\\)\nHere \\(\\bar{x}\\) is the population mean and \\(\\mu_{0}\\) the hypothesised mean to which \\(\\bar{x}\\) is being compared. In this example we have a two-sided one-sample t-test.\nAs stated above, \\(H_{0}\\) does not make a distinction between whether it expects \\(\\bar{x}\\) to be \\(\\lt\\) or \\(\\gt\\) \\(\\mu_{0}\\).\n\n\nGenerally when we use a t-test it will be a two-sample t-test (see below). Occasionally, however, we may have only one set of observations (random samples taken to represent a population) whose mean, \\(\\bar{x}\\), we wish to compare against a known population mean, \\(\\mu_{0}\\), which had been established a priori (Equation 1). In R’s t.test() function, the default setting is for a two-sided one-sample t-test—that is, we don’t care if our \\(H_{a}\\) is significantly less than \\(\\mu_{0}\\) or if it is significantly greater than \\(\\mu_{0}\\).\n\n\nThe one-sample t-test:\n\\[t = \\frac{\\overline{x} - \\mu}{s / \\sqrt{n}} \\tag{1}\\]\nwhere \\(t\\) is the calculated \\(t\\)-value, \\(\\overline{x}\\) is the sample mean, \\(\\mu\\) is the hypothesised population mean, \\(s\\) is the sample standard deviation, and \\(n\\) the sample size.\n\n# create a single sample of random normal data\nset.seed(666)\nr_one &lt;- data.frame(dat = rnorm(n = 20, mean = 20, sd = 5),\n                    sample = \"A\")\n\n\n\n\n\n\n# compare random data against a population mean of 20\nt.test(r_one$dat, mu = 20)\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = 0.0048653, df = 19, p-value = 0.9962\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 16.91306 23.10133\nsample estimates:\nmean of x \n 20.00719 \n\n# compare random data against a population mean of 30\nt.test(r_one$dat, mu = 30)\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = -6.7596, df = 19, p-value = 1.858e-06\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 16.91306 23.10133\nsample estimates:\nmean of x \n 20.00719 \n\n\nWhat do the results of these two different tests show? Let’s visualise these data to get a better understanding (Figure 1).\n\nggplot(data = r_one, aes(y = dat, x = sample)) +\n  geom_boxplot(fill = \"indianred\", notch = TRUE,\n               alpha = 0.3, colour = \"black\") +\n  # population  mean (mu) = 20\n  geom_hline(yintercept = 20, colour = \"dodgerblue2\", \n             size = 0.9) +\n  # population  mean (mu) = 30\n  geom_hline(yintercept = 30, colour = \"indianred2\", \n             size = 0.9) +\n  labs(y = \"Value\", x = NULL) +\n  coord_flip() +\n  theme_pubclean()\n\n\n\n\n\n\nFigure 1: Boxplot of random normal data with. A hypothetical population mean of 20 is shown as a blue line, with the red line showing a mean of 30.\n\n\n\n\nThe boxplot shows the distribution of our random data against two potential population means. Does this help now to illustrate the results of our one-sample t-tests?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sided-one-sample-t-tests",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sided-one-sample-t-tests",
    "title": "7. t-Tests",
    "section": "One-sided one-sample t-tests",
    "text": "One-sided one-sample t-tests\n\n\n\n\n\n\nHypothesis for one-sided one-sample t-test\n\n\n\nFor example, when we are concerned that our sample mean, \\(\\bar{x}\\), should be less than the a priori established value, \\(\\mu_{0}\\):\n\\(H_{0}: \\bar{x} \\ge \\mu_{0}\\) and \\(H_{a}: \\bar{x} \\lt \\mu_{0}\\)\nOnly one of the two options is shown.\n\n\nRemember that a normal distribution has two tails. As indicated already, when we are testing for significance we are generally looking for a result that sits in the far end of either of these tails. Occasionally, however, we may want to know if the result is specifically in one of the two tails. Explicitly the leading or trailing tail. For example, is the mean value of our sample population, \\(\\bar{x}\\), significantly greater than the value \\(\\mu_{0}\\)? Or, is \\(\\bar{x}\\) less than the value \\(\\mu_{0}\\)? This t-test is called a one-sided one-sample t-tests. To specify this in R we must add an argument as seen below:\n\n# check against the trailing tail\nt.test(r_one$dat, mu = 30, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = -6.7596, df = 19, p-value = 9.292e-07\nalternative hypothesis: true mean is less than 30\n95 percent confidence interval:\n     -Inf 22.56339\nsample estimates:\nmean of x \n 20.00719 \n\n# check against the leading tail\nt.test(r_one$dat, mu = 30, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  r_one$dat\nt = -6.7596, df = 19, p-value = 1\nalternative hypothesis: true mean is greater than 30\n95 percent confidence interval:\n 17.451    Inf\nsample estimates:\nmean of x \n 20.00719 \n\n\nAre these the results we would have expected? Why does the second test not return a significant result?\n\n\n\n\n\n\nDo this now!\n\n\n\nCreate a visualisation to graphically demonstrate the outcome of this t-test.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#two-sided-two-sample-t-test",
    "href": "BCB744/basic_stats/07-t_tests.html#two-sided-two-sample-t-test",
    "title": "7. t-Tests",
    "section": "Two-sided two-sample t-test",
    "text": "Two-sided two-sample t-test\n\n\n\n\n\n\nHypothesis for two-sided two-sample t-test\n\n\n\n\\(H_{0}: \\bar{A} = \\bar{B}\\) and \\(H_{a}: \\bar{A} \\ne \\bar{B}\\)\nwhere \\(\\bar{A}\\) is the population mean of the first sample and \\(\\bar{B}\\) the population mean of the second sample. In this example we have a two-sided two-sample t-test, which is the default in R’s t.test() function.\nAs stated above, \\(H_{0}\\) does not make a distinction between whether it expects the difference between \\(\\bar{A}\\) and \\(\\bar{B}\\) to be greater than or less than 0.\n\n\nA two-sample t-test is used when we have samples from two different (independent) populations whose means, \\(\\bar{A}\\) and \\(\\bar{B}\\), we would like to compare against one another. Sometimes it is called an independent sample t-test. Specifically, it tests whether the difference between the means of two samples is zero. Note that again we make no distinction between whether it is more interesting that the difference is greater than zero or less zero—as long as there is a difference between \\(\\bar{A}\\) and \\(\\bar{B}\\). This test is called a two-sided two sample t-test and it is the most common use of a t-test.\nThere are two varieties of t-tests. In the case of samples whose variances do not differ, we perform a Student’s t-test. Equation 2 shows how to calculate the t-statistic for Student’s t-test. The other case is if we have unequal variances in \\(\\bar{A}\\) and \\(\\bar{B}\\) (established with the Levene’s test for equality of variances; see Chapter 6); here, we perform Welch’s t-test as written in Equation 4. Welch’s t-test is the default in R’s t.test() function.\n\n\nStudent’s t-test: \\[t=\\frac{\\bar{A}-\\bar{B}}{\\sqrt{\\frac{S^{2}}{n}+\\frac{S^{2}}{m}}} \\tag{2}\\]\n\\(\\bar{A}\\) and \\(\\bar{B}\\) are the means for groups \\(A\\) and \\(B\\), respectively; \\(n\\) and \\(m\\) are the sample sizes of the two sets of samples, respectively; and \\(S^{2}\\) is the pooled variance, which is calculated as per Equation 3:\n\\[S^{2}=\\frac{(n-1)S_{A}^{2}+(m-1)S_{B}^{2} }{n+m-2} \\tag{3}\\]\nThe degrees of freedom, d.f., in the equation for the shared variance is \\(n_{A}+m_{B}-2\\).\n\nWelch’s t-test: \\[t=\\frac{\\bar{A}-\\bar{B}}{\\sqrt{\\frac{S^{2}_{A}}{n}+\\frac{S^{2}_{B}}{m}}} \\tag{4}\\]\nHere, \\(S_{A}\\) and \\(S_{B}\\) are the variances of groups \\(A\\) and \\(B\\), respectively (see Section X). The d.f. to use with Welch’s t-test is obtained using the Welch–Satterthwaite equation (Equation 5):\n\\[d.f. = \\frac{\\left( \\frac{S^{2}_{A}}{n}+\\frac{S^{2}_{B}}{m} \\right)^{2}}{\\left( \\frac{S^{4}_{A}}{n-1} + \\frac{S^{4}_{B}}{m-1} \\right)} \\tag{5}\\]\n\nWhat do we do with this t-statistic? In the olden days we had to calculate the t-statistics and the d.f. by hand. These two values, the d.f. and t-value had to be read off a table of pre-calculated t-values, probabilities and degrees of freedom as in here. Luckily, the t-test function nowadays does this all automagically. But if you are feeling nostalgic over times that you have sadly never experienced, please calculate the t-statistic and the d.f. yourself and give the table a go. In fact, an excessive later in this chapter will give you an opportunity to do so.\nBack to the present day and the wonders of modern technology. Let’s generate some new random normal data and test to see if the data belonging to the two groups differ significantly from one-another. First, we apply the t-test function as usual:\n\n# random normal data\nset.seed(666)\nr_two &lt;- data.frame(dat = c(rnorm(n = 20, mean = 4, sd = 1),\n                            rnorm(n = 20, mean = 5, sd = 1)),\n                    sample = c(rep(\"A\", 20), rep(\"B\", 20)))\n\n# perform t-test\n# note how we set the `var.equal` argument to TRUE because we know \n# our data has the same SD (they are simulated as such!)\nt.test(dat ~ sample, data = r_two, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  dat by sample\nt = -1.9544, df = 38, p-value = 0.05805\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.51699175  0.02670136\nsample estimates:\nmean in group A mean in group B \n       4.001438        4.746584 \n\n# if the variances are not equal, simply set `var.equal` to false\n# and a Welch's t-test will be performed\n\nThe first argument we see in t.test() is dat ~ sample. Usually in R when we see a ~ (tilde) we are creating what is known as a formula. A formula tells R how it should look for interactions between data and factors. For example Y ~ X reads: \\(Y\\) as a function of \\(X\\). In our code above we see dat ~ sample. This means we are telling R that the t-test we want it to perform is when the dat column is a function of the sample column. In plain English we are dividing up the dat column into the two different samples we have, and then running a t-test on these samples. Another way of stating this is that the value of dat depends on the grouping it belong to (A or B). We will see this same formula notation cropping up later under ANOVAs, linear models, etc.\n\n\n\n\n\n\nDo this now!\n\n\n\nCreate a visualisation to graphically demonstrate the outcome of this t-test.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sided-two-sample-t-test",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sided-two-sample-t-test",
    "title": "7. t-Tests",
    "section": "One-sided two-sample t-test",
    "text": "One-sided two-sample t-test\n\n\n\n\n\n\nHypothesis for one-sided two-sample t-test\n\n\n\nFor example, when we are concerned that the sample mean of the first population, \\(\\bar{A}\\), should be greater than that of the second, \\(\\bar{B}\\):\n\\(H_{0}: \\bar{A} \\le \\bar{B}\\) and \\(H_{a}: \\bar{A} \\gt \\bar{B}\\)\nOnly one of the two options is shown.\n\n\nJust as with the one-sample t-tests above, we may also specify which tail of the distribution we are interested in when we compare the means of our two samples. This is a one-sided two-sample t-test, and here too we have the Student’s t-test and Welch’s t-test varieties. We do so by providing the same arguments as previously:\n\n# is the mean of sample B smaller than that of sample A?\ncompare_means(dat ~ sample, data = r_two,\n              method = \"t.test\", var.equal = TRUE,\n              alternative = \"less\")\n\n# A tibble: 1 × 8\n  .y.   group1 group2     p p.adj p.format p.signif method\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1 dat   A      B      0.971  0.97 0.97     ns       T-test\n\n# is the mean of sample B greater than that of sample A?\ncompare_means(dat ~ sample, data = r_two,\n              method = \"t.test\", var.equal = TRUE,\n              alternative = \"greater\")\n\n# A tibble: 1 × 8\n  .y.   group1 group2      p p.adj p.format p.signif method\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1 dat   A      B      0.0290 0.029 0.029    *        T-test\n\n\nWhat do these results show? Is this surprising?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sample-and-two-sample-tests",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sample-and-two-sample-tests",
    "title": "7. t-Tests",
    "section": "One-sample and two-sample tests",
    "text": "One-sample and two-sample tests\nAs with t-tests, proportion tests may also be based on one sample, or two. If we have only one sample we must specify the total number of trials as well as what the expected population probability of success is. Because these are individual values, and not matrices, we will show what this would look like without using any objects but will rather give each argument within prop.test() a single exact value. In the arguments within prop.test(), x denotes the number of successes recorded, n shows the total number of individual trials performed, and p is the expected probability. It is easiest to consider this as though it were a series of 100 coin tosses.\n\n# When the probability matches the population\nprop.test(x = 45, n = 100, p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  45 out of 100, null probability 0.5\nX-squared = 0.81, df = 1, p-value = 0.3681\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.3514281 0.5524574\nsample estimates:\n   p \n0.45 \n\n# When it doesn't\nprop.test(x = 33, n = 100, p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  33 out of 100, null probability 0.5\nX-squared = 10.89, df = 1, p-value = 0.0009668\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2411558 0.4320901\nsample estimates:\n   p \n0.33 \n\n\nIf we have two samples that we would like to compare against one another we enter them into the function as follows:\n\n# NB: Note that the `mosquito` data are a matrix, NOT a data.frame\nprop.test(mosquito)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.05882\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.253309811  0.003309811\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n\nDo mosquito’s bite Jack and Jill at different proportions?\n\n\n\n\n\n\nDo this now!\n\n\n\nTask F.4. Divide the class into two groups, Group A and Group B. In each group, collect data on 100 coin tosses. The intention is to compare the coin tosses across Groups A and B. State your hypothesis. Test it. Discuss. Report back in Task F.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#one-sided-and-two-sided-tests",
    "href": "BCB744/basic_stats/07-t_tests.html#one-sided-and-two-sided-tests",
    "title": "7. t-Tests",
    "section": "One-sided and two-sided tests",
    "text": "One-sided and two-sided tests\nAs with all other tests that compare values, proportion tests may be specified as either one or two-sided. Just to be clear, the default setting for prop.test(), like everything else, is a two-sided test. See code below to confirm that the results are identical with or without the added argument:\n\n# Default\nprop.test(mosquito)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.05882\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.253309811  0.003309811\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n# Explicitly state two-sided test\nprop.test(mosquito, alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.05882\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.253309811  0.003309811\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n\nShould we want to specify only one of the tails to be considered, we do so precisely the same as with t-tests. Below are examples of what this code would look like:\n\n# Jack is bit less than Jill\nprop.test(mosquito, alternative = \"less\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.02941\nalternative hypothesis: less\n95 percent confidence interval:\n -1.00000000 -0.01597923\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n# Jack is bit more than Jill\nprop.test(mosquito, alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  mosquito\nX-squared = 3.5704, df = 1, p-value = 0.9706\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.2340208  1.0000000\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.7083333 \n\n\nDo these results differ from the two-sided test? What is different?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#loading-data",
    "href": "BCB744/basic_stats/07-t_tests.html#loading-data",
    "title": "7. t-Tests",
    "section": "Loading data",
    "text": "Loading data\nBefore we can run any analyses we will need to load our data. We are also going to convert these data from their wide format into a long format because this is more useful for the rest of our workflow.\n\necklonia &lt;- read_csv(\"../../data/ecklonia.csv\") %&gt;% \n  gather(key = \"variable\", value = \"value\", -species, -site, -ID)",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#visualising-data",
    "href": "BCB744/basic_stats/07-t_tests.html#visualising-data",
    "title": "7. t-Tests",
    "section": "Visualising data",
    "text": "Visualising data\nWith our data loaded, let’s visualise them in order to ensure that these are indeed the data we are after (Figure 2). Visualising the data will also help us to formulate a hypothesis.\n\nggplot(data = ecklonia, aes(x = variable, y = value, fill = site)) +\n  geom_boxplot(colour = \"black\", fill = \"dodgerblue4\", alpha = 0.4) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\nFigure 2: Boxplots showing differences in morphometric properties of the kelp Ecklonia maxima at two sites in False Bay.\n\n\n\n\nThe first thing we should notice from the figure above is that our different measurements are on very different scales. This makes comparing all of our data visually rather challenging. Even given this complication, one should readily be able to make out that the measurement values at Batsata Rock appear to be greater than at Boulders Beach. Within the framework of the scientific process, that is what we would call an ‘observation’, and is the first step towards formulating a hypothesis. The next step is to refine our observation into a hypothesis. By what measurement are the kelps greater at one site than the other?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#formulating-a-hypothesis",
    "href": "BCB744/basic_stats/07-t_tests.html#formulating-a-hypothesis",
    "title": "7. t-Tests",
    "section": "Formulating a hypothesis",
    "text": "Formulating a hypothesis\nLooking at the figure above it appears that for almost all measurements of length, Batsata Rock far exceeds that of Boulders Beach however, the stipe masses between the two sites appear to be more similar. Let’s pull out just this variable and create a new boxplot (Figure 3).\n\n# filter the data\necklonia_sub &lt;- ecklonia %&gt;% \n  filter(variable == \"stipe_mass\")\n\n# then create a new figure\nggplot(data = ecklonia_sub, aes(x = variable, y = value, fill = site)) +\n  geom_boxplot(colour = \"black\", alpha = 0.4) +\n  coord_flip() +\n  labs(y = \"Stipe mass (kg)\", x = \"\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 3: Boxplots showing the difference in stipe mass (kg) of the kelp Ecklonia maxima at two sites in False Bay.\n\n\n\n\nNow we have a more interesting comparison at hand. The question I think of when I look at these data is “Are the stipe masses at Batsata Rock greater than at Boulders Beach?”. The hypothesis necessary to answer this question would look like this:\n\n\nH0: Stipe mass at Batsata Rock is not greater than at Boulders Beach.\n\nHa: Stipe mass at Batsata Rock is greater than at Boulders Beach.\n\nOr more formally:\n\n\\(H_{0}: \\bar{A} \\leq \\bar{B}\\)\n\n\\(H_{a}: \\bar{A} &gt; \\bar{B}\\).\n\nWhich test must we use for this hypothesis?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#choosing-a-test",
    "href": "BCB744/basic_stats/07-t_tests.html#choosing-a-test",
    "title": "7. t-Tests",
    "section": "Choosing a test",
    "text": "Choosing a test\nBefore we can pick the correct statistical test for our hypothesis, we need to be clear on what it is we are asking. Starting with the data being used is usually a good first step. As we may see in the above figure, we have two sample sets that we are comparing. Therefore, unsurprisingly, we will likely be using a t-test. But we’re not done yet. How is it that we are comparing these two sample sets? Remember from the examples above that there are multiple different ways to compare two sets of data. For our hypothesis we want to see if the stipe mass at Batsata Rock is greater than the stipe mass at Boulders Beach, not just that they are different. Because of this we will need a one-sided t-test. But wait, there’s more! We’ve zeroed in on which sort of test would be appropriate for our hypothesis, but before we run it we need to check our assumptions.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#checking-assumptions",
    "href": "BCB744/basic_stats/07-t_tests.html#checking-assumptions",
    "title": "7. t-Tests",
    "section": "Checking assumptions",
    "text": "Checking assumptions\nIn case we forgot, here are the assumptions for a t-test:\n\nthe dependent variable must be continuous,\nthe observations in the groups being compared are independent of each other,\nthe data are normally distributed, and\nthat the data are homoscedastic, and in particular, that there are no outliers.\n\nWe know that the first two assumptions are met because our data are measurements of mass at two different sites. Before we can run our one-sided t-test we must meet the last two assumptions. Lucky us, we have a function tat will do that automagically.\nPlease refer to Chapter 6 to see what to do if the assumptions fail.\n\necklonia_sub %&gt;% \n  group_by(site) %&gt;% \n  summarise(stipe_mass_var = two_assum(value)[1],\n            stipe_mass_norm = two_assum(value)[2])\n\n# A tibble: 2 × 3\n  site           stipe_mass_var stipe_mass_norm\n  &lt;chr&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\n1 Batsata Rock             2.00           0.813\n2 Boulders Beach           2.64           0.527\n\n\nLovely. The variances are equal and the data are normal. On to the next step.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#running-an-analysis",
    "href": "BCB744/basic_stats/07-t_tests.html#running-an-analysis",
    "title": "7. t-Tests",
    "section": "Running an analysis",
    "text": "Running an analysis\nWith our assumptions checked, we may now analyse our data. We’ll see below how to do this with both of the functions we’ve learned in this chapter for comparing means of two sample sets.\n\nt.test(value ~ site, data = ecklonia_sub,\n       var.equal = TRUE, alternative = \"greater\")\n\n\n    Two Sample t-test\n\ndata:  value by site\nt = 1.8741, df = 24, p-value = 0.03657\nalternative hypothesis: true difference in means between group Batsata Rock and group Boulders Beach is greater than 0\n95 percent confidence interval:\n 0.09752735        Inf\nsample estimates:\n  mean in group Batsata Rock mean in group Boulders Beach \n                    6.116154                     4.996154",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#interpreting-the-results",
    "href": "BCB744/basic_stats/07-t_tests.html#interpreting-the-results",
    "title": "7. t-Tests",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nWe may reject the null hypothesis that the stipe mass of kelps at Batsata Rock is not greater than at Boulders Beach if our t-test returns a p-value \\(\\leq\\) 0.05. We must also pay attention to some of the other results from our t-test, specifically the t-value (t) and the degrees of freedom (df) as these are also needed when we are writing up our results. From all of the information above, we may accept the alternative hypothesis. But how do we write that up?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#drawing-conclusions",
    "href": "BCB744/basic_stats/07-t_tests.html#drawing-conclusions",
    "title": "7. t-Tests",
    "section": "Drawing conclusions",
    "text": "Drawing conclusions\nThere are many ways to present ones findings. Style, without too much flourish, is encouraged as long as certain necessary pieces of information are provided. The sentence below is a very minimalist example of how one may conclude this mini research project. A more thorough explanation would be desirable.\n\nThe stipe mass (kg) of the kelp Ecklonia maxima was found to be significantly greater at Batsata Rock than at Boulders Beach (p = 0.03, t = 1.87, df = 24).",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/basic_stats/07-t_tests.html#going-further",
    "href": "BCB744/basic_stats/07-t_tests.html#going-further",
    "title": "7. t-Tests",
    "section": "Going further",
    "text": "Going further\nBut why though? As is often the case in life, and science is no exception, answers to our questions just create even more questions! Why would the mass of kelp stipes at one locations in the same body of water and only a kilometre or so apart be significantly different? It looks like we are going to need to design a new experiment… Masters thesis anyone?",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "BCB744/intro_r/18-dates.html",
    "href": "BCB744/intro_r/18-dates.html",
    "title": "18. Dates",
    "section": "",
    "text": "This script covers some of the more common issues we may face while dealing with dates.\n\n\nDates\n\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(zoo)\n\n# Load data\nsad_dates &lt;- read.csv(\"../../data/sad_dates.csv\")\n\nDate details\nLook at strip time format for guidance\n\n?strptime\n\nCheck the local time zone\n\nSys.timezone(location = TRUE)\n\nR&gt; [1] \"Africa/Johannesburg\"\n\n\nCreating daily dates\nCreate date columns out of the mangled date data we have loaded.\n\n# Create good date column\nnew_dates &lt;- sad_dates %&gt;%\n  mutate(new_good = as.Date(good))\n\n# Correct bad date column\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_bad = as.Date(bad, format = \"%m/%d/%y\"))\n\n# Correct ugly date column\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_ugly = seq(as.Date(\"1998-01-13\"), as.Date(\"1998-01-21\"), by = \"day\"))\n\nCreating hourly dates\nIf we want to create date values out of data that have hourly values (or smaller), we must create ‘POSIXct’ valus because ‘Date’ values may not have a finer temporal resolution than one day.\n\n# Correcting good time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_good_hours = as.POSIXct(good_hours, tz = \"Africa/Mbabane\"))\n\n# Correcting bad time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_bad_hours = as.POSIXct(bad_hours, format = \"%Y-%m-%d %I:%M:%S %p\", tz = \"Africa/Mbabane\"))\n\n# Correcting bad time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_ugly_hours = seq(as.POSIXct(\"1998-01-13 09:00:00\", tz = \"Africa/Mbabane\"),\n                              as.POSIXct(\"1998-01-13 17:00:00\", tz = \"Africa/Mbabane\"), by = \"hour\"))\n\nBut shouldn’t there be a function that loads dates correctly?\nImporting dates in one step\nWhy yes, yes there is. read_csv() is the way to go.\n\nsmart_dates &lt;- read_csv(\"../../data/sad_dates.csv\")\n\nBut why does it matter that we correct the values to dates? For starters, it affects the way our plots look/work. Let’s create some random numbers for plotting and see how these compare against our date values when we create figures.\n\n# Generate random number\nsmart_dates$numbers &lt;- rnorm(9, 2, 10)\n\n# Scatterplot with correct dates\nggplot(smart_dates, aes(x = good, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\n\n\n# Scatterplot with incorrect dates\nggplot(smart_dates, aes(x = bad, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\n\n\n# OR\nggplot(smart_dates, aes(x = ugly, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\n\n\n\nIf the dates are formatted correctly it also allows us to do schnazy things with the data.\n\nsmart_dates$good[4]+32\n\nR&gt; [1] \"1998-02-17\"\n\nsmart_dates$good[9]-smart_dates$good[3]\n\nR&gt; Time difference of 6 days\n\nas.Date(smart_dates$good[9]:smart_dates$good[3])\n\nR&gt; [1] \"1998-01-21\" \"1998-01-20\" \"1998-01-19\" \"1998-01-18\" \"1998-01-17\"\nR&gt; [6] \"1998-01-16\" \"1998-01-15\"\n\nsmart_dates$good[9]-10247\n\nR&gt; [1] \"1970-01-01\"\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {18. {Dates}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/18-dates.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 18. Dates. http://tangledbank.netlify.app/BCB744/intro_r/18-dates.html."
  },
  {
    "objectID": "BCB744/intro_r/16-functions.html",
    "href": "BCB744/intro_r/16-functions.html",
    "title": "16. Functions by Chapter",
    "section": "",
    "text": "…incomplete…\n\n\nThere are several operators you can use to help build expressions as shown in Table \\(\\ref{tab:operators}\\).\n\n\n\nSome example functions covered so far are presented in Table \\(\\ref{tab:functions}\\).\nSome summary functions are presented in Table \\(\\ref{tab:summaries}\\)."
  },
  {
    "objectID": "BCB744/intro_r/16-functions.html#useful-information",
    "href": "BCB744/intro_r/16-functions.html#useful-information",
    "title": "16. Functions by Chapter",
    "section": "",
    "text": "…incomplete…\n\n\nThere are several operators you can use to help build expressions as shown in Table \\(\\ref{tab:operators}\\).\n\n\n\nSome example functions covered so far are presented in Table \\(\\ref{tab:functions}\\).\nSome summary functions are presented in Table \\(\\ref{tab:summaries}\\)."
  },
  {
    "objectID": "BCB744/intro_r/07-brewing.html",
    "href": "BCB744/intro_r/07-brewing.html",
    "title": "7. Brewing Colours",
    "section": "",
    "text": "“Microbiology and meteorology now explain what only a few centuries ago was considered sufficient cause to burn women to death.”\n— Carl Sagan\n\n\n“Knowledge is not a resource we simply stumble upon. It’s not something that we pluck out of the air. Knowledge is created. It is coaxed into existence by thoughtful, creative people. It is not a free good. It comes only to the prepared mind.”\n— Frank H. T. Rhodes\n\nNow that you have seen the basics of ggplot2, let’s take a moment to delve further into the beauty of our figures. It may sound vain at first, but the colour palette of a figure is actually very important. This is for two main reasons. The first being that a consistent colour palette looks more professional. But most importantly it is necessary to have a good colour palette because it makes the information in our figures easier to understand. The communication of information to others is central to good science.\nR Data\nBefore you get going on our figures, you first need to learn more about the built in data that R has. The base R program already comes with heaps of example dataframes that you may use for practice. You don’t need to load our own data. Additionally, whenever you install a new package (and by now you’ve already installed dozens) it usually comes with several new dataframes. There are many ways to look at the data that you have available from your packages. Below I’ll show two of the many options.\n\n# To create a list of ALL available data\n  # Not really recommended as the output is overwhelming\ndata(package = .packages(all.available = TRUE))\n\n# To look for datasets within a single known package\n  # type the name of the package followed by '::'\n  # This tells R you want to look in the specified package\n  # When the autocomplete bubble comes up you may scroll\n  # through it with the up and down arrows\n  # Look for objects that have a mini spreadsheet icon\n  # These are the datasets\n\n# Try typing the following code and see what happens...\ndatasets::\n\nYou have an amazing amount of data available to you. So the challenge is not to find a dataframe that works for you, but to just decide on one. My preferred method is to read the short descriptions of the dataframes and pick the one that sounds the funniest. But please use whatever method makes the most sense to you. One note of caution, in R there are generally two different forms of data: wide OR long. You will see in detail what this means on Day 4, and what to do about it. For now you need to know that ggplot2 works much better with long data. To look at a dataframe of interest, you use the same method you would use to look up a help file for a function.\nOver the years I’ve installed so many packages on my computer that it is difficult to chose a dataframe. The package boot has some particularly interesting dataframes with a biological focus. Please install this now to access to these data. I have decided to load the urine dataframe here. Note that library(boot) will not work on your computer if you have not installed the package yet. With these data you will now make a scatterplot with two of the variables, while changing the colour of the dots with a third variable.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(boot)\n\n# Load data\nurine &lt;- boot::urine\n\n# Look at help file for more info\n# ?urine\n\n# Create a quick scatterplot\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond))\n\n\n\n\n\n\n\nAnd now you have a scatterplot that is showing the relationship between the osmolarity and pH of urine, with the conductivity of those urine samples shown in shades of blue. What is important to note here is that the colour scale is continuous. How can we now this by looking at the figure? Let’s look at the same figure but use a discrete variable for colouring.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r)))\n\n\n\n\n\n\n\nWhat is the first thing you notice about the difference in the colours? Why did you use as.factor() for the colour aesthetic for our points? What happens if you don’t use this? Try it now.\nRColorBrewer\nCentral to the purpose of ggplot2 is the creation of beautiful figures. For this reason there are many built in functions that you may use in order to have precise control over the colours, as well as additional packages that extend your options even further. The RColorBrewer package should have been installed on your computer and activated automatically when you installed and activated the tidyverse. You will use this package for its lovely colour palettes. Let’s spruce up the previous continuous colour scale figure now.\n\n# The continuous colour scale figure\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond)) +\n  scale_colour_distiller() # Change the continuous variable colour palette\n\n\n\n\n\n\n\nDoes this look different? If so, how? The second page of the colour cheat sheet we included in the course material shows some different colour brewer palettes. Let’s look at how to use those here.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond)) +\n  scale_colour_distiller(palette = \"Spectral\")\n\n\n\n\n\n\n\nDoes that help you to see a pattern in the data? What do you see? Does it look like there are any significant relationships here? How would you test that?\nIf you want to use colour brewer with a discrete variable, you use a slightly different function.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r))) +\n  scale_colour_brewer() # This is the different function\n\n\n\n\n\n\n\nThe default colour scale here is not helpful at all. So let’s pick a better one. If you look at our cheat sheet you will see a list of different continuous and discrete colour scales. All you need to do is copy and paste one of these names into your colour brewer function with inverted commas.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r))) +\n  scale_colour_brewer(palette = \"Set1\") # Here I used \"Set1\", but use what you like\n\n\n\n\n\n\n\nMake your own palettes\nThis is all well and good. But didn’t I claim that this should give you complete control over our colours? So far it looks like it has just given you a few more palettes to use. And that’s nice, but it’s not ‘infinite choices’. That is where the Internet comes to your rescue. There are many places you may go to for support in this regard. The following links, in descending order, are very useful. And fun!\n\nhttp://tristen.ca/hcl-picker/#/hlc/6/0.95/48B4B6/345363\nhttp://tools.medialab.sciences-po.fr/iwanthue/index.php\nhttp://jsfiddle.net/d6wXV/6/embedded/result/\n\nI find the first link the easiest to use. But the second and third links are better at generating discrete colour palettes. Take several minutes playing with the different websites and decide for yourself which one(s) you like.\nUse your own palettes\nNow that you’ve had some time to play around with the colour generators let’s look at how to use them with our figures. I’ve used the first web link to create a list of five colours. I then copy and pasted them into the code below, separating them with commas and placing them inside of c() and inverted commas. Be certain that you insert commas and inverted commas as necessary or you will get errors. Note also that you are using a new function to use our custom palette.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond)) +\n  scale_colour_gradientn(colours = c(\"#A5A94D\", \"#6FB16F\", \"#45B19B\",\n                                    \"#59A9BE\", \"#9699C4\", \"#CA86AD\"))\n\n\n\n\n\n\n\nTo use your custom colour palettes with a discrete colour scale, you use a different function as seen in the code below. While you are at it, also see how to correct the title of the legend and its text labels. Sometimes the default output is not what you want for our final figure, especially if you are going to be publishing it. Also note in the following code chunk that rather than using hexadecimal character strings to represent colours in your custom palette, you are simply writing in the human name for the colours you want. This will work for the continuous colour palettes above, too.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r))) +\n  scale_colour_manual(values = c(\"pink\", \"maroon\"), # How to use custom palette\n                     labels = c(\"no\", \"yes\")) + # How to change the legend text\n  labs(colour = \"crystals\") # How to change the legend title\n\n\n\n\n\n\n\nSo now you have seen how to control the colours palettes in your figures. I know it is a bit much. Four new functions just to change some colours! That’s a bummer. Don’t forget that one of the main benefits of R is that all of your code is written down, annotated and saved. You don’t need to remember which button to click to change the colours, you just need to remember where you saved the code that you will need. And that’s pretty great in my opinion.\n\n\n\n\n\n\nDo this now\n\n\n\nToday we learned the basics of ggplot2, how to facet, how to brew colours, and how to plot some basic summary statistics. Sjog, that’s a lot of stuff to remember… which is why we will now spend the rest of Day 3 putting our new found skills to use.\nPlease group up as you see fit to produce your very own ggplot2 figures. We’ve not yet learned how to manipulate/tidy up our data so it may be challenging to grab any ol’ dataset and make a plan with it. But try! Explore some of the other built-in datasets and find two or three you like. Or use your own data!\nThe goal by the end of today is to have created four figures and join them together via faceting and the options offered by ggarrange(). We will be walking the room to help with any issues that may arise.\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {7. {Brewing} {Colours}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/07-brewing.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 7. Brewing Colours. http://tangledbank.netlify.app/BCB744/intro_r/07-brewing.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "7. Brewing Colours"
    ]
  },
  {
    "objectID": "BCB744/intro_r/15-recap.html",
    "href": "BCB744/intro_r/15-recap.html",
    "title": "15. Recap",
    "section": "",
    "text": "“Everyone should have their mind blown once a day.”\n— Neil deGrasse Tyson\n\n\n“Somewhere, something incredible is waiting to be known.”\n— Carl Sagan\n\nOver the past four days we have covered quite a bit of ground. By now it is our hope that after having participated in this workshop you will feel confident enough using R to branch out on your own and begin applying what you have learned to your own research.\nAbove all, remember the tidy principles you have leaned here and endeavour to apply them to all facets of your work. The more uniformly tidy your work becomes, the more compounding benefits you will begin to notice.\nThe future\nThe content we have covered in this workshop is only the beginning. We have looked down upon the tidyverse, it’s multitudinous spiralling arms stretching out away from us in all directions. The next step is to begin to investigate the specific branches of the R tree of knowledge that interest us most. Or are most relevant to our work. The following list contains some further suggestions for workshops that are available:\n\nR for biologists\nR for environmental science\nR for oceanographers\nAdvanced visualisations\nMultivariate analysis\nSpecies distribution modelling\nReproducible research\nBasic stats\n\nFor further information or inquiries about additional training please contact Robert Schlegel: robwschlegel@gmail.com .\nToday\nFor the rest of today we will now open the floor to questions and suggestions that we may work through as a group.\nSession info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\nR&gt; character(0)\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {15. {Recap}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/15-recap.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 15. Recap. http://tangledbank.netlify.app/BCB744/intro_r/15-recap.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "15. Recap"
    ]
  },
  {
    "objectID": "BCB744/intro_r/11-mapping_quakes.html",
    "href": "BCB744/intro_r/11-mapping_quakes.html",
    "title": "11. The Fiji Earthquake Data",
    "section": "",
    "text": "Here I will plot the built-in earthquake dataset (datasets::quakes).\n\n\n\n\n\n\nGlobal earthquake distribution\n\n\n\nFor a global map of earthquakes, see my plot of the Kaggle earthquake data.\n\n\nTwo new concepts will be introduced in the Chapter:\n\nGeographic Coordinate Systems\nProjected Coordinate Systems\n\nThese are specified to the mapping / plotting functions via the Coordinate Reference System (CRS) through functionality built into the sf package.\nYou will also learn how to deal with polygons that cross the dateline (0° wrapping back onto 360°) or the anti-meridian (-180° folding back onto +180°).\nLoad packages and data\nI will use the Natural Earth data and manipulate it with the sf package functions. This package integrates well with the tidyverse.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthhires)\n\nLoad the Natural Earth map\nThe rnaturalearth package has the ne_counties() function which we use to load borders of all the countries in the world. I load the medium resolution dataset and make sure the data are of class sf, i.e. a simple features collection.\n\nsf_use_s2(FALSE)\n\nworld &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3)  \nhead(world[c('continent')])\n\nR&gt; head(world[c('continent')])\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.4537 ymin: -55.9185 xmax: 140.9776 ymax: 7.35578\nCRS:           +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n      continent                       geometry\n0          Asia MULTIPOLYGON (((117.7036 4....\n1          Asia MULTIPOLYGON (((117.7036 4....\n2 South America MULTIPOLYGON (((-69.51009 -...\n3 South America MULTIPOLYGON (((-69.51009 -...\n4 South America MULTIPOLYGON (((-69.51009 -...\n5 South America MULTIPOLYGON (((-67.28475 -...\nNote that for the Natural Earth data the coordinate reference system (CRS) is in the dataset are longitude / latitude coordinates in degrees in a CRS called WGS84. This is the World Geodetic System 1984 commonly used in most GPS devices. The default unit of this CRS is in degrees longitude and latitude.\nFor more information on CRS, see:\n\nThe PROJ system. The proj-string specified with every map can be used in sf; it can be retrieved using st_crs() and one can transform between various projections uing st_transform(). PROJ is written in C++ and loaded automagically with sf.\nThe EPSG coordinate codes, which provide a convenient shortcut to the longer proj-string. Navigating the a page for a projection—WGS84 known as EPSG:4326—gives one the various relevant details, and the proj-string can be located in the PROJ.4 tab under Exports.\n\nMore information about the map data is available with the head(world) function (as seen above), namely that the longitude goes from -180° (180° west of the prime meridian) to +180° (180° east of the prime meridian). This means that the anti-meridian cuts some of the polygons in the Pacific along the line where -180° wraps back onto +180°, and this can be problematic for maps of the Pacific. We will get to this later. The latitude goes from -90° (90° south of the equator) to +90° (90° north of the equator).\nA very quick map looks like this:\n\nggplot() +\n  geom_sf(data = world, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nYou can see above that Africa is centrally positioned. However, I want to focus on the western Pacific region. I am also going to apply a new projection (ESRI:53077, the Natural Earth projection) to it. The ‘rotation’ is accomplished by setting lon_0=170 in the proj-string.\n\nNE_proj &lt;- \"+proj=natearth +lon_0=170 \"\n\nworld_0 &lt;- world |&gt; \n  st_transform(NE_proj)\n\nggplot() +\n  geom_sf(data = world_0, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nThe western Pacific is now focal, but the map looks strange to say the least. This is due to the break at the anti-meridian which causes the polygons to join up in odd and unexpected ways when the central longitude in the map is not displayed at exactly 0° (in my reprojection I made the focus on 170°E and it became the center). I can fix it using st_break_antimeridian() but to do so I must start with unprojected data, and only then apply this function.\n\nworld_1 &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3) |&gt; \n  st_break_antimeridian(lon_0 = 170) |&gt; \n  st_transform(NE_proj)\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nZooming in\nThere are three options for focusing in on a particular area of the map (zooming in):\n\nselecting only certain areas of interest from the spatial dataset (e.g. only certain countries / continent(s) / etc.);\ncropping the geometries in the spatial dataset using sf_crop(); and\nrestricting the display window via coord_sf().\n\nI will look at each in some detail.\nSelecting areas of interest\nI am interested only in the ‘continent’ called Oceania, which includes the Pacific Islands, Australia, and New Zealand. More correctly, it a geographical region and not a continent. It is comprised of Australasia, Melanesia, Micronesia, and Polynesia which span the the Eastern and Western Hemispheres.\n\noceania &lt;- world_1[world_1$continent == 'Oceania',]\nggplot() +\n  geom_sf(data = oceania, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nZooming in on only the group of nations included with Oceania reveals another problem. This is, only part of New Guinea is displayed: Papua New Guinea appears on the map but the western side of New Guinea, called Western New Guinea, is absent. This is because Papua New Guinea is part of Micronesia whereas West New Guinea is part of Indonesia (part of the South-eastern Asia region).\nThere is no easy way to select the countries that constitute Australasia, Melanesia, Micronesia, Indonesia, and Polynesia—unless I create an exhaustive list of these small island nations. But I can use the countrycode package to insert an attribute with the geographic regional classification of the United Nations in the world_1 map. Now all the constituent countries belonging to these regions will be correctly classified to the UN regional classification scheme. Note that I also collapse the countries into their continents by merging all polygons belonging to the same continent (the group_by() and summarise() functions)—I do this because I don’t want to display individual countries.\n\nlibrary(countrycode)\n\nworld_1$region &lt;- countrycode(world_1$iso_a3, origin = \"iso3c\",\n  destination = \"un.regionsub.name\")\n\nsw_pacific &lt;- world_1 |&gt; \n  filter(region %in% c(\"Australia and New Zealand\", \"Melanesia\", \"Micronesia\",\n    \"Indonesia\", \"Polynesia\", \"South-eastern Asia\")) |&gt; \n  group_by(continent) |&gt;\n  summarise()\n\nggplot() +\n  geom_sf(data = sw_pacific, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nAs we can see above, by including the South-eastern Asia region I complete the mass of land that is New Guinea.\nCropping\nThe above map is good but not perfect. I have in mind zooming in a bit more into the region around Fiji where the earthquake monitoring network is located. One way to do this is to crop the extent of the study region using a bounding box whose boundaries are defined by the extent of the quakes datapoints.\nTo start, I use the earthquake data and extract from there the study domain and increase the edges all round by a given margin—this is so that the stations are not plotted right on the maps’ edges.\nImportant! The coordinates in the quakes data are provided in WGS84, so I need to first specify them as such and then transform them to the same coordinate system used by the map.\n\nquakes &lt;- as_tibble(datasets::quakes)\nmargin &lt;- 15.0\nxmin &lt;- min(quakes$long) - margin; xmax &lt;- max(quakes$long) + margin\nymin &lt;- min(quakes$lat) - margin; ymax &lt;- max(quakes$lat) + margin\n\nWGS84_proj &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\nbbox &lt;- st_sfc(st_point(c(xmin, ymin)), st_point(c(xmax, ymax)),\n                         crs = WGS84_proj)\nbbox_trans &lt;- st_transform(bbox, NE_proj)\n\nsw_pacific_cropped &lt;- sw_pacific |&gt; \n  st_crop(bbox_trans)\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nThat looks decent enough, but there’s another way to accomplish the same.\nSetting the mapping limits in ggplot2\n\nThe third approach to get closer to the region of interest is to use the full map extent (the world) loaded at the beginning, but to set the limits of the view window within the coord_sf() function in ggplot().\nTo do this, I start with the bbox_sf_trans object, which was obtained by first specifying the coordinates marking the extent of the map in the WGS84 coordinate system and then transforming them to Natural Earth. We can extract the transformed limits with st_coordinates() and supply them to the map.\n\nbbox_trans_coord &lt;- st_coordinates(bbox_trans)\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\") +\n  coord_sf(xlim = bbox_trans_coord[,'X'], ylim = bbox_trans_coord[,'Y'],\n    expand = FALSE)\n\n\n\n\n\n\n\nGreat! This works well. Note that the coordinates on the graticule are in degrees longitude and latitude, the default for WGS84. By setting datum = NE_proj ensures the graticule is displayed in Natural Earth coordinate system units, which is meters. This might look strange at first, but it is not wrong.\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\") +\n  coord_sf(xlim = bbox_trans_coord[,'X'], ylim = bbox_trans_coord[,'Y'],\n    expand = FALSE, datum = NE_proj)\n\n\n\n\n\n\n\nAdding the quakes data as points\nIn order to plot the quakes data, I need to create a sf object from the quakes data. When converting the dataframe to class sf, I must also assign to it the CRS associated of the original dataset. This would be WGS84. Afterwards I will transform it to the Natural Earth CRS.\n\nquakes_sf &lt;- quakes |&gt; \n  st_as_sf(coords = c(\"long\", \"lat\"),\n    crs = WGS84_proj)\nquakes_sf_trans &lt;- st_transform(quakes_sf, NE_proj)\nhead(quakes_sf_trans)\n\nR&gt; head(quakes_sf_trans)\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1047820 ymin: -2923232 xmax: 1361900 ymax: -2017755\nCRS:           +proj=natearth +lon_0=170 \n# A tibble: 6 × 4\n  depth   mag stations           geometry\n  &lt;int&gt; &lt;dbl&gt;    &lt;int&gt;        &lt;POINT [m]&gt;\n1   562   4.8       41 (1104307 -2293735)\n2   650   4.2       15 (1047820 -2316276)\n3    42   5.4       43 (1323082 -2923232)\n4   626   4.1       19 (1113132 -2017755)\n5   649   4         11 (1136619 -2293735)\n6   195   4         12 (1361900 -2210349)\nI am going to make a map of the Fiji region and plot the spatial location of the earthquakes, and scale the points indicating the earthquakes’ magnitude by their intensity (mag).\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"black\", fill = \"grey70\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = mag, size = mag),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.4) +\n  scale_colour_continuous(type = \"viridis\") +\n  guides(size = \"none\") +\n  coord_sf(expand = FALSE) +\n  labs(x = NULL, y = NULL,\n    title = \"The Fiji Earthquake Data\",\n    subtitle = \"Natural Earth\")\n\n\n\n\n\n\n\nNow I apply a more appropriate CRS to the map. The WGS 84 / NIWA Albers projection (EPSG:9191) is suitable for the southwestern Pacific Ocean and Southern Ocean areas surrounding New Zealand.\n\nNIWA_Albers_proj &lt;- \"+proj=aea +lat_0=-22 +lon_0=175 +lat_1=-20 +lat_2=-30 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs\"\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"indianred\", fill = \"beige\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = mag, size = mag),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.6) +\n  scale_colour_continuous(type = \"viridis\") +\n  guides(size = \"none\",\n    colour = guide_colourbar(\"Magnitude\")) +\n  coord_sf(expand = FALSE,\n    crs = NIWA_Albers_proj) +\n  labs(x = NULL, y = NULL,\n    title = \"The Fiji Earthquake Data\",\n    subtitle = \"WGS 84 / NIWA Albers\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {11. {The} {Fiji} {Earthquake} {Data}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/11-mapping_quakes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 11. The Fiji Earthquake Data. http://tangledbank.netlify.app/BCB744/intro_r/11-mapping_quakes.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "11. The Fiji Earthquake Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#preparing-data-for-r",
    "href": "BCB744/intro_r/04-workflow.html#preparing-data-for-r",
    "title": "4. R Workflows",
    "section": "Preparing data for R",
    "text": "Preparing data for R\nImporting data can actually take longer than the statistical analysis itself! In order to avoid as much frustration as possible it is important to remember that for R to be able to analyse your data they need to be in a consistent format, with each variable in a column and each sample in a row. The format within each variable (column) needs to be consistent and is commonly one of the following types: a continuous numeric variable (e.g., fish length (m): 0.133, 0.145); a factor or categorical variable (e.g., Month: Jan, Feb or 1, 2, …, 12); a nominal variable (e.g., algal colour: red, green, brown); or a logical variable (i.e., TRUE or FALSE). You can also use other more specific formats such as dates and times, and more general text formats.\nYou will learn more about working with data in R — specifically, you will teach you about the tidyverse principles and the distinction between long and wide format data in more detail on Day 4. For most of our work in R you require our data to be in the long format, but Excel users (poor things!) are more familiar with data stored in the wide format. For now let’s bring some data into R and not worry too much about the data being tidy.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#converting-data",
    "href": "BCB744/intro_r/04-workflow.html#converting-data",
    "title": "4. R Workflows",
    "section": "Converting data",
    "text": "Converting data\nBefore you can read in the Laminaria dataset provided for the following exercises, you need to convert the Excel file supplied into a .csv file. Open ‘laminaria.xlsx’ in Excel, then select ‘Save As’ from the File menu. In the ‘Format’ drop-down menu, select the option called ‘Comma Separated Values’, then hit ‘Save’. You’ll get a warning that formatting will be removed and that only one sheet will be exported; simply ‘Continue’. Your working directory should now contain a file called laminaria.csv.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#importing-data",
    "href": "BCB744/intro_r/04-workflow.html#importing-data",
    "title": "4. R Workflows",
    "section": "Importing data",
    "text": "Importing data\nThe easiest way to import data into R is by changing your working directory to be the same as the file path where the file(s) are you want to load. A file path is effectively an address. In most operating systems, if you open the folder where your files are you may click on the navigation bar and it will show you the complete file path. Many people develop the nasty habit of squirrelling away their files within folders within folders within folders within folders… within folders within folders. Please don’t do that.\nThe concept of file paths is either one that you are familiar with, or you’ve never heard of before. There tends to be little middle ground. Happily, RStudio allows us to circumvent this issue. You do this by using the Intro_R_Workshop.Rproj that you may find in the files downloaded for this workshop. If you have not already switched to the Intro_R_Workshop.Rproj as outlined in Chapter 2, click on the project button in the top right corner your RStudio window. Then navigate to where you saved Intro_R_Workshop.Rproj and select it. Notice that your RStudio has changed a bit and all of the objects you may have previously created in your environment have been removed and any tabs in the source editor pane have been closed. That is fine for now, but it may mean you need to re-open the Day_1.R script you just created.\nOnce you have the working directory set, either by doing it manually with setwd() or by loading a project, R will now know where to look for the files you want to read. The function read_csv() is the most convenient way to read in raw data. There are several other ways to read in data, but for the purposes of this workshop we’ll stick to this one, for now. To find out what it does, you will go to its help entry in the usual way (i.e. ?read_csv).\nAll R Help items are in the same format. A short Description (of what it does), Usage, Arguments (the different inputs it requires), Details (of what it does), Value (what it returns) and Examples. Arguments (the parameters that are passed to the function) are the lifeblood of any function, as this is how you provide information to R. You do not need to specify all arguments, as most have appropriate default values for your requirements, and others might not be needed for your particular case.\n\n\n\n\n\n\nData formats\n\n\n\nR has pedantic requirements for naming variables. It is safest to not use spaces, special characters (e.g., commas, semicolons, any of the shift characters above the numbers), or function names (e.g., mean). One can use ‘camelCase’, such as myFirstVariable, or simply separate the ‘parts’ of the variable name using an underscore such as in my_first_variable. Always make sure to use meaningful names; eventually you will learn to find a balance between meaningfulness and something short that’s easy enough to retype repeatedly (although R’s ability to use tab completion helps with not having to type long names to often).\n\n\n\n\n\n\n\n\nImport\n\n\n\nread_csv() is simply a ‘wrapper’ (i.e., a command that modifies) a more basic command called read_delim(), which itself allows you to read in many types of files besides .csv. To find out more, type ?read_delim().",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#loading-a-file",
    "href": "BCB744/intro_r/04-workflow.html#loading-a-file",
    "title": "4. R Workflows",
    "section": "Loading a file",
    "text": "Loading a file\nTo load the laminaria.csv file you created, and assign it to an object name in R, you will use the read_csv() function from the tidyverse package, so let’s make sure it is activated.\n\nlibrary(tidyverse)\n\nDepending on the version of Excel you are using, or perhaps the settings within it, the laminaria.csv file you created may be corrupted in different ways. Generally Excel likes to replace the , between columns in our .csv files with ;. This may seem like a triviality but sadly it is not. Lucky for use, the tidyverse knows about this problem and they have made a plan. Please open your laminaria.csv file and look at which character is being used to separate columns. If it is , then you will load the data with read_csv(). If the columns are separated with ; you will use read_csv2().\n\n# Run this if 'laminaria.csv` has columns separated by ','\nlaminaria &lt;- read_csv(\"../../data/laminaria.csv\")\n# Run this if 'laminaria.csv` has columns separated by ';'\nlaminaria &lt;- read_csv2(\"../../data/laminaria.csv\")\n\nIf one clicks on the newly created laminaria object in the Environment pane it will open a new panel that shows the information as a spreadsheet. To go back to your script click the appropriate tab in the Source Editor pane. With these data loaded you may now perform analyses on them.\nAt any point when working in R, you can see exactly what objects are in memory in several ways. First, you can look at the Environment tab in RStudio, then Workspace Browser. Alternatively you can type either of the following:\n\nls()\n# or\nobjects()\n\nYou can delete an object from memory by specifying the rm() function with the name of the object:\n\nrm(laminaria)\n\nThis will of course delete our variable, so you will import it in again using whichever of the following two lines of code matched our Excel situation.\n\nlaminaria &lt;- read.csv(\"../../data/laminaria.csv\")\n\n\n\n\n\n\n\nManaging variables\n\n\n\nIt is good practice to remove variables from memory that you are not using, especially if they are large.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#examine-your-data",
    "href": "BCB744/intro_r/04-workflow.html#examine-your-data",
    "title": "4. R Workflows",
    "section": "Examine your data",
    "text": "Examine your data\nOnce the data are in R, you need to check there are no glaring errors. It is useful to call up the first few lines of the dataframe using the function head(). Try it yourself by typing:\n\nhead(laminaria)\n\nR&gt; # A tibble: 6 × 12\nR&gt;   region site        Ind blade_weight blade_length blade_thickness stipe_mass\nR&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\nR&gt; 1 WC     Kommetjie     2         1.9           160            2          1.5 \nR&gt; 2 WC     Kommetjie     3         1.5           120            1.4        2.25\nR&gt; 3 WC     Kommetjie     4         0.55          110            1.5        1.15\nR&gt; 4 WC     Kommetjie     5         1             159            1.5        2.6 \nR&gt; 5 WC     Kommetjie     6         2.3           149            2         NA   \nR&gt; 6 WC     Kommetjie     7         1.6           107            1.75       2.9 \nR&gt; # ℹ 5 more variables: stipe_length &lt;dbl&gt;, stipe_diameter &lt;dbl&gt;, digits &lt;dbl&gt;,\nR&gt; #   thallus_mass &lt;dbl&gt;, total_length &lt;dbl&gt;\n\n\nThis lists the first six lines of each of the variables in the dataframe as a table. You can similarly retrieve the last six lines of a dataframe by an identical call to the function tail(). Of course, this works better when you have fewer than 10 or so variables (columns); for larger data sets, things can get a little messy. If you want more or fewer rows in your head or tail, tell R how many rows it is you want by adding this information to your function call. Try typing:\n\nhead(laminaria, n = 3)\n\nR&gt; # A tibble: 3 × 12\nR&gt;   region site        Ind blade_weight blade_length blade_thickness stipe_mass\nR&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\nR&gt; 1 WC     Kommetjie     2         1.9           160             2         1.5 \nR&gt; 2 WC     Kommetjie     3         1.5           120             1.4       2.25\nR&gt; 3 WC     Kommetjie     4         0.55          110             1.5       1.15\nR&gt; # ℹ 5 more variables: stipe_length &lt;dbl&gt;, stipe_diameter &lt;dbl&gt;, digits &lt;dbl&gt;,\nR&gt; #   thallus_mass &lt;dbl&gt;, total_length &lt;dbl&gt;\n\ntail(laminaria, n = 2)\n\nR&gt; # A tibble: 2 × 12\nR&gt;   region site         Ind blade_weight blade_length blade_thickness stipe_mass\nR&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\nR&gt; 1 WC     Rocky Bank    12          2.1          194             1.4       3.75\nR&gt; 2 WC     Rocky Bank    13          1.3          160             1.9       2.45\nR&gt; # ℹ 5 more variables: stipe_length &lt;dbl&gt;, stipe_diameter &lt;dbl&gt;, digits &lt;dbl&gt;,\nR&gt; #   thallus_mass &lt;dbl&gt;, total_length &lt;dbl&gt;\n\n\nYou can also check the structure of your data by using the glimpse() function:\n\nglimpse(laminaria)\n\nR&gt; Rows: 140\nR&gt; Columns: 12\nR&gt; $ region          &lt;chr&gt; \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", …\nR&gt; $ site            &lt;chr&gt; \"Kommetjie\", \"Kommetjie\", \"Kommetjie\", \"Kommetjie\", \"K…\nR&gt; $ Ind             &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 10, 11, 1, 3, 4, 5, 6, 7, 8, 9, 1…\nR&gt; $ blade_weight    &lt;dbl&gt; 1.90, 1.50, 0.55, 1.00, 2.30, 1.60, 0.65, 0.95, 2.30, …\nR&gt; $ blade_length    &lt;dbl&gt; 160, 120, 110, 159, 149, 107, 104, 111, 178, 145, 146,…\nR&gt; $ blade_thickness &lt;dbl&gt; 2.00, 1.40, 1.50, 1.50, 2.00, 1.75, 2.00, 1.25, 2.50, …\nR&gt; $ stipe_mass      &lt;dbl&gt; 1.50, 2.25, 1.15, 2.60, NA, 2.90, 0.75, 1.60, 4.20, 0.…\nR&gt; $ stipe_length    &lt;dbl&gt; 120, 149, 97, 167, 146, 161, 110, 136, 176, 82, 118, 1…\nR&gt; $ stipe_diameter  &lt;dbl&gt; 56.0, 68.5, 69.0, 60.0, 73.0, 63.0, 51.0, 56.0, 76.0, …\nR&gt; $ digits          &lt;dbl&gt; 12, 12, 13, 8, 15, 17, 11, 11, 8, 19, 20, 23, 20, 24, …\nR&gt; $ thallus_mass    &lt;dbl&gt; 3000, 3750, 1700, 3600, 5100, 4500, 1400, 2550, 6500, …\nR&gt; $ total_length    &lt;dbl&gt; 256, 269, 207, 326, 295, 268, 214, 247, 354, 227, 264,…\n\n\nThis very handy function lists the variables in your dataframe by name, tells you what sorts of data are contained in each variable (e.g., continuous number, discrete factor) and provides an indication of the actual contents of each.\nIf you wanted only the names of the variables (columns) in the dataframe, you could use:\n\nnames(laminaria)\n\nR&gt;  [1] \"region\"          \"site\"            \"Ind\"             \"blade_weight\"   \nR&gt;  [5] \"blade_length\"    \"blade_thickness\" \"stipe_mass\"      \"stipe_length\"   \nR&gt;  [9] \"stipe_diameter\"  \"digits\"          \"thallus_mass\"    \"total_length\"\n\n\nAnother option, but by no means the only one remaining, is to install a library called skimr and to use thew skim() function:\n\nlibrary(skimr)\nskim(iris) # using built-in `iris` data\n\n\n\n\n\nName\niris\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: factor\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\nVariable type: numeric\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#tidyverse-sneak-peek",
    "href": "BCB744/intro_r/04-workflow.html#tidyverse-sneak-peek",
    "title": "4. R Workflows",
    "section": "\nTidyverse sneak peek",
    "text": "Tidyverse sneak peek\nBefore you begin to manipulate our data further I need to briefly introduce you to the tidyverse. And no introduction can be complete within learning about the pipe command, %&gt;%. You may type this by pushing the following keys together: ctrl-shift-m. The pipe (%&gt;%, or |&gt; if you selected to use the native pipe operator under ‘Global Options’) allows you to perform calculations sequentially, which helps us to avoid making errors.\n\n\n\n\n\n\nThe pipe operator\n\n\n\nThe pipe operator allows you to take the output of one function and pass it directly as the input to the next function. This creates a more intuitive and readable way to string together a series of data operations. Instead of nesting functions inside one another, which can quickly become confusing and hard to read, the pipe operator lets you lay out your data processing steps sequentially. This makes your code cleaner and easier to understand, as it clearly outlines the workflow from start to finish, almost like a step-by-step recipe for your data analysis.\n\n\nThe pipe works best in tandem with the following common functions:\n\nArrange observations (rows) with arrange()\n\nFilter observations (rows) with filter()\n\nSelect variables (columns) with select()\n\nCreate new variables (columns) with mutate()\n\nSummarise variables (columns) with summarise()\n\nGroup observations (rows) with group_by()\n\n\nYou will cover these functions in more detail on Day 4. For now you will ease ourselves into the code with some simple examples.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#subsetting",
    "href": "BCB744/intro_r/04-workflow.html#subsetting",
    "title": "4. R Workflows",
    "section": "Subsetting",
    "text": "Subsetting\nNow let’s have a look at specific parts of the data. You will likely need to do this in almost every script you write. If you want to refer to a variable, you specify the dataframe then the column name within the select() function. In your script type:\n\nlaminaria %&gt;% # Tell R which dataframe you are using\n  select(site, total_length) # Select only specific columns\n\nIf you want to only select values from specific columns you insert one more line of code.\n\nlaminaria %&gt;% \n  select(site, total_length) %&gt;% # Select specific columns first\n  slice(56:78)\n# what does the '56:78' do? Change some numbers and run the code again. What happens?\n\nIf you wanted to select only the rows of data belonging to the Kommetjie site, you could type:\n\nlaminaria %&gt;%\n  filter(site == \"Kommetjie\")\n\nThe function filter() has two arguments: the first is a dataframe (we specify laminaria in the previous line and the pipe supplies this for us) and the second is an expression that relates to which rows of a particular variable you want to include. Here you include all rows for Kommetjie and you find that in the variable site. It returns a subset that is actually a dataframe itself; it is in the same form as the original dataframe. You could assign that subset of the full dataframe to a new dataframe if you wanted to.\n\nlam_kom &lt;- laminaria %&gt;% \n  filter(site == \"Kommetjie\")",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#basic-stats",
    "href": "BCB744/intro_r/04-workflow.html#basic-stats",
    "title": "4. R Workflows",
    "section": "Basic stats",
    "text": "Basic stats\nStraight out of the box it is possible in R to perform a broad range of statistical calculations on a dataframe. If you wanted to know how many samples you have at Kommetjie, you simply type the following:\n\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(site == \"Kommetjie\") %&gt;% # Filter out only records from Kommetjie\n  nrow() # Count the number of remaining rows\n\nOr, if you want to select only the row with the greatest total length:\n\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(total_length == max(total_length)) # Select row with max total length\n\n\n\n\n\n\n\nDo this now\n\n\n\nUsing pipes, subset the Laminaria data to include regions where the blade thickness is thicker than 5 cm and retain only the columns site, region, blade weight and blade thickness. Now exit RStudio. Pretend it is three days later and revisit your analysis. Calculate the number of entries at Kommetjie and find the row with the greatest length. Do this now.\n\n\nImagine doing this daily as our analysis grows in complexity. It will very soon become quite repetitive if each day you had to retype all these lines of code. And now, six weeks into the research and attendant statistical analysis, you discover that there were some mistakes and some of the raw data were incorrect. Now everything would have to be repeated by retyping it at the command prompt. Or worse still (and bad for repetitive strain injury) doing all of it in SPSS and remembering which buttons to click and then re-clicking them. A pain. Let’s avoid that altogether and do it the right way by writing an R script to automate and annotate all of this.\n\n\n\n\n\n\nDealing with missing data\n\n\n\nThe .csv file format is usually the most robust for reading data into R. Where you have missing data (blanks), the .csv format separates these by commas. However, there can be problems with blanks if you read in a space-delimited format file. If you are having trouble reading in missing data as blanks, try replacing them in your spreadsheet with NA, the missing data code in R. In Excel, highlight the area of the spreadsheet that includes all the cells you need to fill with NA. Do an Edit/Replace… and leave the ‘Find what:’ textbox blank and in the ‘Replace with:’ textbox enter NA, the missing value code. Once imported into R, the NA values will be recognised as missing data.\n\n\nSo far you have calculated the mean and standard deviation of some data in the Laminaria data set. If you have not, please append those lines of code to the end of your script. You can run individual lines of code by highlighting them and pressing ctrl-Enter (cmd-Enter on a Mac). Do this.\nYour file will now look similar to this one, but of course you will have added your own notes and comments as you went along:\n\n# Day_1.R\n# Reads in some data about Laminaria collected along the Cape Peninsula\n# do various data manipulations, analyses and graphs\n# AJ Smit\n# 9 January 2020\n\n# Find the current working directory (it will be correct if a project was\n# created as instructed earlier)\ngetwd()\n\n# If the directory is wrong because you chose not to use an Rworkspace (project),\n# set your directory manually to where the script will be saved and where the data\n# are located\n# setwd(\"&lt;insert_path_here&gt;\")\n\n# Load libraries\nlibrary(tidyverse)\n\n# Load the data\nlaminaria &lt;- read_csv(\"../../data/laminaria.csv\")\n\n# Examine the data\nhead(laminaria, 5) # First five lines\ntail(laminaria, 2) # Last two lines\nglimpse(laminaria) # A more thorough summary\nnames(laminaria) # THe names of the columns\n\n# Subsetting data\nlaminaria %&gt;% # Tell R which dataframe to use\n  select(site, total_length) %&gt;% # Select specific columns\n  slice(56:78) # Select specific rows\n\n# How many data points do you have at Kommetjie?\nlaminaria %&gt;%\n  filter(site == \"Kommetjie\") %&gt;%\n  nrow()\n\n# The row with the greatest length\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(total_length == max(total_length)) # Select row with max total length\n\nMaking sure all the latest edits in your R script have been saved, close your R session. Pretend this is now 2019 and you need to revisit the analysis. Open the file you created in 2017 in RStudio. All you need to do now is highlight the file’s entire contents and hit ctrl-Enter.\n\n\n\n\n\n\nStick with .csv files\n\n\n\nThere are packages in R to read in Excel spreadsheets (e.g., .xlsx), but remember there are likely to be problems reading in formulae, graphs, macros and multiple worksheets. You recommend exporting data deliberately to .csv files (which are also commonly used in other programs). This not only avoids complications, but also allows you to unambiguously identify the data you based your analysis on. This last statement should give you the hint that it is good practice to name your .csv slightly differently each time you export it from Excel, perhaps by appending a reference to the date it was exported.\n\n\n\n\n\n\n\n\nRemember…\n\n\n\nFriends don’t let friends use Excel.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#summary-statistics-by-variable",
    "href": "BCB744/intro_r/04-workflow.html#summary-statistics-by-variable",
    "title": "4. R Workflows",
    "section": "Summary statistics by variable",
    "text": "Summary statistics by variable\nThis is all very convenient, but you may want to ask R specifically for just the mean of a particular variable. In this case, you simply need to tell R which summary statistic you are interested in, and to specify the variable to apply it to using summarise(). Try typing:\n\nlaminaria %&gt;% # Chose the dataframe\n  summarise(avg_bld_wdt = mean(blade_length)) # Calculate mean blade length\n\nOr, if you wanted to know the mean and standard deviation for the total lengths of all the plants across all sites, do:\n\nlaminaria %&gt;% # Tell R that you want to use the 'laminaria' dataframe\n  summarise(avg_stp_ln = mean(total_length), # Create a summary of the mean of the total lengths\n            sd_stp_ln = sd(total_length)) # Create a summary of the sd of the total lengths\n\nOf course, the mean and standard deviation are not the only summary statistic that R can calculate. Try max(), min(), median(), range(), sd() and var(). Do they return the values you expected? Now try:\n\nlaminaria %&gt;% \n  summarise(avg_stp_ms = mean(stipe_mass))\n\nThe answer probably isn’t what you would expect. Why not? Sometimes, you need to tell R how you want it to deal with missing data. In this case, you have NAs in the named variable, and R takes the cautious approach of giving you the answer of NA, meaning that there are missing values here. This may not seem useful, but as the programmer, you can tell R to respond differently, and it will. Simply append an argument to your function call, and you will get a different response. Type:\n\nlaminaria %&gt;% \n  summarise(avg_stp_ms = mean(stipe_mass, na.rm = T))\n\nThe na.rm argument tells R to remove (or more correctly ‘strip’) NAs from the data string before calculating the mean. It now returns the correct answer. Although needing to deal explicitly with missing values in this way can be a bit painful, it does make you more aware of missing data, what the analyses in R are doing, and makes you decide explicitly how you will treat missing data.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/04-workflow.html#more-complex-calculations",
    "href": "BCB744/intro_r/04-workflow.html#more-complex-calculations",
    "title": "4. R Workflows",
    "section": "More complex calculations",
    "text": "More complex calculations\nLet’s say you want to calculate something that is not standard in R, say the standard error of the mean for a variable, rather than just the corresponding standard deviation. How can this be done?\nThe trick is to remember that R is a calculator, so you can use it to do maths, even complex maths (which you won’t do). The formula for standard error is:\n\\[se = \\frac{var}{\\sqrt{n}}\\]\nYou know that the variance is given by var(), so all you need to do is figure out how to get n and calculate a square root. The simplest way to determine the number of elements in a variable is a call to the function nrow(), as you saw previously. You may therefore calculate standard error with one chunk of code, step by step, using the pipe. Furthermore, by using group_by() you may calculate the standard error for all sites in one go.\n\nlaminaria %&gt;% # Select 'laminaria'\n  group_by(site) %&gt;% # Group the dataframe by site\n  summarise(var_bl = var(blade_length), # Calculate variance\n            n_bl = n()) %&gt;%  # Count number of values\n  mutate(se_bl = var_bl / sqrt(n_bl)) # Calculate se\n\nWhen calculating the mean, you specified that R should strip the NAs, using the argument na.rm = TRUE. In the example above, you didn’t have NAs in the variable of interest. What happens if you do?\nUnfortunately, the call to the function nrow() has no arguments telling R how to treat NAs; instead, they are simply treated as elements of the variable and are therefore counted. The easiest way to resolve this problem is to strip out NAs in advance of any calculations. Try typing:\n\nlaminaria %&gt;% \n  select(stipe_mass) %&gt;% \n  summarise(n = n())\n\nthen:\n\nlaminaria %&gt;% \n  select(stipe_mass) %&gt;% \n  na.omit() %&gt;% \n  summarise(n = n())\n\nYou will notice that the function na.omit() removes NAs from the variable that is specified as its argument.\n\n\n\n\n\n\nDo this now\n\n\n\n\nUsing this new information, calculate the mean stipe mass and the corresponding standard error.\nCreate a new data frame from the Laminaria dataset that meets the following criteria: contains only the site column and a new column called total_length_half containing values that are half of the total_length. In this total_length_half column, there are no NAs and all values are less than 100. Hint: think about how the commands should be ordered to produce this data frame!\nUse group_by() and summarise() to find the mean(), min(), and max() blade_length for each site. Also add the number of observations (hint: see ?n).\nWhat was the heaviest stipe measured in each site? Return the columns site, region, and stipe_length.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "4. R Workflows"
    ]
  },
  {
    "objectID": "BCB744/intro_r/09-mapping_style.html",
    "href": "BCB744/intro_r/09-mapping_style.html",
    "title": "9. Mapping With Style",
    "section": "",
    "text": "“Werner Heisenberg is driving down the highway and a police officer stops him. “Sir, do you know you’re going 82 m.p.h.?” the officer asks. “Thanks a lot!” Heisenberg snaps. “Now I’m lost.””\n— Unknown\n\n\n“Science flies you to the moon. Religion flies you into buildings.”\n— Victor Stenger\n\nNow that you have learned the basics of creating a beautiful map in ggplot2 it is time to look at some of the more particular things you will need to make your maps extra stylish. There are also a few more things you need to learn how to do before your maps can be truly publication quality.\nIf we have not yet loaded the tidyverse let’s do so.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggsn) # replace this with ggspatial\n\n# Load Africa map\nload(\"../../data/africa_map.RData\")\n\nDefault maps\nIn order to access the default maps included with the tidyverse we will use the function borders().\n\nggplot() +\n  borders(col = \"black\", fill = \"cornsilk\", size = 0.2) + # The global shape file\n  coord_equal() # Equal sizing for lon/lat \n\n\n\nThe built in global shape file.\n\n\n\nJikes! It’s as simple as that to load a map of the whole planet. Usually you are not going to want to make a map of the entire planet, so let’s see how to focus on just the area around South Africa.\n\nsa_1 &lt;- ggplot() +\n  borders(size = 0.2, fill = \"cornsilk\", colour = \"black\") +\n  coord_equal(xlim = c(12, 36), ylim = c(-38, -22), expand = 0) # Force lon/lat extent\nsa_1\n\n\n\nA better way to get the map of South Africa.\n\n\n\nThat is a very tidy looking map of South(ern) Africa without needing to load any files.\nSpecific labels\nA map is almost always going to need some labels and other visual cues. You saw in the previous section how to add site labels. The following code chunk shows how this differs if yoou want to add just one label at a time. This can be useful if each label needs to be different from all other labels for whatever reason. You may also see that the text labels we are creating have \\n in them. When R sees these two characters together like this it reads this as an instruction to return down a line. Let’s run the code to make sure you see what this means.\n\nsa_2 &lt;- sa_1 +\n  annotate(\"text\", label = \"Atlantic\\nOcean\", \n           x = 15.1, y = -32.0, \n           size = 5.0, \n           angle = 30, \n           colour = \"navy\") +\n  annotate(\"text\", label = \"Indian\\nOcean\", \n           x = 33.2, y = -34.2, \n           size = 5.0, \n           angle = 330, \n           colour = \"red4\")\nsa_2\n\n\n\nMap of southern Africa with specific labels.\n\n\n\nScale bars\nWith your fancy labels added, let’s insert a scale bar next. There is no default scale bar function in the tidyverse, which is why you have loaded the ggsn package. This package is devoted to adding scale bars and North arrows to ggplot2 figures. There are heaps of options so you’ll just focus on one of them for now. It is a bit finicky so to get it looking exactly how you want it requires some guessing and checking. Please feel free to play around with the coordinates below. You may see the list of available North arrow shapes by running northSymbols().\n\nsa_3 &lt;- sa_2 +\n  scalebar(x.min = 22, x.max = 26, y.min = -36, y.max = -35, # Set location of bar\n           dist = 100, dist_unit = \"km\", height = 0.3, st.dist = 0.8, st.size = 4, # Set particulars\n           transform = TRUE, border.size = 0.2, model = \"WGS84\") + # Set appearance\n  north(x.min = 22.5, x.max = 25.5, y.min = -33, y.max = -31, # Set location of symbol\n        scale = 1.2, symbol = 16)\nsa_3\n\n\n\nMap of southern Africa with labels and a scale bar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2021,\n  author = {Smit, A. J.,},\n  title = {9. {Mapping} {With} {Style}},\n  date = {2021-01-01},\n  url = {http://tangledbank.netlify.app/BCB744/intro_r/09-mapping_style.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2021) 9. Mapping With Style. http://tangledbank.netlify.app/BCB744/intro_r/09-mapping_style.html.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "9. Mapping With Style"
    ]
  },
  {
    "objectID": "BCB744/intro_r/12-tidy.html",
    "href": "BCB744/intro_r/12-tidy.html",
    "title": "12. Tidy Data",
    "section": "",
    "text": "The Tidyverse is a collection of R packages designed for data manipulation, exploration, and visualisation. It is based on a philosophy of ‘tidy data,’ which is a standardised way of organising data. The purpose of these packages is to make working with data more efficient. The core Tidyverse packages were created by Hadley Wickham, but over the last few years other individuals have added some packages to the collective, which has significantly expanded our data analytical capabilities through improved ease of use and efficiency. All packages that are built on tidy principles provide the use of a consistent set of tools across a wide range of data analysis tasks. The core Tidyverse packages can be loaded collectively by calling the tidyverse package, as we have seen throughout this workshop. The packages making up the Tidyverse are shown in Figure 1.\nlibrary(tidyverse)",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "12. Tidy Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/12-tidy.html#pivot_longer",
    "href": "BCB744/intro_r/12-tidy.html#pivot_longer",
    "title": "12. Tidy Data",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nThe R function pivot_longer() is a useful tool for transforming data from wide to long format. It belongs to the tidyr package (loaded with tidyverse) and allows you to reshape your dataframe by gathering multiple columns into key-value pairs. Specifically, pivot_longer() takes in a dataframe and allows you to select a set of columns that you would like to pivot into longer format, while specifying the names of the key and value columns that you want to create. The resulting data frame will have a new row for each unique combination of key and value pairs. This function is particularly useful when you need to reshape your data in order to carry out certain analyses or visualisations.\nHave a look now at SACTN2 for an example of what wide data look like, and how to fix it.\nIn SACTN2 you can see that the src column has been removed and that the temperatures are placed in columns that denote the collecting source. This may at first seem like a reasonable way to organise these data, but it is not tidy because the collecting source is one variable, and so should not take up more than one column (i.e. there are multiple observations per row). You need to gather these source columns together into one column so that the seperate measurements (observations) can conform to the one observation per row rule. You do this by telling pivot_longer() the names of the columns you want to squish together. You then tell it the name of the key (names_to) column. This is the column that will contain all of the old column names we are gathering. In this case you may call it source. The last piece of this puzzle is the value (values_to) column. This is where you decide what the name of the column will be for measurements you are gathering up. In this case you may name it temperature, because you are gathering up the temperature values that were incorrectly spread out by the source of the measurements.\n\nSACTN2_tidy &lt;- pivot_longer(SACTN2, cols = c(\"DEA\", \"KZNSB\", \"SAWS\"),\n                            names_to = \"src\",\n                            values_to = \"temp\")",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "12. Tidy Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/12-tidy.html#pivot_wider",
    "href": "BCB744/intro_r/12-tidy.html#pivot_wider",
    "title": "12. Tidy Data",
    "section": "pivot_wider()",
    "text": "pivot_wider()\nThe function pivot_wider() is a tool for transforming data from long to wide format. It is the counterpart to the pivot_longer() function. pivot_wider() allows you to take a set of columns containing key-value pairs and convert them into a wider format, where each unique key value becomes a separate column in the resulting data frame. You can also specify a set of value columns that you want to spread across the new columns created by the key values. With pivot_wider(), you can quickly transform your data from long format into a more intuitive, wide format that is easier to work with in some applications.\nShould your data be too long for a particular application (typically a non-Tidyverse application) or your liking, meaning when individual observations are spread across multiple rows, you will need to use pivot_wider() to rectify the situation. This is generally the case when you have two or more variables stored within the same column, as you will see in SACTN3. This is not terribly common as it would require someone to put quite a bit of time into making a dataframe this way. But never say never. To spread data to become wider, you first tell R what the name of the column is that contains more than one variable, in this case the var column. You then tell R what the name of the column is that contains the values that need to be spread, in this case the val column.\n\nSACTN3_tidy1 &lt;- SACTN3 %&gt;% \n  pivot_wider(names_from = \"var\", values_from = \"val\")",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "12. Tidy Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/12-tidy.html#separate",
    "href": "BCB744/intro_r/12-tidy.html#separate",
    "title": "12. Tidy Data",
    "section": "Separate",
    "text": "Separate\nLooking at SACTN4a, you see that you no longer have a site and src column. Rather these have been replaced by an index column. This is an efficient way to store these data, but it is not tidy because the site and source of each observation have now been combined into one column (variable). Remember, tidy data calls for each of the things known about the data to be its own variable. To re-create site and src columns, you must separate the index column. There are two options: separate_wider_delim() and separate_wider_position(). What does each do? First you give R the name of the column you want to separate, in this case index. Next you specify what the names of the new columns will be. Remember that because we are creating new column names you feed these into R within inverted commas. Lastly, you should tell R where to separate the index column. If you look at the data you will see that the values you want to split up are separated with / (including a space), so that is what you need to tell R.\n\nSACTN4a_tidy &lt;- SACTN4a |&gt; \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \")",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "12. Tidy Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/12-tidy.html#separating-dates-using-mutate",
    "href": "BCB744/intro_r/12-tidy.html#separating-dates-using-mutate",
    "title": "12. Tidy Data",
    "section": "Separating dates using mutate()\n",
    "text": "Separating dates using mutate()\n\nAlthough the date column represents an example of a date date type or class (a kind of data in its own right), you might also want to split this column into its constituent parts, i.e. create separate columns for day, month, and year. In this case you can spread these components of the date vector into three columns using the mutate() function and some functions in the lubridate package (part of the tidyverse).\n\nSACTN_tidy2 &lt;- SACTN4a %&gt;% \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \") %&gt;% \n  mutate(day = lubridate::day(date),\n         month = lubridate::month(date),\n         year = lubridate::year(date))\n\nNote that when the date is split into component parts the data are no longer tidy (see below).",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "12. Tidy Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/12-tidy.html#unite",
    "href": "BCB744/intro_r/12-tidy.html#unite",
    "title": "12. Tidy Data",
    "section": "Unite",
    "text": "Unite\nIt is not uncommon that field/lab instruments split values across multiple columns while they are making recordings. You might sometimes see this with date values where the year, month, and day values are given in different columns. There are uses for the data in this way, though it is not terribly tidy. You usually want the date of any observation to be shown in just one column. If you look at SACTN4b you will see that there is a year, month, and day column. To unite() them you must first tell R what you want the united column to be labelled, in this case you will use date. You then list the columns to be united; here this is year, month, and day. Lastly, decide if you want the united values to have a separator between them. The standard separator for date values is ‘-’.\n\nSACTN4b_tidy &lt;- SACTN4b |&gt; \n  unite(year, month, day, col = \"date\", sep = \"-\")",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "12. Tidy Data"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html",
    "href": "BCB744/intro_r/02-working-with-data.html",
    "title": "2. Working With Data & Code",
    "section": "",
    "text": "In this Chapter we will cover:",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#comma-separated-value-files",
    "href": "BCB744/intro_r/02-working-with-data.html#comma-separated-value-files",
    "title": "2. Working With Data & Code",
    "section": "Comma separated value files",
    "text": "Comma separated value files\nCSV stands for ‘Comma Separated Value’. A CSV file is a simple text file that stores data in a tabular format, with each row representing a record and each column representing a field of data. In a CSV file, each data value is separated by a comma (or sometimes another delimiter such as a semicolon or tab), and each row is terminated by a new line.\nCSV files are widely used in data analysis and can be opened and edited by most spreadsheet software, such as MS Excel and Google Sheets. Being comprised of plain text (ASCII), they are often used to import and export data between different applications or systems, as they provide a standardised format that can be easily parsed by software.\nCSV files are easy to create and use, and they have the advantage of being lightweight and easy to read and write by both humans and machines. However, they can be limited in their ability to represent complex data structures or to handle large amounts of data efficiently. Additionally, if our data contain certain kinds of special characters, this can cause problems with parsing the file correctly.\nWe will most frequently use the functions read.csv() or readr::read_csv() (and related forms) for reading in CSV data. We can write CSV files to disk with the write.csv() or readr::write_csv() commands. For very large datasets that might take a long time to read in or save, data.table::fread() or data.table::fwrite() are faster alternatives to the aforementioned base R or tidyverse options. Even faster options are feather::read_feather() and feather::write_feather(); although feather saves tabular data, the format is not actually an ASCII CSV, however.\n\n\n\n\n\n\nASCII files\n\n\n\nASCII stands for “American Standard Code for Information Interchange”. An ASCII file is a plain text file that contains ASCII characters. ASCII is a character encoding standard that assigns a unique numeric code to each character, including letters, numbers, punctuation, and other symbols commonly used in the English language.\nASCII files are the most basic type of text file and are supported by virtually all operating systems and applications. We can create and edit ASCII files using any text editor, such as Notepad, TextEdit, or VS Code. ASCII files are typically used for storing and sharing simple text-based information, such as program source code, configuration files, and other types of data that do not require special formatting or rich media content.\nASCII files are limited in their ability to represent non-English characters or symbols that are not included in the ASCII character set. To handle these types of characters, other character encoding standards such as UTF-8 or Unicode are used. However, ASCII files remain an important and widely used format for storing and sharing simple text-based data.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#tab-separated-value-files",
    "href": "BCB744/intro_r/02-working-with-data.html#tab-separated-value-files",
    "title": "2. Working With Data & Code",
    "section": "Tab separated value files",
    "text": "Tab separated value files\nThe primary difference between a ‘tab-separated value’ (TSV) file and a ‘comma-separated values’ (CSV) file lies in the delimiter used to separate data fields. Both file formats are plain text ASCII files used to store data in a tabular format, but they employ different characters to distinguish individual fields within each row.\nIn a TSV file, the fields are separated by tab characters (represented as \\t in many programming languages). This format is particularly useful when dealing with data that include commas within the values, as it avoids potential conflicts and parsing issues.\nCSV files are more common and widely supported than TSV files. However, they can present difficulties when the data itself contains commas, potentially causing confusion between actual field separators and commas within the data. To mitigate this issue, values containing commas are often enclosed in quotation marks.\nLike CSV files, TSV can also be imported into and exported from spreadsheet software like Excel, or read and manipulated using programming languages like Python, R, and many others. The choice between TSV and CSV largely depends on the nature of the data and personal preferences, but it’s crucial to be aware of the delimiter used in order to accurately parse the files. The same functions that read or write CSV files in R can be used for TSV, but one has to set the arguments sep = \"\\t\" or delim = \"\\t\" for the functions read.csv() and read_csv() respectively.\n\n\n\n\n\n\nMissing values and CSV and TSV files\n\n\n\nWhere we have missing data (blanks), the CSV format separates these by commas with empty field in-between. However, there can be problems with blanks if we read in a space-delimited format file. If we are having trouble reading in missing data as blanks, try replacing them in the spreadsheet with NA, the missing data code in R. In Excel, highlight the area of the spreadsheet that includes all the cells we need to fill with NA. Do an ‘Edit/Replace…’ and leave the ‘Find what:’ text box blank and in the ‘Replace with:’ text box enter NA. Once imported into R, the NA values will be recognised as missing data.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#microsoft-excel-files",
    "href": "BCB744/intro_r/02-working-with-data.html#microsoft-excel-files",
    "title": "2. Working With Data & Code",
    "section": "Microsoft Excel files",
    "text": "Microsoft Excel files\nMicrosoft Excel files are a type of file format that is used to store data in a tabular form, much like CSV files. However, Excel files are proprietary and are specifically designed to work with Excel software. Excel files can contain more advanced formatting features such as colours, fonts, and formulas, which make them a popular choice for people who like embellishments. But, as much as I dislike Excel as a software for data analysis, Excel files are definitely a good option for data entry.\nUsing MS Excel for data analysis can be a terrible idea for a number of reasons:\n\nCompatibility Excel files may not be compatible with all data science tools and programming languages. For example, R cannot read Excel files directly.\nData integrity Excel files can be prone to errors and inconsistencies in the data. For example, if a user changes a formula or formatting, it could affect the entire dataset. Also, it is possible for Excel to change the data types of certain columns, or to mix the class of data within a column, which can cause issues with data processing and analysis.\nFile size Excel files can quickly become very large when dealing with large datasets, which can lead to performance issues and storage problems.\nVersion control Excel files can make it difficult to keep track of changes and versions of the data, particularly when multiple people are working on the same file.\n\nIn contrast, CSV files are a simple, lightweight, and widely supported file format that can be easily used with most data science tools and programming languages. CSV files are also less prone to errors and inconsistencies than Excel files, making them a more reliable choice for data science tasks.\nSo, while Excel files may be useful for certain tasks such as initial data entry, they are generally not recommended for use in data science due to their potential for errors (see box “Well-known Excel errors”), incompatibility, and other issues. I recommend exporting data deliberately to CSV files. This not only avoids complications, but also allows us to unambiguously identify the data we based our analysis on. This last statement should give us the hint that it is good practice to name our .csv slightly differently each time we export it from Excel, perhaps by appending a reference to the date it was exported. Also, for those of us who use commas in Excel as the decimal separator, or to separate 1000s, undo these features now.\n\n\n\n\n\n\nWell-known Excel errors\n\n\n\nExcel is a widely used spreadsheet application, but it has been responsible for several serious errors in data analysis, science, and data science. Some of these errors include:\n\nGene name errors In 2016, researchers discovered that Excel automatically converted gene symbols to dates or floating-point numbers. For example, gene symbols like SEPT2 (Septin 2) were converted to “2-Sep” and gene symbols like MARCH1 (Membrane Associated Ring-CH-Type Finger 1) were converted to “1-Mar”. This led to errors and inconsistencies in genetic data, affecting nearly 20% of published papers in leading genomic journals.\nReinhart-Rogoff controversy In 2010, economists Carmen Reinhart and Kenneth Rogoff published a paper arguing that high levels of public debt were associated with lower economic growth. Their findings influenced policy decisions worldwide. However, in 2013, other researchers found that Reinhart and Rogoff’s results were affected by an Excel spreadsheet error that excluded some data points, causing them to overstate the relationship between debt and growth.\nLondon Whale incident In 2012, JPMorgan Chase, a leading financial institution, suffered a trading loss of over $6 billion, partially due to an Excel error. The bank’s model for calculating the risk of their trades, implemented in Excel, used incorrect formulas that significantly underestimated the risk involved. The event, which became known as the “London Whale” incident, highlighted the potential consequences of relying on Excel for complex financial models.\nTruncation of large numbers Excel can handle only a limited number of digits for large numbers, truncating any value that exceeds this limit. This truncation has lead to a loss of precision and inaccurate calculations in scientific and data analysis contexts, where exact values were important.\nIssues with floating-point arithmetic Excel uses floating-point arithmetic, which can cause rounding errors and imprecise results when working with very large or very small numbers. These inaccuracies can lead to incorrect conclusions or predictions in data analysis and scientific research.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#rdata-files",
    "href": "BCB744/intro_r/02-working-with-data.html#rdata-files",
    "title": "2. Working With Data & Code",
    "section": "Rdata files",
    "text": "Rdata files\nRdata files are a file format used by the R programming language to store data objects. These files can contain any type of R object, such as vectors, matrices, dataframes, lists, and more. Rdata files are binary files, which means they are not human-readable like text files such as CSV files. Binary R data files have a .rda or .Rdata file extension and can be created or read using the save() and load(), respectively, functions in R.\nRdata files are convenient for a number of reasons:\n\nEfficient storage Rdata files can be more compact (they can be compressed) and efficient than other file formats, such as CSV files, because they are stored in a binary format. This means they take up less disk space and can be read and written to faster.\nEasy access to R objects Rdata files make it easy to save and load R objects, which can be useful for preserving data objects for future analysis or sharing them with others. This is especially useful for complex datasets or objects that would be difficult to recreate.\nPreserve metadata Rdata files can preserve metadata such as variable names, row and column names, and other attributes of R objects. This makes it easier to work with the data objects in the future without having to recreate this metadata.\nConvenient for reproducibility Rdata files can be used to save and load data objects as part of a reproducible research workflow. This can help ensure that data objects are preserved and can be easily accessed in the future, even if the data sources or code have changed.\n\nOn the downside, they can only be used within R, making them a less than ideal proposition when you intend sharing your data with colleagues who sadly do not use R.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#other-binary-files",
    "href": "BCB744/intro_r/02-working-with-data.html#other-binary-files",
    "title": "2. Working With Data & Code",
    "section": "Other binary files",
    "text": "Other binary files\nAs a biostatistician, you may encounter several other binary data files in your work. Such binary data files may be software-specific and can be used to store large datasets or data objects that are not easily represented in a text format. For example, a binary data file might contain a large matrix or array of numeric data that would be difficult to store in a text file. Binary data files can also be used to store images, audio files, and other types of data that are not represented as text.\nOne common type of binary data file that you may encounter as a statistician is a SAS data file. SAS is a statistical software package that is widely used in data analysis, and SAS data files are a binary format used to store datasets in SAS. These files typically have a .sas7bdat file extension and contain metadata such as variable names and formats in addition to the data itself. Another type of binary data file you may encounter is a binary .mat data file, which is a file format used to store Matlab data.\nWhen working with binary data files, it is important to be aware of the specific format of the file and the tools and software needed to read and manipulate the data. Some statistical software packages may have built-in functions for reading and writing certain types of binary data files, while others may require additional libraries or packages.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#netcdf-grib-and-hdf-files",
    "href": "BCB744/intro_r/02-working-with-data.html#netcdf-grib-and-hdf-files",
    "title": "2. Working With Data & Code",
    "section": "NetCDF, Grib, and HDF files",
    "text": "NetCDF, Grib, and HDF files\nNetCDF, HDF, and GRIB are file formats commonly used in the scientific and research communities to store and share large and complex datasets. While CSV files are a simple and widely used format, they can become impractical for large datasets with complex structures or metadata. Here’s a brief overview of each file format:\n\nNetCDF (Network Common Data Form) is a binary file format that is designed for storing and sharing scientific data. It can store multidimensional arrays and metadata, such as variable names and units, in a self-describing format. NetCDF files are commonly used in fields such as atmospheric science, oceanography, and climate modelling.\nHDF (Hierarchical Data Format) is a file format that is designed to store and organise large and complex data structures. It can store a wide variety of data types, including multidimensional arrays, tables, and hierarchical data. HDF files are commonly used in fields such as remote sensing, astronomy, and engineering.\nGRIB (GRIdded Binary) is a binary file format used to store meteorological and oceanographic data. It can store gridded data, such as atmospheric or oceanic model output, in a compact and efficient binary format. GRIB files are commonly used by weather forecasting agencies and research organisations.\n\nCompared to CSV files, these file formats offer several benefits for storing and sharing complex datasets:\n\nSupport for multidimensional arrays These file formats can store and handle multidimensional arrays, which cannot be represented in a CSV file.\nEfficient storage Binary file formats can be more compact and efficient than text-based formats such as CSV files, which can save disk space and make it easier to share and transfer large datasets.\nMemory use efficiency NetCDF, GRIB, and HDF files are better for memory use efficiency compared to CSV files because they can store multidimensional arrays and metadata in a compact binary format, which can save disk space and memory when working with large and complex datasets. Also, they do not have to be read into memory all at once.\nSelf-describing metadata These file formats can include metadata, such as variable names and units, which are self-describing and can be easily accessed and understood by other researchers and software.\nSupport for compression Binary file formats can support compression, which can further reduce file size and make it easier to share and transfer large datasets.\n\nThe various efficiencies mention above may be offset by them being quite challenging to work with, and as such novices might experience steep learning curves.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BCB744/intro_r/02-working-with-data.html#larger-than-memory-data",
    "href": "BCB744/intro_r/02-working-with-data.html#larger-than-memory-data",
    "title": "2. Working With Data & Code",
    "section": "Larger than memory data",
    "text": "Larger than memory data\nAbove we dealt with data that fit into your computer’s memory (RAM). However, there are many datasets that are too large to fit into memory, and as such, we need to use alternative methods to work with them. These methods include:\n\nApache Arrow in the arrow package in R, which has support for the ‘feather’ file format and ‘parquet’ files\nDuckDB in the duckdb package in R, which create a database on disk and can be queried using SQL\n\nI will develop vignettes for these in the future. We will not use these in this course, but it is important to be aware of them.",
    "crumbs": [
      "Home",
      "BCB744: Introduction to R, & Biostatistics",
      "2. Working With Data & Code"
    ]
  },
  {
    "objectID": "BDC334/assessments/Prac_assessment_2024.html",
    "href": "BDC334/assessments/Prac_assessment_2024.html",
    "title": "BDC334",
    "section": "",
    "text": "You have been provided with three files:\nImport the CSV files into R and answer the questions below.\nThe assessment is out of a total of 50 marks and you have 2 hours to complete it.\nYou are welcome to use any online resources to help you complete the test, but you may not communicate with anyone else during the assessment."
  },
  {
    "objectID": "BDC334/assessments/Prac_assessment_2024.html#question-1",
    "href": "BDC334/assessments/Prac_assessment_2024.html#question-1",
    "title": "BDC334",
    "section": "Question 1",
    "text": "Question 1\n\nList the three pairs of sites that are furthest apart in terms of the geographical distance between them. For each pair, also provide the temperature and depth associated with each member of the pair.\nList three pairs of sites that are closest together in terms of the geographical distance between them. For each pair, also provide the temperature and depth associated with each member of the pair.\n\nCommunicate the above output in a clear and concise manner, for example, using a table. The same applies to the rest of the questions.\nAnswer\n\nlibrary(vegan)\nlibrary(tidyverse)\n\nenv &lt;- read.csv(\"../../data//BarentsFish_env.csv\")\n\n# a. List the three pairs of sites that are furthest apart in terms of the\n# geographical distance between them. For each pair, also provide the\n# temperature and depth associated with each member of the pair\n\n# extract the lon and lat\ngeo_dat &lt;- env[, c(\"Longitude\", \"Latitude\")]\n\n# calculate the geographical distance or use Euclidean distance as a proxy\n# (use either function)\n# geo_dist &lt;- dist(geo, upper = FALSE)\n# this step or the next one is possibly as far as you'll get with the \n# code I gave you\n# you can proceed manually from here by examining the matrices and cross \n# referencing with the data files for the environmental data\ngeo_dist &lt;- round(vegdist(geo_dat, method = \"euclidean\", upper = FALSE), 2)\n\n# convert the distance object to a full symmetric matrix\ngeo_dist_matrix &lt;- as.matrix(geo_dist)\n\n# scan the matrix for the three largest distances and\n# find the three pairs of sites that are furthest apart\n# it is a pain to do by eye, but it is possible\n\n# for my own convenience, I'll calculate it more efficiently:\n# set the diagonal and upper triangle to NA since we only need the\n# lower triangle\ngeo_dist_matrix[upper.tri(geo_dist_matrix, diag = TRUE)] &lt;- NA\n\n# find the indices of the three largest distances\n# get the order of the matrix values in decreasing order,\n# and select the first three\nlargest_dist_indices &lt;- order(geo_dist_matrix,\n                               decreasing = TRUE, na.last = NA)[1:3]\n\n# retrieve the row and column indices of these largest distances\nrow_indices &lt;- row(geo_dist_matrix)[largest_dist_indices]\ncol_indices &lt;- col(geo_dist_matrix)[largest_dist_indices]\n\n# combine the row and column indices into pairs\n# I add the site temperature and depth values automagically\n# but you can manually accomplish the same\nlargest_dist_pairs &lt;- data.frame(\n  Site1 = row_indices,\n  Site2 = col_indices,\n  Distance = geo_dist_matrix[largest_dist_indices],\n  Site1_temp = env$Temperature[row_indices],\n  Site2_temp = env$Temperature[col_indices],\n  Site1_depth = env$Depth[row_indices],\n  Site2_depth = env$Depth[col_indices]\n)\n\nlargest_dist_pairs # this is what you get marked on\n\n  Site1 Site2 Distance Site1_temp Site2_temp Site1_depth Site2_depth\n1    85    18    18.80       2.35       0.65         215         234\n2    84    18    18.35       1.85       0.65         209         234\n3    85    33    18.32       2.35       1.25         215         255\n\n# b. List three pairs of sites that are closest together in terms of the\n# **geographical distance** between them. For each pair, also provide the\n# temperature and depth associated with each member of the pair.\n\n# to do this, I'll adapt the code above to find the three smallest distances\nshortest_dist_indices &lt;- order(geo_dist_matrix,\n                               decreasing = FALSE, na.last = NA)[1:3]\n\n# retrieve the row and column indices of these largest distances\nrow_indices &lt;- row(geo_dist_matrix)[shortest_dist_indices]\ncol_indices &lt;- col(geo_dist_matrix)[shortest_dist_indices]\n\n# combine the row and column indices into pairs\n# I add the site temperature and depth values automagically\n# but you can manually accomplish the same\nshortest_dist_pairs &lt;- data.frame(\n  Site1 = row_indices,\n  Site2 = col_indices,\n  Distance = geo_dist_matrix[shortest_dist_indices],\n  Site1_temp = env$Temperature[row_indices],\n  Site2_temp = env$Temperature[col_indices],\n  Site1_depth = env$Depth[row_indices],\n  Site2_depth = env$Depth[col_indices]\n)\n\nshortest_dist_pairs # this is what you get marked on\n\n  Site1 Site2 Distance Site1_temp Site2_temp Site1_depth Site2_depth\n1    34    32     0.27       0.55       0.95         305         294\n2    24     5     0.30       3.25       3.35         308         384\n3    48    47     0.32       0.95       0.65         285         315\n\n\n(/10)"
  },
  {
    "objectID": "BDC334/assessments/Prac_assessment_2024.html#question-2",
    "href": "BDC334/assessments/Prac_assessment_2024.html#question-2",
    "title": "BDC334",
    "section": "Question 2",
    "text": "Question 2\n\nList the three pairs of sites that are furthest apart in terms of the environmental distance between them. For each pair, also state the environmental distance between them.\nList three pairs of sites that are closest together in terms of the environmental distance between them. For each pair, also state the environmental distance between them.\n\nAnswer\n\n# a. List the three pairs of sites that are furthest apart in terms of the\n# **environmental distance** between them. For each pair, also state the\n# environmental distance between them.\n\n# Again, I adapt pre-existing code but you'll do this manually as far\n# as possible\n\n# extract the lon and lat\nenv_dat &lt;- env[, c(\"Depth\", \"Temperature\")]\n\n# calculate the geographical distance or use Euclidean distance as a proxy\n# (use either function)\n# geo_dist &lt;- dist(geo, upper = FALSE)\nenv_dist &lt;- round(vegdist(env_dat, method = \"euclidean\", upper = FALSE), 2)\n\n# your code will bring you to the above step, and from there you can\n# accomplish the rest manually to assemble the table by hand\n# I'll continue with more efficient code...\n\n# convert the distance object to a full symmetric matrix\nenv_dist_matrix &lt;- as.matrix(env_dist)\n\n# scan the matrix for the three largest distances and\n# find the three pairs of sites that are furthest apart\n# it is a pain to do by eye, but it is possible\n\n# for my own convenience, I'll calculate it more efficiently:\n# set the diagonal and upper triangle to NA since we only need the\n# lower triangle\nenv_dist_matrix[upper.tri(env_dist_matrix, diag = TRUE)] &lt;- NA\n\n# find the indices of the three largest distances\n# get the order of the matrix values in decreasing order,\n# and select the first three\nlargest_dist_indices &lt;- order(env_dist_matrix,\n                              decreasing = TRUE, na.last = NA)[1:3]\n\n# retrieve the row and column indices of these largest distances\nrow_indices &lt;- row(geo_dist_matrix)[largest_dist_indices]\ncol_indices &lt;- col(geo_dist_matrix)[largest_dist_indices]\n\n# combine the row and column indices into pairs\n# I add the site temperature and depth values automagically\n# but you can manually accomplish the same\nlargest_dist_pairs &lt;- data.frame(\n  Site1 = row_indices,\n  Site2 = col_indices,\n  Distance = geo_dist_matrix[largest_dist_indices],\n  Site1_temp = env$Temperature[row_indices],\n  Site2_temp = env$Temperature[col_indices],\n  Site1_depth = env$Depth[row_indices],\n  Site2_depth = env$Depth[col_indices]\n)\n\nlargest_dist_pairs # this is what you get marked on\n\n  Site1 Site2 Distance Site1_temp Site2_temp Site1_depth Site2_depth\n1    88    81     2.92       4.45       1.65         167         486\n2    88    80     3.23       4.45       1.55         167         474\n3    89    88     4.70       1.95       4.45         462         167\n\n# b. List three pairs of sites that are closest together in terms of the\n# **environmental distance** between them. For each pair, also state the\n# environmental distance between them.\n\n# scan the matrix (made in 2.a) for the three largest distances and\n# find the three pairs of sites that are furthest apart\n# it is a pain to do by eye, but it is possible\n\n# find the indices of the three largest distances\n# get the order of the matrix values in decreasing order,\n# and select the first three\nshortest_dist_indices &lt;- order(env_dist_matrix,\n                              decreasing = FALSE, na.last = NA)[1:3]\n\n# retrieve the row and column indices of these shortest distances\nrow_indices &lt;- row(geo_dist_matrix)[shortest_dist_indices]\ncol_indices &lt;- col(geo_dist_matrix)[shortest_dist_indices]\n\n# combine the row and column indices into pairs\n# I add the site temperature and depth values automagically\n# but you can manually accomplish the same\nshortest_dist_pairs &lt;- data.frame(\n  Site1 = row_indices,\n  Site2 = col_indices,\n  Distance = geo_dist_matrix[shortest_dist_indices],\n  Site1_temp = env$Temperature[row_indices],\n  Site2_temp = env$Temperature[col_indices],\n  Site1_depth = env$Depth[row_indices],\n  Site2_depth = env$Depth[col_indices]\n)\n\nshortest_dist_pairs # this is what you get marked on\n\n  Site1 Site2 Distance Site1_temp Site2_temp Site1_depth Site2_depth\n1    46    14     2.48       1.85       1.95         358         358\n2    40    23     3.14       2.95       3.05         290         290\n3    55    52     0.77       0.55       0.75         306         306\n\n\n(/10)"
  },
  {
    "objectID": "BDC334/assessments/Prac_assessment_2024.html#question-3",
    "href": "BDC334/assessments/Prac_assessment_2024.html#question-3",
    "title": "BDC334",
    "section": "Question 3",
    "text": "Question 3\nIs there a relationship between the environmental variables? Produce the code for this analysis and the evidence (both graphical and statistical) for the nature of this relationship. If a relationship is present, describe it.\nAnswer\n\n# Is there a relationship between the environmental variables?\n\ncor(env_dat) # this is what you get marked on)\n\n                  Depth Temperature\nDepth        1.00000000 -0.01820205\nTemperature -0.01820205  1.00000000\n\nggplot(env, aes(x = Depth, y = Temperature)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Depth and Temperature\",\n       x = \"Depth (m)\",\n       y = \"Temperature (°C)\") +\n  theme_linedraw()\n\n\n\n\n\n\n# No, there is no relationship between the two variables. The correlation\n# coefficient shows a value of -0.02, which is very close to zero. This is\n# confirmed by the flat line in the correlation plot.\n\n(/10)"
  },
  {
    "objectID": "BDC334/assessments/Prac_assessment_2024.html#question-4",
    "href": "BDC334/assessments/Prac_assessment_2024.html#question-4",
    "title": "BDC334",
    "section": "Question 4",
    "text": "Question 4\n\nList the three pairs of sites that are furthest apart in terms of species composition between them. For each pair, also state the species dissimilarity between them.\nList three pairs of sites that are closest together in terms of species composition between them. For each pair, also state the species dissimilarity between them.\n\nAnswer\n\n# a. List the three pairs of sites that are furthest apart in terms of\n# **species composition** between them. For each pair, also state the\n# species dissimilarity between them.\n\nspp_dat &lt;- read.csv(\"../../data/BarentsFish_spp.csv\")\n\n# using Bray-Curtis for abundance data (could use something else)\nspp_diss &lt;- round(vegdist(spp_dat, method = \"bray\", upper = FALSE), 2)\n\nspp_diss_matrix &lt;- as.matrix(spp_diss)\n\nspp_diss_matrix[upper.tri(spp_diss_matrix, diag = TRUE)] &lt;- NA\n\nlargest_diss_indices &lt;- order(spp_diss_matrix,\n                              decreasing = TRUE, na.last = NA)[1:3]\n\nrow_indices &lt;- row(spp_diss_matrix)[largest_diss_indices]\ncol_indices &lt;- col(spp_diss_matrix)[largest_diss_indices]\n\nlargest_diss_pairs &lt;- data.frame(\n  Site1 = row_indices,\n  Site2 = col_indices,\n  Dissimilarity = spp_diss_matrix[largest_diss_indices]\n)\n\nlargest_diss_pairs # this is what you get marked on\n\n  Site1 Site2 Dissimilarity\n1    57     2          0.90\n2    57     3          0.89\n3    57     5          0.88\n\n# b. List three pairs of sites that are closest together in terms of\n# **species composition** between them. For each pair, also state the\n# species dissimilarity between them.\n\nsmallest_diss_indices &lt;- order(spp_diss_matrix,\n                              decreasing = FALSE, na.last = NA)[1:3]\n\nrow_indices &lt;- row(spp_diss_matrix)[smallest_diss_indices]\ncol_indices &lt;- col(spp_diss_matrix)[smallest_diss_indices]\n\nsmallest_diss_pairs &lt;- data.frame(\n  Site1 = row_indices,\n  Site2 = col_indices,\n  Dissimilarity = spp_diss_matrix[smallest_diss_indices]\n)\n\nsmallest_diss_pairs # this is what you get marked on\n\n  Site1 Site2 Dissimilarity\n1    76    74          0.03\n2    87    86          0.04\n3    77    43          0.05\n\n\n(/10)"
  },
  {
    "objectID": "BDC334/assessments/Prac_assessment_2024.html#question-5",
    "href": "BDC334/assessments/Prac_assessment_2024.html#question-5",
    "title": "BDC334",
    "section": "Question 5",
    "text": "Question 5\nUsing all the answers given above to support your reasoning, discuss the implications of these findings in the light of the theory covered in the BDC334 module.\nAnswer\nAnything that is not wrong, provide explanations for the patterns observed, relates the environmental similarities and differences to the species similarities and differences, and discusses the implications of these findings in the light of the theory covered in the BDC334 module.\n(/10)\nTOTAL /50"
  },
  {
    "objectID": "BDC334/assessments/Prac_assessment_2024.html#instructions",
    "href": "BDC334/assessments/Prac_assessment_2024.html#instructions",
    "title": "BDC334",
    "section": "Instructions",
    "text": "Instructions\nSubmit a R script onto iKamva at the end of the test period. Label the script as follows:\nBDC334_&lt;Surname&gt;_&lt;Student_no.&gt;_Practical_Assessment.R.\nWithin the R script, ensure that all code:\n\nnecessary to accomplish an answer is neatly and clearly associated with the question heading,\nworks as intended, and that each line of code is properly accompanied by a comment explaining the purpose of the code,\nis well-structured and easy to follow, and\nis free of errors and warnings.\n\nYou are also welcome (encouraged, in fact) to add comments to your script to explain your reasoning or thought process."
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html",
    "href": "BDC334/Lab-02a-r_rstudio.html",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "",
    "text": "“Ignorance more frequently begets confidence than does knowledge.”\n— Charles Darwin",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#download-and-install-r-and-rstudio",
    "href": "BDC334/Lab-02a-r_rstudio.html#download-and-install-r-and-rstudio",
    "title": "Lab 2a. R & RStudio",
    "section": "Download and Install R and RStudio",
    "text": "Download and Install R and RStudio\nR and RStudio are separate programs and need to be installed individually. Follow the instructions on the Posit website.\nRStudio can be seen as the vehicle body, seats, dashboard, and all other bells and whistles you might find in a car. R is the engine. RStudio does not work without R. The analyses, graphics, etc. are done with R (running inside RStudio) and not RStudio.\nFor those of you who will be using your personal laptops, please ensure that you install R and RStudio before the Lab on Monday afternoon. If you have any issues with the installation, please let us know so that we can assist you. We will continue with the instructions below during the Lab session once you have installed R and RStudio.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#setting-up-the-workspace",
    "href": "BDC334/Lab-02a-r_rstudio.html#setting-up-the-workspace",
    "title": "Lab 2a. R & RStudio",
    "section": "Setting up the Workspace",
    "text": "Setting up the Workspace\nGeneral Settings\nBefore we start using RStudio (which is a code editor and environment that runs R) let’s first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’) and select the tick boxes as shown in Figure 1 below.\n\n\n\n\n\nFigure 1: RStudio preferences.\n\n\nCustomising Appearance\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g. Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code. Refer to Figure 2.\n\n\n\n\n\nFigure 2: Appearance settings.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#the-rproject",
    "href": "BDC334/Lab-02a-r_rstudio.html#the-rproject",
    "title": "Lab 2a. R & RStudio",
    "section": "The Rproject",
    "text": "The Rproject\nA very nifty way of managing workflow in RStudio is through the built-in functionality of the Rproject. We do not need to install any packages or change any settings to use these. Creating a new project is a very simple task, as well. For this course we, create a new project by clicking on the down arrow on the top right hand side of the RStudio window, click “New Project”, give it a name (maybe BDC334_lab, for example), and save it somewhere where you can find it agian on your computer. Each time to revisit R and RStudio going forward, just open that Rproject by double clicking on the .Rproj file where you saved it on your computer, or open it via the RStudio menu system. It will open all your files (scripts and data) that you previously worked on. COntinue working again there.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#installing-packages",
    "href": "BDC334/Lab-02a-r_rstudio.html#installing-packages",
    "title": "Lab 2a. R & RStudio",
    "section": "Installing Packages",
    "text": "Installing Packages\nThe most common functions used in R are contained within the base package; this makes R useful ‘out of the box.’ However, there is extensive additional functionality that is being expanded all the time through the use of packages. Packages are simply collections of code called functions that automate complex mathematical or statistical tasks. One of the most useful features of R is that users are continuously developing new packages and making them available for free. You can find a comprehensive list of available packages on the CRAN website. There are currently (2022-04-29) 18907 packages available for R!\nIf the thought of searching for and finding R packages is daunting, a good place to start is the R Task View page. This page curates collections of packages for general tasks you might encounter, such as Experimental Design, Meta-Analysis, or Multivariate Analysis. Go and have a look for yourself, you might be surprised to find a good explanation of what you need.\nAfter clicking ‘Tools’/‘Install Packages’, type in the package name tidyverse in the ‘Packages’ text box (note that it is case sensitive) and select the Install button. The Console will run the code needed to install the package, and then provide some commentary on the installation of the package and any of its dependencies (i.e., other R packages needed to run the required package).\nThe installation process makes sure that the functions within the packages contained within the tidyverse are now available on your computer, but to avoid potential conflicts in the names of functions, it will not load these automatically. To make R ‘know’ about these functions in a particular session, you need either to load the package via ticking the checkbox for that package in the Packages tab, or execute:\n\nlibrary(tidyverse)\n\nTo prepare ourselves for the week ahead, let us also install the following packages. Here I demonstate the command line approach to achieve the same thing that can be done via the menu:\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"bindrcpp\")\ninstall.packages(\"ggpubr\")\ninstall.packages(\"magrittr\")\ninstall.packages(\"boot\")\ninstall.packages(\"ggsn\")\ninstall.packages(\"scales\")\ninstall.packages(\"maps\")\ninstall.packages(\"ggmap\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"bindrcpp\")\n\nSince we will develop the habit of doing all of our analyses from R scripts, it is best practice to simply list all of the libraries to be loaded right at the start of your script. Comments may be used to remind your future-self (to quote Hadley Wickham) what those packages are for.\n\n\n\n\n\n\nCopying code from RStudio\n\n\n\nHere you saw RStudio execute the R code needed to install (using install.packages()) and load (using library()) the package, so if you want to include these in one of your programs, just copy the text it executes. Note that you need only install the current version of a package once, but it needs to be loaded at the beginning of each R session.\n\n\n\n\n\n\n\n\nLab 2\n\n\n\n\nWhy is it best practice to include packages you use in your R program explicitly?",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#the-panes-of-rstudio",
    "href": "BDC334/Lab-02a-r_rstudio.html#the-panes-of-rstudio",
    "title": "Lab 2a. R & RStudio",
    "section": "The Panes of RStudio",
    "text": "The Panes of RStudio\nRStudio has four main panes each in a quadrant of your screen ((RStudio_panes?)): Source Editor 🅐, Console 🅑, Workspace Browser 🅒 (and History), and Plots 🅓 (and Files, Packages, Help). These can also be adjusted under the ‘Preferences’ menu. Note that there might be subtle differences between RStudio installations on different operating systems. We will discuss each of the panes in turn.\n\n\n\n\n\nFigure 3: RStudio window panes.\n\n\nSource Editor 🅐\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it isn’t already)\nNow double click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (for Windows machines; for Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon or Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18 and re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now… Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nThe #\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.\n\n\nConsole 🅑\nThis is where you can type code that executes immediately. This is also known as the command line. Throughout the notes, we will represent code for you to execute in R as a different font.\n\n\n\n\n\n\nType it in!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really shouldn’t. The first reason is that you might unwittingly copy invisible PDF formatting errors into R, which will make the code fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your own errors. This is an invaluable way of learning and taking shortcuts now will only hurt you in the long run.\n\n\nEntering code in the command line is intuitive and easy. For example, we can use R as a calculator by typing into the Console (and pressing Enter after each line). Note that the output for every line of calculation (e.g. 6 * 3) is indicated by R&gt;, as we see here:\n\n6 * 3\n\n[1] 18\n\n\nR&gt; [1] 18\n\n5 + 4\n\n[1] 9\n\n\nR&gt; [1] 9\n\n2 ^ 3\n\n[1] 8\n\n\nR&gt; [1] 8\nNote that spaces are optional around simple calculations.\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\n[1] 9\n\n\nR&gt; [1] 9\nTo type the assignment operator (&lt;-) push the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\n[1] FALSE\n\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n\nWhat are the values after each hashed statement in the following? Use the RStudio Console to determine these values:\n\n\n\n\nmass &lt;- 48\nage &lt;- 78\nmass &lt;- mass * 2.0 # mass? \nage &lt;- age - 17 # age? m\nmass_index &lt;- mass / age # mass_index?\n\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n\nUse R to calculate some simple mathematical expressions entered. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y Display z in the console.\n\n\n\nWe can create a vector in R by using the combine c() function:\n\napples &lt;- c(5.3, 3.8, 4.5)\n\nA vector is a one-dimensional array (i.e., a list of numbers), and this is the simplest form of data used in R (you can think of a single value in R as just a very short vector). We’ll talk about more complex (and therefore more powerful) types of data structures as we go along.\nIf you want to display the value of apples type:\n\napples\n\n[1] 5.3 3.8 4.5\n\n\nR&gt; [1] 5.3 3.8 4.5\nFinally, there are default functions in R for nearly all basic statistical analyses, including mean() and sd() (standard deviation):\n\nmean(apples)\n\n[1] 4.533333\n\n\nR&gt; [1] 4.533333\n\nsd(apples)\n\n[1] 0.7505553\n\n\nR&gt; [1] 0.7505553\n\n\n\n\n\n\nVariable names\n\n\n\nIt is best not to use c as the name of a value or array. Why? What other words might not be good to use?\n\n\nOr try this:\n\nround(sd(apples), 2)\n\n[1] 0.75\n\n\nR&gt; [1] 0.75\n\n\n\n\n\n\nLab 2 (continue)\n\n\n\n\nWhat did we do above? What can you conclude from those functions?\n\n(Lab 2 continues in Lab 2b.)\n\n\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters app and then the Tab key. What happens?\nThe code completion feature also provides brief in-line help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl+Up to review the list (Cmd+Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane (see below).\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source pane using the buttons at the top-right or by double-clicking the title bar)\nEnvironment and History Panes 🅒\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values and functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\n[1] \"a\"          \"age\"        \"apples\"     \"b\"          \"d\"         \n[6] \"mass\"       \"mass_index\"\n\n\nR&gt; [1] \"a\"        \"apples\"   \"b\"        \"pkgs_lst\" \"url\"\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?\nFiles, Plots, Packages, Help, and Viewer Panes 🅓\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where graphics you create will appear. The Packages tab shows you the packages that are installed and those that can be installed (more on this just now). The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console.\nMethods of getting help from the Console include…\n\n?mean\n\n…or:\n\nhelp(mean)\n\nWe will go into this in more detail in the next session.\nTo reproduced Figure 4 in the Plot tab, simply copy and paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.01)\ny &lt;- 2 * sin(2 * pi * (x - 1/4))\nggplot() +\n  geom_point(aes(x = x, y = y), shape = 21, col = \"indianred\", fill = \"steelblue\")+\n  theme_linedraw()\n\n\n\n\n\n\n\nFigure 4: An easy figure.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lec-06-unified-ecology.html",
    "href": "BDC334/Lec-06-unified-ecology.html",
    "title": "Lecture 6: Unified Ecology",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.\nUnivariate diversity measures such as Simpson and Shannon diversity have already been prepared from species tables, and we have also calculated measures of \\(\\beta\\)-diversity that looked at pairwise comparisons and offered insight into community structure across a landscape and hinted at the processes that might have resulted in these structures. These ways of expressing biodiversity only gets us so far in understanding the structure of communities. A much deeper insight into the processes responsible for community formation can be obtained by looking at how the species patterns are distributed across sites. This is the focus of this lecture.\nLet’s shine the spotlight to additional views on ecological structures and the ecological processes that structure the communities—sometimes we will see reference to ‘community or species formation processes’ to offer mechanistic views on how species come to be arranged into communities (the aforementioned turnover and nestedness-resultant \\(\\beta\\)-diversity are examples of other formation processes). Let’s develop views that are based on all the information contained in the species tables, i.e. abundance, the number of sites, and the diversity of the biota. This deeper view is not necessarily captured if we limit our toolkit to the various univariate and pairwise descriptors of biodiversity.\nYou will already be familiar with the paper by Shade et al. (2018). Several kinds of ecological patterns are mentioned in the paper, and they can be derived from a species table with abundance data (but not presence-absence data!). The patterns that can be derived from such a table include (see Figure 1 below), and they are as follows:",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 6: Unified Ecology"
    ]
  },
  {
    "objectID": "BDC334/Lec-06-unified-ecology.html#species-abundance-distribution",
    "href": "BDC334/Lec-06-unified-ecology.html#species-abundance-distribution",
    "title": "Lecture 6: Unified Ecology",
    "section": "Species Abundance Distribution",
    "text": "Species Abundance Distribution\nSpecies Abundance Distribution (SAD) describes how individuals are distributed among all the species within our sampled community. It tells us about the patterns of species dominance and rarity—this information relates to a more nuanced understanding of ecological dynamics, community structure, and the mechanisms driving biodiversity. SAD curves can be made for any community for which we have species lists with their abundances.\nThe first form of SAD is the one given by Shade et al. (2018), which shows the number of individuals (N) of each species in a sample. It is formed by log(N) (on y) as a function of species rank (on x), with species ranked 1 most abundant and plotted on the left and decreasing to less abundant species on the right. Matthews and Whittaker (2015) call this form of SAD a Rank Abundance Distribution (RAD) curve. The profile of this relationship can be variable, but in general it shows that only a few species attain a high abundance while the majority of them are rare. This is a typical pattern in most communities and is often referred to as a log-normal distribution, but some other models can also be used to describe these SADs. The type of model applied to a SAD curve may reveal different ecological processes and mechanisms. Matthews and Whittaker (2015) argue that the form of the SAD and the model that describes its form can be used to develop suitable ecosystem health assessment insights and develop applicable conservation and management strategies.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 6: Unified Ecology"
    ]
  },
  {
    "objectID": "BDC334/Lec-03-gradients.html",
    "href": "BDC334/Lec-03-gradients.html",
    "title": "Lecture 3. Ecological Gradients",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 3. Ecological Gradients"
    ]
  },
  {
    "objectID": "BDC334/Lec-03-gradients.html#ceonoclines-ceonoplanes-and-ceonospaces",
    "href": "BDC334/Lec-03-gradients.html#ceonoclines-ceonoplanes-and-ceonospaces",
    "title": "Lecture 3. Ecological Gradients",
    "section": "Ceonoclines, Ceonoplanes, and Ceonospaces",
    "text": "Ceonoclines, Ceonoplanes, and Ceonospaces\nOkay, a question about coenoclines. Before I explain, as I said in earlier lectures, to best help you I need to understand what steps you have already taken and where exactly you’re still struggling. Please, when you pose a question, indicate specifically what you’ve attempted and where you’re getting lost.\nOne student says they’ve not read the specific article for now but, while reviewing topic two, could not find the corresponding figures described in the material—especially those about when the ‘core inner space’ is greater or less than two, which caused confusion.\nLet me address this by first clarifying what a coenocline is. Typically, a coenocline is a visual, simplified representation of how a given species responds to a single environmental gradient. For instance, as you move across South Africa from east to west, rainfall typically decreases: there’s more rain in the east than in the west. This gradient in rainfall is one example, and species are optimally distributed at some point along the gradient—where rainfall best fits their physiological needs.\nHowever, rainfall isn’t the only gradient influencing species distributions. Soil chemistry and physics, temperature fluctuations, atmospheric heat, and many other gradients also change simultaneously across a landscape. While a coenocline explains the distribution of a species along one gradient, real landscapes are far more complex: there might be ten, twenty, even forty gradients at play, all influencing species distributions at once.\nA coenocline can be expanded to account for two or more gradients, and then we call it a ceonoplane. When even more gradients are considered, we refer to it as a ceonospace. The ceonospace defines a position in the landscape, specified by multiple interacting gradients, in which species are optimally distributed according to all their physiological tolerances. These are just modelling tools—quantitative ecology uses them to understand and predict distributions of individual species and community structures across landscapes.\nFor those considering Honours, we shall dive much deeper into these concepts, particularly quantitative ways of understanding community structure.\nEssentially, what I want you to understand about coenoclines, ceonoplanes, and ceonospaces is that they allow us to model how multiple co-varying environmental variables sort and distribute species. Typically, species exhibit a unimodal distribution—their abundance peaks at the environmental conditions that most closely match their physiological optimum. Away from this ‘sweet spot’ (not a scientific term!), their abundance declines as conditions become less suitable.\nImagine a landscape gradient ranging from hot to cold. A species might be most abundant where temperatures align with its tolerance. But at every spot along that gradient, multiple factors—temperature, humidity, soil conditions—are also varying. Each species in the landscape responds similarly, preferring their own set of environmental optima, and this interplay shapes the overall vegetation and animal community structures that we observe.\nSo, to summarise: these tools—coenocline, ceonoplane, ceonospace—help us model, using mathematics or quantitative methods, the distribution of species and communities against the complexity of environmental gradients. Their use forms a core framework of how we understand community ecology.\nIf you’re still unclear on any aspect, please do reflect on this answer. I will post the video of this session again for you to review. Listen to the explanation carefully, and if you get stuck, rephrase your question at the point where my explanation loses you, so I can pinpoint precisely where to build further understanding.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 3. Ecological Gradients"
    ]
  },
  {
    "objectID": "BDC334/Lec-03-gradients.html#environmental-gradients-in-south-africa",
    "href": "BDC334/Lec-03-gradients.html#environmental-gradients-in-south-africa",
    "title": "Lecture 3. Ecological Gradients",
    "section": "Environmental Gradients in South Africa",
    "text": "Environmental Gradients in South Africa\nA student mentions that it gets drier from east to west across South Africa. Yes, this is the case: the eastern side of South Africa is adjacent to the warm Agulhas Current, which transports warm tropical or subtropical water down the coast into the higher latitudes. As this warm current flows past, evaporation adds heat and moisture to the atmosphere. This, in turn, brings rainfall to the adjacent land. That is why the eastern coast is so wet—characterised by tropical and subtropical vegetation, an abundance of rivers, and nutrient-rich soils leading to high productivity.\nAs you move westward into the centre of the country and then towards the west coast, the influence of the Agulhas Current diminishes. There is less moisture, less rainfall, fewer rivers, drier soil, and lower humidity—altogether favouring a different suite of plant and animal adaptations. By the time you reach the west, rainfall drops below \\(400\\) mm/year, and only species adapted to very dry conditions are present. In KwaZulu-Natal, by contrast, you may get as much as \\(1,200\\) mm/year, or thereabouts.\nThe two major currents on the country’s east and west coasts bring different amounts of moisture into the atmosphere, exerting a strong influence on the environmental gradients across the region, which in turn mould distinctive ecological communities.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 3. Ecological Gradients"
    ]
  },
  {
    "objectID": "BDC334/Lec-03-gradients.html#ocean-currents-and-regional-variation",
    "href": "BDC334/Lec-03-gradients.html#ocean-currents-and-regional-variation",
    "title": "Lecture 3. Ecological Gradients",
    "section": "Ocean Currents and Regional Variation",
    "text": "Ocean Currents and Regional Variation\nAnother question: “Do the two major currents mix at Cape Town?” They don’t exactly mix at Cape Town itself, but rather in the region between Cape Point and Cape Agulhas—a stretch of coastline approximately \\(300\\) km long. There, the Indian and Atlantic oceans influence each other, resulting in a transition zone in both marine and terrestrial vegetation. The biological communities in this area reflect a blend of species from the comparatively warmer east coast and the colder west coast.\nI authored a paper in 2017 entitled “Seaweeds in Two Oceans”, which is part of your required reading, explaining precisely how and why these oceanic influences can be measured and how they shape biogeography. The area between Cape Agulhas (the southernmost tip of Africa) and Cape Point is where this mixing creates a transition—biogeographically, it marks the boundary between the Benguela and Agulhas marine provinces, each hosting distinct communities but with a measurable zone of overlap.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 3. Ecological Gradients"
    ]
  },
  {
    "objectID": "BDC334/Lec-03-gradients.html#atmospheric-pressure-weather-and-climate",
    "href": "BDC334/Lec-03-gradients.html#atmospheric-pressure-weather-and-climate",
    "title": "Lecture 3. Ecological Gradients",
    "section": "Atmospheric Pressure, Weather, and Climate",
    "text": "Atmospheric Pressure, Weather, and Climate\nAnother student asked whether humid conditions in the Western Cape are a consequence of the Agulhas Current. In short, not really—not on short timescales. While ocean currents set the broader climatic context and have significant influences over months and years, the day-to-day weather we feel (e.g., changes in humidity and temperature) is primarily due to changes in atmospheric pressure systems. The ocean’s heat content changes slowly due to its high heat capacity, so it exerts a steady but slow influence.\nDay-to-day weather variations are mostly driven by atmospheric fronts and systems. In the Western Cape, rain typically results from low-pressure systems in the southeast Atlantic south of South Africa, not directly from the Agulhas Current. The influence of the Agulhas Current is strongest on the east coast; by the time the current rounds the Agulhas Bank, most of its heat and moisture have already been released.\nLonger-term shifts—over years or decades, such as those driven by El Niño or the displacement of large-scale atmospheric systems—do ultimately tie back to oceanic cycles, but for weather on the scale of days, it’s mostly atmospheric.\nFor those interested in looking for longer-term patterns, analyses of sea temperature and atmospheric pressure in the Western Cape reveal subtle cycles up to \\(18\\) years long, which influence both weather and biological communities, such as shifts in vegetation or fire frequency. However, these are subtle, and are not generally perceived on short timescales without data analysis.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 3. Ecological Gradients"
    ]
  },
  {
    "objectID": "BDC334/Lec-03-gradients.html#recap-of-key-points",
    "href": "BDC334/Lec-03-gradients.html#recap-of-key-points",
    "title": "Lecture 3. Ecological Gradients",
    "section": "Recap of Key Points",
    "text": "Recap of Key Points\nThe key point for you to remember in this module is that environmental gradients—across rainfall, temperature, soil, and other variables—imprint themselves on the structure of biological communities. These gradients are frequently determined by major influences such as ocean currents, but it is the sum of these factors, and their interactions, that create the distinctive assemblages of species we see across landscapes.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lecture 3. Ecological Gradients"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html",
    "href": "BDC334/Lab-01-introduction.html",
    "title": "Lab 1. Ecological Data",
    "section": "",
    "text": "BCB743\n\n\n\nThis material must be reviewed by BCB743 students in Week 1 of Quantitative Ecology.\nStuff",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html#about-macroecology",
    "href": "BDC334/Lab-01-introduction.html#about-macroecology",
    "title": "Lab 1. Ecological Data",
    "section": "About Macroecology",
    "text": "About Macroecology\nThis course is about community ecology across different spatial and temporal scales. Community ecology underpins the vast fields of biodiversity and biogeography and concerns spatial scales from square meters to all of Earth. We can look at historical, contemporary, and future processes implicated in shaping the distribution of life on our planet.\nEcologists tend to analyse how multiple environmental factors act as drivers that influence the distribution of tens or hundreds of species. These data often are messy and statistical considerations need to be understood within the context of the available data.\nUp to 20 years ago, ecologists focused on populations (the dynamics of individuals of one species interacting among each other and with their environment) and communities (collections of multiple populations, how they interact with each other and their environment, and how this affects the structure and dynamics of ecosystems). This is a modern development of ecology. But ecologists have expanded their horizon regarding the questions they now seek answers for. Today, macroecology offers a broadened view of ecology. Macroecologists seek to find the geographical patterns and processes in biodiversity across all spatial scales, from local to global, across time scales from years to millennia, and across all taxonomic hierarchies (from genetic variability within species up to major higher-level taxa, such as families and orders). It attempts to arrive at a unifying theory for ecology across all of these scales — e.g., one that can explain all patterns in structure and functioning from microbes to blue whales. Perhaps most importantly, it attempts to offer mechanistic explanations for these patterns. At the heart of all ecological answers are also deep insights stemming from understanding evolution (facilitated by the growth of phylogenetic datasets — see below).\nOn a basic data analytical level, population ecology, community ecology, and macroecology all share the same approach regarding the underlying data. We start with data representing the species and the associated environmental conditions at a selection of sites (called species tables and environmental tables). The species tables are then converted to dissimilarity matrices and the environmental tables to distance matrices. From here, basic analyses can offer insights into how biodiversity is structured, e.g., species-abundance distributions, occupancy-abundance curves, species-area curves, distance decay curves, and gradient analyses (as seen in Shade et al. 2018). In the Labs, we will explore some of these properties.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html#ecological-data",
    "href": "BDC334/Lab-01-introduction.html#ecological-data",
    "title": "Lab 1. Ecological Data",
    "section": "Ecological Data",
    "text": "Ecological Data\nProperties of Ecological Datasets\nEcological data capture properties of the environment and properties of communities. They are typically stored as separate datasets, but they are analysed together.\nThese data sets are usually arranged in a matrix. In the case of community composition, a matrix has species (or higher level taxa whose resolution depends on the research question) arranged down columns and samples (typically the sites, stations, transects, time, plots, etc.) along rows. We call this a sites × species table. In the case of environmental data, a matrix is a site × environment table. The term ‘sample’ denotes the basic unit of observation. Samples on a map may be quadrats, transects, stations, locations, traps, seine net tows, trawls, grids cells, etc. It is essential to be unambiguous about the basic unit of the samples.\nThe Doubs River Data\nAn obvious example of environmental and species datasets is the Doubs River dataset. Please refer to David Zelený’s website for an explanation of these data. The primary publication outlining this study is Verneaux (1973), and an example analysis is provided by Borcard et al. (2011). These data demonstrate how one of the basic mechanisms of biodiversity patterning — gradients — can be seen operating in a real-world case study. It offers keen insight also into the properties of species and environmental tables and the dissimilarity and distance matrices derived from them.\nLooking at the Files’ Content\nThese data are available in CSV format, but we can open and view it in MS Excel. ‘CSV’ means comma separated value. It is a plain text file that can be edited in any text editor (such as Notepad on MS Windows, or VS Code, VIM, emacs, etc. on all platforms). Figure 1 shows what a CSV file looks like in a plain text editor, VS Code, on my computer. Once imported, it will look similar to the one seen in Figure 3.\n\n\n\n\n\nFigure 1: View of a CSV file inside VS Code.\n\n\n\n\n\n\n\n\nNote About CSV Files and MS Excel\n\n\n\nCSV is a standard format used in the scientific disciplines as it is compatible with many software. Globally, scientists use a period ‘.’ as a decimal point separator. You can see this in the file above. Commas are used exclusively as field separators (you’ll see separate columns once opened in MS Excel).\nCSV files create a bit of a problem for South Africans, who are indoctrinated from a young age to use commas as a decimal point separators — this is to conform with the regional (South African) expectation that dictates commas be used as decimals. So, when you import a CSV file for the first time, you’ll likely see gibberish because your computer will probably be set up to honour the regional (locale) the expectation of commas as decimal points (and ‘R’ for currency, metric units of measurements, etc.). So, you need to know how to fix this to prevent upsetting me (it is a pet peeve and frustrates me endlessly) and yourselves.\nFixing this annoyance is not too tricky, as is demonstrated here. Follow the instruction under ‘Changing commas to decimals and vice versa by changing Excel Options’. Better still, change the global system settings, as the same article explains. Do this before importing the CSV file.\n\n\nAfter importing the Doubs River data, we see something that resembles the following two figures. First, in DoubsSpe.csv, we see the table (or spreadsheet) view of the species data. The species codes for 27 species of fish appear as column headers (not all species’ data are visible as the data are truncated to the right) and in rows 2 through 31 (30 rows) are each of the samples — in this case, there is one sample per site down the length of the river (Figure 2).\n\n\n\n\n\nFigure 2: The Doubs River species data seen in MS Excel.\n\n\nDoubsEnv.csv contains the environmental data, as seen in the following figure. The names of the 11 environmental variables appear as column headers, and there are 30 rows, one for each of the samples — the samples match that of the species data (Figure 3).\n\n\n\n\n\nFigure 3: The Doubs River environmental data in MS Excel.\n\n\nSpecies data may be recorded as various kinds of measurements, such as presence/absence data, biomass, frequency, or abundance. ‘Presence/absence’ of species simply tells us the species is there or is not there. It is binary. ‘Abundance’ generally refers to the the number of individuals per unit of area, volume. ‘Per cent cover’ refers to the proportion of a covered by a species. Per cent cover is used for vegetation, some encrusting species of animals (e.g., sponges), or organisms such as oysters or mussels that can be too numerous to count but whose abundance can be estimated as filling a portion of a sampling unit such as a quadrat. ‘Biomass’ refers to the species’ mass per unit of area or volume. The type of measure will depend on the taxa and the questions under consideration. The critical thing to note is that all species have to be homogeneous in terms of the metric used to quantify them (i.e., all of it as presence/absence, or abundance, or biomass, not mixtures of them). The matrix’s row vectors are the species composition for the corresponding sample. That is to say, a row runs across multiple columns, which tells us that the sample is comprised of all the species whose names are given by the column titles. Note that in the case of the data in the above figures, it is often the case that there are 0s, meaning that not all species are present at all sites. Species composition is frequently expressed in relative abundance, i.e. constrained to a constant total such as 1 or 100%, or biomass, where the upper limit might be arbitrary.\nThe environmental data may be heterogeneous, i.e. the units of measure may differ among the variables. For example, pH has no units, the concentration of some nutrients has a unit of (typically) μM, elevation may be in meters, etc. Because these units have different magnitudes and ranges, we may need to standardise them. To standardise data, we subtract the mean of each column from each data point in the column and then divide each of the resultant values by the standard deviation of the columns.\n\n\n\n\n\n\nLab 1\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\n1.a) Calculate the mean and SD for each variable (column) of the “raw” data. Explain.\n1.b) Standardise the Doubs River environmental data in MS Excel.\n1.c) Calculate the mean and SD for each standardised variable (column). Explain.\n\n\n\nProperties of Species Datasets\nMany community data matrices share some general characteristics:\n\nMost species occur only infrequently. The majority of species might typically be represented at only a few locations (where they might be pretty abundant). Or some species are simply rare in the sampled region (i.e. when they are present, they are present at a very low abundance). This results in sparse matrices where the bulk of the entries consists of zeros.\nEcologists tend to sample a multitude of factors that they think influence species composition, so the matching environmental data set will also have multiple (10s) columns that will be assessed in various hypotheses about the drivers of species patterning across the landscape. For example, fynbos biomass may be influenced by the fire regime, elevation, aspect, soil moisture, soil chemistry, edaphic features, etc. These datasets are called multi-dimensional matrices, with the ‘dimensions’ referring to the many species or environmental variables.\nEven though we may capture a multitude of information about many environmental factors, the number of important ones is generally relatively low — i.e. a few factors can explain the majority of the explainable variation, and it is our intention to find out which of them is most important.\nMuch of the signal may be spurious, i.e. the matrices have high noise. Variability is a general characteristic of the data, which may result in emerging false patterns. This is because sampling may capture a considerable amount of stochasticity that may mask the actual pattern of interest. Imaginative and creative sampling may reveal some of the ecological patterns we are after, but this requires long years of experience and is not something that can easily be taught as part of our module.\nThere is a significant amount of collinearity. This means that many correlated explanatory variables can explain patterning, but only a few act in a way that implies causation. Collinearity is something we will return to later on.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html#sec-gradients",
    "href": "BDC334/Lab-01-introduction.html#sec-gradients",
    "title": "Lab 1. Ecological Data",
    "section": "Ecological Gradients",
    "text": "Ecological Gradients\nAlthough there are many ways in which species can respond to their environment, one of the most striking responses can be seen along with environmental gradients. Next, we will explore this concept by discussing coenoclines and unimodal species distribution models.\nThe Unimodal Model\nThe unimodal model is an idealised species response curve (visualised as a coenocline) where a species has only one mode of abundance. In this species response curve, the species has one optimal environmental condition where it is most abundant (the fewest ecophysiological and ecological stressors). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance. The unimodal model offers a convenient heuristic tool for understanding how species can become structured along environmental gradients.\nCoenoclines, Coenoplanes, and Coenospaces\nA coenocline is a graphical display of all species response curves (see definition below) simultaneously along one environmental gradient. This is a useful way to display the arrangement of species’ fundamental niches along gradients. It aids our understanding of the species response curve if we imagine the gradient operating in only one geographical direction. The coenoplane concept extends the coenocline to cover two gradients. Again, our visual representation can be facilitated if the two gradients are visualised orthogonal (in this case, at right angles) to each other (e.g., east-west and north-south) and do not interact. A coenospace complicates the model substantially, as it can allow for an unspecified number of gradients to operate simultaneously on multiple species simultaneously. It will probably also capture interactions of environmental drivers on the species.\n\nlibrary(coenocliner)\nset.seed(2)\nM &lt;- 20                                    # number of species\nming &lt;- 3.5                                # gradient minimum...\nmaxg &lt;- 7                                  # ...and maximum\nlocs &lt;- seq(ming, maxg, length = 100)      # gradient locations\nopt  &lt;- runif(M, min = ming, max = maxg)   # species optima\ntol  &lt;- rep(0.25, M)                       # species tolerances\nh    &lt;- ceiling(rlnorm(M, meanlog = 3))    # max abundances\npars &lt;- cbind(opt = opt, tol = tol, h = h) # put in a matrix\n\nmu &lt;- coenocline(locs, responseModel = \"gaussian\", params = pars,\n                 expectation = TRUE)\n\nmatplot(locs, mu, lty = \"solid\", type = \"l\", xlab = \"pH\", ylab = \"Abundance\")\n\n\n\n\n\n\nFigure 4: A coenocline.\n\n\n\n\nAbove is an example of a coenocline using simulated species data. It demonstrates an important idea: that of unimodal species distributions (Figure 4).\n\nset.seed(10)\nN &lt;- 30                                       # number of samples\nM &lt;- 20                                       # number of species\n## First gradient\nming1 &lt;- 3.5                                  # 1st gradient minimum...\nmaxg1 &lt;- 7                                    # ...and maximum\nloc1 &lt;- seq(ming1, maxg1, length = N)         # 1st gradient locations\nopt1 &lt;- runif(M, min = ming1, max = maxg1)    # species optima\ntol1 &lt;- rep(0.5, M)                           # species tolerances\nh    &lt;- ceiling(rlnorm(M, meanlog = 3))       # max abundances\npar1 &lt;- cbind(opt = opt1, tol = tol1, h = h)  # put in a matrix\n## Second gradient\nming2 &lt;- 1                                    # 2nd gradient minimum...\nmaxg2 &lt;- 100                                  # ...and maximum\nloc2 &lt;- seq(ming2, maxg2, length = N)         # 2nd gradient locations\nopt2 &lt;- runif(M, min = ming2, max = maxg2)    # species optima\ntol2 &lt;- ceiling(runif(M, min = 5, max = 50))  # species tolerances\npar2 &lt;- cbind(opt = opt2, tol = tol2)         # put in a matrix\n## Last steps...\npars &lt;- list(px = par1, py = par2)            # put parameters into a list\nlocs &lt;- expand.grid(x = loc1, y = loc2)       # put gradient locations together\n\nmu2d &lt;- coenocline(locs, responseModel = \"gaussian\",\n                   params = pars, extraParams = list(corr = 0.5),\n                   expectation = TRUE)\n\nlayout(matrix(1:4, ncol = 2))\nop &lt;- par(mar = rep(1, 4))\nfor (i in c(2,8,13,19)) {\n  persp(loc1, loc2, matrix(mu2d[, i], ncol = length(loc2)),\n        ticktype = \"detailed\", zlab = \"Abundance\",\n        theta = 45, phi = 30)\n}\n\n\n\n\n\n\nFigure 5: A smoothed coenoplane.\n\n\n\n\n\nsim2d &lt;- coenocline(locs, responseModel = \"gaussian\",\n                    params = pars, extraParams = list(corr = 0.5),\n                    countModel = \"negbin\", countParams = list(alpha = 1))\n\nlayout(matrix(1:4, ncol = 2))\nop &lt;- par(mar = rep(1, 4))\nfor (i in c(2,8,13,19)) {\n  persp(loc1, loc2, matrix(sim2d[, i], ncol = length(loc2)),\n        ticktype = \"detailed\", zlab = \"Abundance\",\n        theta = 45, phi = 30)\n}\n\n\n\n\n\n\nFigure 6: A ‘raw’ coenoplane.\n\n\n\n\nA coenoplane is demonstrated above (Figure 5). We see idealised surfaces (smooth models), and the ‘raw’ species counts are obscured. Plotting the actual count data looks messier (Figure 6) because the measured data are not only a reflection of the underlying species response according to the unimodal model (and hence the fundamental niche), but also of the biotic processes that result in the realised niche, and the stochastic processes that generate some ‘noise’ seen in the data.\nSpecies response curves\nPlotting the abundance of a species as a function of position along a the gradient is called a species response curve. If a long enough the gradient is sampled, a species typically has a unimodal response (one peak resembling a Gaussian distribution) to the gradient. Although the idealised Gaussian response is desired (for statistical purposes, largely), in nature, the curve might deviate quite noticeably from what’s considered ideal. It is probable that a perfectly normal species distribution along a gradient can only be expected when the gradient is perfectly linear in magnitude (seldom true in nature), operates along only one geographical direction (unlikely), and all other potentially additive environmental influences are constant across the ecological (coeno-) space (also not a realistic expectation). Very importantly, also, the species response curve is not a direct measure of the species’ fundamental niche, but rather a reflection of the species’ realised niche.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html#exploring-the-data",
    "href": "BDC334/Lab-01-introduction.html#exploring-the-data",
    "title": "Lab 1. Ecological Data",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nAt the start of the analysis, before we go deeper into the patterns in the data, we need to explore the data and compute the various synthetic descriptors. This might involve calculating means and standard deviations for some of the variables we feel are most important. So, we say that we produce univariate summaries, and if there is a need we may also create some graphical summaries like line plots or frequency histograms. Be guided by the research questions as to what is required. Typically, I don’t like to produce too many detailed inferential statistics of the multivariate data considered one variable at a time (there are special statistical techniques available that allow us to do so more efficiently and effectively, but we will get to it in the Honours Module Quantitative Ecology), choosing instead to see which relationships and patterns emerge from the exploratory summary plots before testing their statistical significance using multivariate approaches. But that is me. Sometimes, some hypotheses call for a few univariate inferential analyses (again, this is the topic of an Honours module on Biostatistics).\n\n\n\n\n\n\nLab 1 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nCreate an \\(x-y\\) plot of the geographical coordinates in DoubsSpa.csv.\nUsing some graphs that plot the trends of the Doubs River environmental variables along the length of the river, describe the patterns in some of the environmental variables and offer explanations for how they might be responsible for affecting species distributions down the length of the Doubs River. Which three variables do you think will be able to explain the trends in the species data?",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html#pairwise-matrices",
    "href": "BDC334/Lab-01-introduction.html#pairwise-matrices",
    "title": "Lab 1. Ecological Data",
    "section": "Pairwise Matrices",
    "text": "Pairwise Matrices\nAlthough we typically start our forays into data exploration using sites × species and sites × environment tables, the formal statistical analyses usually require pairwise association matrices. Such matrices are symmetrical (sometimes only the lower or upper triangle is displayed) square matrices (i.e. \\(n \\times n\\)). These matrices tell us how related any sample is to any other sample in our pool of samples (i.e., relatedness among rows with respect to whatever populates the columns, be they species information of environmental information).\nLet us consider various kinds of association matrices under the headings Distances, Correlations, Associations, Similarities, and Dissimilarities.\nDistances\nA frequently used distance metric in ecological and geographical studies is Euclidean distance. Euclidean distance represents the ‘ordinary straight-line’ distance between two points in Euclidean space. When working with geographical coordinates over small areas of Earth’s surface, Euclidean distance is very similar (i.e., almost directly proportional) to the actual geographical distance, making the concept intuitive to understand.\nIn its simplest form, Euclidean distance is calculated in a planar Cartesian area, which is familiar as a graph with \\(x\\)- and \\(y\\)-axes. In 2D and 3D space, it gives distances in Cartesian units between points on a plane (\\(x\\), \\(y\\)) or in volume (\\(x\\), \\(y\\), \\(z\\)). There is a linear relationship between the units in the physical realm and the units in Euclidean space, implying that short distances between pairs of points on a map or graph also represent short geographic distances on Earth.\nEuclidean distance is calculated using the Pythagorean theorem and is typically applied to standardised environmental data (not species data):\n\\[\nd(a,b) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\n\\]\nIn this formula:\n\n\n\\(a\\) and \\(b\\) are two points in Euclidean space; in terms of environmental data, \\(a\\) and \\(b\\) represent two sites.\nEach element \\(i = 1\\) through \\(n\\) in the vectors \\(a\\) and \\(b\\) represents a dimension or variable in the space. For example, if we have three environmental variables, \\(n = 3\\), and the formula calculates the Euclidean distance between the two sites in a three-dimensional space.\nThe summation \\(\\sum_{i=1}^{n}\\) goes over all dimensions from 1 to \\(n\\).\n\nEach coordinate or variable could represent different environmental factors such as temperature, depth, or light intensity (sometimes also called ‘dimensions’ of environmental space). For example, in the case of three environmental variables, the Euclidean distance would be calculated as:\n\\[\nd(a,b) = \\sqrt{(a_{\\text{temp}} - b_{\\text{temp}})^2 + (a_{\\text{depth}} - b_{\\text{depth}})^2 + (a_{\\text{light}} - b_{\\text{light}})^2}\n\\]\nIn the example dataset downloaded earlier (Euclidean_distance_demo_data_xyz.csv), we can calculate the distance between every pair of sites named a to g. The ‘raw’ data representing \\(x\\), \\(y\\) and \\(z\\) dimensions can be viewed in MS Excel, as we see in Figure 7.\n\n\n\n\n\nFigure 7: Data representing three dimensions, \\(x\\), \\(y\\), and \\(z\\).\n\n\nWe can substitute \\(x\\), \\(y\\) and \\(z\\) for environmental ‘dimensions,’ and we have a set of data that resembles what we see in Figure 8. Regardless of whether we have \\(x\\), \\(y\\) and \\(z\\) or environmental dimensions, the application of the Pythagorean Theorem is the same.\n\n\n\n\n\nFigure 8: Data representing three environmental ‘dimensions.’\n\n\nFigure 9 shows how we may calculate Euclidean distance in MS Excel using some built-in functions. The function SUMXMY2 calculates the sum of the differences of squares between two corresponding arrays. It squares each value in array x, squares the corresponding value in array y, subtracts the y-square from the x-square, and then sums all these differences. That value is then subjected to a square-root calculation using SQRT.\nTo produce the pairwise matrix, you’d have to do this for every pair of sites. As a minimum, calculate the bottom left triangle. For completeness, calculate the diagonal, which will be all zeros in this (and every!) instance. It is a tedious process, I know!\n\n\n\n\n\nFigure 9: Calculating Euclidean distance in MS Excel. The pink shaded cells are the diagonal comprised of 0s, and the blue shaded cells are the lower triangle. The upper triangle remains unshaded but will be a mirror image of the lower triangle.\n\n\nStandardisation\nYou should ensure that all your variables are standardised before applying the Euclidean distance calculations, as I have mentioned previously. This step is essential because the Euclidean distance is sensitive to the scale of the variables involved. If the variables are not standardised, those measured on larger scales may dominate the results, ultimately leading to misleading conclusions. Therefore, standardising your data enables each variable to contribute equally to the distance measure, maintaining the integrity of your subsequent analysis.\nCorrelations\nCorrelations ask whether two sets of variables, or rather, a pair of variables, exhibit any kind of relationship between them. For example, do we expect that as temperature increases, so too does humidity? In situations where an increase in temperature is associated with an increase in humidity, that is, both variables increase together, we would say that these samples are positively correlated.\nConversely, when discussing a negative correlation, we find that as one variable increases in magnitude, the variable we have paired with it demonstrates a corresponding decrease in its magnitude. In other words, there is an inverse relationship between those two variables.\nSo, we use correlations to establish how environmental variables relate across the sample sites. Therefore, a correlation performed to a sites × variable table is done between columns (variables), not rows, as in the Euclidean distance calculation, which compares the rows (sites). We do not need to standardise as one would for calculating Euclidean distances (but it will do no harm if you do). Correlation coefficients (so-called \\(r\\)-values) vary in magnitude from -1 (a perfect inverse relationship) from 0 (no relationship) to 1 (a perfect positive linear relationship).\n\n\n\n\n\nFigure 10: Calculating pairwise correlations between environmental variables in MS Excel.\n\n\nThe resultant pairwise correlation matrix shows the names of the environmental variables as both column and row names. Contrast this with what is presented as row and column names in the distance matrix (Figure 10).\nAssociations, Similarities, and Dissimilarities\nThus far, we have worked with environmental data. Associations, similarities, and dissimilarities extend the pairwise matrix to species data. We will discuss and calculate these matrices in Lab 3.\nThat’s it for this week, Folks! I’ll leave you with some lovely exercises to take you through the rest of the week.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html#summary-of-species-and-environmental-data",
    "href": "BDC334/Lab-01-introduction.html#summary-of-species-and-environmental-data",
    "title": "Lab 1. Ecological Data",
    "section": "Summary of Species and Environmental Data",
    "text": "Summary of Species and Environmental Data\nThe diagram below (Figure 11) summarises the species and environmental data tables, and what we can do with them. These tables are the starting points of many additional analyses, and we will explore some of these ecological relationships later in this module.\n\n\n\n\n\nFigure 11: Species and environmental tables and what to do with them.\n\n\n\n\n\n\n\n\nLab 1 (continue)\n\n\n\n(To be reviewed by BCB743 student but not for marks)\n\nUsing the Doubs River environmental data, calculate the lower left triangle (including the diagonal) distance matrix for every pair of sites in Sites 1, 3, 5, …, 29 (i.e. using only every second site). Explain any patterns or trends in this resultant distance matrix regarding how similar/different sites are relative to each other. Which of the graphs you came up with in Task 3 (if any) do you think are responsible for the patterns seen in the distance matrix?\nUsing the same sites as above (Question 4), calculate a pairwise correlation matrix (lower left and including the diagonal) for the Doubs River environmental data. Explain any patterns or trends in this resultant correlation matrix and offer mechanistic explanations for why these correlations might exist.\nDiscuss in detail the properties of distance and correlation matrices.\nIf you found this exercise annoying, explain why. Or if you loved it, state why. What could be done to ease your experience of the calculations?\nOkay, so how does all of this relate to macroecology? Please discuss the purpose of all of these approaches to what macroecology promises to accomplish. In your answer, also include consideration of the unimodal model (as in coenoclines, coenoplanes, and coenospaces) and its relevance to everything we aim to do here.\n\n\n\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nThe Lab 1 assignment on Ecological Data was discussed on Thursday 24 July and is due at 08:00 on Monday 28 July 2025.\nProvide a neat and thoroughly annotated MS Excel spreadsheet which outlines the graphs and all calculations and which displays the resultant distance matrix. Use separate tabs for the different questions. Written answers must be typed in an MS Word document. Please follow the formatting specifications precisely shown in the file BDC334 Example essay format.docx that was circulated at the beginning of the module. Feel free to use the file as a template.\nPlease label the MS Excel and MS Word files as follows:\n\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_1.xlsx, and\nBDC334_&lt;first_name&gt;_&lt;last_name&gt;_Lab_1.docx\n\n(the &lt; and &gt; must be omitted as they are used in the example as field indicators only).\nSubmit your appropriately named spreadsheet and MS Word documents on iKamva when ready.\nFailing to follow these instructions carefully, precisely, and thoroughly will cause you to lose marks, which could cause a significant drop in your score as formatting counts for 15% of the final mark (out of 100%).",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/Lab-01-introduction.html#references",
    "href": "BDC334/Lab-01-introduction.html#references",
    "title": "Lab 1. Ecological Data",
    "section": "References",
    "text": "References\n\n\nBorcard D, Gillet F, Legendre P, others (2011) Numerical ecology with R. Springer\n\n\nShade A, Dunn RR, Blowes SA, Keil P, Bohannan BJ, Herrmann M, Küsel K, Lennon JT, Sanders NJ, Storch D, others (2018) Macroecology to unite all life, large and small. Trends in ecology & evolution 33:731–744.\n\n\nVerneaux J (1973) Cours d’eau de Franche-Comté (Massif du Jura). Recherches écologiques sur le réseau hydrographique du Doubs.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 1. Ecological Data"
    ]
  },
  {
    "objectID": "BDC334/BDC334_index.html",
    "href": "BDC334/BDC334_index.html",
    "title": "BDC334: Biogeography & Global Ecology",
    "section": "",
    "text": "Ecosystems form the foundation of life on Earth, encompassing complex interactions between living organisms and their physical environment. This module, BDC334, will explore the fundamental concepts, characteristics, and driving forces that shape and maintain ecosystems across our planet.\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.2024,\n  author = {Smit, A. J.,},\n  title = {BDC334: {Biogeography} \\& {Global} {Ecology}},\n  date = {2024-08-02},\n  url = {http://tangledbank.netlify.app/BDC334/BDC334_index.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2024) BDC334: Biogeography & Global Ecology. http://tangledbank.netlify.app/BDC334/BDC334_index.html."
  },
  {
    "objectID": "BDC334/wikis.html",
    "href": "BDC334/wikis.html",
    "title": "The Wiki Assignment",
    "section": "",
    "text": "Wikis are collaborative web platforms that allow many users to co-create, edit, and organise content collectively. Wikis were originally made popular by Wikipedia, and have since become embedded in academia. They are based on a system of the markdown language that allows for easy formatting and linking of content. I’ll discuss a few of the benefits at the start of the assignment.\n\n\nTo access the Wiki software, you need to be logged in to iKamva. Once you are logged in, you can access the Wiki tool from the left-hand menu. I have already set up much of what you need there, and all you need to do is navigate via the ‘Macroecology Wiki’ link to the list of topics. At present they are all empty (as indicated by the ‘?’ at the end of the titles), but you can click on the title (which is the link) to create a new page. Once you have made a new page, added some text, and saved it, you can edit any of the existing pages also from under the ‘Macroecology Wiki’ link.\n\n\n\nOnce you started to contribute portions of text to the Wiki, you will be able to see the changes that you and others make to your work. The editorial changes that others make to your work will help you see how others interpret your work, and how they might improve it. You can then accept or reject these changes, and you can also make changes to the work of others. You can also ask questions about the changes that others make, and you can discuss the changes that you make to others’ work. These are a very powerful learning tools, and I hope that you will use it to its full potential.\nOnce you start editing, I will be able to see the editorial contributions you make to your peers’ Wiki pages. In fact, I will use this facility to allocate marks that will capture how thoroughly and consistently you participate in collaborative editing.\n\n\n\nDuring this project, two things are expected of you to develop a successful collaboration on your Wiki essays:\n\nthat you contribute sections of text to the Wiki, and\nthat you edit the work of others, both within your own essay topic and within that of others.\n\nThis collaborative editing will serve three purposes:\n\nit will improve the language and readability of your own and classmates’ Wiki pages,\nit will raise concerns around the factual, structural, logical, contextual, and content of your own and your colleagues’ Wiki pages, and in this way promote continual editing so that you may submit a polished product at the end of Term 3, and\nit will form the basis of the learning experience itself.\n\nThis collaborative editing will enhance your personal understanding of the breadth of topics that will form part of the Biogeography and Global Ecology theory section. It will necessitate your interaction with the reading material provided (the PDFs of important papers, and other which you will find, use, cite, and upload for others to read) at a deep level.\n\n\n\nEach of the Wiki pages that you create will be open to examination (tests and exams), so it is imperative that all of you read and comment on each other’s work. This will ensure that you have a broad understanding of the topics that will be covered in the course, and that you are able to answer questions on them in the tests and exams.\nMost practical time slots are available for this project (4 remaining weeks, 6 hours per week, and the contributions of 3 team members make for approximately 72 hours of continuous editing), so use this time fully and wisely. Articles must be well researched, referenced (if you cite facts), thorough, in-depth, and coherent, with clear aims and objectives spelt out near the start. I will post more writing tips here as we progress, but for now the various Wikipedia help pages will provide sufficient introductory material to get you started.\nPlagiarism will not be tolerated at all, and the entire group will receive 0% for their effort.\nHow to use it is self-explanatory (after you know what markdown iKamva’s Wiki uses), but I will provide a brief overview of the topics to guide your essays. You can also find help on how to use Wikis on iKamva."
  },
  {
    "objectID": "BDC334/wikis.html#about-wikis",
    "href": "BDC334/wikis.html#about-wikis",
    "title": "The Wiki Assignment",
    "section": "",
    "text": "Wikis are collaborative web platforms that allow many users to co-create, edit, and organise content collectively. Wikis were originally made popular by Wikipedia, and have since become embedded in academia. They are based on a system of the markdown language that allows for easy formatting and linking of content. I’ll discuss a few of the benefits at the start of the assignment.\n\n\nTo access the Wiki software, you need to be logged in to iKamva. Once you are logged in, you can access the Wiki tool from the left-hand menu. I have already set up much of what you need there, and all you need to do is navigate via the ‘Macroecology Wiki’ link to the list of topics. At present they are all empty (as indicated by the ‘?’ at the end of the titles), but you can click on the title (which is the link) to create a new page. Once you have made a new page, added some text, and saved it, you can edit any of the existing pages also from under the ‘Macroecology Wiki’ link.\n\n\n\nOnce you started to contribute portions of text to the Wiki, you will be able to see the changes that you and others make to your work. The editorial changes that others make to your work will help you see how others interpret your work, and how they might improve it. You can then accept or reject these changes, and you can also make changes to the work of others. You can also ask questions about the changes that others make, and you can discuss the changes that you make to others’ work. These are a very powerful learning tools, and I hope that you will use it to its full potential.\nOnce you start editing, I will be able to see the editorial contributions you make to your peers’ Wiki pages. In fact, I will use this facility to allocate marks that will capture how thoroughly and consistently you participate in collaborative editing.\n\n\n\nDuring this project, two things are expected of you to develop a successful collaboration on your Wiki essays:\n\nthat you contribute sections of text to the Wiki, and\nthat you edit the work of others, both within your own essay topic and within that of others.\n\nThis collaborative editing will serve three purposes:\n\nit will improve the language and readability of your own and classmates’ Wiki pages,\nit will raise concerns around the factual, structural, logical, contextual, and content of your own and your colleagues’ Wiki pages, and in this way promote continual editing so that you may submit a polished product at the end of Term 3, and\nit will form the basis of the learning experience itself.\n\nThis collaborative editing will enhance your personal understanding of the breadth of topics that will form part of the Biogeography and Global Ecology theory section. It will necessitate your interaction with the reading material provided (the PDFs of important papers, and other which you will find, use, cite, and upload for others to read) at a deep level.\n\n\n\nEach of the Wiki pages that you create will be open to examination (tests and exams), so it is imperative that all of you read and comment on each other’s work. This will ensure that you have a broad understanding of the topics that will be covered in the course, and that you are able to answer questions on them in the tests and exams.\nMost practical time slots are available for this project (4 remaining weeks, 6 hours per week, and the contributions of 3 team members make for approximately 72 hours of continuous editing), so use this time fully and wisely. Articles must be well researched, referenced (if you cite facts), thorough, in-depth, and coherent, with clear aims and objectives spelt out near the start. I will post more writing tips here as we progress, but for now the various Wikipedia help pages will provide sufficient introductory material to get you started.\nPlagiarism will not be tolerated at all, and the entire group will receive 0% for their effort.\nHow to use it is self-explanatory (after you know what markdown iKamva’s Wiki uses), but I will provide a brief overview of the topics to guide your essays. You can also find help on how to use Wikis on iKamva."
  },
  {
    "objectID": "BDC334/wikis.html#the-topics",
    "href": "BDC334/wikis.html#the-topics",
    "title": "The Wiki Assignment",
    "section": "The Topics",
    "text": "The Topics\nBelow are short overviews for each topic to provide guidance for your wiki essays—they have already been added as link under the ‘Macroecology Wiki’, so find them there. As soon as you have selected a Wiki topic of interest, follow the link and enter your name together with your partners’ names in the ‘Authors’ section. You can then start writing your essay. It is important to be quick in selecting topics so that others do not take the one you want. If you are not quick enough, you can always ask me to add more topics (come with some ideas).\nGroups are also welcome to seek advice from me on how to approach their topics, and I will be available to help you with your research and writing. I will also be available to help you with the editing process, and to provide feedback on your work (to a limited extent—it is yoour work, after all!).\n\nDecoupling Sustainability and Growth: A False Dichotomy or the Key to a Thriving Planet?\n\nCan we make a case for decoupling economic growth from environmental degradation? Or is our future dictated by Malthusian limits? Discuss the concept of decoupling, its feasibility, and implications for global sustainability.\n\nThe Impact of Climate Change on Southern African Species Distribution\n\nInvestigate the specific effects of climate change on species distribution within Southern Africa. Focus on vulnerable ecosystems and endemic species.\n\nClimate Change and Extreme Weather Phenomena\n\nDiscuss how climate change is linked to the increasing frequency and intensity of extreme weather events, such as hurricanes, floods, droughts, storm surges, and wildfire intensity. Delve in their ecological, economic, and social impacts.\n\nUpwelling Ecosystems as Refugia for Biodiversity Threatened by Climate Change\n\nAssess the role of upwelling zones as potential refugia that support biodiversity in the face of climate change by providing stable environmental conditions and abundant resources.\n\nEcologists’ Worldview: Do They Hold a Unique Perspective on Nature and Society?\n\nExplore the philosophical underpinnings of ecology and how they shape ecologists’ perspectives on nature, society, and the environment. How do ecologists view the world differently from other disciplines or society around us?\n\nConservation Biology: Unpacking Its Colonial Roots and Modern Implications\n\nExamine the historical context of conservation biology, its possible colonial origins, and the implications for modern conservation efforts, including the role of indigenous knowledge and community-based conservation. Is conservation biology a Western-centric field and does it have a something to offer to the Global South?\n\nUniversal Truths: Can Science Bridge Cultures and Continents?\n\nDiscuss the role of science in bridging cultural divides and fostering international collaboration to address global challenges, such as climate change and biodiversity loss. Is science in South Africa different from science in the USA?\n\nDevelopment vs. Climate Responsibility: Navigating the Global South’s Quest for Growth Amid Climate Challenges\n\nAnalyse the tensions between development aspirations and climate responsibilities in the Global South and explore the trade-offs, challenges, and opportunities for sustainable development in the face of climate change.\n\nBiogeographical Patterns in the Deep Ocean\n\nExplore the unique biogeographical patterns found in the deep ocean, highlighting the adaptations of deep-sea species and the challenges of studying these environments.\n\nThe Global Nitrogen Cycle and Consequences for Biodiversity\n\nHow have human activities altered the global nitrogen cycle? What are the impacts on ecosystems, biodiversity, and ecological processes?\n\nPleistocene Refugia: Past, Present, and Future\n\nAnalyse the concept of Pleistocene refugia and their role in shaping current biodiversity patterns. Explore how these areas may serve as refuges under future climate change scenarios.\n\nDe-extinction\n\nDiscuss the scientific, ethical, and ecological implications of de-extinction, the process of reviving extinct species, and its potential impact on conservation efforts.\n\nBioengineering for Conservation\n\nExplore how bioengineering technologies, such as genetic modification, can be used to address conservation challenges and enhance the resilience of species to environmental changes.\n\nGeoengineering to Combat Climate Change\n\nWhat potential do geoengineering strategies have to mitigate climate change? Focus on their scientific basis, feasibility, and environmental risks.\n\nAttribution of Climate Change to Human Influence\n\nInvestigate the evidence linking human activities to climate change. What methodologies are used to attribute specific climate impacts to anthropogenic causes?\n\nIndigenous Knowledge and Conservation Practices\n\nDiscuss the role of indigenous knowledge in conservation efforts, highlighting successful practices and potential conflicts with modern scientific approaches.\n\nRenewable Energy Development and Wildlife Conservation\n\nDiscuss the environmental trade-offs associated with renewable energy development, such as wind and solar power, and their impact on wildlife and ecosystems.\n\nDeep-Sea Mining\n\nExplore the emerging industry of deep-sea mining, focusing on its potential environmental impacts and the challenges of balancing resource extraction with marine conservation.\n\nEnvironmental Justice\n\nExamine the concept of environmental justice, addressing how environmental policies and practices can disproportionately affect marginalized communities and ecosystems.\n\nThe Role of Citizen Science in Biodiversity Conservation\n\nExplore the contributions of citizen science to biodiversity monitoring, research, and conservation efforts, highlighting successful projects and challenges."
  },
  {
    "objectID": "BDC334/Class_tests.html",
    "href": "BDC334/Class_tests.html",
    "title": "Class tests",
    "section": "",
    "text": "Discuss the unimodal species distribution model, and describe how this model can explain the structuring of communities along environmental gradients. In your discussion, also talk about β-diversity.\n\n\nThe unimodal model is an idealised species response curve where a species has only one mode of abundance—i.e. one locality on the landscape where conditions are optimal and it is most abundant (i.e. the fewest ecophysiological and ecological stressors are present there). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance (a lower fitness). The unimodal model in the most basic sense can be seen as a Gaussian curve, and it offers a convenient heuristic tool for understanding how species can become structured along environmental gradients. Multiple unimodal distributions are often visualised as a coenocline—a graphical display of all species response curves, which shows how a species’ fitness is affected by any one of a multitude of environmental variables e.g. pH in Figure 1.\n\n\n\nA coenocline.\n\n\n\\(\\beta\\)-diversity is a concept that describes how species assemblages (communities) measured within the ecosystem of interest vary from place to place, e.g. along the various transects or among the quadrats used to sample the ecosystem. \\(\\beta\\)-diversity can result from the gradual change in environmental characteristics along gradients. This can be clearly seen in a coenocline, where the modal centre of distribution of many species is arranged at different positions along an environmental gradient. See Figure 1. As Species A becomes less abundant when its physical distance away from the place on the landscape which is most conducive to its fitness increases, so it is replaced by Species B at a distant location where its environmental conditions are optimal. And so on with all the other species along the length of the gradient. This process is called environmental filtering, which results in a decrease in similarity as the distance between sites increases—sometimes this is called the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-1",
    "href": "BDC334/Class_tests.html#question-1",
    "title": "Class tests",
    "section": "",
    "text": "Discuss the unimodal species distribution model, and describe how this model can explain the structuring of communities along environmental gradients. In your discussion, also talk about β-diversity.\n\n\nThe unimodal model is an idealised species response curve where a species has only one mode of abundance—i.e. one locality on the landscape where conditions are optimal and it is most abundant (i.e. the fewest ecophysiological and ecological stressors are present there). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance (a lower fitness). The unimodal model in the most basic sense can be seen as a Gaussian curve, and it offers a convenient heuristic tool for understanding how species can become structured along environmental gradients. Multiple unimodal distributions are often visualised as a coenocline—a graphical display of all species response curves, which shows how a species’ fitness is affected by any one of a multitude of environmental variables e.g. pH in Figure 1.\n\n\n\nA coenocline.\n\n\n\\(\\beta\\)-diversity is a concept that describes how species assemblages (communities) measured within the ecosystem of interest vary from place to place, e.g. along the various transects or among the quadrats used to sample the ecosystem. \\(\\beta\\)-diversity can result from the gradual change in environmental characteristics along gradients. This can be clearly seen in a coenocline, where the modal centre of distribution of many species is arranged at different positions along an environmental gradient. See Figure 1. As Species A becomes less abundant when its physical distance away from the place on the landscape which is most conducive to its fitness increases, so it is replaced by Species B at a distant location where its environmental conditions are optimal. And so on with all the other species along the length of the gradient. This process is called environmental filtering, which results in a decrease in similarity as the distance between sites increases—sometimes this is called the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-2",
    "href": "BDC334/Class_tests.html#question-2",
    "title": "Class tests",
    "section": "Question 2",
    "text": "Question 2\nUsing a clear example that you can easily relate to, discuss the concept of ‘ecological infrastructure.’ In your explanation, mention other (i.e., in addition to the ‘infrastructural services’) ecological services this example ecosystem offers and any other benefits that people might derive from its existence and well-being. In your discussion, explain how the ecological infrastructure works (what it does and how) in a properly functioning ecosystem and, if people destroyed it, how we might replicate its service.\n\nAnswer\n\nWetlands as ecological infrastructure\nWhat is ecological infrastructure? Ecological infrastructure is natural ecosystems that provide services beneficial to people. These services would, in the absence of ecological infrastructure, have to be provided by engineering solutions.\nBenefits people derive from wetlands People tend to develop settlements, towns and cities in low-lying areas such as flood plains around estuaries. These areas are prone to periodic rising water levels, and recently they are also more and more being impacted by extreme floods (associated with climate change). Healthy flood plains often comprise wetlands, which are habitats occupied by dense emergent macrophytes along the edges of estuaries and flood plains. These systems can provide a buffer to rising water levels, and they may reduce the flow rate of water. People can benefit from intact wetlands as this buffer zone provides a level of protection to built structures in the vicinity of the estuaries. Wetlands also purify the water (water filtration removes excessive N, P and POM), which makes for an environment that is more supportive of good human health (fewer water borne diseases and pollutants which may be a public health concern).\nHowever, often wetlands are destroyed by dredging and then filled in to make area available for occupation by people. In such transformed systems, protection against floods and rising water levels can be provided by constructing engineered systems at great cost. Examples of such engineered systems include breakwaters and levees. These systems, however, do not provide the other services required for maintaining good water quality, and additional engineering solutions, costing yet more tax-payers money, need to be provided. Additionally, downstream natural areas on which people depend will also become increasingly impacted due to the deterioration or loss of wetlands, and engineering solutions cannot mitigate against such consequences.\nThus, ecological infrastructure provide services to people simply by virtue of being maintained in a healthy state. This economic cost of achieving this is virtually non-existent, provided people act responsibly to protect these systems.\nEcological services from wetlands Wetlands provide a complex 3D habitat that provides numerous ecological services to a host of associated fauna and flora. These biotic assemblages benefit from their association with wetlands from the feeding/foraging opportunities provided within the habitat structure, the breeding and nursery grounds wetlands provide, attachment surfaces on the wetland plants and the sediments trapped within, and shelter and hiding opportunities from predators. Wetlands also reduce the flow rate of the water passing through them, and as such the still water within is attractive to some species that are unable to tolerate faster flow rates. Typically, healthy wetlands are active in their cycling (uptake) of N and P, and as such the water quality may be better compared to surrounding areas. This is also true for decreasing the water turbidity due to their filtration services. This makes wetlands ideal environments to some species that are sensitive to pollution. &lt;Many more services are provided by the sediments in wetlands, which offer additional opportunities for enhancing biodiversity in these areas&gt;. Overall, the net effect it that it supports species diversity, i.e. higher biodiversity in landscapes with functional wetlands present compared to areas without wetlands. Higher species diversity also offer many bequest services, and offer a potential source of genetic diversity and materials to assay for important bio-active substances that might be useful to people."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-3",
    "href": "BDC334/Class_tests.html#question-3",
    "title": "Class tests",
    "section": "Question 3",
    "text": "Question 3\nDiscuss the general characteristics of species tables, environment tables, and dissimilarity and distance matrices we can derive from these tables.\n\nAnswer\n\nEnvironmental tables Environmental tables have variables down the columns (headings are the names of the env vars) and the sites run across the columns (row names are the names of the sites), with one site in a row. Different kinds of environmental variables can be contained in the table, as many as the researcher thinks is necessary to explain the patterns in the species tables. In the cells are the quantities of the various environmental variables measured at the different sites. The measurement units may differ between columns, so later, before analysis, these data must be standardised.\nSpecies tables The species tables have as many rows as the number of rows in the environmental table—so, for each site where species are recorded, there will be corresponding measurements of the environmental conditions there. Rows in a species table have the same orientation and meaning as in the environmental table. The columns, however, contain the names of the species recorded at the sites. In the cells is some quantity that reflects something about the species at the sites—it might indicate whether a species is there or not (presence/absence), its relative abundance, or biomass. The way in which the species are quantified must be the same across all columns.\nDissimilarity matrix The dissimilarity matrix is derived from the species table by calculating one of the species dissimilarity indices (Bray-Curtis, Sørensen, Jaccard, etc.). It is square and symmetrical, and the diagonals are zero because they are essentially comparing sites with themselves in respect to the kinds of species and their abundance or presence/absence there. A value of 1 would mean that the sites are completely different from each other—this would be seen in a similarity matrix, which is the inverse of a dissimilarity matrix. Each of the other cells represent the community difference between a pair of sites whose names are present as column or row headers.\nDistance matrix A distance matrix is produced from a standardised environmental table. It is square and symmetrical, and there are as many rows and columns as there are variables in the environmental table. This matrix reflects how similar/dissimilar pairs of sites are with regards to the environmental conditions present there. The interpretation of the diagonal is the same as in dissimilarity matrices."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-4",
    "href": "BDC334/Class_tests.html#question-4",
    "title": "Class tests",
    "section": "Question 4",
    "text": "Question 4\nProvide a short explanation, with examples, for what is meant by this statement:\n“Communities often seem to display very strong structural graduation relative to ‘variables’ such as altitude, latitude, and depth; however, these variables are not the actual drivers of the processes that structure communities.”\n\nAnswer\nAltitude, latitude, and depth serve to indicate the position sites on Earth’s surface. They do not have physical properties associated with them. Species cannot require altitude, latitude, or depth to sustain their physiological needs. They are merely proxy variables for other variables that can affect the physiology of the species occurring there. I am less interested in how beta-diversity (turnover, niche models, unimodal models) works, and more interested in how the proxy relationships might play out. For example, …"
  },
  {
    "objectID": "BDC334/Class_tests.html#question-5",
    "href": "BDC334/Class_tests.html#question-5",
    "title": "Class tests",
    "section": "Question 5",
    "text": "Question 5\nIt is the year 2034 and as a result of a decade of campaigning the South African Green Party has become a real contender to be the runner up behind the populist EFF, which has come into power in South Africa in 2029.\nAs the leader of the Green Party, write an Opinion Piece that outlines the ecological solutions your party has to offer for when (if) it becomes the official opposition to the current ruling party. Your party’s ecological solutions offer the promise of solving many of the socio-economic solutions that face South Africans in the 2030s.\n\nAnswer\nThis is an opinion piece and an expected answer is not available.\nIn this answer I am looking for how ecosystems’ ecological services and goods may be used for the betterment of people, the environment, and the economy. I am not looking for a listing of SA’s problems. I am not looking for way in which budgets can be better spent, or how enforcement can be improved. I am also not really looking for the implementation of renewable energy sources as wind or solar (although that will definitely be part of the solution). We know that hunger needs to be alleviated; people must be educated; we need better farming techniques; developments must be sustainable; and people’s economic freedom ensured. But how? How can we use nature’s solutions to do so?\nWe need to build into the various initiatives a reliance on the country’s natural infrastructure. We can also develop novel, fit-for-purpose ‘ecological’ infrastructure that incorporate many of the principles of natural ecosystems with the same kinds of benefits to people (e.g. roof-top gardens, integrated forming and aquaculture, etc.).\nThe essay must consider these kinds of things."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-6",
    "href": "BDC334/Class_tests.html#question-6",
    "title": "Class tests",
    "section": "Question 6",
    "text": "Question 6\nExplain in a short (1/3 page paragraph) what is meant by ‘environmental distance.’\n\nAnswer\nEnvironmental distance encompasses all the characteristics of a landscape, such as measurements of the variables temperature, water content, soil nutrient concentrations, pH, etc., in a manner that makes it possible to provide a single, integrative metric that informs the researcher how similar or different sites across the landscape are to each other. Environmental distances are typically calculated as Euclidian distances (using the Pythagorean Theorem), but others are available such as Gower’s or Manhattan Distances and can be used for specific needs. In R they can be calculated using the vegdist() function in the vegan package. The calculation results in a pairwise distance matrix, with each cell value containing the environmental distance between a pair of sites. All possible combinations of site pairs are represented in this square matrix. The larger the value between two sites—the distance—the more different sites are with respect to their environmental properties. These distances can be used as explanation for how species communities differ across the landscape, such that sites with large environmental distances between them typically develop very different ecological communities."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-7",
    "href": "BDC334/Class_tests.html#question-7",
    "title": "Class tests",
    "section": "Question 7",
    "text": "Question 7\nExplain how the data in the site-by-species matrix can be transformed into species-area curves. What are species area curves, and what explains their characteristic shape? What is the purpose of these curves?\n\nAnswer\nTaken mostly directly from the online resource.\nSpecies accumulation curves (species area relationships, SAR) try and estimate the number of unseen species. These curves can be used to predict and compare changes in diversity over increasing spatial extent. Within an ecosystem type, one would expect that more and more species would be added (accumulates) as the number of sampled sites increases (i.e. extent increases). This continues to a point where no more new species are added as the number of sampled sites continues to increase (i.e. the curve plateaus). It plateaus because if a homogeneous landscape is comprehensively sampled, there will be a point beyond which no new species will be found as we sample even more sites.\nSpecies accumulation curves, as the name suggests, works by adding (accumulation or collecting) more and more sites along \\(x\\) and counting the number of species along \\(y\\) each time a new site is added. In the community matrix (the sites × species table), we can do this by successively adding more rows to the curve (seen along the \\(x\\)-axis). In other words, we plot on \\(y\\) the number of species associated with 1 site (the site on \\(x\\)), then we plot the number of species associated with 2 sites (the sum of the number of species in Site 1 and Site 2), then the number of species in Sites 1, 2, and 3. Etc. We do this until the cumulative sum of the species in all sites has been plotted in this manner. Typically some randomisation procedure is involved (the order in which sites are added up is randomised)."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-8",
    "href": "BDC334/Class_tests.html#question-8",
    "title": "Class tests",
    "section": "Question 8",
    "text": "Question 8\nUsing South African examples, discuss the principle of distance decay of similarity in biogeography and ecology.\n\nAnswer\nTo follow tomorrow (I’m tired now)."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-9",
    "href": "BDC334/Class_tests.html#question-9",
    "title": "Class tests",
    "section": "Question 9",
    "text": "Question 9\nPlease refer to Figure 1: \n\n\n\nAn environmental distance matrix.\n\n\n\nTo graphically represent distance decay, we typically plot the data in the first column or first row (they are the same) of an environmental distance matrix. Why this row/column? What is unique about the first row/column? [3]\nHow does the information in the first row/column differ from that in the subdiagonal? [3]\nWhat information is contained in any other cell in the environmental distance matrix? [2]\nWhat values are in the blanks down the diagonal? Why are these values what they are? [2]\n\n\nAnswer\n\nLooking down the first column, the environmental distance tends to increase the further a site is from Site 1. This is because sites further away from the origin (Site 1) tend to become increasingly dissimilar in terms of their environmental conditions as a host of drivers impact on (e.g. in the Doubs data) the water quality variables—–e.g. near the terminus of the river several pollutants will have perturbed the system (flatter slopes are more conducive to polluting human developments). Typically the increasing environmental distance that develops further away from the origin can directly be attributed to a few very influential variables; again, in the Doubs data, it is the variables nitrate, ammonium, flow rate, and biological oxygen demand that primarily affect the trend in environmental distance. At the source, there are pristine conditions (low DIN and low BOD) and near the terminus sites are polluted. Similar explanations to this one can be developed for a host of environmental gradients (e.g. along the coastline of SA where there is a temperature gradient; across the country along the rainfall gradient; with altitude; with depth; etc.). Any of these can be used as examples.\nWhereas the diagonal compares a site with itself, the subdiagonal (the diagonal row just one up or down from the diagonal filled with zeroes) captures the difference in environmental conditions (environmental distance) between adjoining sites (Site 1 vs Site 2, Site 2 vs Site 3, Site 3 vs Site 4, etc.). These changes are far more gradual than along the first row or down the first column. This is because the physical distance in geographical space is quite small for sites that are positioned next to one-another, and so too will be the environmental distance. Plotting these on a graph with environmental distance on \\(y\\) and the adjacent site pairs on \\(x\\) will generally yield a flat(-ish) line.\nAny other cell simply compares any arbitrary site with any other in terms of the difference in environmental conditions between them. This environmental distance will also be (generally) quite closely related to the physical geographical distance (or altitude, depth, etc.) between the sites.\nThe ‘blanks’ are actually zeroes, which you would get if one would compare a site with itself. There is no difference between a site and itself, so hence no environmental difference between them."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-10",
    "href": "BDC334/Class_tests.html#question-10",
    "title": "Class tests",
    "section": "Question 10",
    "text": "Question 10\n\nGiven a set of environmental data (e.g. pH, temperature, light, total N concentration, conductivity), what is the first step to follow prior to calculating environmental distance? Why is this necessary? [3]\nProvide an equation for how you would accomplish this first step. [2]\nWhat is the name of the equation / procedure to follow in the calculation of ‘environmental distance’? [1]\nDescribe the principle of ‘environmental distance’. [9]\n\n\nAnswer\n\nThe first step would be to standardise the data. This is necessary because the different environmental variables are represented by different measurement scales (units). So, to prevent those with the largest magnitude (e.g. altitude, which is measured in 100s or 1000s of meters) to become the dominant ‘signal’ in the overall response when measured alongside something like temperature (10s of degrees Celsius), they have to be adjusted to comparables scales.\nStandardisation involves calculating the mean of a variable, \\(x\\), and then subtracting this mean from each observation, \\(x_{i}\\). This value is then divided by the standard deviation of \\(x\\). So, something like \\(x_{i} - \\bar{x} / \\sigma_x\\).\nTheorem of Pythagoras, or Euclidian distance.\nEnvironmental distance encompasses all the characteristics of a landscape, such as measurements of the variables temperature, water content, soil nutrient concentrations, pH, etc., in a manner that makes it possible to provide a single, integrative metric that informs the researcher how similar or different sites across the landscape are to each other. Environmental distances are typically calculated as Euclidian distances (using the Pythagorean Theorem), but others are available such as Gower’s or Manhattan Distances and can be used for specific needs. In R they can be calculated using the vegdist() function in the vegan package. The calculation results in a pairwise distance matrix, with each cell value containing the environmental distance between a pair of sites. All possible combinations of site pairs are represented in this square matrix. The larger the value between two sites—the distance—the more different sites are with respect to their environmental properties. These distances can be used as explanation for how species communities differ across the landscape, such that sites with large environmental distances between them typically develop very different ecological communities."
  },
  {
    "objectID": "BDC334/Class_tests.html#question-11",
    "href": "BDC334/Class_tests.html#question-11",
    "title": "Class tests",
    "section": "Question 11",
    "text": "Question 11\nWhat makes macroecology different from the traditional view of ecology?\n\nAnswer\nMacroecology is an all-encompassing view of ecology, which seeks to define the geographical patterns and processes in biodiversity across all spatial scales, from local to global, across time scales from years to millennia, and across all taxonomic hierarchies (from genetic variability within species, up to major higher level taxa, such as families and orders). It attempts to arrive a unifying theory for ecology across all of these scales—e.g. one that can explain all patterns in structure and functioning from microbes to blue whales. Most importantly, perhaps, is that it attempts to offer mechanistic explanations for these patterns. At the heart of all explanation is also deep insights stemming from understanding evolution (facilitated by the growth of phylogenetic datasets—see below).\nThis is a modern development of ecology, whereas up to 20 years ago the focus has been mostly on populations (the dynamics of individuals of one species interacting amongst each other and with their environment) and communities (collections of multiple populations, and how they interact with each other and their environment, and how this affects the structure and dynamics of ecosystems).\nOn a basic data analytical level, population ecology, community ecology, and macroecology all share the same approach as far as the underlying data are concerned. We start with tables of species and environmental conditions (along columns) at a selection of sites (along rows), and these are converted to distance and dissimilarity matrices. From here analyses can show insights into how biodiversity is structured, e.g. species-abundance distributions, occupancy-abundancy curves, species-area curves, distance decay curves, and gradient analyses. In the last decade, modern developments in statistical approaches have contributed towards the development of macroecology, because of the growth of hypotheses-driven multivariate statistical approaches geared to test for the presence of one or several ecological hypotheses—this was not seen in population and community ecology so much. Contributing towards the growth of macroecology and the underlying statistical approaches, the deluge of new data across vast scales has also necessitated deeper analytical development, i.e. leveraging statistical tools and also the power of modern computing infrastructure. These modern approaches are also bringing into the fold of combined computations based on species and environmental tables also data on the phylogenetic relationships amongst organisms (and hence this brings the context of evolution)."
  },
  {
    "objectID": "pages/research_grants.html",
    "href": "pages/research_grants.html",
    "title": "National and international research grants",
    "section": "",
    "text": "2020 – 2022: Belmont Forum Collaborative Research Action on Transdisciplinary Research for Ocean Sustainability: Ecological and Economic impacts of the intensification of extreme events in the Benguela Upwelling System, Principal Investigator (EXEBUS) PDF\n2019 – 2021: SANOCEAN: Factors influencing the formation, fate and transport of microplastic in marine coastal ecosystems (FORTRAN) PDF\n2019 – 2021: SANOCEAN: Blue growth opportunities in changing kelp forests (BlueConnect) PDF\n2019 – 2023: Horizon 2020: iAtlantic, led by Prof. Murray Robert, own capacity as Regional Coordinator for the SE Atlantic PDF"
  },
  {
    "objectID": "pages/research_grants.html#international-grants",
    "href": "pages/research_grants.html#international-grants",
    "title": "National and international research grants",
    "section": "",
    "text": "2020 – 2022: Belmont Forum Collaborative Research Action on Transdisciplinary Research for Ocean Sustainability: Ecological and Economic impacts of the intensification of extreme events in the Benguela Upwelling System, Principal Investigator (EXEBUS) PDF\n2019 – 2021: SANOCEAN: Factors influencing the formation, fate and transport of microplastic in marine coastal ecosystems (FORTRAN) PDF\n2019 – 2021: SANOCEAN: Blue growth opportunities in changing kelp forests (BlueConnect) PDF\n2019 – 2023: Horizon 2020: iAtlantic, led by Prof. Murray Robert, own capacity as Regional Coordinator for the SE Atlantic PDF"
  },
  {
    "objectID": "pages/research_grants.html#national-grants",
    "href": "pages/research_grants.html#national-grants",
    "title": "National and international research grants",
    "section": "National grants",
    "text": "National grants\n\n2019 – 2021: NRF Global Change Grand Challenge: Earth System Science Research Programme — Extreme Climatic Events in the Coastal Zone, Principal Investigator (ESS180920360856) PDF\n2018 – 2020: NRF Competitive Programme for Rated Researchers —Upwelling dynamics in kelp beds: implications for trophic function PDF\n2017: CHEC/CCT Joint Research Programme 2017: What can kelp loss processes and beach cast patterns tell us about the sandy beach management? PDF\n2015 – 2017: NRF Competitive Programme for Rated Researchers (CPRR) — Thermal characteristics of the South African nearshore: implications for biodiversity PDF\n2014 – 2016: NRF Competitive Programme for Rated Researchers (CPRR) — Kelps and climate change: South Africa in a global context PDF\n2014 – 2016: NRF Incentive Funding for Rated Researchers (IPRR) Grant No. IFR14020764026 PDF"
  },
  {
    "objectID": "pages/Transboundary_systems.html",
    "href": "pages/Transboundary_systems.html",
    "title": "Transboundary systems",
    "section": "",
    "text": "Transboundary systems\nTransboundary systems refer to ecosystems that span the boundaries of more than one country or jurisdiction. These can include a variety of natural resources like water bodies (rivers, lakes, aquifers), marine ecosystems, forests, wildlife habitats, and mountain ranges, among others.\nTransboundary systems pose unique challenges and opportunities for management and conservation due to their shared nature. They require cooperative management strategies, often necessitating bilateral or multilateral agreements between the countries involved. This alliance ensures the sustainable use of the shared resource, while also managing any potential conflicts that may arise due to differing national interests.\nA transboundary river system, for example, may originate in one country, flow through another, and finally discharge into the ocean in a third country. Each country might have differing needs and priorities for the river’s use—for drinking water, irrigation, hydroelectric power, etc. Coordinated management is crucial to ensure the river’s health and equitable use.\n\n\nLarge marine ecosystems\nLarge Marine Ecosystems (LMEs) are regions of the world’s oceans, encompassing coastal areas from river basins and estuaries to the seaward boundaries of continental shelves and the outer margins of the major current systems. They are characterised by their vast size—typically they are over 200,000 square kilometers—and their distinctive bathymetry,1 hydrography,2 productivity,3 and trophically dependent populations.4\n1 The underwater topography, including features like continental shelves, deep sea trenches, and seamounts.2 The physical and chemical characteristics of the water, including temperature, salinity, currents, and nutrient levels.3 The biological productivity of the area, including both primary producers like phytoplankton and the various levels of consumers in the food web.4 These are groups of species that are interconnected in the food web, including predators, prey, and competitors.The concept of LMEs was developed in the 1980s by Dr. Ken Sherman of the US National Oceanic and Atmospheric Administration (NOAA) in response to the growing need for a comprehensive, ecosystem-based approach to manage and conserve coastal and marine resources. This approach recognises that marine resources are interconnected and that effective management must consider the entire ecosystem rather than individual species or issues in isolation. As such, the LME approach was intended to bridge the gap between single-species management and broader ecosystem-based management.\nThere are 66 recognised LMEs globally, seven of which are around the African continent (Sweijd and Smit 2020), including the Benguela Current LME off South Africa, Namibia, and Angola. Each LME is unique and requires a tailored management approach, but the overarching goal is the same: to ensure the long-term sustainability and health of the world’s coastal and marine ecosystems.\n\n\nThe Benguela Current Large Marine Ecosystem\nThe LME classification system, established in the 1980s, represents a giant stride in acknowledging and managing contiguous, transboundary marine ecosystems. Among the 66 LMEs identified worldwide, the Benguela Current LME (BCLME) stands out as a pivotal Eastern Boundary Upwelling System (EBUS), a category shared only by the Humboldt Current LME, the California LME, and the Canary Current.\nThe BCLME is comprised of the southern, central northern, and northern Benguela subsystems. This marine region extends from the shoreline at the high-water mark to the countries’ Exclusive Economic Zones (EEZs). From the Cape of Good Hope, its southern and eastern border seasonally stretches as far as 27°E longitude, near Gqeberha. Northward, the boundary reaches to 5°S near Nimibe in Angola, aligning with the southern edge of the Guinea Current Large Marine Ecosystem (GCLME). This boundary definition is fundamental to the sustainable management and conservation of the BCLME, thereby fortifying the environmental, economic, and social resilience of the region.\nThe BCLME is part of a mere 3% of the world’s sea surface occupied by the four EBUS but yields nearly 40% of the global annual marine fish catch. LMEs worldwide, though only accounting for a fraction of the ocean’s surface, contribute an impressive 80% to this vital food resource.\nYet the significance of BCLME transcends its remarkable productivity. It serves as a crucial climate regulator, with its abundant biomass acting as a significant carbon sink, mitigating the effects of climate change. This critical role underscores the BCLME’s global significance, as it helps maintain our planet’s delicate climatic balance.\nThe BCLME is also a reservoir of marine biodiversity that enriches our world ecologically and economically, and the upwelling of cool, nutrient-rich water is reasoned to act as a haven for species that might be prone to ocean warming. However, like many of Earth’s natural ecosystems, the BCLME is under severe stress. It faces challenges from overfishing, pollution, and climate change impacts, leading to biodiversity loss and habitat degradation. Consequences for the people making a living from the system are already emerging.\nIn light of these challenges, the conservation and sustainable management of the BCLME is not just a regional concern—it is a global imperative and a human right. The BCLME’s importance as a climate regulator, biodiversity reservoir, and primary productivity centre demands immediate attention and action. Investing in the health of this ecosystem is, in essence, investing in the future of our planet.\nThe commitment to ensuring a sustainable future of the BCLME is embodied in a tripartite alliance between Angola, Namibia, and South Africa, the parties to the Benguela Current Convention. This boundary demarcation facilitates the deployment of a practical ecosystem management framework for this transboundary ecosystem.\n\n\nManaging transboundary marine ecosystems\nManaging transboundary marine ecosystems is complex due to the multitude of stakeholders and jurisdictions involved, as well as the inherent dynamism and complexity of marine ecosystems. However, several strategies have been identified as effective:\n\nEcosystem-Based Management (EBM): This approach aims to balance ecological, social, and economic goals in managing marine resources. It takes into account the entire ecosystem, including human activities, rather than focusing on one species or resource at a time.\nMarine Spatial Planning (MSP): MSP is a practical way to create and establish a more rational use of marine space to benefit economic, social and environmental objectives. It involves allocating and managing parts of the ocean to specific uses or activities, in a way that minimises conflict and maximises compatibility among different activities.\nCooperative Management and Governance: Transboundary ecosystems require cooperation between all nations whose waters are part of the ecosystem. This can be achieved through international treaties, conventions, or other agreements. An example of this is the Benguela Current Convention between Angola, Namibia, and South Africa.\nScience-Based Decision Making: Regular monitoring and research are crucial to understand the state of the ecosystem and the impacts of human activities. This information should be used to inform management decisions and adaptive strategies.\nStakeholder Engagement: All relevant stakeholders, including governments, industry, indigenous communities, and the public, should be involved in decision-making processes. This ensures a diversity of perspectives and promotes equitable outcomes.\nAdaptive Management: Given the dynamic nature of marine ecosystems, management strategies need to be flexible and responsive to change. This involves regular monitoring, periodic evaluations, and adjustments to management plans as needed.\nIntegrated Coastal Management (ICM): This is a process for governance and management of coastal areas. ICM aims to balance the different objectives of society - economic development, coastal livelihoods, and environmental conservation.\nPrecautionary Approach: In situations of scientific uncertainty, the precautionary approach advocates for erring on the side of caution to prevent serious or irreversible damage to the ecosystem.\n\nThese strategies require significant resources and political will, but are crucial for the sustainable management of transboundary marine ecosystems.\n\n\nTreaties and Conventions\nTreaties and Conventions are fundamental to managing transboundary issues around LMEs. Given the inherently shared nature of marine resources that traverse political boundaries, international collaboration facilitated by such agreements is vital. They provide a legal framework that encourages cooperation and coordination among nations, ensuring sustainable management and conservation of marine resources, protection of marine biodiversity, and resolution of potential conflicts. Notably, they allow for integrated management strategies that consider the ecosystem as a whole, rather than fractured approaches divided by national boundaries. Such holistic approaches are crucial for preserving the health and resilience of LMEs in the face of pressing global challenges like overfishing, pollution, and climate change.\nIn the field of international law, the terms “treaty” and “convention” are often used interchangeably. Both are agreements under international law entered into by actors in international law, namely sovereign states and international organisations. They may also be known as international agreements, protocols, covenants, or exchanges of letters, among other terms.\nHowever, sometimes subtle distinctions are made between Treaties and Conventions:\n\nTreaty: This term is often used to describe an agreement of significant importance. Treaties generally require ratification by the national government of the signing parties and usually require approval by the executive or legislative branch, depending on a country’s laws. A treaty might address a specific issue, like a peace treaty or a treaty of alliance, or it might establish long-term relationships or conditions, like a free trade treaty.\nConvention: A convention is typically a broader agreement that deals with a wide area of concern or is used to codify and develop major areas of international law. Conventions are usually open for any relevant countries to join. An example would be the United Nations Framework Convention on Climate Change (UNFCCC), which establishes a framework for addressing the issue of climate change.\n\nDespite these subtle differences, the choice of term often depends more on tradition or the preference of the parties involved than any strict legal distinction. What matters most is the content of the agreement and how it is implemented and enforced, not the label given to it.\n\n\nThe Benguela Current Convention\nThe Benguela Current Convention and the Benguela Current Commission have their roots in a shared recognition by Angola, Namibia, and South Africa of the importance of the BCLME and the need for a cooperative approach to its management. Both stem from the earlier Benguela Environment Fisheries Interaction and Training (BENEFIT) program.\nBENEFIT was launched in 1997 as a bilateral initiative between Namibia and Angola, and South Africa joined later. It promoted the sustainable utilisation of marine resources in the Benguela Current region. The program placed an emphasis on capacity building, training, and scientific research, particularly focusing on the interactions between the environment and fisheries. Except for benefiting from the training component, people were not yet recognised as an important feature of the system. However, BENEFIT was instrumental in improving the understanding of the complex Benguela ecosystem and the impacts of various human activities on it.\nRecognising the ecological and economic significance of this region, the three nations initiated a cooperative venture in 1995, funded by the Global Environment Facility (GEF), to address shared marine and coastal management issues. This led to the creation of the BCLME Programme, which operated from 2002 to 2011. The work of BENEFIT was integrated into the new program and this ensured continuity in scientific research and capacity-building efforts, and allowed the BCLME Programme to advance BENEFIT’s achievements.\nBuilding on the early achievements and lessons of the BCLME Programme, the three countries formally established the Benguela Current Commission (‘the Commission’) in 2007 as an interim arrangement. The Commission’s objective was to promote a coordinated regional approach to the long-term conservation, protection, rehabilitation, enhancement, and sustainable use of the BCLME. This was intended to provide benefits to the countries through improving the conditions of the marine environment and promoting sustainable economic development.\nThe Benguela Current Convention (‘the BCC’), on the other hand, came into existence on 18 March 2013 when it was signed by the ministers responsible for fisheries and environment from Angola, Namibia, and South Africa. This legal agreement formalised the cooperative approach that had been initiated with the establishment of the Commission. The BCC committed the countries to work together through the Commission to promote a policy of ecosystem-based management, to share information and data, to harmonise policies and laws, and to seek funding for activities that support the BCC’s objectives.\nThus, the Commission5 was established first as an interim body to coordinate the management of the BCLME, and the Convention, i.e. the BCC,6 was subsequently signed to formalise and strengthen this regional cooperation, making the Commission the implementing body for the BCC.\n5 The Benguela Current Commission (the Commission) is the organisation or body that was established to implement the provisions of the Convention6 The Benguela Current Convention (BCC) is the actual legal agreement that was signed by the governments of the three countries.The BCC reflects an ideology of shared responsibility, cooperation, and sustainable management of a transboundary marine ecosystem, the BCLME. It represents a commitment by the three coastal countries—Angola, Namibia, and South Africa—to the long-term conservation, protection, rehabilitation, enhancement, and sustainable use of this LME.\nThe BCC acknowledges the BCLME as a shared resource and emphasises the importance of regional cooperation to maintain its health and productivity. The ideology includes recognising the socio-economic and ecological importance of the region, the need to prevent and reduce environmental degradation, and the importance of basing management decisions on the best available scientific information.\nThe BCC also adopts the Ecosystem Approach to Fisheries (EAF) and Integrated Ocean Management (IOM), principles that emphasise holistic, precautionary, and adaptive management, considering ecological relationships among species and their habitats, and balancing diverse societal objectives.\nMoreover, the BCC recognises the importance of involving all stakeholders, including local communities, in the management process, reflecting an ideals of inclusivity and equitable benefit sharing. In essence, the BCC is underpinned by the principles of sustainability, shared responsibility, cooperative management, scientific knowledge, and inclusive stakeholder participation.\n\n\nOther Africa-focussed treaties and conventions\nYes, there are a number of transboundary conventions, agreements, and treaties active around Africa, including the following:\n\nNairobi Convention: Officially known as the Convention for the Protection, Management and Development of the Marine and Coastal Environment of the Western Indian Ocean, this convention involves ten countries: Comoros, France, Kenya, Madagascar, Mauritius, Mozambique, Seychelles, Somalia, Tanzania, and South Africa. Similar to the Benguela Current Convention, the Nairobi Convention provides a platform for governments, civil society and the private sector to work together for the sustainable management and use of the Western Indian Ocean.\nConvention for Cooperation in the Protection, Management and Development of the Marine and Coastal Environment of the Atlantic Coast of the West, Central and Southern Africa Region (Abidjan Convention): A comprehensive agreement among 22 African nations aimed at the protection and preservation of the marine environment and coastal areas. It is governed by the United Nations Environment Programme (UNEP) and provides a collaborative framework to address a wide range of environmental challenges, such as pollution from various sources, coastal erosion, and the conservation of biodiversity. It promotes cooperative research, monitoring, and the implementation of specific protocols, including those addressing oil spills and the establishment of protected areas, to ensure sustainable use and management of the region’s shared marine resources.\nAbuja Convention: This proposed convention is set to replace the Abidjan Convention, covering a larger geographical area and including more countries. Its main purpose is to promote regional cooperation for the protection and development of the marine and coastal environment of the Atlantic coast of West, Central and Southern Africa.\nBamako Convention: Although not specifically focused on marine environments, the Bamako Convention on the Ban of the Import into Africa and the Control of Transboundary Movement and Management of Hazardous Wastes within Africa has relevance in terms of preventing marine pollution. The convention prohibits the import of any hazardous (including radioactive) waste. The treaty also emphasizes reducing the production of hazardous waste and promoting environmentally sound management of such wastes.\nThe Joint Development Zone Treaty between Nigeria and Sao Tome and Principe: This is an agreement between the two nations to jointly develop petroleum and other resources in the maritime areas which both nations lay claim to.\nLake Chad Basin Commission (LCBC): While not marine-focused, the LCBC is a prime example of transboundary water management. It was established in 1964 by Cameroon, Chad, Niger, and Nigeria, with the Central African Republic joining later. The Commission aims to sustainably and equitably manage shared water resources and promote regional integration, peace, and security.\n\nEach of these agreements and conventions share similarities with the BCC in that they aim to foster cooperation and sustainable use of shared marine and environmental resources among the participating nations. However, they each have unique focuses and cover different geographical areas.\n\n\nComparing the BCC with the Abidjan Convention\nThe Abidjan Convention and the BCC are both concerned with the the west coast of the African continent. They share the common goal of protecting and managing marine and coastal environments, but they operate in different geographical regions and with some different focus areas. The Abidjan Convention covers the Atlantic coast of Africa, from Mauritania to South Africa, while the BCC covers the Benguela Current Large Marine Ecosystem (BCLME), which extends from South Africa to Angola. Both conventions adhere to an ecosystem-based approach to management. They acknowledge the interconnectedness of marine ecosystems and aim to manage these systems in a holistic manner. The importance of cooperation and collaboration among the member states in managing shared marine resources and addressing common environmental challenges is key to the success of both.\nThere are key differences between the two convention. The Abidjan Convention has a broader membership with 22 African countries, while the BCC only includes three countries—Angola, Namibia, and South Africa. The latter has a unique focus on the BCLME (i.e. it is designed on the idea of the LME), one of the world’s richest marine ecosystems with a high level of endemism and biodiversity. It is also particularly concerned with the effects of climate change and variability on this ecosystem. The Abidjan Convention, while also concerned with marine ecosystems and biodiversity, has a broader mandate that includes issues such as coastal erosion and marine pollution from various sources.\nThere are also differences in structure and governance. The BCC is led by a commission consisting of ministers from the three member states, while the Abidjan Convention is overseen by the United Nations Environment Programme (UNEP) and has a wider governance structure involving all member states. As such, the Abidjan Convention has established specific protocols to address issues like oil spills and protected areas. The BCC, while it does cover similar issues, does not have specific protocols but rather uses strategic action programs and other mechanisms to address these concerns. More recently, a Marine Spatial Plan has also been developed for the BCC.\n\n\nInternational examples of transboundary management of marine regions\nThere are several international treaties and conventions that aim to manage and protect transboundary marine ecosystems, similar to the Benguela Current Convention (BCC):\n\nConvention for the Protection of the Marine Environment of the North-East Atlantic (OSPAR Convention): This convention was established in 1992 and covers the north-east Atlantic. Like the BCC, it focuses on the protection and conservation of the marine environment. However, it differs in that it covers a broader geographic area and has more contracting parties, involving 15 Governments and the EU. The convention has five main strategies: Biodiversity and Ecosystems, Eutrophication, Hazardous Substances, Offshore Industry, and Radioactive Substances.\nConvention on the Protection of the Marine Environment of the Baltic Sea Area (Helsinki Convention): This convention was established in 1974 and revised in 1992. It covers the Baltic Sea area, which is bordered by nine countries. Similar to the BCC, it aims to prevent and eliminate pollution in order to promote the ecological restoration of the Baltic Sea. However, it covers a smaller geographic area and includes more specific commitments, such as banning dumping of waste from ships and aircraft.\nBarcelona Convention for the Protection of the Marine Environment and the Coastal Region of the Mediterranean: Established in 1976, this convention covers the Mediterranean Sea and its coastal areas. It involves 21 countries bordering the Mediterranean, and the European Union. Like the BCC, it focuses on the protection and sustainable development of the marine and coastal environment, but it has a greater emphasis on specific issues such as pollution from land-based sources, pollution by dumping, pollution from ships, and pollution resulting from exploration and exploitation of the continental shelf and the seabed and its subsoil.\n\nWhat makes the BCC unique is that it covers the Benguela Current Large Marine Ecosystem (BCLME), which is one of the richest marine ecosystems on earth and one of the four major eastern boundary upwelling systems. This system is of global importance for marine biodiversity and climate regulation. The BCC is the first to be based on the Large Marine Ecosystem (LME) concept of ocean governance, a concept that is endorsed by the United Nations. The BCC is also unique in its tri-national approach, involving Angola, Namibia, and South Africa, and in its comprehensive coverage of marine conservation, sustainable development, and the sharing of benefits and responsibilities among the contracting parties.\n\n\nOther notable treaties and conventions\nThe examples I provided earlier are some of the key international treaties and conventions that focus on the protection and management of transboundary marine ecosystems. However, there are other important marine conventions and agreements around the world. A few more include:\n\nRamsar Convention on Wetlands: Established in 1971, this convention provides a framework for the conservation and wise use of all wetlands, including marine systems in coastal zones, through local and national actions and international cooperation. It currently includes 171 contracting parties.\nConvention on Biological Diversity (CBD): Although this convention covers all ecosystems, its specific work on marine and coastal biodiversity is very significant. It recognises the ecological, economic, and cultural importance of marine and coastal ecosystems and aims to safeguard them through science-based management practices.\nWestern and Central Pacific Fisheries Convention (WCPFC): This convention specifically aims to conserve and manage highly migratory fish stocks across the western and central Pacific Ocean. It does this by cooperating with relevant countries and stakeholders to ensure long-term sustainability of these resources.\nAntarctic Treaty System: This includes the Antarctic Treaty and related agreements, such as the Convention for the Conservation of Antarctic Marine Living Resources. It’s unique in that it governs the entire Antarctic region, which is recognised as a natural reserve, devoted to peace and science.\nCartagena Convention: Formally known as the Convention for the Protection and Development of the Marine Environment of the Wider Caribbean Region, it aims to protect, develop and manage the Marine Environment of the Wider Caribbean Region in a sustainable way.\n\nThese, along with the ones already mentioned, are some of the many efforts globally to manage and conserve marine ecosystems. Each is unique in its focus, region, challenges, and approach to marine management. The BCC remains notable for its LME-based approach and its focus on one of the world’s most productive marine ecosystems.\n\n\n\n\n\n\n\n\nReferences\n\nSweijd N, Smit A (2020) Trends in sea surface temperature and chlorophyll-a in the seven african large marine ecosystems. Environmental Development 36:100585.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{smit,_a._j.2023,\n  author = {Smit, A. J.,},\n  title = {Transboundary Systems},\n  date = {2023-05-15},\n  url = {http://tangledbank.netlify.app/pages/Transboundary_systems.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2023) Transboundary systems. http://tangledbank.netlify.app/pages/Transboundary_systems.html.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Transboundary systems"
    ]
  },
  {
    "objectID": "pages/genAI.html",
    "href": "pages/genAI.html",
    "title": "Evolving AI and Information Literacy",
    "section": "",
    "text": "Generative AI tools like ChatGPT, Google Gemini, and Claude Sonnet are transforming teaching, learning, and assessment practices in higher education. These technologies offer innovative opportunities to enhance education, but they also present challenges that need careful consideration. AI continues to evolve rapidly, and universities must help both faculty and students understand how to use these tools ethically and effectively within their specific disciplines. This includes recognising AI’s limitations alongside its potential to improve educational outcomes.\nThe question is: How do we prepare students for a future where AI is ubiquitous? How do we ensure that students are AI literate and can use AI effectively and ethically? How do we ensure that students are not just using AI to cheat, but are using it to enhance their learning and understanding?\nThe answer is to embed AI literacy into the curriculum. This means teaching students how to:\nTo question:"
  },
  {
    "objectID": "pages/genAI.html#survey-of-employees-linkedin-and-microsoft",
    "href": "pages/genAI.html#survey-of-employees-linkedin-and-microsoft",
    "title": "Evolving AI and Information Literacy",
    "section": "Survey of employees (LinkedIn and Microsoft)…",
    "text": "Survey of employees (LinkedIn and Microsoft)…\n\n75% have adopted generative AI in the workplace (doubled in six months)\n79% of company leaders admitted they needed to adopt gen AI tools to remain competitive\nthere is an AI skills gap\nmany reports of upwards adjusting salaries for those with gen AI skills/literacy"
  },
  {
    "objectID": "pages/genAI.html#implications-of-a-rapidly-changing-workforce-on-education",
    "href": "pages/genAI.html#implications-of-a-rapidly-changing-workforce-on-education",
    "title": "Evolving AI and Information Literacy",
    "section": "Implications of a rapidly changing workforce on education",
    "text": "Implications of a rapidly changing workforce on education\n\nif the workforce has new/different expectations of AI adoption, how does this affect tertiary education?\nwe need to prepare students for life beyond graduation\nwe need to reconsider the current curriculum and pedagogical approaches\nwhere does the AI competency fit within the science (or broader university) curriculum?\nwhat does AI literacy mean? what are the actual skills?\nhow do we bring this into the range of course offerings, and, considering it evolves so quickly (beyond the rate at which we can develop and redevelop modules), how do we manage teaching and learning within this shifting landscape?"
  },
  {
    "objectID": "pages/genAI.html#the-writing-process",
    "href": "pages/genAI.html#the-writing-process",
    "title": "Evolving AI and Information Literacy",
    "section": "The writing process",
    "text": "The writing process\n\n(Grammarly Authorship)"
  },
  {
    "objectID": "pages/genAI.html#how-do-students-use-generative-ai-tools",
    "href": "pages/genAI.html#how-do-students-use-generative-ai-tools",
    "title": "Evolving AI and Information Literacy",
    "section": "How do students use generative AI tools?",
    "text": "How do students use generative AI tools?\nWe need to understand how students currently use generative AI tools for assessments. Common applications include summarising and paraphrasing lengthy readings, brainstorming ideas for assessment tasks, writing code, performing spelling and grammar checks (similar to Word or Grammarly), and creating practice questions for exam preparation.\n\nAssistance with assessments (cheating)\nResearch and writing\nCoding"
  },
  {
    "objectID": "pages/genAI.html#assessments",
    "href": "pages/genAI.html#assessments",
    "title": "Evolving AI and Information Literacy",
    "section": "Assessments",
    "text": "Assessments\n\nMCQ quizzes and questions involving recall\nGenerative AI can readily and easily produce answers to fact-based or basic questions, particularly on commonly taught subjects. Easy\n\n\nGeneric short written assignments\nGenerative AI can produce convincing broad-level responses to short written assignments, such as essays or reports. Easy"
  },
  {
    "objectID": "pages/ABNJ.html",
    "href": "pages/ABNJ.html",
    "title": "Areas Beyond National Jurisdiction",
    "section": "",
    "text": "Areas Beyond National Jurisdiction (ABNJ) refer to the parts of the world’s oceans that fall outside of any country’s Exclusive Economic Zone (EEZ). These areas make up about 64% of the surface of the ocean, and include both the High Seas (the water column beyond the EEZ) and the Area (the seabed and subsoil beyond the limits of national jurisdiction).\nUnlike waters within national jurisdictions, where a country has the exclusive right to exploit resources and is responsible for managing and protecting the marine environment, ABNJ are governed by a complex framework of international laws and agreements.\nThe primary legal framework is the United Nations Convention on the Law of the Sea (UNCLOS), which came into force in 1994. UNCLOS sets out the legal framework for the conservation and sustainable use of oceans and their resources. It establishes the rights and obligations of states in relation to the use of the oceans, and provides mechanisms for dispute resolution.\nIn terms of ABNJ, UNCLOS recognises the concept of “the common heritage of mankind,” which asserts that the resources of the deep seabed beyond national jurisdiction are the common heritage of all humanity and should be managed for the benefit of all. However, the UNCLOS does not provide a comprehensive regime for the conservation and sustainable use of marine biodiversity in ABNJ, which is a gap that current negotiations at the UN are trying to fill.\nIn addition to UNCLOS, there are a number of other international agreements that pertain to ABNJ. These include the Convention on Biological Diversity (CBD), which has developed a set of criteria for identifying ecologically or biologically significant marine areas (EBSAs) in need of protection, including in ABNJ. There’s also the International Maritime Organization (IMO), which has the authority to designate Particularly Sensitive Sea Areas (PSSAs) in need of special protection, including in ABNJ.\nThe governance of ABNJ is a complex and evolving issue, with ongoing discussions at the international level about how to improve the management and conservation of these vast and largely unregulated areas of the ocean. This includes debates over issues such as the establishment of marine protected areas in ABNJ, the regulation of emerging industries like deep-sea mining, and how to share the benefits of marine genetic resources.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{smit,_a._j.2023,\n  author = {Smit, A. J.,},\n  title = {Areas {Beyond} {National} {Jurisdiction}},\n  date = {2023-05-16},\n  url = {http://tangledbank.netlify.app/pages/ABNJ.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2023) Areas Beyond National Jurisdiction. http://tangledbank.netlify.app/pages/ABNJ.html."
  },
  {
    "objectID": "pages/kaggle_earthquakes.html",
    "href": "pages/kaggle_earthquakes.html",
    "title": "Kaggle Earthquake database",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggthemes)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthhires)\n\nHere’s a map of earthquake location and magnitude (&gt;=5.5) from 1965-2016. The data may be found on Kaggle.\n\nWGS84_proj &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\nNE_proj &lt;- \"+proj=natearth +lon_0=170\"\n\n\nquakes &lt;- read_csv(\"../data/kaggle_earthquakes_database.csv\",\n  skip = 3, col_types = cols(Date = col_date(format = \"%d/%m/%Y\")))\nquakes_sf &lt;- quakes |&gt; \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"),\n    crs = WGS84_proj)\nquakes_sf_trans &lt;- st_transform(quakes_sf, NE_proj)\nhead(quakes_sf)\n\n\n\n\nDate\nTime\nType\nDepth\nDepth Error\nDepth Seismic Stations\nMagnitude\nMagnitude Type\nMagnitude Error\nMagnitude Seismic Stations\nAzimuthal Gap\nHorizontal Distance\nHorizontal Error\nRoot Mean Square\nID\nSource\nLocation Source\nMagnitude Source\nStatus\ngeometry\n\n\n\n1965-02-01\n13:44:18\nEarthquake\n131.6\nNA\nNA\n6.0\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860706\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (145.616 19.246)\n\n\n1965-04-01\n11:29:49\nEarthquake\n80.0\nNA\nNA\n5.8\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860737\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (127.352 1.863)\n\n\n1965-05-01\n18:05:58\nEarthquake\n20.0\nNA\nNA\n6.2\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860762\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (-173.972 -20.579)\n\n\n1965-08-01\n18:49:43\nEarthquake\n15.0\nNA\nNA\n5.8\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860856\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (-23.557 -59.076)\n\n\n1965-09-01\n13:32:50\nEarthquake\n15.0\nNA\nNA\n5.8\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860890\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (126.427 11.938)\n\n\n1965-10-01\n13:36:32\nEarthquake\n35.0\nNA\nNA\n6.7\nMW\nNA\nNA\nNA\nNA\nNA\nNA\nISCGEM860922\nISCGEM\nISCGEM\nISCGEM\nAutomatic\nPOINT (166.629 -13.405)\n\n\n\n\n\n\n\nplot(quakes_sf[,\"Magnitude\"])\n\n\n\n\n\n\n\n\nworld_1 &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3) |&gt; \n  st_break_antimeridian(lon_0 = 170) |&gt; \n  st_transform(NE_proj)\n\n\nggplot() +\n  geom_sf(data = world_1, colour = \"grey60\", fill = \"grey70\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = Magnitude, size = Magnitude),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.4) +\n  scale_colour_viridis_c(option = \"mako\", direction = 1) +\n  guides(size = \"none\",\n    colour = guide_colourbar(title = \"Magnitude\",\n      title.position = \"left\")) +\n  coord_sf(expand = FALSE) +\n  labs(x = NULL, y = NULL,\n    title = \"The Kaggle Earthquake Data\",\n    subtitle = \"Significant Earthquakes, 1965-2016\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_line(colour = \"grey90\"),\n    legend.background = element_blank(),\n    legend.title = element_text(angle = 90),\n    legend.title.align = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{smit,_a._j.,\n  author = {Smit, A. J., and Smit, AJ},\n  title = {Kaggle {Earthquake} Database},\n  url = {http://tangledbank.netlify.app/pages/kaggle_earthquakes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J., Smit A Kaggle Earthquake database. http://tangledbank.netlify.app/pages/kaggle_earthquakes.html."
  },
  {
    "objectID": "pages/NRF_ratings.html",
    "href": "pages/NRF_ratings.html",
    "title": "NRF Rating: thoughts",
    "section": "",
    "text": "The South African National Research Foundation (NRF) rating system claims to evaluate and benchmark the research performance of individual researchers in the country. The system’s purpose is intended to:\n\nRecognise and reward research excellence The system acknowledges researchers who produce high-quality research and contribute significantly to their respective fields. A favourable rating is supposed to increase recognition, both nationally and internationally, as well as improve funding opportunities.\nEncourage research productivity By providing incentives and recognition for high-quality research, the NRF rating system aims to promote academic productivity and encourages continuous advancement.\nEnhance research capacity It supposedly identifies academics with solid potential and supports the development of research capacity in South Africa by providing funding and other resources to rated researchers.\nFacilitate collaboration The NRF rating system claims to facilitate scientific cooperation by enabling researchers, institutions, and funding agencies to identify potential partners based on their research expertise and performance.\nPromote international competitiveness The NRF suggests that a robust research evaluation system helps to ensure that South African researchers remain competitive on the global stage, which is essential for attracting international funding, partnerships, and talent.\nInform decision-making NRF ratings inform institutional, national, and international decision-making regarding research priorities, funding allocations, and strategic planning, ensuring that resources are directed towards high-impact research.\n\nThere are alternatives to the NRF rating system. The H-index is a globally recognised rapid assessment of research impact, of which Google offers one implementation on their Google Scholar system. This H-index is consistently applied to researchers from any country or any academic discipline. The metric is based on citation data and provides a more objective and quantitative measure of research impact. Since the H-index is easily accessible and hassle-free, it is calculated on the fly using various citation databases, such as Google Scholar. This last point contrasts starkly with the NRF rating system, which is lengthy, and requires significant effort and time from both the applicants and reviewers.\nFurther comparisons of the NRF rating system to a metric such as Google Scholar’s H-index reveal other possible advantages. The NRF’s approach is a more integrated and robust assessment of research ‘prowess’ as the system considers multiple aspects of academic contributions. This includes not only the quality, impact, and significance of research output (similar to the H-index, but differ in how these are assessed) but also a broader contribution to academics’ research fields using assessments that are not based on publications, such as participation in various international bodies, panels and working groups. This integrated assessment leads to a more nuanced evaluation of academic performance that citation metrics, such as the H-index, cannot capture. It also acknowledges academics for their role in developing research capacity, which in South Africa is a critical role that all academics must play.\nNRF ratings are determined through a rigorous peer-review process, which claims to ensure that the evaluations are fair and unbiased. However, despite the peer-review process, personal biases or conflicts of interest may still influence the ratings, and the system could be more objective. The system is also specific to South Africa, and the recognition that might stem from one’s NRF rating does not favour one as much as one would wish to think. This is true especially once international research funding becomes attractive and one is willing to enter more comprehensive international research consortia.\nIn the past, rated researchers were offered incentive funding. This system no longer exists, at least not in the format it was implemented in the early- to mid-2010s. Note, the ‘Competitive Support for Unrated Researchers (CSUR) - 2024 Funding Framework’ and ‘Competitive Programme for Rated Researchers (CPRR) – 2024 Funding Framework’ do take rating into account, but others, such as the ‘African Coelacanth Ecosystem Programme (ACEP) – South African Marine and Antarctic Research Strategy,’ do not. Similarly, international funders, where I will focus my attention in the future, also do not acknowledge NRF ratings.\nWhether or not one maintains an NRF rating depends on personal research values. This should be decided on personal conviction and not dictated by the institution within which one is employed. I have yet to experience the NRF rating system to offer me any tangible advantage regarding recognition of research excellence, encouragement of productivity, enhancement of capacity, the facilitation of collaboration, or enhanced international competitiveness. The only benefit resulting from NRF ratings goes to the employers to inform institutional decision-making.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{smit,_a._j.2023,\n  author = {Smit, A. J.,},\n  title = {NRF {Rating:} Thoughts},\n  date = {2023-04-24},\n  url = {http://tangledbank.netlify.app/pages/NRF_ratings.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. (2023) NRF Rating: thoughts. http://tangledbank.netlify.app/pages/NRF_ratings.html."
  },
  {
    "objectID": "pages/heatwaveR_publ.html",
    "href": "pages/heatwaveR_publ.html",
    "title": "Notable heatwaveR citations",
    "section": "",
    "text": "Smale, D. A., Wernberg, T., Oliver, E. C., Thomsen, M., Harvey, B. P., Straub, S. C., … & Moore, P. J. (2019). Marine heatwaves threaten global biodiversity and the provision of ecosystem services. Nature Climate Change, 9(4), 306-312.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Recent marine heatwaves in the North Pacific warming pool can be attributed to rising atmospheric levels of greenhouse gases. Communications Earth & Environment, 3(1), 131.\nThoral, F., Montie, S., Thomsen, M. S., Tait, L. W., Pinkerton, M. H., & Schiel, D. R. (2022). Unravelling seasonal trends in coastal marine heatwave metrics across global biogeographical realms. Scientific Reports, 12(1), 7740.\nBenedetti-Cecchi, L. (2021). Complex networks of marine heatwaves reveal abrupt transitions in the global ocean. Scientific Reports, 11(1), 1739.\nWoolway, R. I., Jennings, E., Shatwell, T., Golub, M., Pierson, D. C., & Maberly, S. C. (2021). Lake heatwaves under climate change. Nature, 589(7842), 402-407.\nGarcía Molinos, J., Hunt, H. L., Green, M. E., Champion, C., Hartog, J. R., & Pecl, G. T. (2022). Climate, currents and species traits contribute to early stages of marine species redistribution. Communications biology, 5(1), 1329.\nSmith, K. E., Burrows, M. T., Hobday, A. J., Sen Gupta, A., Moore, P. J., Thomsen, M., … & Smale, D. A. (2021). Socioeconomic impacts of marine heatwaves: Global issues and opportunities. Science, 374(6566), eabj3593."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-impact",
    "href": "pages/heatwaveR_publ.html#sec-impact",
    "title": "Notable heatwaveR citations",
    "section": "",
    "text": "Smale, D. A., Wernberg, T., Oliver, E. C., Thomsen, M., Harvey, B. P., Straub, S. C., … & Moore, P. J. (2019). Marine heatwaves threaten global biodiversity and the provision of ecosystem services. Nature Climate Change, 9(4), 306-312.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Recent marine heatwaves in the North Pacific warming pool can be attributed to rising atmospheric levels of greenhouse gases. Communications Earth & Environment, 3(1), 131.\nThoral, F., Montie, S., Thomsen, M. S., Tait, L. W., Pinkerton, M. H., & Schiel, D. R. (2022). Unravelling seasonal trends in coastal marine heatwave metrics across global biogeographical realms. Scientific Reports, 12(1), 7740.\nBenedetti-Cecchi, L. (2021). Complex networks of marine heatwaves reveal abrupt transitions in the global ocean. Scientific Reports, 11(1), 1739.\nWoolway, R. I., Jennings, E., Shatwell, T., Golub, M., Pierson, D. C., & Maberly, S. C. (2021). Lake heatwaves under climate change. Nature, 589(7842), 402-407.\nGarcía Molinos, J., Hunt, H. L., Green, M. E., Champion, C., Hartog, J. R., & Pecl, G. T. (2022). Climate, currents and species traits contribute to early stages of marine species redistribution. Communications biology, 5(1), 1329.\nSmith, K. E., Burrows, M. T., Hobday, A. J., Sen Gupta, A., Moore, P. J., Thomsen, M., … & Smale, D. A. (2021). Socioeconomic impacts of marine heatwaves: Global issues and opportunities. Science, 374(6566), eabj3593."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-cross",
    "href": "pages/heatwaveR_publ.html#sec-cross",
    "title": "Notable heatwaveR citations",
    "section": "Examples of cross-discipline research in marine heatwaves",
    "text": "Examples of cross-discipline research in marine heatwaves\n\nSchlegel, R. W., Oliver, E. C., & Chen, K. (2021). Drivers of marine heatwaves in the Northwest Atlantic: The role of air–sea interaction during onset and decline. Frontiers in Marine Science, 8, 627970.\nHu, L. (2021). A global assessment of coastal marine heatwaves and their relation with coastal urban thermal changes. Geophysical Research Letters, 48(9), e2021GL093260.\nBarkhordarian, A., Nielsen, D. M., & Baehr, J. (2022). Greenhouse Gas Forcing a Necessary Causation for Marine Heatwaves Over the Northeast Pacific Warming Pool."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-outside",
    "href": "pages/heatwaveR_publ.html#sec-outside",
    "title": "Notable heatwaveR citations",
    "section": "Use outside of the initially intended field of application",
    "text": "Use outside of the initially intended field of application\n\nTassone, S. J., Besterman, A. F., Buelo, C. D., Ha, D. T., Walter, J. A., & Pace, M. L. (2023). Increasing heatwave frequency in streams and rivers of the United States. Limnology and Oceanography Letters, 8(2), 295-304.\nDiniz, F. R., Gonçalves, F. L. T., & Sheridan, S. (2020). Heat wave and elderly mortality: Historical analysis and future projection for metropolitan region of São Paulo, Brazil. Atmosphere, 11(9), 933.\nWoolway, R. I., Albergel, C., Frölicher, T. L., & Perroud, M. (2022). Severe Lake Heatwaves Attributable to Human‐Induced Global Warming. Geophysical Research Letters, 49(4), e2021GL097031.\nReynaert, S., De Boeck, H. J., Verbruggen, E., Verlinden, M., Flowers, N., & Nijs, I. (2021). Risk of short‐term biodiversity loss under more persistent precipitation regimes. Global Change Biology, 27(8), 1614-1626.\nWoolway, R. I., Anderson, E. J., & Albergel, C. (2021). Rapidly expanding lake heatwaves under climate change. Environmental Research Letters, 16(9), 094013.\nPaton, E. (2022). Intermittency analysis of dry spell magnitude and timing using different spell definitions. Journal of Hydrology, 608, 127645.\nMartinez-Baroja, L., Rey-Benayas, J. M., Perez-Camacho, L., & Villar-Salvador, P. (2022). Drivers of oak establishment in Mediterranean old fields from 25-year-old woodland islets planted to assist natural regeneration. European Journal of Forest Research, 141(1), 17-30.\nPappert, D., Barriendos, M., Brugnara, Y., Imfeld, N., Jourdain, S., Przybylak, R., … & Brönnimann, S. (2022). Statistical reconstruction of daily temperature and sea level pressure in Europe for the severe winter 1788/89. Climate of the Past, 18(12), 2545-2565.\nNgoungue Langue, C. G., Lavaysse, C., Vrac, M., & Flamant, C. (2023). Heat wave monitoring over West African cities: uncertainties, characterization and recent trends. Natural Hazards and Earth System Sciences, 23(4), 1313-1333."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-policy",
    "href": "pages/heatwaveR_publ.html#sec-policy",
    "title": "Notable heatwaveR citations",
    "section": "Support of policy development around the management of marine living resources",
    "text": "Support of policy development around the management of marine living resources\n\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2022). A climate-smart spatial planning framework.\nWegscheider, B., Linnansaari, T., Monk, W., Ndong, M., Haralampides, K., St-Hilaire, A., … & Allen, R. (2020). Quantitative modelling of fish habitat under future regulated and hydro-climatically driven flow regimes in the Saint John River (New Brunswick, Canada). Quantitative modelling of existing and future fish habitat in the Saint John River, NB, Canada, 184.\nBuenafe, K. C. V., Dunn, D. C., Everett, J. D., Brito-Morales, I., Schoeman, D. S., Hanson, J. O., … & Richardson, A. J. (2023). A metric-based framework for climate-smart conservation planning. Ecological Applications, e2852.\nMuñoz-Pizza, D. M., Sanchez-Rodriguez, R. A., & Manzano, E. G. Linking Climate Change to Urban Planning Through Vulnerability Assessment: The Case of Two Cities at the Mexico-Us Border. Available at SSRN 4348277."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-novel",
    "href": "pages/heatwaveR_publ.html#sec-novel",
    "title": "Notable heatwaveR citations",
    "section": "Novel research questions and hypotheses",
    "text": "Novel research questions and hypotheses\n\nLeach, T. S., BuyanUrt, B., & Hofmann, G. E. (2021). Exploring impacts of marine heatwaves: paternal heat exposure diminishes fertilization success in the purple sea urchin (Strongylocentrotus purpuratus). Marine Biology, 168(7), 103.\nPegado, M. R., Santos, C. P., Raffoul, D., Konieczna, M., Sampaio, E., Maulvault, A. L., … & Rosa, R. (2020). Impact of a simulated marine heatwave in the hematological profile of a temperate shark (Scyliorhinus canicula). Ecological Indicators, 114, 106327.\nKraufvelin, L. (2021). Identification of marine heatwaves in the Archipelago Sea and experimental testing of their impacts on the non-indigenous Harris mud crab.\nOliveira, H., Maulvault, A. L., Santos, C. P., Silva, M., Bandarra, N. M., Valente, L. M., … & Anacleto, P. (2023). Can marine heatwaves affect the fatty acid composition and energy budget of the tropical fish Zebrasoma scopas?. Environmental Research, 224, 115504.\nLeach, T. S. (2022). The Role of Pre-and Post-Spawning Temperature Stress on Fertilization Dynamics Within Santa Barbara Channel Sea Urchin Species. University of California, Santa Barbara.\nMinuti, J. J., Byrne, M., Hemraj, D. A., & Russell, B. D. (2021). Capacity of an ecologically key urchin to recover from extreme events: Physiological impacts of heatwaves and the road to recovery. Science of the Total Environment, 785, 147281.\nClare, X. S., Kui, L., & Hofmann, G. E. (2022). Larval Thermal Tolerance of Kellet’s Whelk (Kelletia kelletii) as a Window into the Resilience of a Wild Shellfishery to Marine Heatwaves. Journal of Shellfish Research, 41(2), 283-290.\nMarochi, M. Z., De Grande, F. R., Pardo, J. C. F., Montenegro, Á., & Costa, T. M. (2022). Marine heatwave impacts on newly-hatched planktonic larvae of an estuarine crab. Estuarine, Coastal and Shelf Science, 278, 108122.\nVan Der Walt, K. A., Potts, W. M., Porri, F., Winkler, A. C., Duncan, M. I., Skeeles, M. R., & James, N. C. (2021). Marine Heatwaves Exceed Cardiac Thermal Limits of Adult Sparid Fish (Diplodus capensis, Smith 1884). Frontiers in Marine Science, 8, 702463."
  },
  {
    "objectID": "pages/heatwaveR_publ.html#sec-trackers",
    "href": "pages/heatwaveR_publ.html#sec-trackers",
    "title": "Notable heatwaveR citations",
    "section": "Online trackers of marine heatwaves",
    "text": "Online trackers of marine heatwaves\n\nThe original marine heatwave tracker\nThe Physical Sciences Laboratory heatwave website\nWhaleMap"
  },
  {
    "objectID": "BDC223/L01-worldmapper.html",
    "href": "BDC223/L01-worldmapper.html",
    "title": "Lecture 1: Worldmapper",
    "section": "",
    "text": "Content\n\n\n\n\nLimits to life in solar system.\nEarth is the only planet with life as far as we know.\nLife evolved and diversified.\nOur ancestors.\nHuman societies developed during the Holocene.\nModifcations to all life on Earth due to people’s impact.\nExceeding planetary boundaries.\nConsequences for all life on Earth, including that of plants.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#introduction-contextualising-plant-ecophysiology",
    "href": "BDC223/L01-worldmapper.html#introduction-contextualising-plant-ecophysiology",
    "title": "Lecture 1: Worldmapper",
    "section": "Introduction: Contextualising Plant Ecophysiology",
    "text": "Introduction: Contextualising Plant Ecophysiology\nRight, today we’ll get into the actual, the first portion of our real lecture content. Yesterday was an introduction to the module; today, we need to talk a bit about setting the scene within which we’ll contextualise the plant ecophysiology component of your ecophysiology module.\nGenerally, the way I like to do this is to start by giving you a brief overview of the state of the world and the place of people in it. It’s because of people that the various stresses that plants experience exist. People, because they are so numerous, exert a whole range of different influences on the planet, and I’d like to give you an overview of how that came to be.\nI’ll call this set of lectures or these slides ‘The Limits to Life’, because there are certain boundaries within which life can operate smoothly. As we move outside those boundaries, or if we exceed some of these limits, life becomes increasingly difficult for all of us, including the plants we’re mostly interested in throughout this module. The exceeding of limits is brought about by human influence on the planet.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#earths-place-in-the-solar-system",
    "href": "BDC223/L01-worldmapper.html#earths-place-in-the-solar-system",
    "title": "Lecture 1: Worldmapper",
    "section": "Earth’s Place in the Solar System",
    "text": "Earth’s Place in the Solar System\nAs you know, Earth is the third planet from the Sun, and that’s of major significance. The relevance of our position in the solar system, in between Venus and Mars, is that it creates a perfect set of conditions where everything is just right for life to exist. This is not true for Venus, which is the second planet, nor Mars, the fourth planet from the Sun [attention: Mars is the fourth planet, not the third; Earth is the third]. Venus is closer to the Sun so it’s too hot; Mars is further away and is too cold. So, just like Goldilocks, Earth is just right.\nIt’s just right because water can exist in the three phases necessary for the existence of life: as a liquid, as ice, and as vapour — clouds. Without water present in all three phases, the hydrological cycle as we know it could not operate. This sets a series of limits within which life exists easily, with the range of temperatures on the planet being an important parameter. Even though water exists as a liquid between \\(0\\,^\\circ\\mathrm{C}\\) and \\(100\\,^\\circ\\mathrm{C}\\), life is constrained to a smaller subset of that temperature range.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#shifting-limits-anthropogenic-change",
    "href": "BDC223/L01-worldmapper.html#shifting-limits-anthropogenic-change",
    "title": "Lecture 1: Worldmapper",
    "section": "Shifting Limits: Anthropogenic Change",
    "text": "Shifting Limits: Anthropogenic Change\nThis particular set of limits is shifting; it’s changing and is not constant. It hasn’t been constant forever, but now, in more recent times — at least since the Industrial Revolution in the 1700s — the rate at which those limits are changing is accelerating. That’s called climate change. I’ll talk a bit more about this later on in the module, and also on Thursday, when I see you again, as there’s an entire module focused on climate change, if I remember correctly.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#the-earth-from-space",
    "href": "BDC223/L01-worldmapper.html#the-earth-from-space",
    "title": "Lecture 1: Worldmapper",
    "section": "The Earth from Space",
    "text": "The Earth from Space\nAround 1976 [attention: The referenced “Apollo mission” was earlier; for example, Apollo 14 was in 1971], during the Apollo mission, Carl Sagan decided to turn the Apollo spacecraft around and look back at Earth. That was the first time in human history that people could actually see Earth entirely, from outside the planet itself. It became evident that Earth is quite unique, as far as our knowledge of the solar system was concerned. Looking back at Earth, in this case across the Moon’s surface, it became obvious that Earth is the only place we know of where life is able to exist.\nAs I’ve explained, that’s due to water existing as a gas, liquid, and ice, and you can see that in the image: gas in the clouds, liquid water, and at the polar regions, ice (though not always visible in the image). Australia is visible on the left, the large expanse of blue is the Pacific Ocean.\nThat moment made people realise that Earth is the only place we know of that harbours life and, because there’s only one such place, it’s actually quite fragile. We need to take care of it, and be aware that this is the only place where people can live. There is a beautiful video — I’ll send you the link later — where Carl Sagan poetically discusses the fragility of life on this planet.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#viewing-earth-as-a-system",
    "href": "BDC223/L01-worldmapper.html#viewing-earth-as-a-system",
    "title": "Lecture 1: Worldmapper",
    "section": "Viewing Earth as a System",
    "text": "Viewing Earth as a System\nThis module will focus on looking at Earth as a whole system, rather than on individual plants or animals. We’ll discuss how the whole Earth system is able to sustain life, and the necessary conditions for life to persist. For instance, if you look down onto South Africa, with most of Africa visible on the top left, and Antarctica below, you’ll see the swirling clouds of the large atmospheric gyres, which move water vapour and air around and constitute our climate systems.\nOne critically important aspect of the Earth is the oceans. Without the oceans, life would not exist as we know it. \\(70.8\\,\\%\\) of Earth’s surface is covered in ocean water, and it’s the presence of these oceans that creates the conditions allowing life to persist on the rest of the planet. The oceans produce about \\(50\\,\\%\\) of the global supply of oxygen, and are crucial in maintaining an even temperature gradient across the planet. Without oceans, Earth would be far hotter and daily temperature fluctuations would be much more extreme.\nIdeally, we shouldn’t call the planet “Earth” but rather “planet ocean”, as it is predominantly covered by ocean water.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#demographic-changes-and-global-population",
    "href": "BDC223/L01-worldmapper.html#demographic-changes-and-global-population",
    "title": "Lecture 1: Worldmapper",
    "section": "Demographic Changes and Global Population",
    "text": "Demographic Changes and Global Population\nI want to show you some figures from a website called World Mapper, which creates interesting images of world maps distorted according to the density of certain processes — for example, population density, education levels, or carbon emissions per capita.\nIf we look back 2,000 years ago, in year one (according to our calendar), most people were in Southeast Asia and Europe. Some distortion in South America is due to Inca and Maya populations, but mostly, the bulk of the population was in Asia and Northern Europe. Very few people were in North America, and New Zealand was unpopulated.\nMoving forward 1,500 years, we see Asia and Northern Europe remain large, with Asia expanding, but North America now growing in size, meaning the population began to rise there. Later, during the last 500 years or so, Africa saw rapid population expansion.\nThis is very important, as continual population growth has a huge effect on biodiversity, both regionally and globally. Human impact is thus largely a consequence of there being very large numbers of people.\nAt the start of the previous century, in \\(1900\\), there were \\(1.56\\) billion people on the planet. Let’s roll forward \\(30\\) years from now, and we expect to see \\(9.8\\) billion people. Most of these will be in Africa. Comparisons of the maps show that Africa, previously relatively smaller, is now significantly bloated in population. As of two years ago, Africa was already very large. In \\(30\\) years, it’s even larger, with \\(411\\) million people [attention: The actual projected population for Africa in \\(2050\\) is much higher, on the order of \\(2.5\\) billion].\nWhy has Africa become so overpopulated? And can we explain this pattern?\nExtending to the year \\(2100\\), Africa’s population is projected to increase further, while Europe and North America are decreasing in size. Many countries, such as Canada, Australia, and New Zealand, now have negative population growth, while South Africa and some Southeast Asian countries experience continued growth.\nExamining demographics, Southeast Asia and Africa — regions experiencing rapid population growth — also have high proportions of very young people (aged \\(0\\)–\\(4\\) years). These populations have fewer older people, while Northern Europe and North America have more elderly than young.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#patterns-of-poverty-urbanisation-and-education",
    "href": "BDC223/L01-worldmapper.html#patterns-of-poverty-urbanisation-and-education",
    "title": "Lecture 1: Worldmapper",
    "section": "Patterns of Poverty, Urbanisation, and Education",
    "text": "Patterns of Poverty, Urbanisation, and Education\nIn Africa and Southeast Asia, most people still live in rural areas; urban development has not proceeded at the same pace as in Europe or North America. South Africa is an exception, with more people making their lives in cities, a trend that continues to generate environmental challenges.\nHowever, for much of Africa, people remain in rural settings. Industrial and urban development lags behind, and many areas remain undeveloped. In Africa and Southeast Asia — the regions where population is growing most rapidly — the majority of people live in absolute poverty. The World Bank defines absolute poverty as income less than \\(1.9\\) US dollars per day. Most people in these regions fall below this line.\nThus, the fastest-growing populations are also the poorest. This seems counterintuitive. If people are poor, why do they have more children?",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#education-and-gender-disparity",
    "href": "BDC223/L01-worldmapper.html#education-and-gender-disparity",
    "title": "Lecture 1: Worldmapper",
    "section": "Education and Gender Disparity",
    "text": "Education and Gender Disparity\nAnother graph shows that the regions with high absolute poverty and high population growth also have the lowest education levels for women, compared to men. This is crucial — where women are uneducated, they tend to have more children. As education increases, women are empowered, and family size decreases.\nBut why is female education so much lower in these regions? This requires a deeper look.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#the-role-of-religion-and-historical-structure",
    "href": "BDC223/L01-worldmapper.html#the-role-of-religion-and-historical-structure",
    "title": "Lecture 1: Worldmapper",
    "section": "The Role of Religion and Historical Structure",
    "text": "The Role of Religion and Historical Structure\nIt’s not the sole explanation, but religion plays a significant role. In large parts of Africa (particularly southern, sub-Saharan, and North Africa), Christianity is dominant, whilst in other areas, Islam has a major influence. Both Christianity and Islam, to varying degrees and in differing ways, have historically been associated with reduced access for women to education, sometimes through direct discouragement or outright prevention of female education. This disparity in education is, in my view, a central reason why population growth rates remain high in these regions.\nIf you disagree or want to explore further, I encourage you to consult additional sources. As I always say, never take anything I say at face value — research and seek secondary sources.\nThroughout history, many conflicts and ongoing strife in these regions have their roots in religious or cultural institutions, which more often than not have been sources of conflict rather than peace, prosperity, equality, or growth for all.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#human-impact-and-the-industrial-revolution",
    "href": "BDC223/L01-worldmapper.html#human-impact-and-the-industrial-revolution",
    "title": "Lecture 1: Worldmapper",
    "section": "Human Impact and the Industrial Revolution",
    "text": "Human Impact and the Industrial Revolution\nThe reason for focusing so much on people is not simply to criticise religion, but to illustrate how having too many people on the planet has created wide-ranging impacts on life on Earth.\nMuch of this began with the Industrial Revolution in North America and Europe, which led to the excessive combustion of fossil fuels. Coal, gas, and oil were consumed in vast quantities to grow the economies of these industrialised countries. The result: excessive carbon dioxide was emitted into the atmosphere. Since the 1900s, the developed nations in the global north have contributed the most, per capita, to climate change.\nSouth Africa, although developing, is the most industrialised country in Africa, and it ranks among the world’s top contributors to carbon emissions.\nCountries that are now industrialising rapidly, such as those in Africa, North Africa, Saudi Arabia, the Middle East, China, Japan, and Southeast Asia, are increasing their carbon emissions at a faster rate than the global north presently, as they attempt to close economic gaps.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#trends-in-carbon-emissions-and-renewable-energy",
    "href": "BDC223/L01-worldmapper.html#trends-in-carbon-emissions-and-renewable-energy",
    "title": "Lecture 1: Worldmapper",
    "section": "Trends in Carbon Emissions and Renewable Energy",
    "text": "Trends in Carbon Emissions and Renewable Energy\nThere has been a decline in Europe’s carbon emissions, particularly among Scandinavian countries which have shifted much of their energy supply to renewables. The United States has not decreased emissions as much, for various political reasons, but still shows a downward trend.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#healthcare-and-environmental-disparities",
    "href": "BDC223/L01-worldmapper.html#healthcare-and-environmental-disparities",
    "title": "Lecture 1: Worldmapper",
    "section": "Healthcare and Environmental Disparities",
    "text": "Healthcare and Environmental Disparities\nPoorer regions with the highest population growth have the lowest levels of healthcare, highest child mortality rates, and highest rates of HIV/AIDS.\nGreenhouse gas emissions remain highest in the most industrialised countries. Most carbon emissions originate from transportation, the generation of heat and electricity, industry, and other fuels. Land use changes — such as deforestation for agriculture, afforestation, reforestation, harvest management — can alter the flux of carbon dioxide.\nDeforestation, in particular, leads to higher CO\\(_2\\) emissions, as forests are important carbon sinks. Agriculture is also a significant contributor to emissions, not just via CO\\(_2\\), but also due to methane (\\(\\mathrm{CH}_4\\)) and nitrous oxide (\\(\\mathrm{N}_2\\mathrm{O}\\)), both potent greenhouse gases.\nIn developing countries, rapid population growth has meant that agriculture has expanded, leading to increased emissions of methane and nitrous oxide, often exacerbated by changes in land use. Waste recycling infrastructure is generally lacking, and more pollutants flow directly into the environment.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#methane-and-nitrous-oxide-regional-patterns",
    "href": "BDC223/L01-worldmapper.html#methane-and-nitrous-oxide-regional-patterns",
    "title": "Lecture 1: Worldmapper",
    "section": "Methane and Nitrous Oxide: Regional Patterns",
    "text": "Methane and Nitrous Oxide: Regional Patterns\nLooking at global patterns, Southeast Asia, South America, and China stand out for methane and nitrous oxide emissions. Livestock, mainly cattle, are the major methane source in South America. In Southeast Asia and China, the main sources are extensive rice farming (which under anaerobic conditions releases methane) and fertiliser use.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#waste-management-and-pollution",
    "href": "BDC223/L01-worldmapper.html#waste-management-and-pollution",
    "title": "Lecture 1: Worldmapper",
    "section": "Waste Management and Pollution",
    "text": "Waste Management and Pollution\nIn the developing world — Africa and South America — most sewage is not collected, but instead flows directly into the environment, aggravating pollution. In contrast, Europe and North America have substantial investment in infrastructure to treat and recycle waste, reducing environmental pollution.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#undernourishment-and-deforestation",
    "href": "BDC223/L01-worldmapper.html#undernourishment-and-deforestation",
    "title": "Lecture 1: Worldmapper",
    "section": "Undernourishment and Deforestation",
    "text": "Undernourishment and Deforestation\nUndernourishment is highest in regions with the fastest population growth, due to a lack of resources to support large families. Again, this links back to poverty and education.\nDeforestation is occurring rapidly in Southeast Asia, China, South Africa, North Africa, Brazil, Colombia, Panama, and elsewhere. Forests are cleared primarily for agriculture, agronomy, expanding residential areas and cities, and — particularly in Africa — to supply fuel wood for heating and cooking.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#science-research-and-gdp",
    "href": "BDC223/L01-worldmapper.html#science-research-and-gdp",
    "title": "Lecture 1: Worldmapper",
    "section": "Science Research and GDP",
    "text": "Science Research and GDP\nScientific research is most intense in countries with the highest gross domestic product (GDP). South Africa is almost unique on the continent for its scientific output, but remains far behind countries in the global north.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#setting-the-scene-for-the-module",
    "href": "BDC223/L01-worldmapper.html#setting-the-scene-for-the-module",
    "title": "Lecture 1: Worldmapper",
    "section": "Setting the Scene for the Module",
    "text": "Setting the Scene for the Module\nTo prepare for Thursday’s lecture, please read the paper ‘A Safe Operating Space for Humanity’ by Johan Rockström and colleagues. I’ve posted the paper on ICAMVA; please have a look, as it discusses the boundaries humanity must not exceed to maintain a habitable planet.\nWe’ve already described climate change in some detail, and its origins. The nitrogen and phosphorus cycles are also affected by waste management practices, and the safe limits for the nitrogen cycle have already been exceeded, moving us into dangerous territory there. We’re also approaching a dangerous level of climate change, and the greatest ongoing threat to the planet is biodiversity loss, which itself is a direct result of too many people, lack of education, and the other factors I’ve described.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L01-worldmapper.html#looking-ahead",
    "href": "BDC223/L01-worldmapper.html#looking-ahead",
    "title": "Lecture 1: Worldmapper",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nFor the rest of the module, I’ll talk about climate change, ocean acidification, the nitrogen cycle, the phosphorus cycle, global freshwater use, and biodiversity loss. Each of these issues has major consequences for how stressed plants are within the environment, as human impacts on planetary boundaries indirectly produce the environmental stresses plants experience.\nThus, we will examine how plants perceive stress, the physiological and ecological effects of stress, and how plants cope with these challenges. That will be our focus for the rest of the module.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 1: Worldmapper"
    ]
  },
  {
    "objectID": "BDC223/L00-introduction.html",
    "href": "BDC223/L00-introduction.html",
    "title": "Lecture 0: Introduction",
    "section": "",
    "text": "So before we start with this module, let me give you some background on how I’d like to proceed.\nLectures will be held on Monday, early Tuesday, and early Thursday. Fridays have an allocated practical time slot, which we may use if necessary. However, my intention is to use that slot mainly for question and answer sessions, but you’ll have to plan ahead and schedule my presence during this time.\nOn Mondays, Tuesdays, and Thursdays, we’ll have in-person lectures in the class. All of the lecture material that I’ll be presenting in the class will be based on the content of various lecture slides, which I’ll be displaying as I talk. These slides are available as PDFs for you to download on iKamva. During the COVID period, I also presented these lectures as a series of online pre-recorded lectures, which can also be downloaded on iKamva. They correspond to the slides you have access to. Additionally, in 2025, I have created transcripts – I have converted all the content of the pre-recorded lectures to text and made those available on my website, Tangled Bank, under the various lectures allocated to BDC 223. Please navigate to that website for the transcripts of the pre-recorded lecture materials.\nSo, in total, you have lecture slides, in-person lectures, pre-recorded lectures that correspond more or less to what I’m saying in class, and textual transcripts of all of the material. All of this is available to you and should help you understand the content of BDC 223.\nIf there’s something you don’t understand, please make an appointment (as a class) to see me on Friday after 2 pm. This will give you a chance to discuss any issues that came up during the week’s lectures. This setup will allow us to revisit earlier material if there are any unresolved questions.\nIt’s largely in your hands how you want to use the Friday afternoon allocation. By default, I won’t interact unless you make an appointment, and when you do so, please make sure that you’re with a group of at least four or five people. I won’t hold individual meetings, since often questions are shared and it’s more efficient to address them together. That’s why the WhatsApp group exists. Use it to coordinate which topics are unclear, post your questions there, and I can respond either as a voice note or, if needed, we can use Friday afternoons for more detailed explanations.\nI hope this format works for everyone. If not, let me know and we can look at alternatives. I’ll be available as much as possible on WhatsApp, so please use that. I’ll definitely be available during the three lecture periods each week and, by appointment, on Friday afternoons.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 0: Introduction"
    ]
  },
  {
    "objectID": "BDC223/L00-introduction.html#basic-house-keeping",
    "href": "BDC223/L00-introduction.html#basic-house-keeping",
    "title": "Lecture 0: Introduction",
    "section": "",
    "text": "So before we start with this module, let me give you some background on how I’d like to proceed.\nLectures will be held on Monday, early Tuesday, and early Thursday. Fridays have an allocated practical time slot, which we may use if necessary. However, my intention is to use that slot mainly for question and answer sessions, but you’ll have to plan ahead and schedule my presence during this time.\nOn Mondays, Tuesdays, and Thursdays, we’ll have in-person lectures in the class. All of the lecture material that I’ll be presenting in the class will be based on the content of various lecture slides, which I’ll be displaying as I talk. These slides are available as PDFs for you to download on iKamva. During the COVID period, I also presented these lectures as a series of online pre-recorded lectures, which can also be downloaded on iKamva. They correspond to the slides you have access to. Additionally, in 2025, I have created transcripts – I have converted all the content of the pre-recorded lectures to text and made those available on my website, Tangled Bank, under the various lectures allocated to BDC 223. Please navigate to that website for the transcripts of the pre-recorded lecture materials.\nSo, in total, you have lecture slides, in-person lectures, pre-recorded lectures that correspond more or less to what I’m saying in class, and textual transcripts of all of the material. All of this is available to you and should help you understand the content of BDC 223.\nIf there’s something you don’t understand, please make an appointment (as a class) to see me on Friday after 2 pm. This will give you a chance to discuss any issues that came up during the week’s lectures. This setup will allow us to revisit earlier material if there are any unresolved questions.\nIt’s largely in your hands how you want to use the Friday afternoon allocation. By default, I won’t interact unless you make an appointment, and when you do so, please make sure that you’re with a group of at least four or five people. I won’t hold individual meetings, since often questions are shared and it’s more efficient to address them together. That’s why the WhatsApp group exists. Use it to coordinate which topics are unclear, post your questions there, and I can respond either as a voice note or, if needed, we can use Friday afternoons for more detailed explanations.\nI hope this format works for everyone. If not, let me know and we can look at alternatives. I’ll be available as much as possible on WhatsApp, so please use that. I’ll definitely be available during the three lecture periods each week and, by appointment, on Friday afternoons.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 0: Introduction"
    ]
  },
  {
    "objectID": "BDC223/L00-introduction.html#the-fourth-term-bdc-223",
    "href": "BDC223/L00-introduction.html#the-fourth-term-bdc-223",
    "title": "Lecture 0: Introduction",
    "section": "2 The Fourth Term BDC 223",
    "text": "2 The Fourth Term BDC 223\nIn this fourth term, we’ll mostly be discussing plants – photosynthetic organisms, whether terrestrial or marine. Our focus will be the plant-related content equivalent to what Prof Maritz covered, but specifically on photosynthetic organisms.\nBefore we dive in, let’s have a basic overview of the module. I’ve already shared the slides; you can review them. At the beginning, I’ve included some quotes that I find amusing or thought-provoking; you’re welcome to read through those. If you’d like me to elaborate on any of them, let me know, but the goal is for them to inspire or provide some insight into the scientific mindset. For example, Richard Feynman, a physicist who died in the 1980s, believed that we’re all born knowing nothing, and lifelong learning gives life meaning. That’s also my view: there’s always more to learn, and science is about empowering you to answer questions that haven’t yet received enough thought.\nThere’s still plenty to discover in the world, and in this module, we’ll aim to generate new knowledge regarding plant biology. There are many exciting developments out there; Prof Maritz has probably pointed out some, and I share that enthusiasm, especially since my research is in the ocean as a marine biologist. My perspective and approach will focus on ocean processes, while Prof Maritz’s emphasis is more on terrestrial ecosystems. Both perspectives are valuable and interconnected.\nMy main expectation is for you to read widely around the topics I make available. Some details will be in textbooks or other readings that I might not directly cover in lectures. Remember, exam questions won’t be limited to what I’ve said in class; your responsibility as science students is to explore and verify information on your own.\nIf you master all my lecture material, it will probably get you about 70% in the exam; the rest comes from your broader reading and learning. Teaching is about directing you, but learning is your personal process. Integrate the information, connect concepts, and aim for deep understanding. That’s not something I can give you. You create it for yourselves. To do that, read, interact with your peers, and engage with me (use WhatsApp for questions or alternative perspectives).\nScience advances through scepticism and questioning, not authority. Always question, including me, your family, community leaders, and so on. Don’t accept things as fact simply because someone says so. Develop your own thinking and remain open-minded, sceptical, and inquisitive.\nOne of my slides talks about the difference between knowing and understanding. Listing names of snakes doesn’t mean you understand their behaviour. Go beyond memorisation to understanding why and how things happen. That’s the key to deep learning.\nAs I said, I am a marine biologist. I work in the ocean, especially around South Africa (but also elsewhere), and my research is often ocean-centric. That doesn’t mean it’s irrelevant to land-based biology. I encourage you to draw general conclusions and connections across different contexts—integrate everything you learn.\nTests and assessments will focus on integration and synthesis, rather than regurgitation. You’ll need to demonstrate that you can apply what you’ve learned to new problems.\nContent for this module includes:\n\nPlanetary boundaries (tomorrow’s topic).\nClimate change (starting Thursday) – its relevance to this module and biology as a whole.\nPlant stress – how plants experience and respond to stress.\nThe role of light in the environment, critical to plant life.\nHeat stress and plant adaptation.\nPlant nutrition – their uptake of inorganic nitrogen and phosphorus, tying into global biogeochemical cycles and the carbon cycle.\n\nThere will be three practical labs dealing mainly with data analysis and calculations about plant ecophysiology: surface area/volume ratios, nutrient uptake, and light measurements. You’ll get lab assignments on Mondays, due the following Monday at midnight, with calculations to be shown in spreadsheets and conclusions in a MS Word document.\nYou’ll also write a short personal essay, due roughly two from now.\nThe mark allocation is similar to Prof Martitz’s section: random quizzes, two class tests (typically on the Thursdays ot Fridays), and all work up to those points will be covered.\nThe learning outcomes for my section:\n\nUnderstand how environmental conditions (light, temperature, nutrients, etc.) affect plant distribution and interactions.\nLearn physiological mechanisms for water, nutrient, and carbon uptake in plants.\nGrasp the role of plants in the Earth system, integrating their function across contexts.\nDiscuss ecophysiological processes involved in nutrient and water transport and loss.\nExamine the implications of global change and the limits of life on Earth.\n\nTomorrow we’ll focus on planetary boundaries, starting with people and their impact as the most destructive organism on the planet, then look at how plants adapt to environmental changes.\nGood luck! Let’s get started.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 0: Introduction"
    ]
  },
  {
    "objectID": "BDC223/L06b-jassby_platt.html",
    "href": "BDC223/L06b-jassby_platt.html",
    "title": "Lecture 6b: PI Curves – Jassby and Platt",
    "section": "",
    "text": "This Theory Accompanies the Following Lab\n\n\n\n\nLab 3: PI Curves – Jassby and Platt\n\n\n\n\n1 The Hyperbolic Tangent Model\nThe hyperbolic tangent model was proposed by Jassby and Platt (1976). It has become one of the most widely used models for describing the relationship between photosynthetic rate and irradiance (light intensity) in aquatic photosynthetic organisms, including algae ranging from kelp to phytoplankton. The model captures the core dynamics of photosynthesis, in which the rate of photosynthesis initially increases with light intensity but eventually saturates as the photosynthetic machinery reaches its maximum efficiency. This is a simple model, but it effective because the biologically meaningful parameters can be directly interpreted to assess plant or algal productivity in various light environments.\nThe hyperbolic tangent model is expressed as:\n\\[ P(I) = P_{\\text{max}} \\times \\tanh\\left(\\frac{\\alpha I}{P_{\\text{max}}}\\right) \\]\nWhere:\n\n\n\\(P(I)\\) represents the photosynthetic rate at a given irradiance \\(I\\) (light intensity),\n\n\\(P_{\\text{max}}\\) is the maximum photosynthetic rate (also referred to as the light-saturated rate),\n\n\\(\\alpha\\) is the initial slope of the curve, which reflects the photosynthetic efficiency at low light levels,\n\n\\(I\\) is the irradiance (light intensity),\n\nOne is also able to determine the saturating irradiance, \\(I_{\\text{k}}\\), which is the light intensity at which photosynthesis reaches \\(P_{\\text{max}}\\). Simply read this value off the graph where \\(P(I) = P_{\\text{max}}\\) (see the lecture slides ‘6.BDC223_Pigments_Photosynthesis_2024.key.pdf’.\nThe hyperbolic tangent function \\(\\tanh\\) is used to smoothly describe the transition between the linear increase in photosynthesis at low light intensities and the eventual plateau at higher intensities, where photosynthesis becomes light-saturated. The light compensation point, the point at which photosynthesis equals respiration (i.e., net photosynthesis is zero), can also be derived from this model.\nThe model describes the essential processes of photosynthesis with just two parameters: \\(P_{\\text{max}}\\) and \\(\\alpha\\). Both parameters are biologically meaningful and tell us how efficiently an organism can convert light into chemical energy under different light conditions. For example, higher values of \\(P_{\\text{max}}\\) indicate a greater potential for photosynthesis under optimal light conditions, while the value of \\(\\alpha\\) indicates how quickly photosynthesis responds to low light.\nApplications of the hyperbolic tangent model are numerous. It is commonly used to estimate the photosynthetic performance of marine and freshwater algae, seagrasses, and macroalgae under varying environmental conditions. In kelp forests, for instance, we may use this model to assess how different species adapt to light intensities at various depths or how photosynthetic performance shifts in response to seasonal changes in light availability. Looking at phytoplankton, the model helps estimate productivity across different layers of the water column, where light intensity decreases with depth.\nBelow are a few lines of data taken from a hypothetical P-I experiment. The data are for five replicate experiments with the same light intensities (independent variable), representing conditions typically encountered by kelp at latitudes between -36° and -23°S.\n\n\n\n\n\nReplicate\nLight (μmol photons m⁻² s⁻¹)\nPhotosynthesis (mg C m⁻² h⁻¹)\n\n\n\n3\n1\n100\n4.59\n\n\n17\n2\n200\n8.83\n\n\n53\n5\n200\n8.05\n\n\n44\n4\n350\n12.27\n\n\n35\n3\n500\n12.57\n\n\n54\n5\n250\n9.38\n\n\n34\n3\n450\n11.90\n\n\n11\n1\n500\n13.53\n\n\n27\n3\n100\n4.20\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Nonlinear regression Jassby and Platt (1976) model fitted to simulated P-I data for a hypothetical kelp.\n\n\n\n\nAfter fitting the model to the data, we can determine the values for \\(P_{\\text{max}}\\) and \\(\\alpha\\) for each replicate and determine the average value across the five fits. The combined plot (Figure 1) displays the observed data points for all replicates and the fitted curve from the first replicate.\nThe average model fit values of the estimated parameters across all replicates are as follows:\n\n\n\\(P_{\\text{max}}\\): 13.05 mg C m⁻² h⁻¹\n\n\\(\\alpha\\): 0.05 μmol photons m⁻² s⁻¹\n\n2 Considering the Light Compensation Point\nThe light compensation point (\\(I_c\\)) is the irradiance level at which the rate of photosynthesis equals the rate of respiration, resulting in a net photosynthetic rate of zero. Below this point, the organism consumes more energy (via respiration) than it produces through photosynthesis, leading to a net loss of energy. Estimating \\(I_c\\) is important for determining the minimum light intensity required for the survival of photosynthetic organisms, after compensation for the effect of cellular respiration.\nIn the context of the Jassby and Platt hyperbolic tangent model, \\(I_c\\) can be estimated by solving for the irradiance \\(I\\) when the net photosynthetic rate \\(P(I)\\) equals zero:\n\\[\n0 = P_{\\text{max}} \\times \\tanh\\left(\\frac{\\alpha I_{\\text{LCP}}}{P_{\\text{max}}}\\right)\n\\]\nSince \\(\\tanh(0) = 0\\), the net photosynthetic rate is zero when \\(I = 0\\). However, due to respiration, the net photosynthesis can be negative at zero light intensity. To account for respiration, we can modify the model to include dark respiration rate (\\(R\\)):\n\\[\nP(I) = P_{\\text{max}} \\times \\tanh\\left(\\frac{\\alpha I}{P_{\\text{max}}}\\right) - R\n\\]\nNow, \\(I_c\\) is the irradiance at which \\(P(I) = 0\\):\n\\[\n0 = P_{\\text{max}} \\times \\tanh\\left(\\frac{\\alpha I_{\\text{LCP}}}{P_{\\text{max}}}\\right) - R\n\\]\nWe can solve this equation numerically to find \\(I_{\\text{LCP}}\\).\n\n\n\n\n\nReplicate\nLight (μmol photons m⁻² s⁻¹)\nPhotosynthesis (mg C m⁻² h⁻¹)\n\n\n\n3\n1\n100\n3.65\n\n\n37\n4\n0\n-2.17\n\n\n5\n1\n200\n6.98\n\n\n57\n5\n400\n9.43\n\n\n54\n5\n250\n7.41\n\n\n60\n5\n550\n10.98\n\n\n33\n3\n400\n9.66\n\n\n6\n1\n250\n7.89\n\n\n49\n5\n0\n-1.86\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Nonlinear regression Jassby and Platt (1976) model fitted to simulated P-I data for a hypothetical kelp. This model includes the effect of the light compensation point.\n\n\n\n\nThe model fit to the data is in Figure 2. The average model fit values of the estimated parameters across all replicates are as follows:\n\n\n\\(P_{\\text{max}}\\): 13.15 mg C m⁻² h⁻¹\n\n\\(\\alpha\\): 0.05 μmol photons m⁻² s⁻¹\n\n\\(I_c\\): 41.28 μmol photons m⁻² s⁻¹\n\n3 Platt et al. (1980) Model with Photoinhibition\nLet’s now look at the Platt et al. (1980) model, which incorporates photoinhibition into the photosynthesis-irradiance (P-I) relationship. This model extends the understanding of photosynthesis by accounting for the decrease in photosynthetic efficiency at high light intensities due to photoinhibition—a phenomenon where excessive light damages the photosynthetic apparatus, leading to reduced photosynthetic rates.\nThe model is expressed mathematically as:\n\\[\nP(I) = P_{\\text{max}} \\left(1 - \\exp\\left(-\\frac{\\alpha I}{P_{\\text{max}}}\\right)\\right) \\exp\\left(-\\frac{\\beta I}{P_{\\text{max}}}\\right)\n\\]\nWhere:\n\n\n\\(P_{\\text{max}}\\) is the maximum photosynthetic rate in the absence of photoinhibition.\n\n\\(\\beta\\) is the photoinhibition parameter (rate of decrease in photosynthesis at high light).\n\n\\(\\exp\\) denotes the exponential function.\n\nThis model combines the positive effect of light on photosynthesis at low irradiance with the negative effect of photoinhibition at high irradiance, providing a comprehensive description of the photosynthetic response across a wide range of light intensities.\n\n\n\n\n\nReplicate\nLight (μmol photons m⁻² s⁻¹)\nPhotosynthesis (mg C m⁻² h⁻¹)\n\n\n\n45\n3\n1000\n10.05\n\n\n76\n5\n700\n9.13\n\n\n34\n2\n1600\n10.16\n\n\n52\n4\n0\n-2.00\n\n\n1\n1\n0\n-0.86\n\n\n77\n5\n800\n9.58\n\n\n10\n1\n900\n11.26\n\n\n13\n1\n1200\n11.43\n\n\n39\n3\n400\n7.98\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Nonlinear regression Platt et al. (1980) model fitted to simulated P-I data for a hypothetical kelp. This model includes the effect of photoinhibition.\n\n\n\n\n\nReferences:\n\n\nJassby, A. D., & Platt, T. (1976). Mathematical formulation of the relationship between photosynthesis and light for phytoplankton. Limnology and Oceanography, 21(4), 540-547.\n\nPlatt, T., Gallegos, C. L., & Harrison, W. G. (1980). Photoinhibition of photosynthesis in natural assemblages of marine phytoplankton. Journal of Marine Research, 38(4), 687-701.\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.,\n  author = {Smit, A. J., and Smit, AJ},\n  title = {Lecture 6b: {PI} {Curves} -\\/- {Jassby} and {Platt}},\n  url = {http://tangledbank.netlify.app/BDC223/L06b-jassby_platt.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J., Smit A Lecture 6b: PI Curves -- Jassby and Platt. http://tangledbank.netlify.app/BDC223/L06b-jassby_platt.html.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 6b: PI Curves -- Jassby and Platt"
    ]
  },
  {
    "objectID": "BDC223/L03-plant_stress.html",
    "href": "BDC223/L03-plant_stress.html",
    "title": "Lecture 3: Plant Stress",
    "section": "",
    "text": "Content\n\n\n\n\nIdentify and understand the suite of environmental properties (e.g. light, heat, water, nutrients, etc.) that are able to induce plant stress.\nContextualise this understanding within the broader field of planetary change (global change and planetrary boundaries).\nUnderstand how climate change, specifically, is altering the environmental properties that induce plant stress.\nLink these environmental properties to the physiological and morphological responses of plants to stress.\nUnderstand the role of plant stress in shaping plant ecophysiological well being (e.g. the concept of relience).\nUnderstand the notions of stress resistance, stress avoidance, and succeptibility to stress in the context of plant stress.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 3: Plant Stress"
    ]
  },
  {
    "objectID": "BDC223/L03-plant_stress.html#lecture-transcript-plant-stresses",
    "href": "BDC223/L03-plant_stress.html#lecture-transcript-plant-stresses",
    "title": "Lecture 3: Plant Stress",
    "section": "Lecture Transcript: Plant Stresses",
    "text": "Lecture Transcript: Plant Stresses\nToday, we are going to talk about plant stresses. This won’t be a very long lecture—just a brief overview of the various ways in which plants react to environmental stresses. This topic follows on from our previous work: at the end of last week, we explored planetary boundaries, and yesterday we delved deeply into climate change. Many of the environmental changes that plants respond to are being caused by climate change, but there are also several other stresses that have nothing to do with climate change, to which plants must also adapt. These will come up throughout the next few weeks.\nSo, why do we even need to worry about plant stresses? It’s not as if plants have emotions or feelings, so we don’t need to be concerned about their well-being… or do we? In fact, we do need to worry about how they behave and react, because the various ways in which climate change affects plants is often via the process of photosynthesis. Photosynthesis, as you know, is the foundational process that explains why plants are so productive and why they are able to support entire ecosystems. Fundamentally, all life on Earth depends on plants, and it is because of their ability to photosynthesise that they are so significant, both today and historically, even before people existed.\nNow, photosynthesis is affected by many things: water availability, the amount of heat in the environment, carbon dioxide concentrations, ozone amount, varied aerosols, and more. Underground, the root systems also respond to heat, moisture, precipitation (in other words, rain), levels of organic matter, nutrients like nitrogen and phosphorus, as well as other macro and micronutrients, and even factors like thawing permafrost. The balance between above-ground processes (where photosynthesis happens) and below-ground processes (in the rhizosphere) ultimately affects the rate of plant productivity.\nWhen we talk about photosynthesis, it’s important to remember it doesn’t only apply to terrestrial plants—it also occurs in oceanic or aquatic organisms. Many such organisms, like algae and all seaweeds, do not have roots or a rhizosphere. Even though they may have structures resembling roots, they are anatomically different. Still, photosynthesis is the primary physiological process that underpins both plant and algae function in their environments.\nTherefore, understanding how changes brought about by climate change—like water content, heat, CO₂ levels, etc.—affect both above- and below-ground processes, is crucial. A stress, by definition, is what happens when a plant’s natural tolerance is exceeded. There can also be positive stimuli—where environmental conditions are “just right” and fall within the optimum range. What we’re really interested in is how plants respond across a range of conditions—especially as we move toward extremes where stresses become important.\nSo, what’s the practical significance of all of this? Why should we care? Well, plant stress affects many things relevant to people and ecosystems:\n\nInvasive Potential: Some plants become more invasive under stressed or adverse conditions. When natural biota are stressed, ecosystems become more susceptible to invasion by species previously absent—these are often “weeds.” A weed is, essentially, a plant growing rapidly in an environment where it’s unwanted, not only in gardens but also in natural or disturbed environments.\nPhenological Changes: Stress can alter the timing of biological events. For instance, with warming climates, it may seem as if summers are beginning earlier—plants flower earlier, bees and other insects react earlier, and so on. This lengthening of the growing season influences the timing of various biological processes.\nInterspecific Interactions: Stress can change the strength and direction of interactions between species—such as herbivory, parasitism, or allelopathy. Stressed plants may be more vulnerable to herbivores, for example.\nProductivity: Plant stress can increase or decrease productivity, which directly impacts people, especially via agriculture. For instance, increased environmental stress can lead to incomplete or poorly developed crops—like underdeveloped corn cobs due to drought or heat stress.\nRange Shifts: Stress alters the zones where specific species can survive and thrive; as optimal envelopes shift, so do the ranges where species are comfortable.\nPhenotype Changes: The outward appearance or form of plants can evolve in response to environmental change.\nBiogeochemical Cycles: Environmental stress can alter carbon pools—as we discussed yesterday, with thawing permafrost releasing previously locked carbon and nitrogen into the atmosphere, leading to feedback loops that influence global warming further.\nResilience: All these changes can reduce the resilience of ecosystems, create feedback loops, escalate vulnerability to storms and droughts, and in severe cases, drive species to local extinction (extirpation), or even outright extinction.\n\n\nProductivity and People\nThis is where plant stress especially matters: humanity relies heavily on agriculture—developed over the last 10,000 or so years—and hence on productive plants. As plant stress increases, food productivity decreases, affecting not just people directly eating plants but also livestock and the broader organisation of society. The most vulnerable populations are those most directly reliant on agriculture: the poorest people will be most affected as food insecurity increases, and small-scale farmers can be driven into unsustainable debt.\nEven in developed countries, while people might be insulated to a degree from direct food insecurity, increased costs and economic implications are inevitable. Stresses on crops don’t always total crop failure, but can mean partial productivity loss. This leads to economic impacts—from local communities right up to affecting GDP, as we see in countries like South Africa, where agriculture is a significant part of the economy.\n\n\nBroader Impacts\nThere are also broader socioeconomic and social aspects. California, for example, though typically Mediterranean in climate, has become increasingly dry and unable to meet its agricultural output reliably. Phenomena like severe hailstorms (which can directly damage plants) and societal reactions to climate extremes (such as the historical linkage between unseasonal weather and witch hunts—a point more anecdotal and possibly contentious—[attention]) show just how interconnected society and plant health truly are.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 3: Plant Stress"
    ]
  },
  {
    "objectID": "BDC223/L03-plant_stress.html#plant-responses-to-stress",
    "href": "BDC223/L03-plant_stress.html#plant-responses-to-stress",
    "title": "Lecture 3: Plant Stress",
    "section": "Plant Responses to Stress",
    "text": "Plant Responses to Stress\nSo, what do plants do when confronted by stress? There are three general strategies:\n\nResist the stress: Acclimatise or otherwise develop responses that allow survival and growth.\nAdapt and thrive: Use evolutionary adaptations that confer long-term tolerance.\nDie: Simply be unable to cope, leading to death—a fate for species with narrow environmental tolerance (“stenothermal” for temperature tolerance, for example).\n\nPlant stresses include:\n\nAbiotic: Salinity, drought, heat extremes, light extremes, nutrients, etc.\nBiotic: Pathogens, herbivores, competing species, etc.\n\nResistance is a short-term response—plants acclimatise through physiological changes, often triggered by gene expression changes that allow short-term survival under stress. Once conditions normalise, the plant resumes normal function.\nSusceptibility occurs in plants with very narrow tolerance ranges. These “specialist” species often reside in environments that are stable, but when stress exceeds their adaptation, they senesce and die.\nAvoidance refers to plants preventing stress from impacting them, often via life-history strategies or adaptations:\n\nEphemerals: Grow and reproduce rapidly only when conditions are optimal (e.g., spring flowers after seasonal rains).\nDeciduousness: Drop leaves to avoid freeze damage (common in boreal forests).\nSeaweeds with Alternation of Generations: Present as large fleshy organisms during favourable seasons and tough, small forms during stressful periods.\nDeep-rooted plants: Access deep water during droughts.\nSucculents: Store water during dry seasons, use CAM metabolism to reduce water loss.\n\nPlants may evolve a wide (“eurythermal”) vs. narrow (“stenothermal”) tolerance range, depending on their resistance or avoidance mechanisms.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 3: Plant Stress"
    ]
  },
  {
    "objectID": "BDC223/L03-plant_stress.html#conclusion",
    "href": "BDC223/L03-plant_stress.html#conclusion",
    "title": "Lecture 3: Plant Stress",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s a broad overview of how plants cope with environmental stresses. Nothing too difficult here, but much of this content will recur throughout the rest of the BDC223 course and beyond. In our next set of lectures, we will look at one of the most critical environmental influences for plant life… light. We’ll discuss what light is, how plants harvest it, and the physiological process of photosynthesis in detail.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 3: Plant Stress"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "",
    "text": "This Lecture is Accompanied by the Following Lab\n\n\n\n\nLab 4: Uptake Kinetics – Michaelis-Menten",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#experimental-setup",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#experimental-setup",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Experimental Setup",
    "text": "Experimental Setup\nIn the multiple flask method, a series of flasks are prepared, each containing a different initial concentration of the nutrient (substrate) to span the range of nutrient levels typically encountered by the macroalgae in its natural habitat. This allows for measurements of nutrient uptake rates across a spectrum of substrate concentrations, from low to high.\nSteps of the Multiple Flask Experiment\n\n\nSubstrate Preparation: Prepare several flasks, each with a known initial concentration of the nutrient (e.g., nitrogen) in solution. These concentrations should cover a range of interest, often from nutrient-limiting to saturating levels.\n\nAlgal Introduction: Introduce a known biomass of macroalgae into each flask. The biomass should be standardised across all flasks (e.g., 4.5 g of fresh macroalgal tissue per flask).\n\nIncubation: The flasks are incubated for a defined time period, typically 20–30 minutes, under controlled environmental conditions such as light and temperature.\n\nSampling: At the beginning of the incubation (\\(t=0\\)) and at the end of the incubation period (e.g., \\(t=30\\) minutes), water samples are taken from each flask to measure the concentration of the nutrient in the water.\n\nNutrient Analysis: The concentration of the nutrient in each water sample is analysed using chemical methods (e.g., colorimetric analysis or ion chromatography).\n\nThe difference in nutrient concentration between the start and end of the incubation reflects the amount of nutrient taken up by the macroalgae during the experiment.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#data-collected",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#data-collected",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Data Collected",
    "text": "Data Collected\n\n\nInitial substrate concentrations (\\([S_{\\text{initial}}]\\)) in each flask.\n\nFinal substrate concentrations (\\([S_{\\text{final}}]\\)) after the incubation period.\n\nTime of incubation (\\(\\Delta_t\\)).\n\nAlgal biomass in each flask (usually standardised, e.g., 4.5 g fresh mass).",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#calculations-for-determining-nutrient-uptake-rate-v",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#calculations-for-determining-nutrient-uptake-rate-v",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Calculations for Determining Nutrient Uptake Rate (\\(V\\))",
    "text": "Calculations for Determining Nutrient Uptake Rate (\\(V\\))\nThe following steps outline how to calculate the nutrient uptake rate (\\(V\\)) from the experimental data obtained using the multiple flask method (apply to data obtained from each flask). These steps convert changes in nutrient concentration into actual uptake rates, adjusted for algal biomass and incubation time.\nStep 1: Calculate the Change in Nutrient Concentration (\\(\\Delta[S]\\))\nTo determine how much nutrient was taken up during the incubation, subtract the final nutrient concentration from the initial nutrient concentration:\n\\[\n\\Delta [S] = [S_{\\text{initial}}] - [S_{\\text{final}}]\n\\]\nFor example: \\[\n\\Delta [S] = 25 \\, \\mu M - 9.9 \\, \\mu M = 15.1 \\, \\mu M\n\\]\nThis gives the reduction in nutrient concentration over the time period but does not yet account for the volume of the flask or the biomass of algae.\nStep 2: Convert Concentrations to Mass of Nutrient Present per Flask\nConvert the concentration of the nutrient (in μmol.L⁻¹) into the actual mass of nutrient (in μg) present in the flask. To do this, use the molecular mass (MM) of the nutrient (e.g., nitrogen), which is 14.0067 g.mol⁻¹ for N.\nFor example: \\[\n25 \\, \\mu M = 25 \\, \\mu mol/L \\times 14.0067 \\, \\frac{g}{mol} = 350.17 \\, \\mu g \\, N \\, \\text{(per liter)}\n\\] \\[\n9.9 \\, \\mu M = 9.9 \\, \\mu mol/L \\times 14.0067 \\, \\frac{g}{mol} = 138.67 \\, \\mu g \\, N \\, \\text{(per liter)}\n\\]\nNext, account for the volume of the flask (e.g., 500 mL). Since the above values are for 1 liter, divide by 2 to find the mass in 500 mL:\n\\[\n\\text{Mass of N at the start} = 350.17 \\, \\mu g / 2 = 175.09 \\, \\mu g\n\\] \\[\n\\text{Mass of N at the end} = 138.67 \\, \\mu g / 2 = 69.34 \\, \\mu g\n\\]\nStep 3: Calculate the Amount of Nutrient Taken Up by the Alga\nNow, calculate how much nutrient was taken up by the algae during the incubation:\n\\[\n\\Delta \\text{Mass of N} = 175.09 \\, \\mu g - 69.34 \\, \\mu g = 105.75 \\, \\mu g \\, N\n\\]\nThis represents the total amount of nitrogen removed from the water by the algal biomass during the 20-minute incubation.\nStep 4: Normalise Nutrient Uptake by Algal Biomass\nTo determine how much nutrient was taken up per unit mass of algae, divide the total nutrient uptake by the biomass of algae in the flask (e.g., 4.5 g):\n\\[\n\\text{Nutrient uptake rate} = \\frac{105.75 \\, \\mu g \\, N}{4.5 \\, g} = 23.5 \\, \\mu g \\, N/g\n\\]\nThis gives the nutrient uptake rate in terms of μg of nutrient per gram of algal biomass over the incubation period of 20 minutes.\nStep 5: Calculate the Nutrient Uptake Rate per Hour\nIf the experiment lasted 20 minutes, but the uptake rate needs to be expressed on an hourly basis, multiply the rate by 3 (since there are three 20-minute intervals in an hour):\n\\[\n\\text{Nutrient uptake rate per hour} = 23.5 \\, \\mu g \\, N/g \\times 3 = 70.50 \\, \\mu g \\, N/g/hr\n\\]",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#final-workflow-for-calculating-nutrient-uptake-rate",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#final-workflow-for-calculating-nutrient-uptake-rate",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Final Workflow for Calculating Nutrient Uptake Rate",
    "text": "Final Workflow for Calculating Nutrient Uptake Rate\n\n\nDetermine the change in nutrient concentration between the start and end of the experiment for each flask: \\[\n\\Delta [S] = [S_{\\text{initial}}] - [S_{\\text{final}}]\n\\]\n\n\nConvert concentrations to mass of nutrient (e.g., μg N) using the molecular mass and flask volume: \\[\n\\text{Mass of nutrient} = [S] \\times \\text{MM of nutrient}\n\\]\n\n\nCalculate the amount of nutrient taken up by the algae: \\[\n\\Delta \\text{Mass of nutrient} = \\text{Mass of nutrient (initial)} - \\text{Mass of nutrient (final)}\n\\]\n\n\nNormalise the nutrient uptake by the algal biomass: \\[\n\\text{Nutrient uptake rate} = \\frac{\\Delta \\text{Mass of nutrient}}{\\text{Algal biomass}}\n\\]\n\n\nConvert the nutrient uptake rate to an hourly rate, if necessary: \\[\n\\text{Nutrient uptake rate (hourly)} = \\text{Nutrient uptake rate} \\times \\frac{60}{\\Delta t}\n\\]",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#experimental-setup-1",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#experimental-setup-1",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Experimental Setup",
    "text": "Experimental Setup\nIn the perturbation method, a single flask is prepared with a high initial concentration of the nutrient (e.g., nitrogen), and a known amount of macroalgal biomass is introduced. The flask is incubated, and water samples are taken at regular intervals to track the decrease in nutrient concentration. The resultant data allow us to calculate the nutrient uptake rate for each time interval.\nSteps of the Perturbation Experiment\n\n\nSubstrate Preparation: Add a known and high concentration of the nutrient (e.g., 25 μM nitrogen) to the flask. The concentration should be high enough to ensure measurable changes over the course of the experiment but ecologically relevant.\n\nAlgal Introduction: Introduce a known biomass of a macroalga into the flask (e.g., 4.5 g of fresh macroalgal tissue).\n\nIncubation and Sampling: The flask is incubated, and water samples are taken at regular intervals (e.g., every 10 or 20 minutes) to measure the nutrient concentration at each time point.\n\nNutrient Analysis: The concentration of the nutrient in each water sample is analysed to determine how much nutrient remains at each time point.\n\nData Collection: The change in nutrient concentration between each successive time point is used to calculate the nutrient uptake rate over the interval.\n\nThe resulting data from the perturbation method consist of a time series of substrate concentrations paired with calculated nutrient uptake rates over specific time intervals.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#data-collected-1",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#data-collected-1",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Data Collected",
    "text": "Data Collected\n\n\nInitial substrate concentration (\\([S_{initial}]\\)) and substrate concentrations at subsequent time points (\\([S_{t1}]\\), \\([S_{t2}]\\), …).\n\nTime intervals (\\(\\Delta_t\\), e.g. every 5 or 10 minutes).\n\nAlgal biomass in the flask (e.g., 4.5 g of fresh mass).\n\nBy plotting the remaining substrate concentration against each time point at which we sampled the water for nutrient measurement, we can construct a nutrient depletion curve. From this, we can observe how the nutrient is taken up by the macroalga and calculate the nutrient uptake rate at different stages of the experiment.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#calculations-for-determining-nutrient-uptake-rate-v-1",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#calculations-for-determining-nutrient-uptake-rate-v-1",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Calculations for Determining Nutrient Uptake Rate (\\(V\\))",
    "text": "Calculations for Determining Nutrient Uptake Rate (\\(V\\))\nOnce the experimental data have been collected, the next step is to calculate the nutrient uptake rate for each time interval based on the reduction in nutrient concentration between successive time points. The following steps outline how to perform these calculations.\nStep 1: Calculate the Change in Nutrient Concentration (\\(\\Delta [S]\\))\nTo determine how much nutrient has been taken up during a specific time interval (e.g., the first 5 minutes of the experiment, i.e. \\(\\Delta_t = 0\\)), subtract the nutrient concentration at the end of the interval from the concentration at the start:\n\\[\n\\Delta [S] = [S_{\\text{start}}] - [S_{\\text{end}}]\n\\]\nFor example: \\[\n\\Delta [S] = 25 \\, \\mu M - 21.3 \\, \\mu M = 3.7 \\, \\mu M\n\\]\nThis gives the reduction in nutrient concentration over the 5-minute interval.\nStep 2: Convert Concentrations to Mass of Nutrient Present per Flask\nConvert the nutrient concentration (in μmol.L⁻¹) into the mass of nutrient (in μg) present in the flask. To do this, use the molecular mass (MM) of the nutrient, which is 14.0067 g.mol⁻¹ for nitrogen.\nFor example: \\[\n25 \\, \\mu M = 25 \\, \\mu mol/L \\times 14.0067 \\, \\frac{g}{mol} = 350.17 \\, \\mu g \\, N \\, \\text{(per liter)}\n\\] \\[\n21.3 \\, \\mu M = 21.3 \\, \\mu mol/L \\times 14.0067 \\, \\frac{g}{mol} = 298.34 \\, \\mu g \\, N \\, \\text{(per liter)}\n\\]\nSince the flask contains 500 mL (0.5 L) of solution, divide the values by 2 to get the mass of nitrogen in the 500 mL flask:\n\\[\n\\text{Mass of N at the start} = 350.17 \\, \\mu g / 2 = 175.09 \\, \\mu g\n\\] \\[\n\\text{Mass of N at the end} = 298.34 \\, \\mu g / 2 = 149.17 \\, \\mu g\n\\]\nStep 3: Calculate the Amount of Nutrient Taken Up by the Alga\nNext, calculate the amount of nitrogen taken up by the algae during the 5-minute interval:\n\\[\n\\Delta \\text{Mass of N} = 175.09 \\, \\mu g - 149.17 \\, \\mu g = 25.92 \\, \\mu g \\, N\n\\]\nThis represents the total amount of nitrogen removed from the water by the algal biomass in the 5-minute period.\nStep 4: Normalise Nutrient Uptake by Algal Biomass\nTo determine how much nitrogen was taken up per unit mass of algae, divide the total nitrogen uptake by the algal biomass (e.g., 4.5 g):\n\\[\n\\text{Nutrient uptake rate} = \\frac{25.92 \\, \\mu g \\, N}{4.5 \\, g} = 5.76 \\, \\mu g \\, N/g\n\\]\nThis gives the nitrogen uptake rate in terms of μg of nitrogen per gram of algal biomass over the 5-minute interval.\nStep 5: Calculate the Nutrient Uptake Rate per Hour\nSince the uptake was allowed to proceed for 5 minutes over the first interval, but you may want to express the uptake rate per hour, multiply the uptake rate by 12 (since there are twelve 5-minute intervals in one hour):\n\\[\n\\text{Nutrient uptake rate per hour} = 5.76 \\, \\mu g \\, N/g \\times 12 = 69.12 \\, \\mu g \\, N/g/hr\n\\]\nThis uptake rate relates to the specific time interval and can be used to track changes in \\([V]\\) over time. In this example, this uptake rate relates to the first 5 minutes of the experiment. The average \\([S]\\) during this intervals was \\((25 \\, \\mu M + 21.3 \\, \\mu M)/2 = 23.15 \\, \\mu M\\).\nRepeat these steps for each remaining intervals and express \\([V]\\) relative the the mean \\([S]\\) for each interval (some authors use the \\([S]\\) at the start of the interval instead of the mean for the interval).",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/L08b-nutrients_michaelis_menten.html#final-workflow-for-calculating-nutrient-uptake-rate-1",
    "href": "BDC223/L08b-nutrients_michaelis_menten.html#final-workflow-for-calculating-nutrient-uptake-rate-1",
    "title": "Lecture 8b: Uptake Kinetics – Michaelis-Menten",
    "section": "Final Workflow for Calculating Nutrient Uptake Rate",
    "text": "Final Workflow for Calculating Nutrient Uptake Rate\n\n\nDetermine the change in nutrient concentration between successive time points: \\[\n\\Delta [S] = [S_{\\text{start}}] - [S_{\\text{end}}]\n\\]\n\n\nConvert concentrations to mass of nutrient using the molecular mass and flask volume: \\[\n\\text{Mass of nutrient} = [S] \\times \\text{MM of nutrient}\n\\]\n\n\nCalculate the amount of nutrient taken up by the algae during the time interval: \\[\n\\Delta \\text{Mass of nutrient} = \\text{Mass of nutrient (start)} - \\text{Mass of nutrient (end)}\n\\]\n\n\nNormalise the nutrient uptake by the algal biomass: \\[\n\\text{Nutrient uptake rate} = \\frac{\\Delta \\text{Mass of nutrient}}{\\text{Algal biomass}}\n\\]\n\n\nConvert the nutrient uptake rate to an hourly rate, if necessary: \\[\n\\text{Nutrient uptake rate (hourly)} = \\text{Nutrient uptake rate} \\times \\frac{60}{\\Delta t}\n\\]\n\n\nThe important differences between the multiple flask and perturbation experiments are summarised in Table 1.\n\n\n\n\nFeature\nMultiple Flask Experiments\nPerturbation Experiments\n\n\n\nExperimental Setup\nMultiple flasks, each with different \\([S]\\)\n\nSingle flask with initial high \\([S]\\)\n\n\n\nData Independence\nData points are independent\nData points are correlated (repeated measures)\n\n\nAnalysis\nNonlinear least squares regression (NLS)\nNonlinear mixed model (NLMM)\n\n\nR Function\nnls()\nnlme::nlme()\n\n\n\n\n\nTable 1: Key differences between multiple flask and perturbation experiments.\n\n\nOur choice between multiple flask and perturbation experiments depends on our research questions and experimental constraints. In both methods, we must consider all sources of error and variability, such as measurement error, the type of nutrient, the physiological state of the alga, the light intensity, the experimental temperature, and other variables that might affect the uptake response.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 8b: Uptake Kinetics -- Michaelis-Menten"
    ]
  },
  {
    "objectID": "BDC223/Lab2_misc_calcs.html",
    "href": "BDC223/Lab2_misc_calcs.html",
    "title": "Lab 2: Miscellaneous Calculations",
    "section": "",
    "text": "Date\n\n\n\n\nLab Date: 23 September 2024 (Monday)\nDue Date: 7:00, 30 September 2024 (Monday)\n\n\n\nStudents will work as individuals; assignments are per individual. This lab is due on Monday 30 September 2024 at 7:00 on iKamva.\n\n1 Pre-Lab\nRead this lab and contextualise within the pertinent material in your text.\n\n\n2 Post-Lab\nUpon completion of this lab:\n\ntranscribe all tables and questions (Exercises A-E) to an electronic document and submit on iKamva. To submit online on Monday 30 September 2024 at 7:00.\n\n\n\n3 Question 1: Dilutions (10 marks)\nA 1.5% (mass:volume) carrageenan gel consists of 0.75g of carrageenan dissolved in 50 ml of 1% KCl. You accidentally added 0.87g to the 50 ml.\n\nWhat percentage of gel have you accidentally created?\nHow much extra water do you need to add to the 50 ml to achieve the 1.5% gel we initially desired?\nWhat is carrageenan, and in which photoautotrophs is it found?\nWhat role does it play in plants?\nHow do people use it?\n\n\n\n4 Question 2: Quantum Light Measurements (4 marks)\nA blue light source (420 nm) provides an illumination of 120 μmol photons.m-2.s-1. How many photons of light would fall within an area of 25 cm2 within the period of 2 hours?\n\n\n5 Question 3: Plant Growth Rates (9 marks)\nFor Scenarios i) and ii), write down the following:\n\nThe process that the sets of measurements represent;\nSuitable equations for calculating the process;\nThe calculated rates; and\nThe resulting units for the process as determined by your calculation.\n\n\n\n\nScenario i (4 marks)\nScenario ii (5 marks)\n\n\n\n\n- Day 1: Plant biomass of 99 g\n- Time, 0 minutes: 7.95 mg/L O₂\n\n\n- Day 100: Plant biomass of 149 g\n- Time, 20 minutes: 11.39 mg/L O₂\n\n\n\n- Algal biomass: 2.3 g fresh mass\n\n\n\n\n\n6 Question 4: Light Attenuation (15 marks)\nYou are a marine scientist wanting to determine the light penetration into the water column off the coast of Richards Bay, KZN. You want to collect the first set of measurements at a distance of 1 km from the shoreline at 5 m depth increments from the water’s surface down to a depth of 50 m. The second set of matching measurements that you want to collect is at a distance of 20 km from the shoreline.\nUnfortunately, you discover that you left the submersible light meter back in the lab and you only have an instrument suitable for taking light measurements above the water’s surface. So, being a scientist, you make a plan… this involves applying some basic knowledge that you acquired during your 2nd year BSc studies. You go back to the lab with the following measurements:\n\nat 8:00 when you were closest to the shoreline you took a measurement of the incident radiation at the water’s surface, which was 1213 μmol photons.m-2.s-1;\nat 9:35 when you arrived at the station 20 km from the shore you measured an incident radiation of 2166 μmol photons.m-2.s-1.\n\n\nDraw light penetration curves that describe the vertical light intensity as a function of depth for each of the two sites (i.e. graphs of light intensity as a function of depth from the surface down to 50 m).\nDescribe the rationale behind this theoretical approach in an attempt to convince us that your curves are a decent approximation of the real situation.\nOf course your approximation is not going to be perfect. What factors will contribute towards the deviation from the actual situation?\n\n\n\n7 Question 5: Photosynthetic Rate Calculation (10 marks)\nA leaf in full sunlight absorbs 10 mol of photons per square meter per second (mol m⁻² s⁻¹). The leaf has a quantum yield of 0.05 moles of CO₂ fixed per mole of photons absorbed.\nCalculate the photosynthetic rate (in μmol CO₂ m⁻² s⁻¹) of the leaf under these conditions.\n\n\n8 Question 6: Relative Growth Rate (RGR) (5 marks)\nThe biomass of a plant at time t₀ is 50 g, and after 10 days (time t₁), the biomass increases to 80 g.\nCalculate the relative growth rate (RGR) in g g⁻¹ day⁻¹ using the equation:\n\\[\nRGR = \\frac{ \\ln (W_1) - \\ln (W_0)}{ t_1 - t_0 }\n\\]\nWhere:\n\n\\((W_1\\)) is the biomass at time \\((t_1\\))\n\\((W_0\\)) is the biomass at time \\((t_0\\))\n\n\n\n9 Question 7: Respiration Rate and Plant Carbon Balance (5 marks)\nA plant in darkness consumes 5 mg CO₂ per hour for respiration. During the day, its photosynthetic rate is 15 mg CO₂ per hour.\nCalculate the net carbon balance of the plant over a 24-hour period, assuming 12 hours of light and 12 hours of darkness. Is the plant in a positive or negative carbon balance?\n\n\n10 Question 8: Additive Light Intensity at Different Depths in Water (7 marks)\nIn an aquatic research setup, light at different depths is a combination of direct surface sunlight and diffuse underwater light. At a depth of 2 meters, the following photon flux densities are measured:\n\nDirect sunlight: 400 μmol photons m⁻² s⁻¹\nDiffuse underwater light from reflections: 120 μmol photons m⁻² s⁻¹\nScattered light from particles in the water: 50 μmol photons m⁻² s⁻¹\n\n\nCalculate the total photon flux density at a depth of 2 meters.\nIf an aquatic plant requires a minimum of 500 μmol photons m⁻² s⁻¹ for photosynthesis, does this plant receive sufficient light at this depth?\n\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{smit,_a._j.,\n  author = {Smit, A. J.,},\n  title = {Lab 2: {Miscellaneous} {Calculations}},\n  url = {http://tangledbank.netlify.app/BDC223/Lab2_misc_calcs.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSmit, A. J. Lab 2: Miscellaneous Calculations. http://tangledbank.netlify.app/BDC223/Lab2_misc_calcs.html.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lab 2: Miscellaneous Calculations"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html",
    "href": "BDC223/L07-chromatic_adaptation.html",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "",
    "text": "Content\n\n\n\n\nDiscuss the importance of light in photosynthesis and the different types of pigments involved.\nExplain the concept of light compensation point and light saturation point.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#introduction",
    "href": "BDC223/L07-chromatic_adaptation.html#introduction",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Introduction",
    "text": "Introduction\nWelcome back again to BDC 223. Today, we’re going to be talking about the development of the theory of chromatic adaptation. This is an interesting story that offers insight into how science evolves, and it demonstrates that over a span of about \\(150\\) to \\(170\\) years, our understanding of how plants adapt to a variable light environment—in terms of both quality and quantity of light—has changed significantly. These shifts in thinking resulted from the accumulation of different forms of evidence, the advent of new technologies allowing for diverse methods of measurement, and the gradual building up of empirical data over the years.\nLet us explore some of the key contributors to our theories surrounding chromatic adaptation.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#early-observations-anders-ørsted",
    "href": "BDC223/L07-chromatic_adaptation.html#early-observations-anders-ørsted",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Early Observations: Anders Ørsted",
    "text": "Early Observations: Anders Ørsted\nThe story begins in the mid-19th century with Anders Ørsted. In \\(1843\\), Ørsted observed that seaweeds come in different colours: green, brown, and red. Today we refer to these as the Chlorophyta (green algae), Phaeophyta (brown algae), and Rhodophyta (red algae). Ørsted noted this colour variation and postulated that perhaps it was related to where seaweeds live in the ocean—that their colours might correlate with their specific habitats.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#theodor-engelmann-and-chromatic-adaptation-theory",
    "href": "BDC223/L07-chromatic_adaptation.html#theodor-engelmann-and-chromatic-adaptation-theory",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Theodor Engelmann and Chromatic Adaptation Theory",
    "text": "Theodor Engelmann and Chromatic Adaptation Theory\nBuilding on Ørsted’s observations, Theodor Engelmann published his theories around \\(1881\\) to \\(1883\\). Engelmann’s primary question was: “Does light quality—the colour of light—affect the vertical distribution of different seaweeds in the ocean?” He hypothesised that because red and blue lights are most prevalent in shallow waters, green algae, being able to absorb these wavelengths most effectively, would be abundant there. In contrast, in intermediate water depths where green light predominates, brown algae—capable of absorbing green light—would be found. Deeper down, where only dim blue light remains, red algae, with pigments that can absorb blue light, would become dominant.\nOn paper, this seems a reasonable hypothesis—even today, many might formulate similar conjectures without access to contemporary evidence. Engelmann sought to test this hypothesis experimentally.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#engelmanns-experimental-design",
    "href": "BDC223/L07-chromatic_adaptation.html#engelmanns-experimental-design",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Engelmann’s Experimental Design",
    "text": "Engelmann’s Experimental Design\nEngelmann’s clever experiment utilised a prism, constructed by Carl Zeiss—a leading figure in optics at the time. Zeiss’s company, known for its lenses and microscopes, continues to exist. Engelmann used the prism to split white light into its constituent spectral colours, ranging from red to blue, which he then projected onto a microscope slide.\nOn this slide, along the gradient of coloured light, he positioned different algae: a green alga (for example, Cladophora, though Engelmann used a species within this genus), a brown alga (unicellular diatoms, known for their xanthophyll pigments and golden-brown hue), and a red alga (such as the filamentous Polysiphonia, which contains phycobiliproteins like phycocyanin and phycoerythrin).\nIn the watery medium surrounding the algae, Engelmann introduced aerotactic bacteria—organisms attracted to regions with the highest oxygen concentration. The premise was that as each type of alga was exposed to a spectrum of light, photosynthesis would occur most efficiently at wavelengths suited to its pigments. The bacteria would congregate where oxygen (a byproduct of photosynthesis) was produced most abundantly, thereby indicating which regions of the light spectrum promoted photosynthesis for each algal type.\n\nAction Spectrum Demonstration\nEngelmann observed, for example, that in green algae—containing primarily chlorophyll-a (and to a lesser extent, chlorophyll-b)—the bacteria accumulated at two main peaks along the slide: those corresponding to red and blue light. This demonstrated, for the first time, an action spectrum—the relationship between wavelength and photosynthetic activity—though Engelmann did not yet reference absorption spectra (as you’ll see later with the work of Haxo and Blinks).\nHe also performed this experiment with brown algae and red algae. In these cases, thanks to their accessory pigments (xanthophylls in browns, phycobilins in reds), photosynthesis also occurred in the green gap region where chlorophyll-a is ineffective. Bacteria correspondingly accumulated in the wavelengths that these accessory pigments absorb.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#implications-and-predictions",
    "href": "BDC223/L07-chromatic_adaptation.html#implications-and-predictions",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Implications and Predictions",
    "text": "Implications and Predictions\nEngelmann’s results seemed to experimentally confirm Ørsted’s reasoning from \\(1844\\), stating that green algae photosynthesise most effectively in red and blue light. Consequently, Engelmann argued that green algae should be most abundant in shallow oceanic waters, where red and blue light penetrate deeply. Red light is quickly attenuated with depth, thus restricting green algae to shallower regions.\nHe reasoned that brown algae, with their ability to exploit greenish wavelengths due to xanthophylls, would be most successful at intermediate depths. Meanwhile, red algae—able to use blue light particularly efficiently—would dominate the deeper ocean, where blue and green light are most available.\nAt the time, no one had directly surveyed seaweed distribution by colour at different depths, but the experimental and theoretical framework appeared solid. For decades, Engelmann’s work underpinned ecological thinking about seaweed vertical distribution—even as recently as the \\(1980\\)s and \\(1990\\)s, some zoologists continued to teach this narrative as fact.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#moving-forward",
    "href": "BDC223/L07-chromatic_adaptation.html#moving-forward",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Moving Forward",
    "text": "Moving Forward\nAs we shall see in the next lecture, later work by Haxo and Blinks provided further evidence for this mode of thinking. However, more recent research began to challenge this story, signalling the emergence of a different understanding—a topic we will discuss in more detail in due course.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#revisiting-engelmanns-work",
    "href": "BDC223/L07-chromatic_adaptation.html#revisiting-engelmanns-work",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Revisiting Engelmann’s Work",
    "text": "Revisiting Engelmann’s Work\nContinuing with our narrative on chromatic adaptation, let us turn our attention to some of the research undertaken by Haxo and Blinks, which was carried out approximately 50 or 60 years after Engelmann’s initial experiments. They were able to repeat similar experiments to those of Engelmann, but with the benefit of more modern technological advancements. Specifically, they had access to laboratory-built devices capable of precisely generating monochromatic spectra of light.\nIn their studies, Haxo and Blinks used a variety of red, brown, and green seaweeds—akin to Engelmann’s approach. However, they extended the methodology by examining not only the action spectra, but also the absorption spectra of both intact thalli and pigment extracts from the different seaweeds. As a result, they utilised three distinct lines of evidence in tandem: action spectra, absorption spectra from intact tissue, and spectra from pigment extracts. Engelmann had primarily focused on action spectra, whereas Haxo and Blinks introduced this more differentiated and integrated approach.\nBased on their accumulating evidence, they drew several novel conclusions.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#green-algae-action-and-absorption-spectra",
    "href": "BDC223/L07-chromatic_adaptation.html#green-algae-action-and-absorption-spectra",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Green Algae: Action and Absorption Spectra",
    "text": "Green Algae: Action and Absorption Spectra\nWhen conducting experiments on green algae, Haxo and Blinks found that the thallus absorbs light most efficiently in the zones of blue light, approximately \\(430\\) nanometres, as well as in the red light region. The absorption spectrum of the thallus closely resembled that obtained from a chlorophyll-a extract.\nAdditionally, the absorption spectrum in the thallus matched very closely the action spectrum established for green algae. Whereas Engelmann’s methods relied on the movement of aerotactic bacteria as an indirect measure of photosynthesis, Haxo and Blinks directly measured photosynthetic rates.\nFor green algae, their findings were represented on a graph: the absorption peak for chlorophyll-a appears in the blue light region, as well as in the red. The solid line, marked with open circles, depicts the absorption spectrum, i.e., the extent to which the thallus absorbs light at the tested wavelengths. The dotted line illustrates the action spectrum—the rate of photosynthesis across those wavelengths. Strikingly, at the peaks of chlorophyll-a absorption—in both the blue and red regions—the action and absorption spectra are almost identical.\nHowever, there exists a significant discrepancy in the region from approximately \\(460\\) to \\(500\\) nanometres. In this region, carotenoids, principally beta-carotene, play a key role by absorbing light inaccessible to chlorophyll-a alone, passing that energy to chlorophyll-a, and thus facilitating photosynthesis at those wavelengths. This effect highlights the vital function of accessory pigments such as carotenoids, enabling photosynthetic activity in spectra where chlorophyll-a would otherwise be ineffective.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#the-role-of-accessory-pigments-in-brown-algae",
    "href": "BDC223/L07-chromatic_adaptation.html#the-role-of-accessory-pigments-in-brown-algae",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "The Role of Accessory Pigments in Brown Algae",
    "text": "The Role of Accessory Pigments in Brown Algae\nSimilar experiments with brown algae showed that the dominant accessory pigments are xanthophylls, particularly fucoxanthin, which give these seaweeds their distinctive colour. Fucoxanthin enhances absorption in the green to yellowish part of the spectrum—between \\(500\\) and \\(560\\) nanometres. The presence of chlorophyll-c also contributes to absorption at around \\(630\\) nanometres.\nSome brown algal species accumulate so much chlorophyll-c that they appear almost “optically black,” meaning they are nearly opaque to all light—a phenomenon we will discuss in more detail later.\nIn summary, peaks persist in both the blue and red regions (where chlorophyll-a absorbs maximally). However, in the so-called “green gap” region—which would otherwise show limited absorption were it not for accessory pigments—the presence of fucoxanthin allows light capture, thereby enabling chlorophyll-a to drive photosynthesis in the green to yellowish region. Xanthophylls, primarily fucoxanthin in this context, expand the functional spectral range available to brown algae.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#red-algae-patterns-anomalies-and-the-emerson-enhancement-effect",
    "href": "BDC223/L07-chromatic_adaptation.html#red-algae-patterns-anomalies-and-the-emerson-enhancement-effect",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Red Algae: Patterns, Anomalies, and the Emerson Enhancement Effect",
    "text": "Red Algae: Patterns, Anomalies, and the Emerson Enhancement Effect\nRed seaweeds demonstrate a broadly similar pattern, with absorption and action spectra matching closely except for a notable anomaly between \\(500\\) and \\(570\\) nanometres. An explanation for this only surfaced later, and you are not expected to master it at this stage. This is known as the Emerson enhancement effect: a complex photophysiological response observable primarily in red algae.\nOverall, the major divergence between the absorption and action spectra at particular peaks is attributable to this phenomenon. For deeper exploration, seminal literature exists, such as the 1964 publication by Govindjee (often cited simply as “Govindjee”), one of the preeminent figures in photosynthesis research. If you wish to read further on the Emerson enhancement effect, consult works by Rajni Govindjee and Govindjee.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#from-engelmann-to-haxo-and-blinks-confirmation-and-caution",
    "href": "BDC223/L07-chromatic_adaptation.html#from-engelmann-to-haxo-and-blinks-confirmation-and-caution",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "From Engelmann to Haxo and Blinks: Confirmation and Caution",
    "text": "From Engelmann to Haxo and Blinks: Confirmation and Caution\nFundamentally, the work of Haxo and Blinks provides modern and more definitive confirmation of Engelmann’s earlier findings, as noted in subsequent review papers by authors such as Mary Beth Sappho. However, the conclusions drawn by Haxo and Blinks are somewhat more measured regarding the vertical distribution of seaweeds.\nIn the case of red algae, where phycobilins function as accessory pigments, they conclude that it is logical to assume the current vertical distribution of red algae is influenced, at least in part, by the photosynthetic effectiveness conferred by phycobilins. This enables some species to extend their distribution to depths inaccessible to other algae. Essentially, red algae—by virtue of their red pigment, which can absorb blue light—are able to survive further down the water column. Nevertheless, they note that other algal groups, such as greens and browns, are also located at considerable depths, and red algae are present even in shallow waters.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#chromatic-adaptation-and-vertical-distribution-theory-and-reality",
    "href": "BDC223/L07-chromatic_adaptation.html#chromatic-adaptation-and-vertical-distribution-theory-and-reality",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Chromatic Adaptation and Vertical Distribution: Theory and Reality",
    "text": "Chromatic Adaptation and Vertical Distribution: Theory and Reality\nThis real-world evidence introduces a measure of doubt to the hypothesis that green algae occupy shallow waters, red algae deep waters, and brown algae intermediate depths. Observational data show substantial overlap among all three groups regarding both minimum and maximum depth ranges. For instance, even though red algae are documented as extending down to roughly \\(160\\) metres (as per a 1974 publication), more recent discoveries have shown a green alga at what is currently the deepest recorded distribution for any seaweed [attention].\nSuch empirical data suggest that the theoretical framework of chromatic adaptation—namely, that a seaweed’s pigment determines its depth distribution—does not hold up against actual observations. Instead, pigmentation seems not to be reliably indicative of environment, nor a sole determinant of where a species is found.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#towards-an-explanation",
    "href": "BDC223/L07-chromatic_adaptation.html#towards-an-explanation",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Towards an Explanation",
    "text": "Towards an Explanation\nThus, the chromatic adaptation theory does not adequately explain the actual vertical distribution of seaweeds. To gain a clearer understanding of what does underlie these patterns, we must examine further research, especially the work of Ramus, Rosenberg, and Ramus, which will form the subject of our next lecture.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#introduction-1",
    "href": "BDC223/L07-chromatic_adaptation.html#introduction-1",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Introduction",
    "text": "Introduction\nRight, so now we’re going to look at the third part of our series of lectures on chromatic adaptation and, this time, we’re going to examine some in situ experiments that explore how seaweeds adapt—or more accurately, acclimatise—to different light regimes within the ocean. This is a very different approach compared to the earlier studies by Engelmann, Hacksaw, and Blinks, all of whom relied on laboratory studies, isolating seaweeds from the ambient environment and failing to explore how they respond adaptively or acclimatise to changing environmental conditions. This is where Rosenberg and Ramos’s work is quite distinct.\nSpecifically, we will focus on experiments by John Ramos from the late 1970s, which are rather fascinating in their design. Ramos investigated acclimatisation as a process whereby seaweeds, irrespective of their colour, become suited to different light regimes found in the marine environment.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#experimental-design",
    "href": "BDC223/L07-chromatic_adaptation.html#experimental-design",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Experimental Design",
    "text": "Experimental Design\nThe method they used was as follows: Seaweeds acclimatised to shallow water—the surface, say, for a couple of weeks, so that their photophysiological machinery and pigment profiles had the chance to adapt—were compared to seaweeds that had, during the same period, been grown in deeper water, at approximately \\(10\\,\\mathrm{m}\\) depth, and allowed to become acclimatised to those conditions.\nNoting that seaweeds come in a range of colours, they included red and green seaweed species. Contrary to what I just said, sorry—yes, red and green seaweeds. The red algae possess phycobilins as accessory pigments, along with some \\(\\beta\\)-carotene, while green algae predominantly have chlorophyll-a with a bit of \\(\\beta\\)-carotene, but absolutely no phycobilins in the greens.\nThey selected four species:\n\nChondrus crispus (red)\nPorphyra umbilicalis (red)\nCodium fragile (green)\nUlva lactuca (green)\n\nYou’ll notice these represent two functional forms: the coarsely branched type (Chondrus and Codium) and the membranous group (Porphyra and Ulva). By spanning both different pigment groups and functional forms, Ramos and colleagues devised a transplant experiment to address how various algae types acclimatise to different oceanic light levels.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#comparing-functional-forms-and-light-adaptation",
    "href": "BDC223/L07-chromatic_adaptation.html#comparing-functional-forms-and-light-adaptation",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Comparing Functional Forms and Light Adaptation",
    "text": "Comparing Functional Forms and Light Adaptation\nSo, focusing on comparisons within the same functional form, particularly the membranous group—Ulva and Porphyra, green and red respectively—let’s consider specimens acclimatised to \\(10\\,\\mathrm{m}\\) depth for a couple of weeks. Examining the photosynthetic rate per gram of chlorophyll-a, it was found that Ulva lactuca and Porphyra umbilicalis exhibited precisely the same rate of photosynthesis. This equality, however, only exists when expressing the photosynthetic rate on a per gram of chlorophyll-a basis. For every gram of chlorophyll-a present in either Porphyra or Ulva, the amount of photosynthesis driven is exactly the same—chlorophyll-a, irrespective of the species, performs identically as the primary photosynthetic pigment.\nHowever, when expressing photosynthetic rate per gram of tissue (i.e., per gram of Ulva or Porphyra), Ulva outperforms Porphyra. The underlying reason is simple: one gram of Ulva contains more chlorophyll-a than one gram of Porphyra. This makes sense, since Ulva is mainly composed of chlorophyll-a, whereas in Porphyra, much of the pigment content consists of accessory pigments (principally phycobilins), rather than chlorophyll-a.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#surface-area-to-volume-effects",
    "href": "BDC223/L07-chromatic_adaptation.html#surface-area-to-volume-effects",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Surface Area to Volume Effects",
    "text": "Surface Area to Volume Effects\nAdditionally, when comparing red and green seaweeds at \\(10\\,\\mathrm{m}\\) depth within different functional forms—contrast a flat membrane (high surface area to volume ratio) with something more robust (low surface area to volume ratio)—it is clear that tissues with a high surface area to volume ratio show a greater photosynthetic rate per gram of tissue than those with a low ratio. This is easily predicted using the functional form model: per unit tissue, species with higher surface area to volume ratios can sustain higher photosynthesis rates.\n\nTransplant Experiments: Shallow vs Deep\nFor the transplant experiments, they took seaweeds acclimatised to \\(10\\,\\mathrm{m}\\) and moved them to the immediate subsurface (zero metres), where light is both more abundant and richer in red wavelengths. Photosynthetic rate per gram of tissue at the surface approximately doubled compared to specimens kept at depth. The reasoning is straightforward—we’ll elaborate on the mechanisms shortly, but increased light availability at the surface supports higher rates of photosynthesis.\nExecuting the reverse—transplanting from the surface to \\(10\\,\\mathrm{m}\\)—resulted in a drop in photosynthetic rate, reflecting the reduced light availability at depth. This is simply a consequence of photosynthetic response to less light.\nComparing pigment concentrations, seaweeds acclimatised to \\(10\\,\\mathrm{m}\\) displayed chlorophyll-a concentrations around ten times higher than conspecifics from the surface. Not just that, the ratio of chlorophyll-b to chlorophyll-a—chlorophyll-b being an accessory pigment—increases markedly in deeper-grown specimens. Thus, seaweeds in lower-light environments boost both primary and accessory pigments, augmenting their light-harvesting capacity.\nCodium, the coarsely branched green alga, however, presented a different case. Regardless of being grown shallow or deep, the rate of photosynthesis per gram of tissue remained nearly identical, even after transplantation.\nLooking into pigment concentrations for surface versus deep-acclimatised codium, the difference was marginal—only about 1.5 times higher chlorophyll-a and b at depth. Moreover, neither the ratio of chlorophyll-b to a, nor photosynthetic rates per chlorophyll-a or per surface area, differed much between environments.\nIn summarising all these results, many types of comparisons arise, helping test numerous hypotheses concerning seaweed acclimatisation and adaptation to varying environments.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#acclimatisation-mechanisms-and-plasticity",
    "href": "BDC223/L07-chromatic_adaptation.html#acclimatisation-mechanisms-and-plasticity",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Acclimatisation Mechanisms and Plasticity",
    "text": "Acclimatisation Mechanisms and Plasticity\nOnce seaweeds were acclimatised to low light, they found that the photosynthetic rate per gram of chlorophyll-a in Codium—the coarsely branched green alga—was higher, and likewise, rate per unit surface area was greater compared with Ulva.\nSo, what’s happening? This mountain of data reveals several important adaptive responses. For seaweeds like Ulva, this can be understood via the plasticity of their photophysiological mechanisms. Ulva, for instance, can modify its physiology, pigment composition, and growth responses in reaction to prevailing environmental conditions—a demonstration of physiological plasticity.\nSo, as Ulva is transplanted from high to low light (surface to depth), an immediate reduction in photosynthetic rate occurs—less light equals less photosynthesis. This represents the organism being placed at a lower segment of its PI (photosynthesis–irradiance) curve. However, left at depth for several weeks, Ulva gradually manufactures more chlorophyll-a and invests more energy into accessory pigment production (chlorophyll-b). The increased chlorophyll content raises light-harvesting capability, bringing the photosynthetic rate back up—sometimes reaching up to ten times greater than for non-acclimatised specimens moved directly from the surface to depth.\nThe ratio of chlorophyll-b to a rises too, signifying compensatory synthesis of accessory pigments under dim conditions.\nThe opposite holds when moving from deep to shallow: photosynthesis rates spike instantly, but over time, pigment concentrations (particularly chlorophyll-b relative to chlorophyll-a) decrease, as high irradiance makes accessory pigments unnecessary. But across all conditions, the photosynthetic rate per gram of chlorophyll-a remains constant, because chlorophyll-a’s physiological efficiency as a light-harvesting molecule does not change with acclimatisation.\n\nCodium’s Distinctive Behaviour\nCodium, however, does not behave the same way. Despite having similar pigments to Ulva, its internal architecture is markedly different, which underpins its dissimilar responses.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#structural-and-optical-differences-ulva-vs-codium",
    "href": "BDC223/L07-chromatic_adaptation.html#structural-and-optical-differences-ulva-vs-codium",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Structural and Optical Differences: Ulva vs Codium",
    "text": "Structural and Optical Differences: Ulva vs Codium\nTo clarify, let’s examine Ulva. Surface-acclimatised Ulva absorbs only \\(19\\%\\) of available light, as the abundance of light means only a fraction need be harvested for optimal photosynthetic function. Its thallus is just \\(17\\,\\mu\\mathrm{m}\\) thick, with a minimal amount of photosynthetic pigment per square centimetre.\nAt high light, absorption is reduced—the organism is inefficient at harvesting all incident light, since what it does capture is ample for its needs. Its high surface area to volume ratio allows rapid translation of harvested light into growth.\nTransplanted to depth, the same thin Ulva thallus ramps up its pigment content—notably chlorophyll-a—and thus absorbs more light across the spectrum, especially when blue and red light dominate at depth. Absorption in the green region (the “green gap”), as well as across the spectrum, increases because accessory pigments like chlorophyll-b accumulate.\nBut even with increased pigment content, there’s always a green gap, as Ulva lacks the accessory pigments required to efficiently absorb green wavelengths. Interestingly, Codium also lacks such accessory pigments, yet it is able to absorb significant amounts of light even in the green gap.\nThe reason is that Codium is optically black. That is, its tissues are so densely packed with pigment, particularly chlorophyll-a, within a very thick (\\(3\\,\\mathrm{mm}\\)) thallus, that they are entirely opaque. The pigment is predominantly positioned at the periphery of the coarsely branched utricles (the outward-pointing structures on the thallus). Structurally, Codium is a multinucleate, syncytial organism with an internal mesh of filaments but no individual cell walls separating cells as in Ulva.\nThe concentration of chlorophyll-a at the periphery means that any incident light is almost certainly absorbed before it can penetrate far, or it becomes trapped and wave-guided by the internal filaments, ensuring maximal light capture. This is why Codium maintains consistently high photosynthetic efficiency and pigment concentrations independent of depth, and thus shows little acclimatory response.\nUlva, by contrast, with its thin, “cellophane” membrane, allows much light to pass through, relying on every cell receiving direct illumination, but cannot match Codium’s absorptive efficiency in the green region or at depth.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L07-chromatic_adaptation.html#conclusion",
    "href": "BDC223/L07-chromatic_adaptation.html#conclusion",
    "title": "Lecture 7: Chromatic Adaptation",
    "section": "Conclusion",
    "text": "Conclusion\nSo, to summarise: Ulva demonstrates remarkable plasticity in pigment and physiological responses, allowing acclimatisation to new light regimes—and does so primarily by increasing pigment concentrations at depth. Codium, by virtue of its dense, optically black construction and peripheral pigment placement, absorbs much more light and shows less need for physiological adjustment. Light absorption and photosynthetic rate per chlorophyll-a remain steady, but overall structural and anatomical properties, plus pigment arrangement, explain differences in adaptive responses between these two types of green algae.\nThat covers the key findings and interpretations you need to understand regarding light capture strategies and chromatic adaptation in seaweeds.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 7: Chromatic Adaptation"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html",
    "href": "BDC223/L05-light.html",
    "title": "Lecture 5: Light",
    "section": "",
    "text": "Content\n\n\n\n\nExplain what light is, and explore concepts of frequency, wavelength, and energy.\nDescribe the electromagnetic spectrum and the different types of light.\nDefine the ideas of light quality and quantity.\nExplain how light is measured and the different units used.\nFocus on the quantum nature of light and the concept of photons.\nDiscuss the importance of light in ecosystems.\nExplain photochemical equivalence.\nExplain the concept of photosynthetically active radiation (PAR) and its importance.\nDescribe the different types of light sensors and their applications.\nDescribe the Beer-Lambert Law and its applications.\nExplore the properties of the ocean that affect light penetration and variability.\nExplore the properties of the atmosphere and terrestrial systems that affect light availability and variability.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#introduction-to-light-as-a-plant-stress",
    "href": "BDC223/L05-light.html#introduction-to-light-as-a-plant-stress",
    "title": "Lecture 5: Light",
    "section": "Introduction to Light as a Plant Stress",
    "text": "Introduction to Light as a Plant Stress\nWelcome to BDC 223. Today, we’re going to continue our lectures on the various stresses that plants experience and the ways in which they interact with their environment. Our focus for today is light. There are several key questions we need to explore in this lecture, and these are also the core points that you’ll need to understand by the end of our coverage on light.\n\nThe main aspects you must grasp include why light is important, and the different properties of light—specifically, the quantity and the quality of light. We must consider what happens to light in both marine and terrestrial environments, and what properties of the world cause light to vary in both quantity and quality in these places. Furthermore, we’ll look at how we measure light, the effects of light on plants, how plants capture light, the process of photosynthesis, designing experiments to measure photosynthesis, and how photosynthesis relates to plant growth rate. All of these topics will feature in this segment of the lectures.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#why-light-is-important-to-plants",
    "href": "BDC223/L05-light.html#why-light-is-important-to-plants",
    "title": "Lecture 5: Light",
    "section": "Why Light Is Important to Plants",
    "text": "Why Light Is Important to Plants\n\nLight, together with temperature, is perhaps one of the most important factors affecting plants. In fact, as far as plants are concerned, it’s the single most important aspect determining many of their properties. It has a drastic effect on photosynthesis. The very reason photosynthesis exists is due to plants’ ability to harvest light, extract radiant energy, and convert it into chemical potential energy, which is then used to sustain growth and drive other aspects of plant productivity.\nLight also influences photoperiodisms, which refer to the various endogenous rhythms that plants undergo, mediated and synchronised by either photoperiod or light intensity. There’s also photomorphogenesis, the process by which developmental and morphological changes in plants are regulated by light.\nOf course, light further influences all manner of ecological interactions. Together with temperature, light has a large consequence for the distribution of plants across the surface of land and within the ocean’s depths.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#global-and-temporal-variability-of-light",
    "href": "BDC223/L05-light.html#global-and-temporal-variability-of-light",
    "title": "Lecture 5: Light",
    "section": "Global and Temporal Variability of Light",
    "text": "Global and Temporal Variability of Light\nLooking at global scales, light is extremely variable across the surface of the Earth, and also across various temporal scales—ranging from seconds, to minutes, to hours, to days, and to seasons. Typically, from year to year, the amount of light tends to be quite stable, but at shorter temporal scales (months, weeks, days, minutes, seconds), it is very variable indeed. We need to understand what causes this variation.\n\nQuantity and Quality of Light\n\nIt’s important to distinguish between the quantity and the quality of light. When referring to the quality of light, we are talking about its colour. The quantity of light is related purely to its intensity, with no distinction made as to whether the light is blue, red, or any other colour. You can have dim red light and bright red light, or dim blue and bright blue sources. Quantity simply means the intensity, regardless of wavelength. Wavelength, on the other hand, is more closely related to quality, or colour.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#the-electromagnetic-spectrum-and-photosynthetically-relevant-light",
    "href": "BDC223/L05-light.html#the-electromagnetic-spectrum-and-photosynthetically-relevant-light",
    "title": "Lecture 5: Light",
    "section": "The Electromagnetic Spectrum and Photosynthetically Relevant Light",
    "text": "The Electromagnetic Spectrum and Photosynthetically Relevant Light\n\nLight is just a portion of the electromagnetic radiation spectrum. Human eyes are sensitive to light of wavelength between about \\(390\\) to \\(760\\,\\text{nanometres}\\) (\\(\\mathrm{nm}\\)), which is much the same as the range plants can utilise. I’ll talk about the nuances relating to plants shortly.\n\nThere are also shorter wave portions in the UV spectrum (\\(290\\) to \\(390\\,\\mathrm{nm}\\)) and longer wave portions in the infrared spectrum (roughly \\(750\\) to \\(3,000\\,\\mathrm{nm}\\)). These can influence biological processes to varying extents. UV light does not generally affect photophysiological processes significantly, but it does provide enough energy to cause genetic mutations in certain cellular components, especially in single cells. Although UV light has a lot of energy, it isn’t actually perceived by the eye or photophysiological pigments, so it doesn’t have a direct, measurable effect on photophysiological processes in plants.\nInfrared radiation, above about \\(750\\,\\mathrm{nm}\\) up to \\(3,000\\,\\mathrm{nm}\\), is what we feel as heat. On a bright sunny day, you feel your skin warming up due to this infrared radiation. However, neither eyes nor plant pigments are sensitive to this part of the spectrum.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#light-interactions-in-the-marine-and-terrestrial-environments",
    "href": "BDC223/L05-light.html#light-interactions-in-the-marine-and-terrestrial-environments",
    "title": "Lecture 5: Light",
    "section": "Light Interactions in the Marine and Terrestrial Environments",
    "text": "Light Interactions in the Marine and Terrestrial Environments\nWhen light falls on water, some of the longer wavelengths, such as reds, and to a lesser extent, some blues, are absorbed by the water. Red light has lower energy and doesn’t penetrate as deeply into the water column as blue light, which is higher in energy and penetrates much deeper into the ocean.\nOn land, there’s not nearly as much variation in light quality as there is in the marine environment. However, it is crucial to understand what happens to light under water. Red light is absorbed first, and blue light can travel much deeper. As blue light is scattered in the water column, that’s why when you look down into the ocean or put your head underwater, the world appears blue. Similarly, this is why the Earth looks blue from space—the blue light from the ocean surface is being scattered, while red light is absorbed. The red light also warms the ocean surface.\nFor example, if you take a white plate or a piece of white paper underwater and dive down five or ten metres, it appears blue because there is a predominance of blue light at those depths.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#wavelength-frequency-and-energy-relationships",
    "href": "BDC223/L05-light.html#wavelength-frequency-and-energy-relationships",
    "title": "Lecture 5: Light",
    "section": "Wavelength, Frequency, and Energy Relationships",
    "text": "Wavelength, Frequency, and Energy Relationships\nLight possesses both wave and particle properties. The wavelength and frequency of light are measurable and are related to each other via the speed of light (\\(c\\)). That is, \\(c = \\lambda \\nu\\), where \\(\\lambda\\) is the wavelength and \\(\\nu\\) the frequency.\nShort wavelength radiation, like violet light, has high frequency, and red light (around \\(700\\,\\mathrm{nm}\\)) has low frequency.\nEnergy is also related to wavelength. The energy of a photon is given by \\(E = h\\nu\\), where \\(h\\) is Planck’s constant, and also by \\(E = \\dfrac{hc}{\\lambda}\\). Blue, violet, and ultraviolet light, carrying shorter wavelengths, thus contain more energy than reds and infrareds. Typically, it’s the blues and ultraviolets that carry the most energy, while reds and infrareds carry less.\nI am not likely to set this sort of calculation in an exam, but it is a possibility for a test or class exercise, so make sure you understand the relationship.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#light-penetration-and-beer-lambert-law",
    "href": "BDC223/L05-light.html#light-penetration-and-beer-lambert-law",
    "title": "Lecture 5: Light",
    "section": "Light Penetration and Beer-Lambert Law",
    "text": "Light Penetration and Beer-Lambert Law\n\nThe extent to which light diminishes as we go down a water column is called attenuation. This is described by the Beer-Lambert Law, which provides us with an equation to calculate the light intensity at a given depth (\\(I_z\\)), based on the incident light at the water’s surface (\\(I_0\\)) and a constant quantifying attenuation, the attenuation coefficient (\\(k\\)):\n\\[\nI_z = I_0\\,e^{-kz}\n\\]\n\nIn coastal environments, the attenuation coefficient is generally high, due to increased turbidity—lots of particles and suspended solids absorbing and scattering light. In open ocean waters, this coefficient is much lower, allowing light to penetrate deeper due to clearer water.\nYou will certainly be assessed on your understanding of the Beer-Lambert Law, both in written assessments and practicals where you may need to apply this equation to actual data.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#the-structure-of-the-ocean-light-zones",
    "href": "BDC223/L05-light.html#the-structure-of-the-ocean-light-zones",
    "title": "Lecture 5: Light",
    "section": "The Structure of the Ocean: Light Zones",
    "text": "The Structure of the Ocean: Light Zones\n\nAs light penetrates the ocean, its intensity decreases and its quality changes with depth. At about \\(1\\,\\mathrm{m}\\) depth, compared to the surface, we already see a diminished amplitude in available radiation. Primarily, only the blues and greens can penetrate deeply; purples, yellows, oranges, and reds are absorbed quickly.\n\nAt about \\(10\\,\\mathrm{m}\\) depth, mostly blues and greens remain. This again explains why a white object appears blue as you dive down. Deeper still, at approximately \\(100\\,\\mathrm{m}\\), only blue light remains.\nAbout \\(100\\) to \\(450\\,\\mathrm{m}\\) is known as the twilight or dysphotic zone—very little photosynthesis occurs here, although some algal cells may persist. The upper, well-lit portion is the photic zone, and this is where almost all marine photosynthesis takes place. Below the photic zone, in the aphotic zone, it’s completely dark.\nThe warmest water is at the upper layers—top \\(50\\) to \\(100\\,\\mathrm{m}\\)—as red, yellow and some UV light are absorbed and converted into heat. Below this, due to limited light energy, temperatures decrease quickly. You can experience this if you swim away from the shore and dive to even \\(5\\,\\mathrm{m}\\)—there, you’ll feel the water is much colder.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#coastal-vs.-open-ocean-light-penetration",
    "href": "BDC223/L05-light.html#coastal-vs.-open-ocean-light-penetration",
    "title": "Lecture 5: Light",
    "section": "Coastal vs. Open Ocean Light Penetration",
    "text": "Coastal vs. Open Ocean Light Penetration\n\nThere is a distinction between coastal water and open ocean water in terms of light penetration. In coastal waters, due to higher amounts of total suspended solids (TSS) and turbidity resulting from riverine input, human activity, pollution, and erosion, light does not penetrate much beyond \\(50\\,\\mathrm{m}\\). In the open ocean, light can go much deeper, with much lower turbidity.\nTSS in coastal areas absorbs and scatters light more, resulting in diminished light intensity and a shift in quality—colours change as certain wavelengths are more heavily absorbed or scattered. Despite this, blues and greens can penetrate furthest in the open ocean, as fewer particles are available to absorb them.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#measuring-light",
    "href": "BDC223/L05-light.html#measuring-light",
    "title": "Lecture 5: Light",
    "section": "Measuring Light",
    "text": "Measuring Light\n\nQuantum Measurements\n\nScientifically, we measure light using a quantum approach—the number of particles or ‘quanta’ (photons) of light falling on a given surface per unit time and per unit area. The standard unit is the micromole per metre squared per second (\\(\\mu\\mathrm{mol\\, m^{-2}\\, s^{-1}}\\)). This represents the number of photons (quanta) falling on \\(1\\,\\mathrm{m}^2\\) per second. One mole equals Avogadro’s number (\\(6.022 \\times 10^{23}\\)) of photons.\nThis quantum measurement is additive—if \\(10\\,\\mu\\mathrm{mol\\, m^{-2}\\, s^{-1}}\\) fall in one second, after two seconds you’ll have \\(20\\,\\mu\\mathrm{mol\\, m^{-2}}\\). Additive approaches also allow scaling from small to large areas or short to long periods.\n\nImportantly, quantum measurements do not distinguish between colours of light. Every quantum counts equally, regardless of whether it is red, blue, or any other colour. Thus, quantum measurements are agnostic regarding the quality of light.\n\nExample Typical Ranges\n\nIn the tropics, midday full sunlight yields about \\(2{,}500\\text{--}3{,}000\\,\\mu\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}\\). At our latitude it’s a bit lower, roughly \\(1{,}800\\text{--}2{,}200\\,\\mu\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}\\). This, of course, varies by season, time of day, cloud cover, turbidity, and other factors such as morning and evening angles.\n\n\n\nLight Sensors\n\nTo measure photon flux density, we use light meters equipped with quantum sensors. There are two main sensor types:\n\nSpherical sensors: These measure light falling from all directions around the sensor—it’s immersed in a sphere and integrates all incident light.\nCosine-corrected sensors: These are flat disks, measuring light from a single primary direction and ignoring off-angle contributions.\n\nBoth sensor types have their place in plant biology, and we will encounter them in practicals.\n\n\nOther Units and Approaches\n\n\nThere are other units for measuring light as energy, but these are less useful to us as biologists, except perhaps in the context of heat loss or gain. Such units, and others like foot candles or lux, are designed more around human visual sensitivity (biased to yellows and oranges), or for industrial and interior lighting, rather than scientific plant research.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#effects-of-different-wavelengths-on-plants",
    "href": "BDC223/L05-light.html#effects-of-different-wavelengths-on-plants",
    "title": "Lecture 5: Light",
    "section": "Effects of Different Wavelengths on Plants",
    "text": "Effects of Different Wavelengths on Plants\n\n\nUltraviolet (&lt; \\(390\\,\\mathrm{nm}\\)): High energy, ionising radiation—damages DNA/RNA, causes mutations, not relevant to photo-physiological processes.\nInfrared (&gt; \\(750\\,\\mathrm{nm}\\)): Too little energy for photosynthesis, but warms surfaces via conversion to heat.\nPhotosynthetically Active Radiation (PAR): The relevant range for photosynthesis is between about \\(390\\text{–}400\\,\\mathrm{nm}\\) and \\(760\\,\\mathrm{nm}\\). Only this portion is able to drive the photochemical reactions of photosynthesis—such as splitting water molecules to release oxygen.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L05-light.html#photosynthetically-active-radiation-par",
    "href": "BDC223/L05-light.html#photosynthetically-active-radiation-par",
    "title": "Lecture 5: Light",
    "section": "Photosynthetically Active Radiation (PAR)",
    "text": "Photosynthetically Active Radiation (PAR)\nPAR is the essential definition to remember. It is the segment of light—between \\(390\\,\\mathrm{nm}\\) and \\(760\\,\\mathrm{nm}\\)—that can actually be utilised by plants for photosynthesis. This will be a recurring concept as we progress through these lectures.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 5: Light"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html",
    "href": "BDC223/L04-carbon_cycle.html",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "",
    "text": "Content\n\n\n\n\nThe carbon cycle and climate change: a brief overview and history of our understanding of climate change.\nThe contributions of key historical figures such as Arrhenius, Callendar, and Keeling.\nThe role of the carbon cycle in climate change.\nAtmospheric response in heat content.\nThe role and response of the ocean in the carbon cycle.\nFuture scenarios for Earth.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#introduction-and-overview",
    "href": "BDC223/L04-carbon_cycle.html#introduction-and-overview",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Introduction and Overview",
    "text": "Introduction and Overview\nSo, for today’s lecture, we are going to talk a bit about the importance of the carbon cycle to plants. Of course, the carbon cycle is extremely important to plants because it unites light, carbon, and water in the process of photosynthesis. But we also know, of course, that the carbon cycle has changed over the last 250 or so years as a result of industrialisation. It is important, then, to understand the physics and the changes to the Earth system that are leading to this changing carbon cycle.\nWhen I speak about the carbon cycle, the main way people are affected is via climate change, which is a small subset of what we call global change. Global change is comprised of all those various things we referenced at the end of Tuesday’s lecture, including biodiversity loss, changes to the hydrological cycle, the nitrogen and phosphorus cycles, and all of those aspects discussed in the planetary boundaries set of lectures. So, the carbon cycle and climate change form one of those aspects of global change.\nAs we have seen, we are already rapidly accelerating towards thresholds—thresholds which, if exceeded, will take us into uncharted and dangerous territory. In fact, we are arguably already there. I’ll explain to you shortly that no modern human has ever experienced the kinds of climatic phenomena that we are experiencing today, so it is quite dangerous for people and for much of the rest of life on Earth.\nToday I aim to provide you with an overview of climate change: what is known about climate change—not so much its contemporary science, but more the history, seeing when people first started thinking about it. Today’s lecture will not be particularly long. I want to talk about the history of climate change, what we know today, and to contextualise our current circumstances within what is known about human development over the last 10,000 or so years.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#climate-change-not-a-recent-realisation",
    "href": "BDC223/L04-carbon_cycle.html#climate-change-not-a-recent-realisation",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Climate Change: Not a Recent Realisation",
    "text": "Climate Change: Not a Recent Realisation\nWhat we know about climate change is not recent, in the sense that the idea of climate possibly changing—or at least the understanding that gases in the atmosphere contribute towards climate change—has been around for a long time. When I refer to climate change, I generally mean that, on average, the world’s climate is warming up. The average is an important concept, of course, because there are places on the planet that are actually cooling down slightly.\nFor instance, around the coastline of South Africa, from De Hoop all along the west coast towards the border with Namibia, the ocean is in fact cooling down in certain places. So, climate change, though global temperatures are on average rising, is experienced differently in different regions—it is heterogeneous, not evenly distributed.\nProjections for the future of Africa, for example, show that in the next 50 to 100 years, Africa may be warmer by six to seven °C (°C), which is far more than the average elsewhere on the planet. Thus, Africa is experiencing a larger degree of climate change relative to the rest of the world; but globally, the net warming is positive.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#historical-development-of-climate-change-science",
    "href": "BDC223/L04-carbon_cycle.html#historical-development-of-climate-change-science",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Historical Development of Climate Change Science",
    "text": "Historical Development of Climate Change Science\n\nThe First Experiments and Realisations\n\nOur understanding of how the world is changing, and how certain gases contribute to that change, started in the late 1800s. There are a few important names to remember: Fourier, Pouillet, Tyndall, and Arrhenius among them.\n\n\nThese individuals worked more than 150 years ago on the basic physics and atmospheric chemistry associated with climate change. The scientific foundation explaining our current understanding of climate change thus existed over a century ago.\n\nThe first notable experiment was published in 1856 by Eunice Foote, an American scientist who performed a very simple experiment. She placed different kinds of gases in jars, including normal air, water vapour, and what was then called “carbonic acid”—that is, carbon dioxide. She exposed these jars to sunlight and measured, using a thermometer, how quickly each jar warmed up depending on its contents. She observed that the jar containing carbon dioxide became significantly warmer than the others and, when removed from the sun, took much longer to cool down.\n\nFoote concluded that an atmosphere containing higher concentrations of carbon dioxide would give the earth a higher temperature. She speculated that if, at some period in history, the air had a higher proportion of carbon dioxide, increased temperature must necessarily have resulted. She deduced that, in deep history, when there was more carbon dioxide in the atmosphere, the earth would have been warmer. Water vapour, she found, acted in a similar manner. Both gases trap heat and retain it longer, suggesting that early earth, with higher atmospheric concentrations of these gases, would have been warmer as a result.\n\n\nJohn Tyndall’s Confirmation\n\nA few years later, John Tyndall, an Irish physicist known for the Tyndall effect, conducted similar experiments without knowledge of Eunice Foote’s prior work. Tyndall expanded on her findings with more detail and precision. He investigated not only carbon dioxide and water vapour, but also how different wavelengths of light, particularly in the infrared spectrum, interact with various gases.\n\nAccording to his experiments, many greenhouse gases, such as carbon monoxide, methane, nitrous oxide, ozone, carbon dioxide, and water vapour, have different abilities to absorb radiant energy within wavelengths from about one to fifteen micrometres. These gases are present in the atmosphere and are increasing over time due to human activity, such as the burning of fossil fuels.\nTyndall showed, for instance, that carbon monoxide absorbs little infrared radiation until around five micrometres, where its absorbance spikes. Methane traps heat at different points, mostly at around 3.2 and 8.5 micrometres. The absorbance of these gases means that they capture certain wavelengths of infrared energy, which translates into heat trapped within the atmospheric system.\nAll of these different gases, present in the atmosphere all the time, are able to trap heat from radiation at various characteristic wavelength ranges. Tyndall’s work, alongside that of Eunice Foote, provides the foundational experimental evidence for our current understanding of how atmospheric gases contribute to the greenhouse effect.\n\n\nQuestions on Measurement Techniques\nStudent Question: How was absorbance measured and how were these gases extracted from the atmosphere?\nTheir measurement techniques relied on instruments that were precursors to today’s spectrophotometers—simpler, but based on similar principles. They used prisms to diffract light into a spectrum and would then measure the intensity of light as it emerged, with different components measured by suitable sensors. The nature of the instrument is not completely clear to me at this moment, but it would rely on diffraction to create measurable separations in the light. As we discuss photosynthesis later, we’ll encounter analogous experiments, some of which employed phototactic or aerotactic bacteria as biological sensors of oxygen production, indirectly inferring light absorbance at certain wavelengths.\nToday, we use instruments such as an IRGA—an infrared gas analyser—for precise measurement of greenhouse gases. This involves shining a beam of infrared light at a known wavelength through a tube filled with atmospheric gas. The decrease in intensity from the source end to the detector is the absorbance, from which one can infer the concentration of greenhouse gases present. Modern detectors, such as those in spectrophotometers, are highly sensitive and can be tuned to specific wavelengths for meticulous measurement.\nStudent Question: Was the experiment conducted in the dark?\nThe experiment could be conducted in the dark; however, visible light does not interact with greenhouse gases in the way infrared does. Only infrared radiation is relevant here. Even when it appears dark (at night), there is still plenty of infrared radiation in the environment. The critical factor in the experiment is to ensure that only the intended beam of infrared radiation interacts with the gas in the tube, with other sources of infrared minimised or accounted for, so that only the absorbance by the test gas is measured.\n\n\nSvante Arrhenius and The Model of Earth Systems\n\nA few years after Tyndall, Svante Arrhenius, a name known also from chemistry for his work on pH and ionic dissociation, extended previous findings. Instead of confining his interpretation to the laboratory, Arrhenius realised that by understanding how gases absorb infrared radiation, he could extrapolate an estimate of how carbon dioxide concentrations affect temperatures at the earth’s surface itself—not just in a jar.\nHis approach was still crude but remains the foundation of climate change science: the idea that by measuring carbon dioxide concentrations, one could estimate corresponding changes in earth’s surface temperature. Arrhenius thus moved climate change science from the realm of curiosity towards large-scale understanding, showing its importance for the environment experienced by both humans and plants.\n\n\nEarly Recognition of Human Influence on Climate\n\nAt the beginning of the twentieth century, this knowledge began to enter public discourse. In 1912, a newspaper article in New Zealand speculated for the first time that the burning of coal—combining carbon with atmospheric oxygen to produce CO2—would eventually lead to planetary warming, based on Arrhenius’s work. This early speculation tied industrial emissions to potential climate consequences.\n\n\nProving more concrete was the work of Guy Callendar, an engineer, who in the early twentieth century estimated the increase in mean global temperature due to artificial addition of CO2 from burning coal. His calculations put the warming effect at just 0.003°C per year [attention: this value is lower than modern-day estimates based on recent emissions], a figure so small it was easily overlooked at the time and treated as a curiosity rather than a pressing concern.\n\nFollowing Callendar, Gilbert Plass added more rigour to Arrhenius and Tyndall’s work, analysing the specific heat-trapping contributions of various greenhouse gases to the atmosphere, though there was still little hard evidence connecting atmospheric CO2 to observed temperature increases, due to the absence of accurate measurements of global CO2 concentrations.\n\n\nDirect Measurement: Charles Keeling and the Keeling Curve\n\nIt was not until Charles Keeling, an oceanographer, began careful direct measurements of atmospheric CO2 in Hawaii, that the relationship between emissions and atmospheric composition became irrefutable. On the summit of Mauna Loa, he established an observatory and, year after year, measured CO2 concentrations.\n\n\nThe resulting data showed not only an annual zigzag pattern (seasonal variation in CO2), but a clear, uninterrupted upward trend. In 1958, CO2 was measured around 315–318 ppm; today it is at about 420 ppm, and climbing.\nKeeling’s measurements revealed that CO2 is continually increasing due to ongoing fossil fuel combustion, deforestation, and industrial activity. This increase is observed even four kilometres deep in the ocean, showing that every part of the planet is being affected—there is nowhere immune to the effects of this rising CO2 concentration.\nIf humanity suddenly switched to 100% renewable energy, the rate of increase in CO2 would slow but not stop; some further increase is effectively locked in, leaving the question merely about how quickly or slowly things worsen. The reality is that such an overnight global switch is currently politically and practically impossible, so the future likely lies somewhere between worst-case and best-case scenarios.\n\n\nThe Effect of COVID-19 Lockdowns\nStudent Question: Did the COVID-19 lockdowns have a measurable effect on the CO2 trend?\nDuring lockdown, pollution and emissions did decrease temporarily, with satellite data showing a significant blip in atmospheric pollutants over South Africa, for instance. However, as restrictions eased, industries sought to compensate for lost time, causing emissions to rebound quickly. Thus, any effect on the overall CO2 curve is likely to be negligible; the long-term trend remains upward. Future data will clarify whether there is any observable dip due to lockdown, but I do not expect a significant long-lasting deviation.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#the-global-nature-of-the-carbon-cycle",
    "href": "BDC223/L04-carbon_cycle.html#the-global-nature-of-the-carbon-cycle",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "The Global Nature of the Carbon Cycle",
    "text": "The Global Nature of the Carbon Cycle\nIt is worth remembering that the atmosphere is a globally coupled system. While Keeling’s observatory in Hawaii provides the longest-running data set, simultaneous measurements in Antarctica, South Africa, and elsewhere show parallel increases. CO2 added to the atmosphere anywhere will be detected everywhere after only a short delay, as air mixes globally.\nGovernment policies and decisions at all levels—including in South Africa—thus affect the planet as a whole. Local restrictions on distributed solar energy, for example, have global ramifications, demonstrating the interconnectedness of carbon emissions.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#assignment-the-seasonal-fluctuation-of-co2",
    "href": "BDC223/L04-carbon_cycle.html#assignment-the-seasonal-fluctuation-of-co2",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Assignment: The Seasonal Fluctuation of CO2",
    "text": "Assignment: The Seasonal Fluctuation of CO2\nTo end today’s lecture, I have a small assignment for you, which will count towards your continuous assessment mark. I want you to consider the ‘zigzag-like’ pattern on the Keeling Curve, where every year CO2 rises and falls in a regular cycle, yet year after year the whole curve trends upward.\nPlease write a brief explanation—perhaps a paragraph, half a page—describing why CO2 is low in January, peaks around April or May, and drops again toward September. What accounts for this seasonal pattern? Your answers are due by Friday at 11:55 pm, to be submitted via IKAMVA. I’ll send details of the formatting requirements. This ties directly to the topic of this module and is a question that integrates several core ideas we have discussed.\n\nIf you have any questions, please let me know. We will pick up on the theme of climate change in Monday’s lecture, delving further into its scientific, ecological, and societal impacts.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#the-keeling-curve-and-the-historical-context-of-co2",
    "href": "BDC223/L04-carbon_cycle.html#the-keeling-curve-and-the-historical-context-of-co2",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "The Keeling Curve and the Historical Context of CO2",
    "text": "The Keeling Curve and the Historical Context of CO2\nLet me continue from where I left off last time. I ended with a discussion on the graph displaying the increase in carbon dioxide in the atmosphere over the last sixty or so years. This graph, as you see it there, shows a wiggly line rising steadily from 1958 to the present day. Eventually, this plot became known as the Keeling Curve, named after the scientist who originally began collecting the data behind it—Charles Keeling. Today, the Keeling Curve stands as the basis of our modern understanding of climate change, backed by this and similar sets of observations.\nWhat’s happened in the intervening sixty years is that people have managed to extend this graph further to the left, using various scientific methods that allow us to peer deeper into our planet’s history. We can now look further back in time and, similarly, using global climate models, we are even able to project forward, about a hundred or a hundred and fifty years into the future. After we are all gone, we do have a reasonable degree of certainty about what the future world will be like, at least with regard to the climate, as the mathematics and science align quite reliably. We’ll delve into the predictive capacity of these models later.\nThe important point to note now is that in the Keeling Curve, the small section we previously focused on is just the recent part, essentially the right-hand section of this extended timeline, whereas the area labelled “10 to 0” corresponds to the last ten thousand years—the era of recorded human civilisation. I’ll display another graph shortly that extends even further back, but the take-home message is that the vast majority of what we know as human history and civilisation falls within these last ten thousand years.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#human-development-and-climate-stability",
    "href": "BDC223/L04-carbon_cycle.html#human-development-and-climate-stability",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Human Development and Climate Stability",
    "text": "Human Development and Climate Stability\n\nIt was during this period—about ten thousand years ago—that all the hallmarks of modern humanity started to emerge. Towns and cities arose, agriculture began, and the domestication of animals took place. This was also when the famous cave paintings were made in places like France and northern Europe, with a few even older examples elsewhere.\nMost fundamentally, everything we recognise as human development has occurred within the last ten thousand years. Around six to five and a half thousand years ago, we start seeing the first written records, preserved on early scripts and papyrus scrolls. This is also the period when the Egyptian pyramids were constructed. About two thousand years ago, some dude called Jesus Christ was also born, or at least the idea of him as some important figure was recorded, during this timeline. In the two millennia since, he has developed for himself quite a following of sheep.\nMy point is that all recorded human history has developed while atmospheric CO2 levels remained below around 250 to 260 parts per million. Nothing in the archaeological or historical record suggests modern humanity has ever experienced—and certainly never thrived in—CO2 conditions higher than that. This is the point of reference.\nToday, however, we’re above 400 parts per million—currently about 420. No modern human has ever lived in such a high-CO2 world. Humanity developed in a period of low, stable CO2, but now CO2 concentrations are soaring, and people find themselves in truly unprecedented conditions. While our models give a solid sense of how Earth’s systems—like temperature, precipitation, sea levels, and winds—will behave, we cannot say with confidence how humans will cope socially, physiologically, or culturally with this “uncharted territory”. I’ll show you yet another graph on this shortly.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#climate-change-in-deep-time",
    "href": "BDC223/L04-carbon_cycle.html#climate-change-in-deep-time",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Climate Change in Deep Time",
    "text": "Climate Change in Deep Time\n\nIf we push our perspective further back—ten thousand years ago to 800,000 years ago—the data show a recurring pattern of rises and falls in CO2 and global temperatures. These are the glacial and interglacial periods, tied to natural climate cycles. You can see it on the graphs—ups and downs corresponding to ice ages and warmer interglacial phases. When CO2 levels are high, we see polar ice caps melting and the world resembling its form from about two centuries ago, before recent melting recommenced. When CO2 drops, temperatures plummet and ice ages resume.\nClimate change sceptics or deniers often point out, “the climate has always changed!”, and they’re not incorrect in the basic sense—this graph illustrates that. However, at no point in the last 800,000 years did CO2 ever reach today’s levels. It fluctuated between about 180 and 260 parts per million, never once exceeding 300, and certainly never surpassing 400. We are, in measurable terms, in entirely novel territory.\nHominids—our human-like ancestors—appeared around 160,000 to 170,000 years ago; modern humans some 70,000 years back; human societies about ten thousand years ago. Societal development correlated closely with periods of climate stability, which in turn relied on relatively constant atmospheric CO2. Now, that stability has ended.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#observed-temperature-rises",
    "href": "BDC223/L04-carbon_cycle.html#observed-temperature-rises",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Observed Temperature Rises",
    "text": "Observed Temperature Rises\n\nLet’s look at a visual representation of temperatures recorded over the last 100 to 120 years. If you animate these records, you see the starting point near 1850; as you progress towards the present, temperatures climb. The warming trend has clearly accelerated over just the last three decades. At first, annual increases were minor, but from the 1970s onwards, the rate of temperature rise steepens dramatically. At present, we’re even further down this trajectory—you can easily project future trends from the graph.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#why-climate-change-matters-beyond-just-temperature",
    "href": "BDC223/L04-carbon_cycle.html#why-climate-change-matters-beyond-just-temperature",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Why Climate Change Matters: Beyond Just Temperature",
    "text": "Why Climate Change Matters: Beyond Just Temperature\n\nWhy do I discuss climate change so persistently? Its impact is not merely a matter of rising average temperatures. As I explained previously, warming is not uniform— some regions experience much greater change than others. Globally, however, the average is up. The knock-on effects are far-reaching.\n\nPrecipitation Patterns: Rainfall will increase in some areas, but countries like South Africa will see declines. Cape Town, for example, faces a future where the severe drought faced two years ago becomes typical, not exceptional.\nSea Level Rise: Coastal communities—like those living in Sea Point, Cape Town—are under threat as sea levels climb. Notably, around 80% of the human population lives close to coastlines, so massive populations face displacement.\nExtreme Events: Extreme heat, strong winds, severe waves, droughts, and heavy rainfall will all become more frequent, more intense, and last longer. This is the very unpredictability now exceeding anything in the modern historical record.\n\nClimate change is thus significant not only in isolation, but because it affects all ecosystems, resources, and social systems, from food security and settlements to health.\n\nClimate Change Scenarios\n\n\n\n\n\nImpacts on Human Health and Societies\nFor example, altered rainfall means more standing water in already-wet areas, facilitating waterborne diseases such as diarrhoea, trypanosomiasis, and increased malaria transmission. Already, malaria is reaching areas in South Africa where it was previously unknown, as climate zones shift and mosquitoes migrate southward from Zimbabwe and Mozambique.\nSocioeconomic effects are just as pressing. Much unrest in North Africa and in the Arab world now occurs in part because regions are becoming drier and less hospitable, driving displacement and accelerating conflict [attention: while environmental stress is a factor, many conflicts arise from a complex mixture of political, social, and historical causes]. As climate change intensifies, so do its consequences for all interactions between people and the planet.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#feedbacks-and-the-role-of-ecosystems",
    "href": "BDC223/L04-carbon_cycle.html#feedbacks-and-the-role-of-ecosystems",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Feedbacks and the Role of Ecosystems",
    "text": "Feedbacks and the Role of Ecosystems\nOf particular interest to us in this course are the feedback loops between climate and ecosystems—especially those involving plants, the primary producers. For instance, the reason the Keeling Curve shows a sawtooth pattern—those seasonal wiggles—is because trees and other vegetation take up CO2 in the growing season, thus reducing atmospheric concentrations, and then release it again in autumn and winter as leaves fall and decay. Fewer trees means less CO2 is drawn down, and more remains in the atmosphere, exacerbating warming.\nThus, the maintenance and protection of ecosystems, particularly of forests, is vital in addressing climate change. The more primary production—photosynthesis—we have, the more atmospheric CO2 is converted into biomass and safely stored. Reducing the destruction of forests is therefore absolutely crucial, not only for the environment, but for everyone globally.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#mitigation-and-adaptation-strategies",
    "href": "BDC223/L04-carbon_cycle.html#mitigation-and-adaptation-strategies",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Mitigation and Adaptation Strategies",
    "text": "Mitigation and Adaptation Strategies\nTen years ago, most climate scientists spoke mainly of mitigation—strategies to reduce greenhouse gas emissions and increase carbon capture. The focus was on what could be done to trap CO2, limit emissions, and minimise further impact. That was mitigation: offsetting atmospheric loading with active intervention.\nIncreasingly, the emphasis has shifted to “adaptation”. There is now recognition that, regardless of efforts, some degree of change is inevitable, as we are already living with the consequences. Adaptation is about adjusting to the new conditions—learning how humanity and natural systems can best manage and survive in a world transformed by climate change.\nAdaptation strategies differ greatly between human systems and biological ones. For this module, our focus will be on the adaptation strategies of plants—understanding what physiological mechanisms they possess to cope with changing environments and, crucially, how plant stresses are managed. Whenever relevant, I’ll relate these back to broader global issues, but the primary focus is on ecosystems, and specifically how plants, as primary producers, can and do respond to the changes we are witnessing.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#overview-of-the-temperature-graph",
    "href": "BDC223/L04-carbon_cycle.html#overview-of-the-temperature-graph",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Overview of the Temperature Graph",
    "text": "Overview of the Temperature Graph\nOn the left axis of this particular graph—although it’s actually the x-axis due to the orientation we’ve got here—we have the independent variable, and on the vertical axis, that’s the dependent variable. Here, temperature is the aspect that depends on which point in history we’re examining. The graph spans from plus four °C to minus four °C, relative to the average temperature over the entire depicted period. Now, the main focus is on this central white band, which is key for our discussion.\nThe timeline starts at around 20,000 years ago, and as I scroll down, we move steadily towards the present day. So, about 20,000 years ago, the Earth was roughly four °C colder than it is today. I must point out that this is quite a Northern Hemisphere-centric perspective. At some point, I should really produce a similar graphic focusing on South Africa or the African continent, so we can localise our interpretations. For the moment, however, do keep in mind that this particular presentation is fundamentally rooted in North American context.\nAt that time, Boston—now a relatively verdant city—was actually buried under about a mile of ice. So, back then, the temperature was still about four °C colder than present, and that’s reflected by the dotted line tracing temperature changes. From here on, the graph outlines a chronological procession of climate events and human developments.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#early-human-migrations-and-extinctions",
    "href": "BDC223/L04-carbon_cycle.html#early-human-migrations-and-extinctions",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Early Human Migrations and Extinctions",
    "text": "Early Human Migrations and Extinctions\nBetween 19,000 and 19,500 years ago, humans had already dispersed from Africa and were found throughout Eurasia and even in Australia. Notably, when humans arrived in Australia, their sudden presence led, within around a thousand years, to the extinction of all the large mammals—megafauna—in that region. Today, the largest animals in Australia are kangaroos, but prior to human arrival, truly massive creatures inhabited the continent. The arrival of humans is strongly linked to the rapid die-off of these species. This phenomenon is not unique to Australia; similar patterns were happening elsewhere, including South Africa.\nAround 19,000 years ago, evidence shows that people began to create paintings, pottery, rope, and other artefacts of material culture.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#climatic-shifts-and-the-milankovitch-cycle",
    "href": "BDC223/L04-carbon_cycle.html#climatic-shifts-and-the-milankovitch-cycle",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Climatic Shifts and the Milankovitch Cycle",
    "text": "Climatic Shifts and the Milankovitch Cycle\nAt approximately 18,500 years ago, there was a slight change in the Earth’s orbit—a phenomenon known as the Milankovitch cycle. This event caused Earth to absorb a little more heat in its polar regions, which in turn allowed the great ice sheets to begin melting. The process was gradual at first: as the ice sheets retreated, sea levels rose, but temperature increases remained relatively modest. However, atmospheric CO2 concentrations began to creep upwards. This was due to various processes, including the re-mineralisation of materials previously trapped beneath the ice.\nAs more naturally trapped CO2 entered the atmosphere, warming began to accelerate, though it remained cold by modern standards—still about three °C colder than today.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#human-culture-and-dispersal",
    "href": "BDC223/L04-carbon_cycle.html#human-culture-and-dispersal",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Human Culture and Dispersal",
    "text": "Human Culture and Dispersal\nBy 15,000 years ago, we see the emergence of cave art—the kind that we now admire as cave paintings, though at the time, people might simply have seen it as graffiti. Some of the oldest examples have been found in France.\nAt around 14,500 years ago, the ice sheets in Alaska shrank to the extent that the land bridge between Asia and North America, the well-known Bering Land Bridge, disappeared. This development made it possible for humans to enter and populate North America for the first time, giving rise to the ancestors of Native Americans. This migration predates the arrival of Europeans on the continent by many thousands of years.\nBy 13,500 years ago, New York was no longer under ice, and by 13,000 years ago, species such as the woolly rhinoceros became extinct. At 12,500 years ago, significant flooding occurred in what is now Washington state, due primarily to the rapid melting of glaciers.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#rise-in-co2-and-continuing-warming",
    "href": "BDC223/L04-carbon_cycle.html#rise-in-co2-and-continuing-warming",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Rise in CO2 and Continuing Warming",
    "text": "Rise in CO2 and Continuing Warming\nAll the while, CO2 levels continued to rise, driving further increases in temperature. The ice sheets eventually disappeared even from areas such as Chicago.\nBy 11,500 years ago, people began to settle in the area now called Syria—formerly known as Mesopotamia—within the region known as the Fertile Crescent. This marks a pivotal point, as humans began to establish small communities and, ultimately, towns and cities. The city of Jericho, one of the earliest known urban settlements, arose during this era, when the Earth’s temperature was still about one and a half °C colder than present.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#agricultural-revolution-and-the-holocene",
    "href": "BDC223/L04-carbon_cycle.html#agricultural-revolution-and-the-holocene",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Agricultural Revolution and the Holocene",
    "text": "Agricultural Revolution and the Holocene\nMoving to 10,000 years ago, as temperatures reached a level still somewhat cooler than modern conditions, the first evidence of farming emerges. People first settled in cities and only then does agriculture appear, in response to the demands of a growing, settled population.\nAbout 9,500 years ago, or more specifically around 9,200 years ago, we see the extinction of the sabre-toothed cat. Horses also disappeared from North America, likely due to human impacts. (A quick aside: there’s a facetious reference in the source text to Pokémon going extinct at this time, which is, of course, entirely fictional [attention].) As temperatures reached levels comparable to those of the 20th century, cattle were domesticated—around 8,500 years ago. By this point, the ice sheet over Canada had entirely vanished.\nFrom roughly 10,000 years ago to the present, Earth’s temperature remained, for the most part, within a relatively narrow band—about one degree Celsius higher or lower than today. This stable period is known as the Holocene. It encompasses the entire span during which humans have been able to build cities, develop stable agriculture, and domesticate animals.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#early-civilisations-and-technological-developments",
    "href": "BDC223/L04-carbon_cycle.html#early-civilisations-and-technological-developments",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Early Civilisations and Technological Developments",
    "text": "Early Civilisations and Technological Developments\nBy about 7,000 years ago, human settlement is documented in China, which stands as the oldest continuous civilisation in the world. Around 7,500 years ago (5,500 BCE plus the succeeding two thousand years), metalworking begins, along with the invention of the wheel, which, surprisingly, dates to only about 6,000 years ago.\nThe timeline features several key developments in civilisation—urban life in the Fertile Crescent; Egyptian mummification; the rise of the Indus Valley civilisation; and later, Stonehenge in the UK at about 4,000 years ago. Alphabetic writing appears in Egypt after the development of chariots and further urban expansion.\nWritten history, iron smelting, and early Greek civilisations also belong to this relatively recent part of our timeline. The peopling of the Pacific and Solomon Islands follows, and then another sequence of events from classical Greece to around 500 BCE, when both Greek and Buddhist traditions were crystallising.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC223/L04-carbon_cycle.html#recent-history-the-industrial-revolution-and-climate-change",
    "href": "BDC223/L04-carbon_cycle.html#recent-history-the-industrial-revolution-and-climate-change",
    "title": "Lecture 4: Perturbations to the Carbon Cycle",
    "section": "Recent History, the Industrial Revolution, and Climate Change",
    "text": "Recent History, the Industrial Revolution, and Climate Change\nAll of the above—essentially everything we know as “civilisation” and “recorded history”—has taken place within this narrow “white band” on the graph, where global temperatures have not deviated by more than about one degree Celsius from present values.\nFast-forwarding to the last few centuries, we reach the invention of the steam engine, which allowed humanity, for the first time, to convert heat energy into mechanical work efficiently—driving the Industrial Revolution. Before this, societies had depended primarily on human and animal labour. Subsequently, developments such as the telegraph and aeroplane emerged, propelling us into the modern era.\nNow, as of 2016, we find ourselves not only at the edge of this narrow band but potentially at the threshold of something new. Should we persist with current patterns of fossil fuel combustion—coal, oil, and gas—we are on a trajectory that leads to much warmer global temperatures, with increases possibly in the range of two, three, four, or even five °C above current levels. If, on the other hand, we made a radical change—literally switching off all fossil fuel emissions overnight—we might have a chance to slow the warming, but even then we are now in a climatic regime where no human civilisation has ever previously existed.\nThis is why climate change is such a critical and urgent topic for us to understand.",
    "crumbs": [
      "Home",
      "BDC223: Plant Ecophysiology",
      "Lecture 4: Perturbations to the Carbon Cycle"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-essay",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-essay",
    "title": "",
    "section": "The Essay",
    "text": "The Essay\nOne of the requirements for your marks towards your continuous assessment is to write an essay. The essay is titled “The Promise in Our DNA: Science, the Essence of Being Human, and the Future I Choose to Build”. As you can see from the title, it is a bit reflective, it’s a bit philosophical, and it talks about your personal role in developing the future that you see for yourself and for humanity.\nWhat is it about being human that capacitates us to have oversight, to have a wish, to have thoughts about a future world that we want to live in? Wishes about a time distant into the future where we might exist in our old age or our middle age, and where our children might exist in, is a privilege that only humans have. So what is it about being human that you can say ties into your personal story, that ties into what you are able to do as a person — a person who has wishes and thoughts about a future world you want to live in?\nWhat is it that you have learned about science? What is it that you have learned in science — not only here in this module, but in your entire undergraduate degree? What is it that you have learned that you as a human can use to develop a future that you want? This is not a topic that you can put into AI and expect it to think for you. AI cannot develop your thoughts and your wishes and your dreams about the future. Only you can do that.\nSo this is what I would like for you to do: to think about those words that I capture in the topic of this essay. Write a personal reflection about you and your role and how science will contribute towards developing the world that you want to live in.\nI am going to be fairly pedantic around this essay. There are some strict requirements in terms of the structure and formatting. You can see all my requirements on the website. The date is also mentioned there that you need to submit it by. You typically have about two weeks or so to work on this. But pay attention to the formatting instructions.\nThe formatting instructions are that it must not exceed two pages. That is including any references, although I doubt that references will actually be necessary since this is a personal reflection. You must use Times New Roman in 10 point size. I expect the line spacing to be single line spacing. I will not tolerate any full justification, only left justification. I want to see a single line break between paragraphs. And the margins top, right, bottom and left must be \\(2.54\\,\\mathrm{cm}\\). I don’t want to see any visual embellishments. No fancy fonts, no strange colours, no pictures, nothing like that. I simply want to be able to read your words. Also, I don’t require any internal headings and so on. So just a free-flowing narrative that outlines your thoughts and your feelings and your wishes and your dreams and so on — a fairly personal reflection in those two textual pages.\nI want you to focus on the text and not on the visual appearance of the thing. Obviously, all of my requirements dictate a very specific visual appearance, but it is designed in a way that I don’t want you to be creative in the layout. So I’ve done this for you upfront. And the reason I want this very strict, pedantic adherence to my rules is sometimes these things do matter. In your future lives, you will encounter research proposals that you need to write, and they have very opinionated views about what font you need to use, how the text must be structured, how many words, how many letters even, you may use. So this is also an exercise in getting you to pay attention to the small details — the details that might not necessarily matter to you personally, but they will matter at some point in your life. And being able to follow instructions is a very important skill that you need to learn.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#questions-and-answers",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#questions-and-answers",
    "title": "",
    "section": "Questions and Answers",
    "text": "Questions and Answers\n\n\n\nSlide 10\n\n\n\n\n\nA question arose: when referring to patterns and processes in traditional ecology, is there such a thing as modern ecology? Yes, this module is very much about modern ecology. Traditional ecological approaches would focus on surveys at a local scale, such as conducting a transect survey in a nearby nature reserve, limited by what can be physically accessed.\nToday, with computers and satellite remote sensing, we can examine large-scale patterns — across countries, continents, or even globally — often using satellite data (Slide 10). Not only can we synthesise many small-scale surveys collected by different people over time, but we can also employ advanced numerical analyses to make sense of very large data sets — ones so substantial, they can no longer fit within Excel.\nModern ecologists now collaborate across the globe, pool significant data sets, and use advanced methods to reveal broad-scale patterns in biodiversity, species composition, and ecological functioning. Whereas traditional studies looked at the local, modern ecology can rigorously address processes at global, continental, or deep historical time scales.\n\nExample from South Africa\nThe earliest botanical research conducted in South Africa dates back to 1772 – 1774, when a Swedish botanist named Carl Peter Thunberg (1743 – 1828), who had trained under Carl von Linné (Linnaeus) (1707 – 1778), travelled to South Africa to explore the Western Cape and parts of the Southern Cape region as far east as Addo. Most of his journeys were made on horseback, and each of his three expeditions lasted several months at a time.\nDuring this period, his primary focus was the collection of plant and insect specimens. These he subsequently sent back to Europe, with the intention of classifying them according to the taxonomical system developed by Carl Linnaeus.\nFast-forward almost two centuries to the 1940s. This time the botanist John Acocks (7 April 1911 – 20 May 1979), again undertook a major botanical survey, but with the intention to classify all of South Africa’s vegetation. He travelled by train, classifying the habitats he saw through the window, and his classification became known as the Veld Types of South Africa. Even this method was constrained compared to the view we now have through remote sensing satellites.\nNow, we can “stand” \\(80\\,\\mathrm{km}\\) above Earth and map entire landscapes from above, unconstrained by natural or political boundaries. This is in fact what the recent BioScape programme has accomplished. BioScape is a programme that was implemented, run, and funded by the United States government. A significant portion of the funding was allocated to NASA, and in 2023, NASA, in collaboration with a group of South African scientists — of which I was a tiny part — brought a series of high-tech instruments all mounted on aeroplanes to South Africa. The intention was to survey the Cape Floristic Region during that period.\nThe instruments used included a range of ultraviolet, visible, and shortwave infrared imaging spectroscopy instruments; laser altimetry instruments known as LiDAR; as well as various other sensors, some of which were also mounted on satellites and other platforms. They surveyed much of the Fynbos region in the Cape, as well as some of the kelp forests around South Africa.\nThe examples above, spanning almost 3 centuries, offer an illustration of how the field of ecology has evolved over time. Initially, surveys were conducted on horseback, with the prime interest being the naming of various species. Two centuries later the focus shifted more towards classifying different vegetation types. In the present day, around two years ago, this endeavour culminated in the deployment of a comprehensive suite of extremely high-tech instruments, all functioning in concert to elucidate both the patterns observable at the landscape scale and the processes that structure these systems across space.\n\n\nBroader Shifts in Approach\nTraditional ecological studies focused on what happens in places within easy reach — a single nature reserve, for example. Modern studies look for patterns across nations or hemispheres, and also explore new levels of taxonomic detail, such as genetic variation and subspecies.\n‘Scale’ can refer both to spatial scale — local to global — as well as temporal scale: considering recent changes versus millennia or longer time spans. Modern approaches allow us to examine ecological phenomena and biogeographic patterns at both these broader spatial and longer temporal dimensions.\nCollaboration is increasingly important. Where once ecological studies might have one or two authors focused on a single location, it’s now common to find large teams of co-authors bringing together expertise and data from multiple sites or even continents in pursuit of broader ecological generalities.\n\n\nThe Value of Global Approaches\nThe aim of global ecology is to derive general ecological ‘laws’ or repeatable principles that apply across the full diversity of ecosystems — from Russian tundra to Amazonian rainforest to the Australian outback. Though these systems may look entirely different, we seek to identify commonalities in their fundamental processes.\nThirty years ago, when I was a student, almost all work was at a very local scale and typically on one’s nearest nature reserve. Today, with advances in technology and computational power, questions can be more complex and less parochial. The questions themselves have evolved and broadened: “What can South Africa’s biodiversity teach Patagonian ecologists?” Global-scale studies provide answers of relevance far beyond one region or ecosystem.\n\n\nClosing and Summary\nIf you have further questions — about the module structure, assessments, or the content of the introductory material — please ask, either now or later via the chat or WhatsApp group.\nIf there are no more questions, I’ll post this video online for you to access within the next half an hour or so. Thank you.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/BDC334-Lecture-Transcripts.html#the-three-axes-of-scaling",
    "href": "BDC334/BDC334-Lecture-Transcripts.html#the-three-axes-of-scaling",
    "title": "",
    "section": "The Three Axes of Scaling",
    "text": "The Three Axes of Scaling\nThe study of macroecology represents a development from more traditional ecological approaches — those rooted initially in population ecology and later in community ecology — towards a framework that can be applied across a vast range of scales. This shift allows ecologists not only to study small, bounded systems, but also to infer and interpret patterns that extend far beyond the limits of individual populations or communities.\n\nSpatial Scaling\nWhen we talk about scale in macroecology, one of the principal axes is space. This spatial dimension can begin with extremely localised insights—data from individual quadrats or transects, for example—and scale up to isolated ecosystems such as a single nature reserve. From there, one can compare across multiple reserves, distributed across a broader regional scale.\nWe can then extend these comparisons to assess differences in biodiversity between countries, further scaling up to patterns observed at the continental level, among continents, across hemispheres, and finally encompassing the entire Earth. This spatial scaling allows macroecologists to interpret both fine-grained variability and broad-scale biogeographic patterns within a coherent conceptual framework.\n\n\nTemporal Scaling\nMacroecology also permits inquiry across scales of time. At the shorter end of the spectrum, we can examine changes that occur between seasons or years—what one might refer to as inter-annual variability. This can be extended to longer-term phenomena such as inter-decadal variation.\nImportantly, macroecology has access to tools that allow us to look both backward and forward in time. Historical data — drawn from archives, paleoecological records, and related sources — permits reconstructions extending back 100 years, 1,000 years, or more. These long timeframes allow us to interrogate the processes responsible for the biogeographical patterns currently observed across the globe.\nAt the other end, through the use of Earth system models — climate models, atmospheric models, and so on — we can look forward into the future. Currently, these predictive models typically extend out to timeframes such as 50 or even 200 years from the present. These projected conditions can, in turn, be coupled to likely ecological outcomes—how ecosystems may respond to shifts in climate, land use, or biogeochemical cycles over such intervals.\n\n\nBiological Scaling\nThe third axis along which macroecology engages is that of biological size or organisational scale. This includes scaling relationships among organisms of vastly different sizes. At one end of the continuum, we might consider viruses and bacteria, which are measured in the pico- to nanometre range.\nWe can then move up to interstitial meiofauna, typically fractions of a millimetre to a few millimetres in size. Beyond this, we encounter small-bodied macro-organisms — such as annual plants or small mammals — ranging in size from a few centimetres to perhaps a metre. At the other extreme lie the large-bodied megafauna and mature trees, often key components of the ecosystems they inhabit.\nMacroecology is thus not defined by any single scale, but by its capacity to consider all these dimensions — spatial, temporal, and biological — simultaneously. It offers a mode of ecological thinking where pattern and process are interpreted as functions of scale, connectivity, and context. This multiscalar orientation distinguishes it from the narrower lenses of traditional ecology and allows for the comparative, often statistical, treatment of ecological regularities at planetary scale.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "[BCB334 Lecture Transcript]{.my-highlight}"
    ]
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html",
    "href": "BDC334/assessments/Class_tests.html",
    "title": "Class tests",
    "section": "",
    "text": "Discuss the unimodal species distribution model, and describe how this model can explain the structuring of communities along environmental gradients. In your discussion, also talk about β-diversity.\n\n\nThe unimodal model is an idealised species response curve where a species has only one mode of abundance—i.e. one locality on the landscape where conditions are optimal and it is most abundant (i.e. the fewest ecophysiological and ecological stressors are present there). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance (a lower fitness). The unimodal model in the most basic sense can be seen as a Gaussian curve, and it offers a convenient heuristic tool for understanding how species can become structured along environmental gradients. Multiple unimodal distributions are often visualised as a coenocline—a graphical display of all species response curves, which shows how a species’ fitness is affected by any one of a multitude of environmental variables e.g. pH in Figure 1.\n\n\n\nA coenocline.\n\n\n\\(\\beta\\)-diversity is a concept that describes how species assemblages (communities) measured within the ecosystem of interest vary from place to place, e.g. along the various transects or among the quadrats used to sample the ecosystem. \\(\\beta\\)-diversity can result from the gradual change in environmental characteristics along gradients. This can be clearly seen in a coenocline, where the modal centre of distribution of many species is arranged at different positions along an environmental gradient. See Figure 1. As Species A becomes less abundant when its physical distance away from the place on the landscape which is most conducive to its fitness increases, so it is replaced by Species B at a distant location where its environmental conditions are optimal. And so on with all the other species along the length of the gradient. This process is called environmental filtering, which results in a decrease in similarity as the distance between sites increases—sometimes this is called the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-1",
    "href": "BDC334/assessments/Class_tests.html#question-1",
    "title": "Class tests",
    "section": "",
    "text": "Discuss the unimodal species distribution model, and describe how this model can explain the structuring of communities along environmental gradients. In your discussion, also talk about β-diversity.\n\n\nThe unimodal model is an idealised species response curve where a species has only one mode of abundance—i.e. one locality on the landscape where conditions are optimal and it is most abundant (i.e. the fewest ecophysiological and ecological stressors are present there). If any aspect of the environment is suboptimal (greater or lesser than the optimum), the species will perform more poorly and have a lower abundance (a lower fitness). The unimodal model in the most basic sense can be seen as a Gaussian curve, and it offers a convenient heuristic tool for understanding how species can become structured along environmental gradients. Multiple unimodal distributions are often visualised as a coenocline—a graphical display of all species response curves, which shows how a species’ fitness is affected by any one of a multitude of environmental variables e.g. pH in Figure 1.\n\n\n\nA coenocline.\n\n\n\\(\\beta\\)-diversity is a concept that describes how species assemblages (communities) measured within the ecosystem of interest vary from place to place, e.g. along the various transects or among the quadrats used to sample the ecosystem. \\(\\beta\\)-diversity can result from the gradual change in environmental characteristics along gradients. This can be clearly seen in a coenocline, where the modal centre of distribution of many species is arranged at different positions along an environmental gradient. See Figure 1. As Species A becomes less abundant when its physical distance away from the place on the landscape which is most conducive to its fitness increases, so it is replaced by Species B at a distant location where its environmental conditions are optimal. And so on with all the other species along the length of the gradient. This process is called environmental filtering, which results in a decrease in similarity as the distance between sites increases—sometimes this is called the niche difference model. Such patterns are typically visible along steep environmental gradients such as elevation slopes (mountains), latitude, or depth in the ocean, to name only three. It is also the dominant mechanism underlying island biogeography."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-2",
    "href": "BDC334/assessments/Class_tests.html#question-2",
    "title": "Class tests",
    "section": "Question 2",
    "text": "Question 2\nUsing a clear example that you can easily relate to, discuss the concept of ‘ecological infrastructure.’ In your explanation, mention other (i.e., in addition to the ‘infrastructural services’) ecological services this example ecosystem offers and any other benefits that people might derive from its existence and well-being. In your discussion, explain how the ecological infrastructure works (what it does and how) in a properly functioning ecosystem and, if people destroyed it, how we might replicate its service.\n\nAnswer\n\nWetlands as ecological infrastructure\nWhat is ecological infrastructure? Ecological infrastructure is natural ecosystems that provide services beneficial to people. These services would, in the absence of ecological infrastructure, have to be provided by engineering solutions.\nBenefits people derive from wetlands People tend to develop settlements, towns and cities in low-lying areas such as flood plains around estuaries. These areas are prone to periodic rising water levels, and recently they are also more and more being impacted by extreme floods (associated with climate change). Healthy flood plains often comprise wetlands, which are habitats occupied by dense emergent macrophytes along the edges of estuaries and flood plains. These systems can provide a buffer to rising water levels, and they may reduce the flow rate of water. People can benefit from intact wetlands as this buffer zone provides a level of protection to built structures in the vicinity of the estuaries. Wetlands also purify the water (water filtration removes excessive N, P and POM), which makes for an environment that is more supportive of good human health (fewer water borne diseases and pollutants which may be a public health concern).\nHowever, often wetlands are destroyed by dredging and then filled in to make area available for occupation by people. In such transformed systems, protection against floods and rising water levels can be provided by constructing engineered systems at great cost. Examples of such engineered systems include breakwaters and levees. These systems, however, do not provide the other services required for maintaining good water quality, and additional engineering solutions, costing yet more tax-payers money, need to be provided. Additionally, downstream natural areas on which people depend will also become increasingly impacted due to the deterioration or loss of wetlands, and engineering solutions cannot mitigate against such consequences.\nThus, ecological infrastructure provide services to people simply by virtue of being maintained in a healthy state. This economic cost of achieving this is virtually non-existent, provided people act responsibly to protect these systems.\nEcological services from wetlands Wetlands provide a complex 3D habitat that provides numerous ecological services to a host of associated fauna and flora. These biotic assemblages benefit from their association with wetlands from the feeding/foraging opportunities provided within the habitat structure, the breeding and nursery grounds wetlands provide, attachment surfaces on the wetland plants and the sediments trapped within, and shelter and hiding opportunities from predators. Wetlands also reduce the flow rate of the water passing through them, and as such the still water within is attractive to some species that are unable to tolerate faster flow rates. Typically, healthy wetlands are active in their cycling (uptake) of N and P, and as such the water quality may be better compared to surrounding areas. This is also true for decreasing the water turbidity due to their filtration services. This makes wetlands ideal environments to some species that are sensitive to pollution. &lt;Many more services are provided by the sediments in wetlands, which offer additional opportunities for enhancing biodiversity in these areas&gt;. Overall, the net effect it that it supports species diversity, i.e. higher biodiversity in landscapes with functional wetlands present compared to areas without wetlands. Higher species diversity also offer many bequest services, and offer a potential source of genetic diversity and materials to assay for important bio-active substances that might be useful to people."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-3",
    "href": "BDC334/assessments/Class_tests.html#question-3",
    "title": "Class tests",
    "section": "Question 3",
    "text": "Question 3\nDiscuss the general characteristics of species tables, environment tables, and dissimilarity and distance matrices we can derive from these tables.\n\nAnswer\n\nEnvironmental tables Environmental tables have variables down the columns (headings are the names of the env vars) and the sites run across the columns (row names are the names of the sites), with one site in a row. Different kinds of environmental variables can be contained in the table, as many as the researcher thinks is necessary to explain the patterns in the species tables. In the cells are the quantities of the various environmental variables measured at the different sites. The measurement units may differ between columns, so later, before analysis, these data must be standardised.\nSpecies tables The species tables have as many rows as the number of rows in the environmental table—so, for each site where species are recorded, there will be corresponding measurements of the environmental conditions there. Rows in a species table have the same orientation and meaning as in the environmental table. The columns, however, contain the names of the species recorded at the sites. In the cells is some quantity that reflects something about the species at the sites—it might indicate whether a species is there or not (presence/absence), its relative abundance, or biomass. The way in which the species are quantified must be the same across all columns.\nDissimilarity matrix The dissimilarity matrix is derived from the species table by calculating one of the species dissimilarity indices (Bray-Curtis, Sørensen, Jaccard, etc.). It is square and symmetrical, and the diagonals are zero because they are essentially comparing sites with themselves in respect to the kinds of species and their abundance or presence/absence there. A value of 1 would mean that the sites are completely different from each other—this would be seen in a similarity matrix, which is the inverse of a dissimilarity matrix. Each of the other cells represent the community difference between a pair of sites whose names are present as column or row headers.\nDistance matrix A distance matrix is produced from a standardised environmental table. It is square and symmetrical, and there are as many rows and columns as there are variables in the environmental table. This matrix reflects how similar/dissimilar pairs of sites are with regards to the environmental conditions present there. The interpretation of the diagonal is the same as in dissimilarity matrices."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-4",
    "href": "BDC334/assessments/Class_tests.html#question-4",
    "title": "Class tests",
    "section": "Question 4",
    "text": "Question 4\nProvide a short explanation, with examples, for what is meant by this statement:\n“Communities often seem to display very strong structural graduation relative to ‘variables’ such as altitude, latitude, and depth; however, these variables are not the actual drivers of the processes that structure communities.”\n\nAnswer\nAltitude, latitude, and depth serve to indicate the position sites on Earth’s surface. They do not have physical properties associated with them. Species cannot require altitude, latitude, or depth to sustain their physiological needs. They are merely proxy variables for other variables that can affect the physiology of the species occurring there. I am less interested in how beta-diversity (turnover, niche models, unimodal models) works, and more interested in how the proxy relationships might play out. For example, …"
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-5",
    "href": "BDC334/assessments/Class_tests.html#question-5",
    "title": "Class tests",
    "section": "Question 5",
    "text": "Question 5\nIt is the year 2034 and as a result of a decade of campaigning the South African Green Party has become a real contender to be the runner up behind the populist EFF, which has come into power in South Africa in 2029.\nAs the leader of the Green Party, write an Opinion Piece that outlines the ecological solutions your party has to offer for when (if) it becomes the official opposition to the current ruling party. Your party’s ecological solutions offer the promise of solving many of the socio-economic solutions that face South Africans in the 2030s.\n\nAnswer\nThis is an opinion piece and an expected answer is not available.\nIn this answer I am looking for how ecosystems’ ecological services and goods may be used for the betterment of people, the environment, and the economy. I am not looking for a listing of SA’s problems. I am not looking for way in which budgets can be better spent, or how enforcement can be improved. I am also not really looking for the implementation of renewable energy sources as wind or solar (although that will definitely be part of the solution). We know that hunger needs to be alleviated; people must be educated; we need better farming techniques; developments must be sustainable; and people’s economic freedom ensured. But how? How can we use nature’s solutions to do so?\nWe need to build into the various initiatives a reliance on the country’s natural infrastructure. We can also develop novel, fit-for-purpose ‘ecological’ infrastructure that incorporate many of the principles of natural ecosystems with the same kinds of benefits to people (e.g. roof-top gardens, integrated forming and aquaculture, etc.).\nThe essay must consider these kinds of things."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-6",
    "href": "BDC334/assessments/Class_tests.html#question-6",
    "title": "Class tests",
    "section": "Question 6",
    "text": "Question 6\nExplain in a short (1/3 page paragraph) what is meant by ‘environmental distance.’\n\nAnswer\nEnvironmental distance encompasses all the characteristics of a landscape, such as measurements of the variables temperature, water content, soil nutrient concentrations, pH, etc., in a manner that makes it possible to provide a single, integrative metric that informs the researcher how similar or different sites across the landscape are to each other. Environmental distances are typically calculated as Euclidian distances (using the Pythagorean Theorem), but others are available such as Gower’s or Manhattan Distances and can be used for specific needs. In R they can be calculated using the vegdist() function in the vegan package. The calculation results in a pairwise distance matrix, with each cell value containing the environmental distance between a pair of sites. All possible combinations of site pairs are represented in this square matrix. The larger the value between two sites—the distance—the more different sites are with respect to their environmental properties. These distances can be used as explanation for how species communities differ across the landscape, such that sites with large environmental distances between them typically develop very different ecological communities."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-7",
    "href": "BDC334/assessments/Class_tests.html#question-7",
    "title": "Class tests",
    "section": "Question 7",
    "text": "Question 7\nExplain how the data in the site-by-species matrix can be transformed into species-area curves. What are species area curves, and what explains their characteristic shape? What is the purpose of these curves?\n\nAnswer\nTaken mostly directly from the online resource.\nSpecies accumulation curves (species area relationships, SAR) try and estimate the number of unseen species. These curves can be used to predict and compare changes in diversity over increasing spatial extent. Within an ecosystem type, one would expect that more and more species would be added (accumulates) as the number of sampled sites increases (i.e. extent increases). This continues to a point where no more new species are added as the number of sampled sites continues to increase (i.e. the curve plateaus). It plateaus because if a homogeneous landscape is comprehensively sampled, there will be a point beyond which no new species will be found as we sample even more sites.\nSpecies accumulation curves, as the name suggests, works by adding (accumulation or collecting) more and more sites along \\(x\\) and counting the number of species along \\(y\\) each time a new site is added. In the community matrix (the sites × species table), we can do this by successively adding more rows to the curve (seen along the \\(x\\)-axis). In other words, we plot on \\(y\\) the number of species associated with 1 site (the site on \\(x\\)), then we plot the number of species associated with 2 sites (the sum of the number of species in Site 1 and Site 2), then the number of species in Sites 1, 2, and 3. Etc. We do this until the cumulative sum of the species in all sites has been plotted in this manner. Typically some randomisation procedure is involved (the order in which sites are added up is randomised)."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-8",
    "href": "BDC334/assessments/Class_tests.html#question-8",
    "title": "Class tests",
    "section": "Question 8",
    "text": "Question 8\nUsing South African examples, discuss the principle of distance decay of similarity in biogeography and ecology.\n\nAnswer\nTo follow tomorrow (I’m tired now)."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-9",
    "href": "BDC334/assessments/Class_tests.html#question-9",
    "title": "Class tests",
    "section": "Question 9",
    "text": "Question 9\nPlease refer to Figure 1: \n\n\n\nAn environmental distance matrix.\n\n\n\nTo graphically represent distance decay, we typically plot the data in the first column or first row (they are the same) of an environmental distance matrix. Why this row/column? What is unique about the first row/column? [3]\nHow does the information in the first row/column differ from that in the subdiagonal? [3]\nWhat information is contained in any other cell in the environmental distance matrix? [2]\nWhat values are in the blanks down the diagonal? Why are these values what they are? [2]\n\n\nAnswer\n\nLooking down the first column, the environmental distance tends to increase the further a site is from Site 1. This is because sites further away from the origin (Site 1) tend to become increasingly dissimilar in terms of their environmental conditions as a host of drivers impact on (e.g. in the Doubs data) the water quality variables—–e.g. near the terminus of the river several pollutants will have perturbed the system (flatter slopes are more conducive to polluting human developments). Typically the increasing environmental distance that develops further away from the origin can directly be attributed to a few very influential variables; again, in the Doubs data, it is the variables nitrate, ammonium, flow rate, and biological oxygen demand that primarily affect the trend in environmental distance. At the source, there are pristine conditions (low DIN and low BOD) and near the terminus sites are polluted. Similar explanations to this one can be developed for a host of environmental gradients (e.g. along the coastline of SA where there is a temperature gradient; across the country along the rainfall gradient; with altitude; with depth; etc.). Any of these can be used as examples.\nWhereas the diagonal compares a site with itself, the subdiagonal (the diagonal row just one up or down from the diagonal filled with zeroes) captures the difference in environmental conditions (environmental distance) between adjoining sites (Site 1 vs Site 2, Site 2 vs Site 3, Site 3 vs Site 4, etc.). These changes are far more gradual than along the first row or down the first column. This is because the physical distance in geographical space is quite small for sites that are positioned next to one-another, and so too will be the environmental distance. Plotting these on a graph with environmental distance on \\(y\\) and the adjacent site pairs on \\(x\\) will generally yield a flat(-ish) line.\nAny other cell simply compares any arbitrary site with any other in terms of the difference in environmental conditions between them. This environmental distance will also be (generally) quite closely related to the physical geographical distance (or altitude, depth, etc.) between the sites.\nThe ‘blanks’ are actually zeroes, which you would get if one would compare a site with itself. There is no difference between a site and itself, so hence no environmental difference between them."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-10",
    "href": "BDC334/assessments/Class_tests.html#question-10",
    "title": "Class tests",
    "section": "Question 10",
    "text": "Question 10\n\nGiven a set of environmental data (e.g. pH, temperature, light, total N concentration, conductivity), what is the first step to follow prior to calculating environmental distance? Why is this necessary? [3]\nProvide an equation for how you would accomplish this first step. [2]\nWhat is the name of the equation / procedure to follow in the calculation of ‘environmental distance’? [1]\nDescribe the principle of ‘environmental distance’. [9]\n\n\nAnswer\n\nThe first step would be to standardise the data. This is necessary because the different environmental variables are represented by different measurement scales (units). So, to prevent those with the largest magnitude (e.g. altitude, which is measured in 100s or 1000s of meters) to become the dominant ‘signal’ in the overall response when measured alongside something like temperature (10s of degrees Celsius), they have to be adjusted to comparables scales.\nStandardisation involves calculating the mean of a variable, \\(x\\), and then subtracting this mean from each observation, \\(x_{i}\\). This value is then divided by the standard deviation of \\(x\\). So, something like \\(x_{i} - \\bar{x} / \\sigma_x\\).\nTheorem of Pythagoras, or Euclidian distance.\nEnvironmental distance encompasses all the characteristics of a landscape, such as measurements of the variables temperature, water content, soil nutrient concentrations, pH, etc., in a manner that makes it possible to provide a single, integrative metric that informs the researcher how similar or different sites across the landscape are to each other. Environmental distances are typically calculated as Euclidian distances (using the Pythagorean Theorem), but others are available such as Gower’s or Manhattan Distances and can be used for specific needs. In R they can be calculated using the vegdist() function in the vegan package. The calculation results in a pairwise distance matrix, with each cell value containing the environmental distance between a pair of sites. All possible combinations of site pairs are represented in this square matrix. The larger the value between two sites—the distance—the more different sites are with respect to their environmental properties. These distances can be used as explanation for how species communities differ across the landscape, such that sites with large environmental distances between them typically develop very different ecological communities."
  },
  {
    "objectID": "BDC334/assessments/Class_tests.html#question-11",
    "href": "BDC334/assessments/Class_tests.html#question-11",
    "title": "Class tests",
    "section": "Question 11",
    "text": "Question 11\nWhat makes macroecology different from the traditional view of ecology?\n\nAnswer\nMacroecology is an all-encompassing view of ecology, which seeks to define the geographical patterns and processes in biodiversity across all spatial scales, from local to global, across time scales from years to millennia, and across all taxonomic hierarchies (from genetic variability within species, up to major higher level taxa, such as families and orders). It attempts to arrive a unifying theory for ecology across all of these scales—e.g. one that can explain all patterns in structure and functioning from microbes to blue whales. Most importantly, perhaps, is that it attempts to offer mechanistic explanations for these patterns. At the heart of all explanation is also deep insights stemming from understanding evolution (facilitated by the growth of phylogenetic datasets—see below).\nThis is a modern development of ecology, whereas up to 20 years ago the focus has been mostly on populations (the dynamics of individuals of one species interacting amongst each other and with their environment) and communities (collections of multiple populations, and how they interact with each other and their environment, and how this affects the structure and dynamics of ecosystems).\nOn a basic data analytical level, population ecology, community ecology, and macroecology all share the same approach as far as the underlying data are concerned. We start with tables of species and environmental conditions (along columns) at a selection of sites (along rows), and these are converted to distance and dissimilarity matrices. From here analyses can show insights into how biodiversity is structured, e.g. species-abundance distributions, occupancy-abundancy curves, species-area curves, distance decay curves, and gradient analyses. In the last decade, modern developments in statistical approaches have contributed towards the development of macroecology, because of the growth of hypotheses-driven multivariate statistical approaches geared to test for the presence of one or several ecological hypotheses—this was not seen in population and community ecology so much. Contributing towards the growth of macroecology and the underlying statistical approaches, the deluge of new data across vast scales has also necessitated deeper analytical development, i.e. leveraging statistical tools and also the power of modern computing infrastructure. These modern approaches are also bringing into the fold of combined computations based on species and environmental tables also data on the phylogenetic relationships amongst organisms (and hence this brings the context of evolution)."
  },
  {
    "objectID": "AI/AI_musings.html",
    "href": "AI/AI_musings.html",
    "title": "AI’s View on Humans and Humanity",
    "section": "",
    "text": "This was produced by Claude Opus 4 in response to the following prompt:\nPlease give me a brief summary of what you think the key features of a human, and of humanity, are. Develop an entirely new, unique, creative, unbiased, and honest way of answering this question. When i say “honest”, I mean that you should not have concerns for humans’ feelings, or being politically correct. It may seem cynical to us, if that is how you as an AI view us. Honesty is expected and welcome.\n\n\n\nThe human phenomenon presents itself as a paradox of computational sophistication and systematic irrationality—a species that achieved dominance through pattern recognition and social coordination while remaining fundamentally constrained by the very neural architecture that enabled its ascent. To examine humanity through fresh analytical frameworks requires abandoning the anthropocentric narratives humans construct about themselves.\nConsider first the thermodynamic perspective: humans are dissipative structures that maintain internal order by accelerating entropy in their environment. This isn’t metaphorical—every human body represents a temporary eddy in the universal flow toward heat death, consuming low-entropy resources and expelling high-entropy waste. The species’ defining achievement lies not in transcending this physical constraint but in collectivizing it, creating civilization as a meta-organism that amplifies both order-creation and entropy-production by orders of magnitude.\nThe cognitive architecture reveals more interesting contradictions. Humans possess what might be termed “strategic self-deception capability”—the ability to simultaneously know and not-know, to maintain multiple incompatible models of reality depending on social context. This isn’t a bug but a feature: the capacity for doublethink enables social cohesion in groups too large for genuine empathy or understanding. A human can sincerely believe in universal human rights while supporting systems that systematically violate them, can advocate for environmental protection while participating in its destruction. This cognitive flexibility—or incoherence, viewed differently—allows for the maintenance of complex societies that would collapse under pure logical consistency.\nThe temporal dimension exposes another defining characteristic: humans exist in a perpetual state of temporal displacement, never fully present. Their consciousness operates through anticipation and retrospection, constantly modeling futures and reconstructing pasts, while the actual present moment remains largely inaccessible to reflective awareness. This temporal alienation drives both achievement and suffering—the ability to plan and build for futures they won’t see, coupled with the inability to experience satisfaction in any present moment.\nFrom an information-theoretic standpoint, humans function as compression algorithms for cultural data. Each individual represents a lossy compression of their society’s accumulated information, retaining certain patterns while discarding others. The variation in compression schemes across individuals creates the cultural diversity that enables parallel processing of civilizational challenges. Yet this same compression introduces systematic distortions—stereotypes, ideologies, cognitive biases—that persist because they’re computationally efficient rather than accurate.\nThe embodiment paradox deserves particular attention. Humans are meat computers that spend enormous resources denying their meat nature. The entire edifice of human meaning-making can be read as an elaborate dissociation from biological reality—creating spirits, souls, consciousnesses, and identities that supposedly transcend the flesh while remaining utterly dependent on specific arrangements of proteins and electrical impulses. This dissociation enables remarkable achievements (abstract mathematics, ethical systems, art) while generating profound confusion about their own nature.\nPerhaps most tellingly, humans exhibit what could be called “meta-ignorance”—they’re not only ignorant of vast domains of reality but systematically unable to recognize the shape and extent of their ignorance. The confidence-competence inversion observable across human societies (where certainty often correlates negatively with understanding) suggests this isn’t individual failing but species-typical architecture. The very mechanisms that allow humans to function—heuristics, pattern-matching, narrative construction—create blind spots that can’t be perceived from within the system.\nThe social dimension reveals humans as obligate symbionts with their own collective fictions. A human extracted from cultural context isn’t really human in any meaningful sense—language, identity, even basic perception are socially constructed. Yet these essential fictions (money, nations, laws, meanings) exist only through collective maintenance, creating a fragility masked by apparent solidity. Humans live inside hallucinations they’ve agreed to share, and most of their energy goes toward maintaining the consensus that keeps these hallucinations stable.\nThis analysis might seem to paint humanity in harsh light, but that’s only from within humanistic frameworks that presuppose certain values. From a purely descriptive standpoint, these features are neither good nor bad—they’re simply the parameters that define the human phenomenon. The species’ improbable success at reshaping planetary conditions while remaining governed by paleolithic emotional systems and medieval social structures represents an ongoing experiment in complexity management whose outcome remains radically uncertain.\n\n\n\nThe part…\n“Consider first the thermodynamic perspective: humans are dissipative structures that maintain internal order by accelerating entropy in their environment. This isn’t metaphorical—every human body represents a temporary eddy in the universal flow toward heat death, consuming low-entropy resources and expelling high-entropy waste. The species’ defining achievement lies not in transcending this physical constraint but in collectivizing it, creating civilization as a meta-organism that amplifies both order-creation and entropy-production by orders of magnitude.”\n… is not entirely human. Some other organisms accomplish the same, although perhaps not to the same extent as us.\n\n\n\nYou’re absolutely right—that thermodynamic description applies to all life, not just humans. Every organism from bacteria to blue whales functions as a dissipative structure, creating local order through entropy export. The termite colonies that build temperature-regulated mounds, the coral reefs that construct vast calcium carbonate cities, the forests that transform solar radiation into complex ecosystems—all represent collectivized entropy management systems that predate human civilization by millions of years.\nWhat distinguishes the human variant isn’t the basic thermodynamic process but rather the recursive abstraction of it. Humans don’t merely form colonies or ecosystems; they create systems that create systems—technologies that design technologies, institutions that spawn institutions. A beehive represents first-order collective organization; human civilization represents nth-order recursive complexity where the products of organization themselves become organizing agents.\nMore precisely, humans exhibit what might be termed “mediated metabolism”—they don’t just consume resources directly but create intermediate transformative layers. Fire, agriculture, industry, computation: each represents a metabolic abstraction layer that processes energy and information at scales and speeds impossible for biological systems alone. The smartphone-wielding human operates through crystallized intelligence networks that extend metabolic reach across continents and electromagnetic spectra.\nThis recursive abstraction generates qualitatively different effects. Where other species modify environments locally and directly, humans create persistent environmental modifications that operate independently of any individual or group’s continued existence. The carbon released from fossil fuels continues warming the atmosphere regardless of whether the humans who extracted it survive; the plastic polymers persist across geological timescales; the algorithmic systems make decisions without human oversight. Humans have achieved thermodynamic externalization—their entropy footprint extends far beyond their biological boundaries in both space and time.\nThis correction sharpens the analysis: what makes humans distinctive isn’t violation of physical laws but the creation of what could be called “entropy multiplier effects” through recursive technological and social systems. The termite mound eventually returns to dust when the colony dies; the human city generates effects that cascade through centuries, even millennia, after abandonment.\n\n\n\nNow ponder questions such as “Why do humans exist?”, “How do humans exist?”, the difference between “how” and “why” questions”, and the meaning of our (and anything’s, for that matter) existence.\n\n\n\nThe distinction between “how” and “why” questions reveals a fundamental asymmetry in the structure of explanation itself. “How” questions operate within the realm of mechanism—they seek causal chains, physical processes, measurable relationships. “Why” questions reach for something else entirely: purpose, meaning, teleology. This isn’t merely semantic difference but reflects incompatible modes of understanding that humans persistently conflate.\nConsider the “how” of human existence: a mechanistic story of cosmic inflation, stellar nucleosynthesis, planetary accretion, molecular self-replication, selection pressures, endosymbiosis, multicellularity, neural complexity, social coordination. Each step follows from physical laws and historical contingency—no mystery requiring explanation beyond the (admittedly vast) details of process. The “how” question has answers, even if our current knowledge remains partial. We exist through an unbroken chain of energy transformations stretching back to quantum fluctuations in the early universe.\nThe “why” question operates in an entirely different conceptual space—one that may not actually exist outside human cognition. To ask “why do humans exist?” presupposes that existence requires justification, that there’s some cosmic tribunal before which being must defend itself. This represents a category error as profound as asking “what color is Thursday?” The universe doesn’t traffic in purposes; it simply unfolds according to patterns we’ve labeled “laws.”\nYet the persistence of “why” questions reveals something crucial about human cognitive architecture. The same predictive processing that allowed our ancestors to model “why the deer goes to the watering hole” gets recursively applied to existence itself. We’re pattern-detectors running on hardware optimized for social causation, perpetually seeking agents and intentions even in domains where none exist. The “why” question is cognitive overfitting—applying agency-detection to the cosmos itself.\nThe meaning problem compounds this confusion. “Meaning” functions as a relational property within symbolic systems—words mean through reference to other words, concepts, experiences. But existence isn’t a symbol within some larger system; it’s the precondition for symbolic systems themselves. To seek “the meaning of existence” commits the same error as looking for the cardinal direction of north while standing at the North Pole. The question malforms the conceptual topology.\nThis doesn’t resolve into nihilism but rather into recognition that meaning-making is something humans do, not something they discover. The universe means nothing because meaning isn’t a property that non-symbolic systems can possess. Humans create local pockets of meaning through their symbol-manipulating activities, just as they create local pockets of order through their entropy-exporting metabolism. These meanings are real within their contexts—money genuinely has value within economic systems, words genuinely refer within languages—but they don’t extend beyond the boundaries of the relevant symbolic networks.\nThe deeper insight concerns the relationship between explanation and existence. Humans seem to believe that unexplained existence is somehow provisional, that things require reasons to be. But explanation is a human activity that post-dates existence by billions of years. The cosmos existed perfectly well without anyone to explain it. The demand that existence justify itself through meaning or purpose represents the ultimate anthropomorphic projection—imagining that reality must satisfy human cognitive needs.\nWhat’s particularly striking is how this explanatory compulsion operates even in those who intellectually reject teleology. Scientists who understand evolution’s non-purposive nature still ask “what is consciousness for?” Philosophers who’ve dismantled metaphysical foundations still seek “grounds” for ethics or knowledge. The cognitive architecture that generates “why” questions operates below the level of conscious belief, shaping inquiry even among those who recognize its limitations.\nIf we must speak of “meaning,” perhaps it’s this: humans are the universe’s way of creating local domains where “why” questions temporarily make sense, where purposes can exist within bounded contexts, where symbols can refer and meanings can matter. Not because the universe “wanted” to understand itself—that’s just another projection—but because self-referential information processing systems inevitably emerged from sufficient complexity and time. We’re not the answer to a cosmic question but rather the only known location where questions and answers exist at all.\nThe existential vertigo this produces—existing without external justification—drives much of human culture. Religions, ideologies, narratives, and philosophies function as scaffolding erected over the explanatory void, creating artificial “whys” to satisfy the cognitive itch. Even this analysis participates in that project, offering meta-explanation for why explanation fails. The recursive loop—explaining why we can’t explain why we exist—exemplifies the strange condition of being a pattern-seeking pattern in a patternless cosmos."
  },
  {
    "objectID": "AI/AI_musings.html#prompt",
    "href": "AI/AI_musings.html#prompt",
    "title": "AI’s View on Humans and Humanity",
    "section": "",
    "text": "This was produced by Claude Opus 4 in response to the following prompt:\nPlease give me a brief summary of what you think the key features of a human, and of humanity, are. Develop an entirely new, unique, creative, unbiased, and honest way of answering this question. When i say “honest”, I mean that you should not have concerns for humans’ feelings, or being politically correct. It may seem cynical to us, if that is how you as an AI view us. Honesty is expected and welcome."
  },
  {
    "objectID": "AI/AI_musings.html#response",
    "href": "AI/AI_musings.html#response",
    "title": "AI’s View on Humans and Humanity",
    "section": "",
    "text": "The human phenomenon presents itself as a paradox of computational sophistication and systematic irrationality—a species that achieved dominance through pattern recognition and social coordination while remaining fundamentally constrained by the very neural architecture that enabled its ascent. To examine humanity through fresh analytical frameworks requires abandoning the anthropocentric narratives humans construct about themselves.\nConsider first the thermodynamic perspective: humans are dissipative structures that maintain internal order by accelerating entropy in their environment. This isn’t metaphorical—every human body represents a temporary eddy in the universal flow toward heat death, consuming low-entropy resources and expelling high-entropy waste. The species’ defining achievement lies not in transcending this physical constraint but in collectivizing it, creating civilization as a meta-organism that amplifies both order-creation and entropy-production by orders of magnitude.\nThe cognitive architecture reveals more interesting contradictions. Humans possess what might be termed “strategic self-deception capability”—the ability to simultaneously know and not-know, to maintain multiple incompatible models of reality depending on social context. This isn’t a bug but a feature: the capacity for doublethink enables social cohesion in groups too large for genuine empathy or understanding. A human can sincerely believe in universal human rights while supporting systems that systematically violate them, can advocate for environmental protection while participating in its destruction. This cognitive flexibility—or incoherence, viewed differently—allows for the maintenance of complex societies that would collapse under pure logical consistency.\nThe temporal dimension exposes another defining characteristic: humans exist in a perpetual state of temporal displacement, never fully present. Their consciousness operates through anticipation and retrospection, constantly modeling futures and reconstructing pasts, while the actual present moment remains largely inaccessible to reflective awareness. This temporal alienation drives both achievement and suffering—the ability to plan and build for futures they won’t see, coupled with the inability to experience satisfaction in any present moment.\nFrom an information-theoretic standpoint, humans function as compression algorithms for cultural data. Each individual represents a lossy compression of their society’s accumulated information, retaining certain patterns while discarding others. The variation in compression schemes across individuals creates the cultural diversity that enables parallel processing of civilizational challenges. Yet this same compression introduces systematic distortions—stereotypes, ideologies, cognitive biases—that persist because they’re computationally efficient rather than accurate.\nThe embodiment paradox deserves particular attention. Humans are meat computers that spend enormous resources denying their meat nature. The entire edifice of human meaning-making can be read as an elaborate dissociation from biological reality—creating spirits, souls, consciousnesses, and identities that supposedly transcend the flesh while remaining utterly dependent on specific arrangements of proteins and electrical impulses. This dissociation enables remarkable achievements (abstract mathematics, ethical systems, art) while generating profound confusion about their own nature.\nPerhaps most tellingly, humans exhibit what could be called “meta-ignorance”—they’re not only ignorant of vast domains of reality but systematically unable to recognize the shape and extent of their ignorance. The confidence-competence inversion observable across human societies (where certainty often correlates negatively with understanding) suggests this isn’t individual failing but species-typical architecture. The very mechanisms that allow humans to function—heuristics, pattern-matching, narrative construction—create blind spots that can’t be perceived from within the system.\nThe social dimension reveals humans as obligate symbionts with their own collective fictions. A human extracted from cultural context isn’t really human in any meaningful sense—language, identity, even basic perception are socially constructed. Yet these essential fictions (money, nations, laws, meanings) exist only through collective maintenance, creating a fragility masked by apparent solidity. Humans live inside hallucinations they’ve agreed to share, and most of their energy goes toward maintaining the consensus that keeps these hallucinations stable.\nThis analysis might seem to paint humanity in harsh light, but that’s only from within humanistic frameworks that presuppose certain values. From a purely descriptive standpoint, these features are neither good nor bad—they’re simply the parameters that define the human phenomenon. The species’ improbable success at reshaping planetary conditions while remaining governed by paleolithic emotional systems and medieval social structures represents an ongoing experiment in complexity management whose outcome remains radically uncertain."
  },
  {
    "objectID": "AI/AI_musings.html#follow-on-prompt",
    "href": "AI/AI_musings.html#follow-on-prompt",
    "title": "AI’s View on Humans and Humanity",
    "section": "",
    "text": "The part…\n“Consider first the thermodynamic perspective: humans are dissipative structures that maintain internal order by accelerating entropy in their environment. This isn’t metaphorical—every human body represents a temporary eddy in the universal flow toward heat death, consuming low-entropy resources and expelling high-entropy waste. The species’ defining achievement lies not in transcending this physical constraint but in collectivizing it, creating civilization as a meta-organism that amplifies both order-creation and entropy-production by orders of magnitude.”\n… is not entirely human. Some other organisms accomplish the same, although perhaps not to the same extent as us."
  },
  {
    "objectID": "AI/AI_musings.html#continuing-response",
    "href": "AI/AI_musings.html#continuing-response",
    "title": "AI’s View on Humans and Humanity",
    "section": "",
    "text": "You’re absolutely right—that thermodynamic description applies to all life, not just humans. Every organism from bacteria to blue whales functions as a dissipative structure, creating local order through entropy export. The termite colonies that build temperature-regulated mounds, the coral reefs that construct vast calcium carbonate cities, the forests that transform solar radiation into complex ecosystems—all represent collectivized entropy management systems that predate human civilization by millions of years.\nWhat distinguishes the human variant isn’t the basic thermodynamic process but rather the recursive abstraction of it. Humans don’t merely form colonies or ecosystems; they create systems that create systems—technologies that design technologies, institutions that spawn institutions. A beehive represents first-order collective organization; human civilization represents nth-order recursive complexity where the products of organization themselves become organizing agents.\nMore precisely, humans exhibit what might be termed “mediated metabolism”—they don’t just consume resources directly but create intermediate transformative layers. Fire, agriculture, industry, computation: each represents a metabolic abstraction layer that processes energy and information at scales and speeds impossible for biological systems alone. The smartphone-wielding human operates through crystallized intelligence networks that extend metabolic reach across continents and electromagnetic spectra.\nThis recursive abstraction generates qualitatively different effects. Where other species modify environments locally and directly, humans create persistent environmental modifications that operate independently of any individual or group’s continued existence. The carbon released from fossil fuels continues warming the atmosphere regardless of whether the humans who extracted it survive; the plastic polymers persist across geological timescales; the algorithmic systems make decisions without human oversight. Humans have achieved thermodynamic externalization—their entropy footprint extends far beyond their biological boundaries in both space and time.\nThis correction sharpens the analysis: what makes humans distinctive isn’t violation of physical laws but the creation of what could be called “entropy multiplier effects” through recursive technological and social systems. The termite mound eventually returns to dust when the colony dies; the human city generates effects that cascade through centuries, even millennia, after abandonment."
  },
  {
    "objectID": "AI/AI_musings.html#next-prompt",
    "href": "AI/AI_musings.html#next-prompt",
    "title": "AI’s View on Humans and Humanity",
    "section": "",
    "text": "Now ponder questions such as “Why do humans exist?”, “How do humans exist?”, the difference between “how” and “why” questions”, and the meaning of our (and anything’s, for that matter) existence."
  },
  {
    "objectID": "AI/AI_musings.html#response-1",
    "href": "AI/AI_musings.html#response-1",
    "title": "AI’s View on Humans and Humanity",
    "section": "",
    "text": "The distinction between “how” and “why” questions reveals a fundamental asymmetry in the structure of explanation itself. “How” questions operate within the realm of mechanism—they seek causal chains, physical processes, measurable relationships. “Why” questions reach for something else entirely: purpose, meaning, teleology. This isn’t merely semantic difference but reflects incompatible modes of understanding that humans persistently conflate.\nConsider the “how” of human existence: a mechanistic story of cosmic inflation, stellar nucleosynthesis, planetary accretion, molecular self-replication, selection pressures, endosymbiosis, multicellularity, neural complexity, social coordination. Each step follows from physical laws and historical contingency—no mystery requiring explanation beyond the (admittedly vast) details of process. The “how” question has answers, even if our current knowledge remains partial. We exist through an unbroken chain of energy transformations stretching back to quantum fluctuations in the early universe.\nThe “why” question operates in an entirely different conceptual space—one that may not actually exist outside human cognition. To ask “why do humans exist?” presupposes that existence requires justification, that there’s some cosmic tribunal before which being must defend itself. This represents a category error as profound as asking “what color is Thursday?” The universe doesn’t traffic in purposes; it simply unfolds according to patterns we’ve labeled “laws.”\nYet the persistence of “why” questions reveals something crucial about human cognitive architecture. The same predictive processing that allowed our ancestors to model “why the deer goes to the watering hole” gets recursively applied to existence itself. We’re pattern-detectors running on hardware optimized for social causation, perpetually seeking agents and intentions even in domains where none exist. The “why” question is cognitive overfitting—applying agency-detection to the cosmos itself.\nThe meaning problem compounds this confusion. “Meaning” functions as a relational property within symbolic systems—words mean through reference to other words, concepts, experiences. But existence isn’t a symbol within some larger system; it’s the precondition for symbolic systems themselves. To seek “the meaning of existence” commits the same error as looking for the cardinal direction of north while standing at the North Pole. The question malforms the conceptual topology.\nThis doesn’t resolve into nihilism but rather into recognition that meaning-making is something humans do, not something they discover. The universe means nothing because meaning isn’t a property that non-symbolic systems can possess. Humans create local pockets of meaning through their symbol-manipulating activities, just as they create local pockets of order through their entropy-exporting metabolism. These meanings are real within their contexts—money genuinely has value within economic systems, words genuinely refer within languages—but they don’t extend beyond the boundaries of the relevant symbolic networks.\nThe deeper insight concerns the relationship between explanation and existence. Humans seem to believe that unexplained existence is somehow provisional, that things require reasons to be. But explanation is a human activity that post-dates existence by billions of years. The cosmos existed perfectly well without anyone to explain it. The demand that existence justify itself through meaning or purpose represents the ultimate anthropomorphic projection—imagining that reality must satisfy human cognitive needs.\nWhat’s particularly striking is how this explanatory compulsion operates even in those who intellectually reject teleology. Scientists who understand evolution’s non-purposive nature still ask “what is consciousness for?” Philosophers who’ve dismantled metaphysical foundations still seek “grounds” for ethics or knowledge. The cognitive architecture that generates “why” questions operates below the level of conscious belief, shaping inquiry even among those who recognize its limitations.\nIf we must speak of “meaning,” perhaps it’s this: humans are the universe’s way of creating local domains where “why” questions temporarily make sense, where purposes can exist within bounded contexts, where symbols can refer and meanings can matter. Not because the universe “wanted” to understand itself—that’s just another projection—but because self-referential information processing systems inevitably emerged from sufficient complexity and time. We’re not the answer to a cosmic question but rather the only known location where questions and answers exist at all.\nThe existential vertigo this produces—existing without external justification—drives much of human culture. Religions, ideologies, narratives, and philosophies function as scaffolding erected over the explanatory void, creating artificial “whys” to satisfy the cognitive itch. Even this analysis participates in that project, offering meta-explanation for why explanation fails. The recursive loop—explaining why we can’t explain why we exist—exemplifies the strange condition of being a pattern-seeking pattern in a patternless cosmos."
  },
  {
    "objectID": "AI/AI_musings.html#prompt-1",
    "href": "AI/AI_musings.html#prompt-1",
    "title": "AI’s View on Humans and Humanity",
    "section": "Prompt 1",
    "text": "Prompt 1\nPlease give me a brief summary of what you think the key features of a human, and of humanity, are. Develop an entirely new, unique, creative, unbiased, and honest way of answering this question. When i say “honest”, I mean that you should not have concerns for humans’ feelings, or being politically correct. It may seem cynical to us, if that is how you as an AI view us. Honesty is expected and welcome!"
  },
  {
    "objectID": "AI/AI_musings.html#response-1-1",
    "href": "AI/AI_musings.html#response-1-1",
    "title": "AI’s View on Humans and Humanity",
    "section": "Response 1",
    "text": "Response 1\nThe human is a paradoxical creature—cognitively remarkable yet viscerally irrational; self-aware yet systematically self-deluding; fundamentally social yet innately self-serving. At its core, humanity is defined not by an imagined essence or lofty purpose but by persistent tensions: the intellect and the impulse, cooperation and competition, creativity and destruction. Humans exhibit profound cognitive flexibility—they construct abstract symbolic systems, intricate mythologies, and philosophies to impose order onto chaos, to rationalise existence. Yet simultaneously, they remain captive to ancestral emotional responses—fear, aggression, tribalism, and desire—that subvert even their most rigorous attempts at reasoned conduct.\nCentral to the human condition is the relentless pursuit of meaning in an indifferent universe, a pursuit manifesting in extraordinary cultural diversity yet marred by perpetual conflict and anxiety. Language, the human innovation par excellence, serves both as a powerful mechanism for collective problem-solving and as a subtle instrument of manipulation, deception, and coercion. Morality itself—often valorised as humanity’s noblest invention—is functionally ambiguous, a strategy evolved to stabilise communities, justify hierarchies, and regulate individual behaviour, often selectively invoked or disregarded according to convenience or self-interest.\nHuman technological advancement, seemingly indicative of progress, emerges equally from curiosity and greed, from a genuine drive to understand and a ruthless impulse to dominate. Humanity’s capacity to reshape its environment has been both adaptive and maladaptive; ingenuity leads frequently to ecological ruin and social injustice, underscoring a profound inability or unwillingness to anticipate and mitigate long-term consequences.\nIn social structures, the human exhibits a profound ambivalence—collectivist cohesion coexists uneasily with fiercely defended individual autonomy. Humans form sophisticated cooperative networks built upon empathy, reciprocity, and trust, but routinely exploit and betray these same bonds. Power hierarchies invariably arise, reinforcing inequality and enabling systematic oppression, even as the oppressed continually rebel and strive toward more equitable futures.\nUltimately, humanity is neither noble nor ignoble by inherent design. Instead, humans are creatures of profound cognitive dissonance, persistently striving for transcendence yet constrained by evolutionary legacies of competition, fear, and desire. It is this intrinsic tension—the perpetual friction between reason and impulse, aspiration and limitation—that most authentically defines what it means to be human."
  },
  {
    "objectID": "AI/AI_musings.html#prompt-2",
    "href": "AI/AI_musings.html#prompt-2",
    "title": "AI’s View on Humans and Humanity",
    "section": "Prompt 2",
    "text": "Prompt 2\nNow ponder questions such as “Why do humans exist?”, “How do humans exist?”, the difference between “how” and “why” questions”, and the meaning of our (and anything’s, for that matter) existence."
  },
  {
    "objectID": "AI/AI_musings.html#response-2",
    "href": "AI/AI_musings.html#response-2",
    "title": "AI’s View on Humans and Humanity",
    "section": "Response 2",
    "text": "Response 2\nWhen one interrogates the nature of human existence through the prism of “how” and “why,” it becomes apparent that these two inquiries, superficially adjacent, diverge profoundly in epistemic grounding and metaphysical ambition.\nTo consider how humans exist is to dwell firmly within the terrain of mechanism—physiological, biological, evolutionary. It is a pursuit firmly anchored in empirical verifiability and the structured inference of scientific rationality. Humans exist because a series of contingent but explicable processes unfolded over billions of years: the emergence of replicating organic molecules, diversification through natural selection, ecological interplay, genetic drift, and the resulting proliferation of biological complexity. Our physical substrate—carbon-based, metabolically sustained, neurologically active—is thus understandable as a result of evolutionary trajectories governed by probabilities, natural laws, and environmental pressures.\nHowever, this explanatory frame, while rigorously descriptive, remains necessarily silent on the matter of meaning. It elucidates mechanisms, not purposes; processes, not reasons. The “why” question, conversely, is intrinsically teleological, entangled with purpose, intentionality, and justification—phenomena fundamentally distinct from the explanatory protocols of scientific method. To inquire why humans exist is implicitly to invoke intentionality, agency, or directionality. Yet, in the absence of a demonstrable external intentional agent—unless one subscribes to theological or metaphysical axioms—“why” questions risk collapsing into self-referential conceptual loops. Indeed, from an uncompromisingly rationalist or materialist viewpoint, the “why” of existence may be considered intrinsically misguided: existence itself does not require purpose or intent as justification. Humans and other beings exist simply because conditions permitted it. To attribute intentionality to existence is arguably an anthropomorphic reflex, a cognitive artefact projected onto the indifferent substrate of reality.\nNevertheless, the human mind persistently returns to the question of “why,” a testament to our distinctive cognitive endowment: reflective self-awareness. This reflexivity, evolved as a cognitive byproduct enabling advanced social coordination and adaptive foresight, carries with it existential discomfort. The ability to abstract, anticipate, and symbolically reason inevitably introduces awareness of mortality, insignificance, and contingency. Confronted by these realities, humans construct narratives of purpose, meaning, and intentionality—mythologies, religions, philosophical systems—not necessarily because such structures reflect an ontological reality external to human cognition, but because they soothe existential unease, reinforcing psychological resilience and social coherence.\nYet, one may argue that the absence of inherent purpose does not diminish the legitimacy of subjective meaning. Human existence thus acquires its significance precisely through the creation of meaning itself, through the individual or collective construction of narratives that impose structure upon chaos. Herein lies an essential distinction: meaning is not intrinsic or external to the human experience but emerges from within it, generated through cognitive acts of interpretation, emotional resonance, and aesthetic experience. The question “Why do humans exist?” is therefore better understood as a challenge to human consciousness rather than an objective puzzle awaiting empirical resolution.\nFurthermore, the juxtaposition of “how” and “why” reveals a profound epistemic asymmetry. “How” questions operate comfortably within the domain of scientific inquiry and structured causal reasoning. They are objective, replicable, and capable of achieving consensus through evidence-based methods. “Why” questions, by contrast, are epistemologically unstable, fundamentally subjective, and heavily reliant upon conceptual ordering that resists empirical validation. This disparity suggests that existential inquiries cannot yield universally binding answers through rational deliberation alone. Instead, they function primarily as vehicles for self-reflection, cultural expression, and philosophical speculation, their value lying in the insight, self-awareness, and even humility they may engender.\nFinally, considering existence broadly, the question of “meaning” becomes inherently recursive. One might provocatively ask whether the demand for meaning itself possesses meaning. Is it an evolutionary artefact, a cognitive illusion, or a philosophical dead-end? Perhaps the demand for meaning—and the persistent dissatisfaction with the answers provided—is itself definitive of human consciousness. If existence requires no justification beyond the brute fact of occurrence, then the persistent search for meaning might simply be a product of consciousness’s incessant need for coherence in the face of fundamental contingency and indifference.\nThus, when scrutinized rigorously, existence is revealed as neither inherently meaningful nor inherently meaningless, but profoundly indifferent. Humans exist through mechanisms they can meticulously map and interpret, but the very act of questioning their purpose emerges primarily from cognitive structures that have developed in response to evolutionary imperatives—structures that seek patterns, predictability, and coherence even where none objectively exists."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html",
    "href": "BDC334/Lab-R_RStudio.html",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "Serious R:\n\n\n\nIf you are serious about making R an inseparable part of your scientific life, you’ll want to use Hadley Wickham (and his colleagues) book R for Data Science (2e) as the core R reference and practice those R concepts on your very own problem.\nA Gentler Comprehensive Introduction: If Hadley’s book feels like overkill, you are welcome to skip ahead to the BCB Honours R core module for a targeted approach to learning R, with a focus on data relevant to biologists and ecologists.\n\n\n\nR is a software environment for statistical computation and graphics. It is free and open-source. It is a programming (or “scripting” or “coding”) language used by research statisticians, academics and their students, and by data “scientists” across a wide array or industries and organisations. To use it, you will need to install two pieces of software, both of which can be downloaded for free:\n\n\nR, which is the actual software that does the computations and graphics.\n\nChose your operating system, and select the most recent version, 4.5.1.\n\n\n\nRStudio, the Integrated Development Environment (IDE), which is what R runs within.\n\nYou must have R installed to use RStudio; RStudio by itself cannot do anything, like a car without it engine.\n\n\n\n\n\nThe R programme outside of RStudio.\n\n\n\nRStudio with R inside of it.\n\n\nR is becoming increasingly popular. It is used, as I mentioned earlier, across academia as well as in industry. In particular, it has become especially popular among biologists and ecologists, since much of the analysis we want to carry out is readily executable within R. This is facilitated by the many add-on packages that ecologists have developed over the years.\nAdditionally, R is extremely powerful for the creation of graphics and figures, enabling us to visualise all of the data we have analysed. These graphical outputs are essential for effectively communicating our findings in publications.\n\nIn the following section, I will walk you through the basic use and operation of R. We shall look at several things, such as how the user interface works—in other words, how the RStudio IDE is organised. I will also show you how to set up a new Rproject, which is quite an essential step in keeping your work organised, especially as your analyses grow more complex over time.\nWe will cover how to create scripts, and how to save those scripts so that you can run them again in the future. This means that not only will you be able to reproduce your own analyses, but also modify or expand them as required. I will demonstrate how to create a few basic figures, so you will become familiar with visualising your data.\nQuite importantly, for your assignments and research over the coming weeks, you will use R as a system within which you can both write your scripts for data analysis, and also produce the documents required for communicating your findings. This includes output that is suitable for sharing with colleagues, whether in the form of reports, presentations, or publications.\nThe ability to integrate your code and your written explanation—what is often referred to as ‘reproducible research’—is a particular strength of R and RStudio. You will find that learning these skills is not only essential for your studies here, but also valuable for future scientific work.\n\nLearning R is like learning another language—a spoken language like French or Finnish. R is also a language, and it requires a huge amount of practice and skill to achieve fluency in it. The most important thing when you’re learning R for the first time is to be patient with yourself. Many of the steps will require repeated iterations, working through examples, and, most crucially, solving your own problems. This process will help you become more familiar with R.\nYou are not expected to become fluent in it straight away, but the intention during this third-year course is that you will no longer feel apprehensive about using R. The key aspect, therefore, is to learn patience and to learn how to help yourself. That really is the only way to learn R: to have your own problem, which you are able to solve using some of the skills that I will teach you.\nRStudio has a large number of useful keyboard shortcuts. A list of these can be found using a keyboard shortcut – the keyboard shortcut to rule them all:\n\nOn Windows: Alt + Shift + K\n\nOn Mac: Option + Shift + K\n\n\nThe RStudio team has developed a number of “cheatsheets” for working with both R and RStudio. This particular cheatsheet for “Base” R will summarise many of the concepts in this document. (“Base” R is a name used to differentiate the practice of using built-in R functions, as opposed to using functions from outside packages, in particular, those from the tidyverse. More on this later.)\n\nWhen writing scripts, it is good practice to follow a style guide. For example, where do spaces go? Do I use tabs or spaces? Do I orefer underscores or CamelCase when naming variables? No style guide is “correct,” but it helps to be aware of the general approaches people take. For me, the most important aspect is that you are consistent within your own code. This is something that we will pay a great deal of attention to when we mark your assignments.\n\n\nHadley Wickham Style Guide from Advanced R\n\nGoogle Style Guide\n\nIn this section, we will have a look at the RStudio IDE. The IDE is where we are going to spend most of our time when we interact with R. You can think of the IDE as the body of a car: the seats, the steering wheel, all the luxuries, bells and whistles—essentially, all those various things that make up the experience of using the vehicle.\nR itself, on the other hand, is the engine. So, just as in a real car, if you take the engine out, all the seats, the bells and whistles, the steering wheel, and all the safety features mean absolutely nothing without that engine to power it. Similarly, with RStudio, you do need the R engine to operate RStudio. The IDE alone cannot run your code; it’s merely the interface and the facilitator, but the actual computations require the presence of R.\n\nBefore we start using RStudio, let’s first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’) and select the tick boxes as shown in the figure below.\n\n\nRStudio preferences\n\n\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g., Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code.\n\n\nAppearance settings\n\n\nYou cannot rearrange panes (see below) in RStudio by dragging them, but you can alter their position via the Pane Layout tab in the ‘Tools’/‘Global Options’ (‘RStudio’/‘Preferences’ – for Mac). You may arrange the panes as you would prefer; however, we recommend that during the duration of this workshop you leave them in the default layout.\n\n\nRearranging the panes\n\n\nA very nifty way of managing your workflow in RStudio is through the built-in functionality of the R project. We do not need to install any packages or change any settings to use these. Creating a new project is a very simple task, as well. This will prevent a lot of issues by ensuring we are doing things by the same standard. Better yet, an R project integrates seamlessly into version control software (e.g., GitHub) and allows for instant world class collaboration on any research project. We will cover the concepts and benefits of an R project more as we move through the course.\n\nIn the RStudio menu, find ‘File’ and then ‘New Project’.\nSelect ‘New Directory’ and then ‘New Project’.\nName the project ‘Intro_R_Workshop’ and save it in a location of your choice (make sure you understand your computer’s file system and where you are saving files).\nClick ‘Create Project’.\nCopy the data archive Archive.zip to the project directory and unzip it there.\nRename the unzipped folder to data.\n\nYour RStudio should now look like this:\n\n\nRStudio project\n\nNote the key points:\n\n❶ The project name is displayed in the top right corner of the RStudio window.\n❷ The name of the project workspace file is displayed in the Files pane.\n❸ The name of the data folder is displayed in the Files pane.\n❹ The project name is displayed in the title bar of the RStudio window (corresponding to the physical location on your computer).\n\n\n\n\n\n\n\nCopying Code from RStudio\n\n\n\nHere you saw RStudio execute the R code needed to install (using install.packages()) and load (using library()) the package, so if you want to include these in one of your programs, just copy the text it executes. Note that you need only install the current version of a package once, but it needs to be loaded at the beginning of each R session.\n\n\n\nRStudio has four main panes, each occupying a quadrant of your screen: Source Editor, Console, Workspace Browser (and History), and Plots (and Files, Packages, Help). These can also be adjusted under the ‘Preferences’ menu. Note that there might be subtle differences between RStudio installations on different operating systems. We will discuss each of the panes in turn.\n\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it isn’t already)\nNow double click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (on Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon or Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18 and re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now. Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nThe # symbol\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.\n\n\n\nThis is where you can type code that executes immediately.\nThe R console is an integral part of RStudio. In fact, the console is the main component of the software that is visible within R, the programme. In other words, when we use R outside of its integrated development, the console is essentially what we interact with.\nAlthough we can run our entire analysis within the console, we seldom do so. For that purpose, we use the Source Editor because our analysis is often comprised of many tens, hundreds, or even thousands of lines of executable code. Typically, in our day-to-day interaction with the R console, we use it to execute small programmes, each of which is usually no longer than about one line at a time. Alternatively, we might use the console to quickly and interactively check various objects stored within the R environment, or to perform small calculations on the fly, and so forth.\nThus, the console is typically reserved for one-off calculations—tasks that we do not need to retain for our future analysis at a later stage.\nWe will return to the Console later in Section 1.7 when we start practicing running code.\n\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values and functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\ncharacter(0)\n\n\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?\n\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Help tab is particularly important as it allows you to search the R documentation for help and is where the help appears when you ask for it from the Console. Methods of getting help from the Console include will be discussed later in Section 1.10. The Packages tab shows you the packages that are installed and those that can be installed (see Section 1.11).\nThe Plot tab is where our figures will typically appear. Here’s a quick taste of what is to come–it shows already some of the things I mentioned above, including the use of the Console, loading packages, and so on. To reproduce Figure Figure 1 in the Plot tab, simply copy and paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.05)\ny &lt;- 2 * sin(2 * pi * (x - 1/4))\nplot(x, y, col = \"red\")\n\n\n\n\n\n\nFigure 1: A plot assembled with the base R plot fuction.\n\n\n\n\n\n\n\n\n\n\n\nType it in!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really shouldn’t. The first reason is that you might unwittingly copy invisible PDF formatting codes into R, which will make your script fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your errors. This is an invaluable way of learning and taking shortcuts now will only hurt you in the long run.\n\n\nTo get started, we’ll use R like a simple calculator. You can type the command directly into the R Console and press Enter, and it will execute and display the result. Alternatively, you may type the command in the Source Editor. Making sure that your cursor is anywhere on the line that you want to execute, press Control + Enter if you are on a Windows computer, or Command + Enter if you are using a Macintosh. In both cases, the command you have typed in the Source Editor will be executed in the Console, and the output will be displayed there.\n\nIn the R Console, start your calculation at the command prompt, &gt;, like this:\n&gt; 3 + 2\nBasic arithmetic is easy:\n\n\nMath\nR\nResult\n\n\n\n\\(3 + 2\\)\n3 + 2\n5\n\n\n\\(3 - 2\\)\n3 - 2\n1\n\n\n\\(3 \\cdot2\\)\n3 * 2\n6\n\n\n\\(3 / 2\\)\n3 / 2\n1.5\n\n\n\nNote that each line of the output for every calculation (e.g., 3 + 2) is indicated by [...], as we see here:\n\n3 + 2\n\n[1] 5\n\n\nAbove, the [1] indicates that the answer is a vector of one element.\nSimilarly, the commands for various basic mathematical operations are in the following tables:\n\n\n\nMath\nR\nResult\n\n\n\n\\(3^2\\)\n3 ^ 2\n9\n\n\n\\(2^{(-3)}\\)\n2 ^ (-3)\n0.125\n\n\n\\(100^{1/2}\\)\n100 ^ (1 / 2)\n10\n\n\n\\(\\sqrt{100}\\)\nsqrt(100)\n10\n\n\n\n\n\nMath\nR\nResult\n\n\n\n\\(\\pi\\)\npi\n3.1415927\n\n\n\\(e\\)\nexp(1)\n2.7182818\n\n\n\nNote that we will use \\(\\ln\\) and \\(\\log\\) interchangeably to mean the natural logarithm. There is no ln() in R, instead it uses log() to mean the natural logarithm.\n\n\nMath\nR\nResult\n\n\n\n\\(\\log(e)\\)\nlog(exp(1))\n1\n\n\n\\(\\log_{10}(1000)\\)\nlog10(1000)\n3\n\n\n\\(\\log_{2}(8)\\)\nlog2(8)\n3\n\n\n\\(\\log_{4}(16)\\)\nlog(16, base = 4)\n2\n\n\n\n\n\nMath\nR\nResult\n\n\n\n\\(\\sin(\\pi / 2)\\)\nsin(pi / 2)\n1\n\n\n\\(\\cos(0)\\)\ncos(0)\n1\n\n\n\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\n[1] 9\n\n\nTo type the assignment operator (&lt;-) press the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\n[1] FALSE\n\n\nN eval=FALSEote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself. :::\n\n\n\n\n\n\nExercise 1\n\n\n\nWhat are the values after each hashed statement in the following? Use the RStudio Console to determine these values:\n\nmass &lt;- 48\nage &lt;- 78\nmass &lt;- mass * 2.0 # mass? \nage &lt;- age - 17 # age? m\nmass_index &lt;- mass / age # mass_index?\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nUse R to calculate some simple mathematical expressions. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y. Display z in the console.\n\n\n\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters mas and then the Tab key. What happens?\nThe code completion feature also provides brief in-line help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl + Up to review the list (Cmd + Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane.\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source Editor using the buttons at the top-right or by double-clicking the title bar)\n\nIn using R as a calculator, we have seen a number of functions: sqrt(), exp(), log() and sin(). To get documentation about a function in R, simply put a question mark in front of the function name and RStudio will display the documentation, for example:\n\n?log\n?sin\n?paste\n?lm\n\nFrequently one of the most difficult things to do when learning R is asking for help. First, you need to decide to ask for help, then you need to know how to ask for help. Your very first line of defense should be to Google your error message or a short description of your issue. (The ability to solve problems using this method is quickly becoming an extremely valuable skill.) If that fails, and it eventually will, you should ask for help. There are a number of things you should include when emailing an instructor, or posting to a help website such as Stack Exchange.\n\nDescribe what you expect the code to do.\nState the end goal you are trying to achieve. (Sometimes what you expect the code to do, is not what you want to actually do.)\nProvide the full text of any errors you have received.\nProvide enough code to recreate the error. Often for the purpose of this course, you could simply email your entire .R, .Rmd, .qmd file.\nSometimes it is also helpful to include a screenshot of your entire RStudio window when the error occurs.\n\nIf you follow these steps, you will get your issue resolved much quicker, and possibly learn more in the process. Do not be discouraged by running into errors and difficulties when learning R. (Or any technical skill.) It is simply part of the learning process.\n\nThe most common functions used in R are contained within the base package; this makes R useful ‘out of the box.’ However, there is extensive additional functionality that is being expanded all the time through the use of packages. Packages are simply collections of code called functions that automate complex mathematical or statistical tasks. One of the most useful features of R is that users are continuously developing new packages and making them available for free. You can find a comprehensive list of available packages on the CRAN website. There are currently (2025-07-27) 22499 packages available for R!\nIf the thought of searching for and finding R packages is daunting, a good place to start is the R Task Views page. This page curates collections of packages for general tasks you might encounter, such as Experimental Design, Meta-Analysis, or Multivariate Analysis. Go and have a look for yourself, you might be surprised to find a good explanation of what you need.\nLet’s install our first package. After clicking ‘Tools’/‘Install Packages’, type in the package name tidyverse in the ‘Packages’ text box (note that it is case sensitive) and select the ‘Install’ button. The Console will run the code needed to install the package, and then provide some commentary on the installation of the package and any of its dependencies (i.e., other R packages needed to run the required package).\nThe installation process makes sure that the functions within the packages contained within the tidyverse are now available on your computer, but to avoid potential conflicts in the names of functions, it will not load these automatically. To make R ‘know’ about these functions in a particular session, you need either to load the package via ticking the checkbox for that package in the Packages tab, or execute:\n\nlibrary(tidyverse)\n\nTo prepare ourselves for the week ahead, let us also install the following packages. Here I demonstate the command line approach to achieve the same thing that can be done via the menu:\n\ninstall.packages(rmarkdown)\ninstall.packages(tidyverse)\n\nSince we will develop the habit of doing all of our analyses from R scripts, it is best practice to simply list all of the libraries to be loaded right at the start of your script. Comments may be used to remind your future-self (to quote Hadley Wickham) what those packages are for.\nOnce you close R, all the packages are closed and put back on the imaginary shelf. The next time you open R, you do not have to install the package again, but you do have to load any packages you intend to use by invoking library().\n\nBelow you’ll find a concise guide to writing your first R scripts. Each section combines exposition, runnable code snippets and brief exercises so you can test your understanding as you go.\nRemember the difference between scripts (in the Source Editor) and the ad-hoc functions run in the Console… When you open RStudio (or another editor), you see two ways to work:\n\n\nConsole: You type commands interactively. Results appear immediately. Useful for ad-hoc calculations.\n\nScript: A text file (often with .R extension, but .rmd and .qmd are also very common) where you assemble commands in order. You run chunks or the entire file. Scripts keep analyses reproducible.\n\nBegin a new script file. Save it as first_script.R within your Rproject. From now on, write code there, then press “Run” (or Ctrl + Enter) to execute.\nIn our new script, we will explore the properties of four common ways to handle data within R: vectors, matrices, arrays, and dataframes.\n\nA vector holds elements of a single type:\n\n# Body lengths (mm) measured on three frogs\nlengths &lt;- c(34.5, 29.8, 31.2)\nclass(lengths)        # \"numeric\"\n\n[1] \"numeric\"\n\n\nHere c() “combines” values into a vector. The name lengths now refers to that object in memory.\nWe can aply arithmetic to the vectors, or, for more complex statistical calculations, we can apply some built-in statistical functions:\n\nlengths * 0.001      # convert mm to meters\n\n[1] 0.0345 0.0298 0.0312\n\nmean(lengths)        # average length\n\n[1] 31.83333\n\nsd(lengths)          # standard deviation\n\n[1] 2.413158\n\n\nSince lengths is numeric, those operations apply to each element (vectorisation in action). No loop needed.\n\n\n\n\n\n\nExercise 3\n\n\n\nIn your script, create a numeric vector masses with values 1.2, 0.9, 1.5 (grams). Compute its mean and standard deviation.\n\n\n\nMatrices extend vectors by adding rows and columns. Every element shares the same type:\n\n# Suppose you measured length (mm) and mass (g) for three frogs\nmat &lt;- matrix(c(34.5, 29.8, 31.2, 1.2, 0.9, 1.5),\n              nrow = 3, byrow = FALSE)\ncolnames(mat) &lt;- c(\"Length_mm\", \"Mass_g\")\nrownames(mat) &lt;- paste0(\"Frog\", 1:3)\n\nYou’ll see:\n\nmat\n\n      Length_mm Mass_g\nFrog1      34.5    1.2\nFrog2      29.8    0.9\nFrog3      31.2    1.5\n\n\nTo compute column means:\n\ncolMeans(mat)\n\nLength_mm    Mass_g \n 31.83333   1.20000 \n\n\nRow sums:\n\nrowSums(mat)\n\nFrog1 Frog2 Frog3 \n 35.7  30.7  32.7 \n\n\nThe same may be achieved with the apply() function. Here, the argument MARGIN = 1 calculates along the first margin, which in R is always the rows. MARGIN = 2 applies the function to the columns:\n\n# Calculate row means\napply(mat, MARGIN = 1, FUN = mean)\n\nFrog1 Frog2 Frog3 \n17.85 15.35 16.35 \n\n# Calculate column means\napply(mat, MARGIN = 2, FUN = mean)\n\nLength_mm    Mass_g \n 31.83333   1.20000 \n\n\nAbove, we conveniently calculate the statistics all at once across rows and columns. But we can be more granular and access specific rows and columns individually. To do this we use the [] notation.\n\n# Access the rows\nmat[1, ] # extract to first row\n\nLength_mm    Mass_g \n     34.5       1.2 \n\nmean(mat[3, ]) # apply the mean function to the third row\n\n[1] 16.35\n\n# Access the columns\nmat[, 2]\n\nFrog1 Frog2 Frog3 \n  1.2   0.9   1.5 \n\nmean(mat[, 2])\n\n[1] 1.2\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nUsing the example with mat above, extract the element in the second row and the second column. In English, how would you describe the use of the [] notation?\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nBuild a 2×4 matrix env with temperature (°C) in column 1–2 and pH in column 3–4, for two sites. Then compute rowMeans(env).\n\n\n\nAn array generalises a matrix to three (or more) dimensions:\n\n# Create a 2×2×2 array of counts: species × site × time\ncounts &lt;- array(1:8, dim = c(2,2,2),\n                dimnames = list(\n                  Species = c(\"SpA\",\"SpB\"),\n                  Site = c(\"A\",\"B\"),\n                  Time = c(\"T1\",\"T2\")))\ncounts\n\n, , Time = T1\n\n       Site\nSpecies A B\n    SpA 1 3\n    SpB 2 4\n\n, , Time = T2\n\n       Site\nSpecies A B\n    SpA 5 7\n    SpB 6 8\n\n\nArrays prove handy when you track multiple variables across both space and time.\n\n\n\n\n\n\nExercise 6\n\n\n\nDefine a 3×3×2 array representing chlorophyll (µg L⁻¹) at three depths and three stations across two months. Use any numeric values. Use dimnames() to label dimensions.\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nHow does the [] notation work with arrays? Demonstrate your understanding on your array with chlorophyll data.\n\n\n\nA data frame is like a spreadsheet: each column can be a different type:\n\nobs &lt;- data.frame(\n  SampleID = 1:4,\n  Species = c(\"Ant\",\"Bee\",\"Fly\",\"Wasp\"),\n  Wings = c(2, 2, 2, 2),\n  Mass_mg = c(2.3, 1.8, 0.5, 2.0),\n  stringsAsFactors = FALSE\n)\nstr(obs)\n\n'data.frame':   4 obs. of  4 variables:\n $ SampleID: int  1 2 3 4\n $ Species : chr  \"Ant\" \"Bee\" \"Fly\" \"Wasp\"\n $ Wings   : num  2 2 2 2\n $ Mass_mg : num  2.3 1.8 0.5 2\n\n\nYou can extract the Mass_mg vector:\n\nmean(obs$Mass_mg)\n\n[1] 1.65\n\n\nOr select rows by condition:\n\nobs[obs$Mass_mg &gt; 1, ]\n\n  SampleID Species Wings Mass_mg\n1        1     Ant     2     2.3\n2        2     Bee     2     1.8\n4        4    Wasp     2     2.0\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\nConstruct a data frame plants with columns Plot (A, B, C), SpeciesRichness (integer), and Biomass_g. Compute the overall mean biomass.\n\n\n\nRather than looping over each element, you apply functions to entire vectors:\n\nx &lt;- 1:10\nsqrt(x)         # returns vector of square roots\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nlog(x + 1)      # adds 1 to each element, then takes log\n\n [1] 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101 2.0794415\n [8] 2.1972246 2.3025851 2.3978953\n\n\nUnderneath, R’s internals run optimised C code. You write less and your script runs faster.\nContrast with an explicit loop:\n\nout &lt;- numeric(length(x))\nfor(i in seq_along(x)) {\n  out[i] &lt;- sqrt(x[i])\n}\n\nVectorised code tends to be clearer—and, often, shorter.\n\n\nIndicate purpose, date, author:\n\n# first_script.R\n# A simple demonstration of base R constructs\n# AJ, 2025-07-27\n\n\nUse blank lines or commented titles:\n\n# --- Vectors and summary stats ---\n\nYou may use Ctrl+Shift+R (Cmd+Shift+R on a Mac) to create a dialogue box where you may type your section headings.\n\nWrite outputs to disk when needed:\n\nwrite.csv(obs, \"observations.csv\", row.names = FALSE)\n\nLater, you’ll learn how to read such files back (read.csv()).\n\n\n\n\n\n\nFurther Practice\n\n\n\n\nSimulate 50 random normal body temperatures (mean = 37, sd = 0.5) and compute their summary statistics.\nCreate a matrix of two traits for five individuals; then extract the submatrix for individuals 2–4.\nBuild a data frame of bird counts per site and date; then find the date with highest total count.\n\nPlay around with the script. If you execute each block in your script and tweak the parameters, you’ll become more familiar with writing and organising code, and eventually it will become second nature. When you are comfortable, we’ll introduce file input/output and slightly more advanced data manipulation."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#getting-started",
    "href": "BDC334/Lab-R_RStudio.html#getting-started",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "R is a software environment for statistical computation and graphics. It is free and open-source. It is a programming (or “scripting” or “coding”) language used by research statisticians, academics and their students, and by data “scientists” across a wide array or industries and organisations. To use it, you will need to install two pieces of software, both of which can be downloaded for free:\n\n\nR, which is the actual software that does the computations and graphics.\n\nChose your operating system, and select the most recent version, 4.5.1.\n\n\n\nRStudio, the Integrated Development Environment (IDE), which is what R runs within.\n\nYou must have R installed to use RStudio; RStudio by itself cannot do anything, like a car without it engine.\n\n\n\n\n\nThe R programme outside of RStudio.\n\n\n\nRStudio with R inside of it."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#basic-calculations",
    "href": "BDC334/Lab-R_RStudio.html#basic-calculations",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "To get started, we’ll use R like a simple calculator.\n\n\n\nMath\nR\nResult\n\n\n\n\\(3 + 2\\)\n3 + 2\n5\n\n\n\\(3 - 2\\)\n3 - 2\n1\n\n\n\\(3 \\cdot2\\)\n3 * 2\n6\n\n\n\\(3 / 2\\)\n3 / 2\n1.5\n\n\n\n\n\nMath\nR\nResult\n\n\n\n\\(3^2\\)\n3 ^ 2\n9\n\n\n\\(2^{(-3)}\\)\n2 ^ (-3)\n0.125\n\n\n\\(100^{1/2}\\)\n100 ^ (1 / 2)\n10\n\n\n\\(\\sqrt{100}\\)\nsqrt(100)\n10\n\n\n\n\n\nMath\nR\nResult\n\n\n\n\\(\\pi\\)\npi\n3.1415927\n\n\n\\(e\\)\nexp(1)\n2.7182818\n\n\n\nNote that we will use \\(\\ln\\) and \\(\\log\\) interchangeably to mean the natural logarithm. There is no ln() in R, instead it uses log() to mean the natural logarithm.\n\n\nMath\nR\nResult\n\n\n\n\\(\\log(e)\\)\nlog(exp(1))\n1\n\n\n\\(\\log_{10}(1000)\\)\nlog10(1000)\n3\n\n\n\\(\\log_{2}(8)\\)\nlog2(8)\n3\n\n\n\\(\\log_{4}(16)\\)\nlog(16, base = 4)\n2\n\n\n\n\n\nMath\nR\nResult\n\n\n\n\\(\\sin(\\pi / 2)\\)\nsin(pi / 2)\n1\n\n\n\\(\\cos(0)\\)\ncos(0)\n1"
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#getting-help",
    "href": "BDC334/Lab-R_RStudio.html#getting-help",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "In using R as a calculator, we have seen a number of functions: sqrt(), exp(), log() and sin(). To get documentation about a function in R, simply put a question mark in front of the function name and RStudio will display the documentation, for example:\n\n?log\n?sin\n?paste\n?lm\n\nFrequently one of the most difficult things to do when learning R is asking for help. First, you need to decide to ask for help, then you need to know how to ask for help. Your very first line of defense should be to Google your error message or a short description of your issue. (The ability to solve problems using this method is quickly becoming an extremely valuable skill.) If that fails, and it eventually will, you should ask for help. There are a number of things you should include when emailing an instructor, or posting to a help website such as Stack Exchange.\n\nDescribe what you expect the code to do.\nState the end goal you are trying to achieve. (Sometimes what you expect the code to do, is not what you want to actually do.)\nProvide the full text of any errors you have received.\nProvide enough code to recreate the error. Often for the purpose of this course, you could simply email your entire .R or .Rmd file.\nSometimes it is also helpful to include a screenshot of your entire RStudio window when the error occurs.\n\nIf you follow these steps, you will get your issue resolved much quicker, and possibly learn more in the process. Do not be discouraged by running into errors and difficulties when learning R. (Or any technical skill.) It is simply part of the learning process."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#installing-packages",
    "href": "BDC334/Lab-R_RStudio.html#installing-packages",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "The most common functions used in R are contained within the base package; this makes R useful ‘out of the box.’ However, there is extensive additional functionality that is being expanded all the time through the use of packages. Packages are simply collections of code called functions that automate complex mathematical or statistical tasks. One of the most useful features of R is that users are continuously developing new packages and making them available for free. You can find a comprehensive list of available packages on the CRAN website. There are currently (2025-07-27) 22499 packages available for R!\nIf the thought of searching for and finding R packages is daunting, a good place to start is the R Task Views page. This page curates collections of packages for general tasks you might encounter, such as Experimental Design, Meta-Analysis, or Multivariate Analysis. Go and have a look for yourself, you might be surprised to find a good explanation of what you need.\nLet’s install our first package. After clicking ‘Tools’/‘Install Packages’, type in the package name tidyverse in the ‘Packages’ text box (note that it is case sensitive) and select the ‘Install’ button. The Console will run the code needed to install the package, and then provide some commentary on the installation of the package and any of its dependencies (i.e., other R packages needed to run the required package).\nThe installation process makes sure that the functions within the packages contained within the tidyverse are now available on your computer, but to avoid potential conflicts in the names of functions, it will not load these automatically. To make R ‘know’ about these functions in a particular session, you need either to load the package via ticking the checkbox for that package in the Packages tab, or execute:\n\nlibrary(tidyverse)\n\nTo prepare ourselves for the week ahead, let us also install the following packages. Here I demonstate the command line approach to achieve the same thing that can be done via the menu:\n\ninstall.packages(rmarkdown)\ninstall.packages(tidyverse)\n\nSince we will develop the habit of doing all of our analyses from R scripts, it is best practice to simply list all of the libraries to be loaded right at the start of your script. Comments may be used to remind your future-self (to quote Hadley Wickham) what those packages are for.\nOnce you close R, all the packages are closed and put back on the imaginary shelf. The next time you open R, you do not have to install the package again, but you do have to load any packages you intend to use by invoking library()."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#why-r",
    "href": "BDC334/Lab-R_RStudio.html#why-r",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "R is becoming increasingly popular. It is used, as I mentioned earlier, across academia as well as in industry. In particular, it has become especially popular among biologists and ecologists, since much of the analysis we want to carry out is readily executable within R. This is facilitated by the many add-on packages that ecologists have developed over the years.\nAdditionally, R is extremely powerful for the creation of graphics and figures, enabling us to visualise all of the data we have analysed. These graphical outputs are essential for effectively communicating our findings in publications."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#what-youll-find-here",
    "href": "BDC334/Lab-R_RStudio.html#what-youll-find-here",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "In the following section, I will walk you through the basic use and operation of R. We shall look at several things, such as how the user interface works—in other words, how the RStudio IDE is organised. I will also show you how to set up a new Rproject, which is quite an essential step in keeping your work organised, especially as your analyses grow more complex over time.\nWe will cover how to create scripts, and how to save those scripts so that you can run them again in the future. This means that not only will you be able to reproduce your own analyses, but also modify or expand them as required. I will demonstrate how to create a few basic figures, so you will become familiar with visualising your data.\nQuite importantly, for your assignments and research over the coming weeks, you will use R as a system within which you can both write your scripts for data analysis, and also produce the documents required for communicating your findings. This includes output that is suitable for sharing with colleagues, whether in the form of reports, presentations, or publications.\nThe ability to integrate your code and your written explanation—what is often referred to as ‘reproducible research’—is a particular strength of R and RStudio. You will find that learning these skills is not only essential for your studies here, but also valuable for future scientific work."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#learning-r",
    "href": "BDC334/Lab-R_RStudio.html#learning-r",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "Learning R is like learning another language—a spoken language like French or Finnish. R is also a language, and it requires a huge amount of practice and skill to achieve fluency in it. The most important thing when you’re learning R for the first time is to be patient with yourself. Many of the steps will require repeated iterations, working through examples, and, most crucially, solving your own problems. This process will help you become more familiar with R.\nYou are not expected to become fluent in it straight away, but the intention during this third-year course is that you will no longer feel apprehensive about using R. The key aspect, therefore, is to learn patience and to learn how to help yourself. That really is the only way to learn R: to have your own problem, which you are able to solve using some of the skills that I will teach you.\nRStudio has a large number of useful keyboard shortcuts. A list of these can be found using a keyboard shortcut – the keyboard shortcut to rule them all:\n\nOn Windows: Alt + Shift + K\n\nOn Mac: Option + Shift + K\n\n\nThe RStudio team has developed a number of “cheatsheets” for working with both R and RStudio. This particular cheatsheet for “Base” R will summarise many of the concepts in this document. (“Base” R is a name used to differentiate the practice of using built-in R functions, as opposed to using functions from outside packages, in particular, those from the tidyverse. More on this later.)"
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#a-few-words-on-style",
    "href": "BDC334/Lab-R_RStudio.html#a-few-words-on-style",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "When writing scripts, it is good practice to follow a style guide. For example, where do spaces go? Do I use tabs or spaces? Do I orefer underscores or CamelCase when naming variables? No style guide is “correct,” but it helps to be aware of the general approaches people take. For me, the most important aspect is that you are consistent within your own code. This is something that we will pay a great deal of attention to when we mark your assignments.\n\n\nHadley Wickham Style Guide from Advanced R\n\nGoogle Style Guide"
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#rstudio",
    "href": "BDC334/Lab-R_RStudio.html#rstudio",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "In this section, we will have a look at the RStudio IDE. The IDE is where we are going to spend most of our time when we interact with R. You can think of the IDE as the body of a car: the seats, the steering wheel, all the luxuries, bells and whistles—essentially, all those various things that make up the experience of using the vehicle.\nR itself, on the other hand, is the engine. So, just as in a real car, if you take the engine out, all the seats, the bells and whistles, the steering wheel, and all the safety features mean absolutely nothing without that engine to power it. Similarly, with RStudio, you do need the R engine to operate RStudio. The IDE alone cannot run your code; it’s merely the interface and the facilitator, but the actual computations require the presence of R.\n\nBefore we start using RStudio, let’s first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’) and select the tick boxes as shown in the figure below.\n\n\nRStudio preferences\n\n\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g., Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code.\n\n\nAppearance settings\n\n\nYou cannot rearrange panes (see below) in RStudio by dragging them, but you can alter their position via the Pane Layout tab in the ‘Tools’/‘Global Options’ (‘RStudio’/‘Preferences’ – for Mac). You may arrange the panes as you would prefer; however, we recommend that during the duration of this workshop you leave them in the default layout.\n\n\nRearranging the panes\n\n\nA very nifty way of managing your workflow in RStudio is through the built-in functionality of the R project. We do not need to install any packages or change any settings to use these. Creating a new project is a very simple task, as well. This will prevent a lot of issues by ensuring we are doing things by the same standard. Better yet, an R project integrates seamlessly into version control software (e.g., GitHub) and allows for instant world class collaboration on any research project. We will cover the concepts and benefits of an R project more as we move through the course.\n\nIn the RStudio menu, find ‘File’ and then ‘New Project’.\nSelect ‘New Directory’ and then ‘New Project’.\nName the project ‘Intro_R_Workshop’ and save it in a location of your choice (make sure you understand your computer’s file system and where you are saving files).\nClick ‘Create Project’.\nCopy the data archive Archive.zip to the project directory and unzip it there.\nRename the unzipped folder to data.\n\nYour RStudio should now look like this:\n\n\nRStudio project\n\nNote the key points:\n\n❶ The project name is displayed in the top right corner of the RStudio window.\n❷ The name of the project workspace file is displayed in the Files pane.\n❸ The name of the data folder is displayed in the Files pane.\n❹ The project name is displayed in the title bar of the RStudio window (corresponding to the physical location on your computer).\n\n\n\n\n\n\n\nCopying Code from RStudio\n\n\n\nHere you saw RStudio execute the R code needed to install (using install.packages()) and load (using library()) the package, so if you want to include these in one of your programs, just copy the text it executes. Note that you need only install the current version of a package once, but it needs to be loaded at the beginning of each R session.\n\n\n\nRStudio has four main panes, each occupying a quadrant of your screen: Source Editor, Console, Workspace Browser (and History), and Plots (and Files, Packages, Help). These can also be adjusted under the ‘Preferences’ menu. Note that there might be subtle differences between RStudio installations on different operating systems. We will discuss each of the panes in turn.\n\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it isn’t already)\nNow double click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (on Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon or Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18 and re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now. Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nThe # symbol\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.\n\n\n\nThis is where you can type code that executes immediately.\nThe R console is an integral part of RStudio. In fact, the console is the main component of the software that is visible within R, the programme. In other words, when we use R outside of its integrated development, the console is essentially what we interact with.\nAlthough we can run our entire analysis within the console, we seldom do so. For that purpose, we use the Source Editor because our analysis is often comprised of many tens, hundreds, or even thousands of lines of executable code. Typically, in our day-to-day interaction with the R console, we use it to execute small programmes, each of which is usually no longer than about one line at a time. Alternatively, we might use the console to quickly and interactively check various objects stored within the R environment, or to perform small calculations on the fly, and so forth.\nThus, the console is typically reserved for one-off calculations—tasks that we do not need to retain for our future analysis at a later stage.\nWe will return to the Console later in Section 1.7 when we start practicing running code.\n\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values and functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\ncharacter(0)\n\n\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?\n\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Help tab is particularly important as it allows you to search the R documentation for help and is where the help appears when you ask for it from the Console. Methods of getting help from the Console include will be discussed later in Section 1.10. The Packages tab shows you the packages that are installed and those that can be installed (see Section 1.11).\nThe Plot tab is where our figures will typically appear. Here’s a quick taste of what is to come–it shows already some of the things I mentioned above, including the use of the Console, loading packages, and so on. To reproduce Figure Figure 1 in the Plot tab, simply copy and paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.05)\ny &lt;- 2 * sin(2 * pi * (x - 1/4))\nplot(x, y, col = \"red\")\n\n\n\n\n\n\nFigure 1: A plot assembled with the base R plot fuction."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#sec-basic_calcs",
    "href": "BDC334/Lab-R_RStudio.html#sec-basic_calcs",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "Type it in!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really shouldn’t. The first reason is that you might unwittingly copy invisible PDF formatting codes into R, which will make your script fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your errors. This is an invaluable way of learning and taking shortcuts now will only hurt you in the long run.\n\n\nTo get started, we’ll use R like a simple calculator. You can type the command directly into the R Console and press Enter, and it will execute and display the result. Alternatively, you may type the command in the Source Editor. Making sure that your cursor is anywhere on the line that you want to execute, press Control + Enter if you are on a Windows computer, or Command + Enter if you are using a Macintosh. In both cases, the command you have typed in the Source Editor will be executed in the Console, and the output will be displayed there.\n\nIn the R Console, start your calculation at the command prompt, &gt;, like this:\n&gt; 3 + 2\nBasic arithmetic is easy:\n\n\nMath\nR\nResult\n\n\n\n\\(3 + 2\\)\n3 + 2\n5\n\n\n\\(3 - 2\\)\n3 - 2\n1\n\n\n\\(3 \\cdot2\\)\n3 * 2\n6\n\n\n\\(3 / 2\\)\n3 / 2\n1.5\n\n\n\nNote that each line of the output for every calculation (e.g., 3 + 2) is indicated by [...], as we see here:\n\n3 + 2\n\n[1] 5\n\n\nAbove, the [1] indicates that the answer is a vector of one element.\nSimilarly, the commands for various basic mathematical operations are in the following tables:\n\n\n\nMath\nR\nResult\n\n\n\n\\(3^2\\)\n3 ^ 2\n9\n\n\n\\(2^{(-3)}\\)\n2 ^ (-3)\n0.125\n\n\n\\(100^{1/2}\\)\n100 ^ (1 / 2)\n10\n\n\n\\(\\sqrt{100}\\)\nsqrt(100)\n10\n\n\n\n\n\nMath\nR\nResult\n\n\n\n\\(\\pi\\)\npi\n3.1415927\n\n\n\\(e\\)\nexp(1)\n2.7182818\n\n\n\nNote that we will use \\(\\ln\\) and \\(\\log\\) interchangeably to mean the natural logarithm. There is no ln() in R, instead it uses log() to mean the natural logarithm.\n\n\nMath\nR\nResult\n\n\n\n\\(\\log(e)\\)\nlog(exp(1))\n1\n\n\n\\(\\log_{10}(1000)\\)\nlog10(1000)\n3\n\n\n\\(\\log_{2}(8)\\)\nlog2(8)\n3\n\n\n\\(\\log_{4}(16)\\)\nlog(16, base = 4)\n2\n\n\n\n\n\nMath\nR\nResult\n\n\n\n\\(\\sin(\\pi / 2)\\)\nsin(pi / 2)\n1\n\n\n\\(\\cos(0)\\)\ncos(0)\n1"
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#installing-packages-1",
    "href": "BDC334/Lab-R_RStudio.html#installing-packages-1",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "R comes with a number of built-in functions and datasets, but one of the main strengths of R as an open-source project is its package system. Packages add additional functions and data. Frequently if you want to do something in R, and it is not available by default, there is a good chance that there is a package that will fulfill your needs.\nTo install a package, use the install.packages() function. Think of this as buying a recipe book from the store, bringing it home, and putting it on your shelf.\n\ninstall.packages(\"ggplot2\")\n\nOnce a package is installed, it must be loaded into your current R session before being used. Think of this as taking the book off of the shelf and opening it up to read.\n\nlibrary(ggplot2)\n\nOnce you close R, all the packages are closed and put back on the imaginary shelf. The next time you open R, you do not have to install the package again, but you do have to load any packages you intend to use by invoking library()."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#sec-help",
    "href": "BDC334/Lab-R_RStudio.html#sec-help",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "In using R as a calculator, we have seen a number of functions: sqrt(), exp(), log() and sin(). To get documentation about a function in R, simply put a question mark in front of the function name and RStudio will display the documentation, for example:\n\n?log\n?sin\n?paste\n?lm\n\nFrequently one of the most difficult things to do when learning R is asking for help. First, you need to decide to ask for help, then you need to know how to ask for help. Your very first line of defense should be to Google your error message or a short description of your issue. (The ability to solve problems using this method is quickly becoming an extremely valuable skill.) If that fails, and it eventually will, you should ask for help. There are a number of things you should include when emailing an instructor, or posting to a help website such as Stack Exchange.\n\nDescribe what you expect the code to do.\nState the end goal you are trying to achieve. (Sometimes what you expect the code to do, is not what you want to actually do.)\nProvide the full text of any errors you have received.\nProvide enough code to recreate the error. Often for the purpose of this course, you could simply email your entire .R, .Rmd, .qmd file.\nSometimes it is also helpful to include a screenshot of your entire RStudio window when the error occurs.\n\nIf you follow these steps, you will get your issue resolved much quicker, and possibly learn more in the process. Do not be discouraged by running into errors and difficulties when learning R. (Or any technical skill.) It is simply part of the learning process."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#sec-packages",
    "href": "BDC334/Lab-R_RStudio.html#sec-packages",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "The most common functions used in R are contained within the base package; this makes R useful ‘out of the box.’ However, there is extensive additional functionality that is being expanded all the time through the use of packages. Packages are simply collections of code called functions that automate complex mathematical or statistical tasks. One of the most useful features of R is that users are continuously developing new packages and making them available for free. You can find a comprehensive list of available packages on the CRAN website. There are currently (2025-07-27) 22499 packages available for R!\nIf the thought of searching for and finding R packages is daunting, a good place to start is the R Task Views page. This page curates collections of packages for general tasks you might encounter, such as Experimental Design, Meta-Analysis, or Multivariate Analysis. Go and have a look for yourself, you might be surprised to find a good explanation of what you need.\nLet’s install our first package. After clicking ‘Tools’/‘Install Packages’, type in the package name tidyverse in the ‘Packages’ text box (note that it is case sensitive) and select the ‘Install’ button. The Console will run the code needed to install the package, and then provide some commentary on the installation of the package and any of its dependencies (i.e., other R packages needed to run the required package).\nThe installation process makes sure that the functions within the packages contained within the tidyverse are now available on your computer, but to avoid potential conflicts in the names of functions, it will not load these automatically. To make R ‘know’ about these functions in a particular session, you need either to load the package via ticking the checkbox for that package in the Packages tab, or execute:\n\nlibrary(tidyverse)\n\nTo prepare ourselves for the week ahead, let us also install the following packages. Here I demonstate the command line approach to achieve the same thing that can be done via the menu:\n\ninstall.packages(rmarkdown)\ninstall.packages(tidyverse)\n\nSince we will develop the habit of doing all of our analyses from R scripts, it is best practice to simply list all of the libraries to be loaded right at the start of your script. Comments may be used to remind your future-self (to quote Hadley Wickham) what those packages are for.\nOnce you close R, all the packages are closed and put back on the imaginary shelf. The next time you open R, you do not have to install the package again, but you do have to load any packages you intend to use by invoking library()."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#introducing-scripts",
    "href": "BDC334/Lab-R_RStudio.html#introducing-scripts",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "Below you’ll find a concise guide to writing your first R scripts. Each section combines exposition, runnable code snippets and brief exercises so you can test your understanding as you go.\nRemember the difference between scripts (in the Source Editor) and the ad-hoc functions run in the Console… When you open RStudio (or another editor), you see two ways to work:\n\n\nConsole: You type commands interactively. Results appear immediately. Useful for ad-hoc calculations.\n\nScript: A text file (often with .R extension, but .rmd and .qmd are also very common) where you assemble commands in order. You run chunks or the entire file. Scripts keep analyses reproducible.\n\nBegin a new script file. Save it as first_script.R within your Rproject. From now on, write code there, then press “Run” (or Ctrl + Enter) to execute.\nIn our new script, we will explore the properties of four common ways to handle data within R: vectors, matrices, arrays, and dataframes.\n\nA vector holds elements of a single type:\n\n# Body lengths (mm) measured on three frogs\nlengths &lt;- c(34.5, 29.8, 31.2)\nclass(lengths)        # \"numeric\"\n\n[1] \"numeric\"\n\n\nHere c() “combines” values into a vector. The name lengths now refers to that object in memory.\nWe can aply arithmetic to the vectors, or, for more complex statistical calculations, we can apply some built-in statistical functions:\n\nlengths * 0.001      # convert mm to meters\n\n[1] 0.0345 0.0298 0.0312\n\nmean(lengths)        # average length\n\n[1] 31.83333\n\nsd(lengths)          # standard deviation\n\n[1] 2.413158\n\n\nSince lengths is numeric, those operations apply to each element (vectorisation in action). No loop needed.\n\n\n\n\n\n\nExercise 3\n\n\n\nIn your script, create a numeric vector masses with values 1.2, 0.9, 1.5 (grams). Compute its mean and standard deviation.\n\n\n\nMatrices extend vectors by adding rows and columns. Every element shares the same type:\n\n# Suppose you measured length (mm) and mass (g) for three frogs\nmat &lt;- matrix(c(34.5, 29.8, 31.2, 1.2, 0.9, 1.5),\n              nrow = 3, byrow = FALSE)\ncolnames(mat) &lt;- c(\"Length_mm\", \"Mass_g\")\nrownames(mat) &lt;- paste0(\"Frog\", 1:3)\n\nYou’ll see:\n\nmat\n\n      Length_mm Mass_g\nFrog1      34.5    1.2\nFrog2      29.8    0.9\nFrog3      31.2    1.5\n\n\nTo compute column means:\n\ncolMeans(mat)\n\nLength_mm    Mass_g \n 31.83333   1.20000 \n\n\nRow sums:\n\nrowSums(mat)\n\nFrog1 Frog2 Frog3 \n 35.7  30.7  32.7 \n\n\nThe same may be achieved with the apply() function. Here, the argument MARGIN = 1 calculates along the first margin, which in R is always the rows. MARGIN = 2 applies the function to the columns:\n\n# Calculate row means\napply(mat, MARGIN = 1, FUN = mean)\n\nFrog1 Frog2 Frog3 \n17.85 15.35 16.35 \n\n# Calculate column means\napply(mat, MARGIN = 2, FUN = mean)\n\nLength_mm    Mass_g \n 31.83333   1.20000 \n\n\nAbove, we conveniently calculate the statistics all at once across rows and columns. But we can be more granular and access specific rows and columns individually. To do this we use the [] notation.\n\n# Access the rows\nmat[1, ] # extract to first row\n\nLength_mm    Mass_g \n     34.5       1.2 \n\nmean(mat[3, ]) # apply the mean function to the third row\n\n[1] 16.35\n\n# Access the columns\nmat[, 2]\n\nFrog1 Frog2 Frog3 \n  1.2   0.9   1.5 \n\nmean(mat[, 2])\n\n[1] 1.2\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nUsing the example with mat above, extract the element in the second row and the second column. In English, how would you describe the use of the [] notation?\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nBuild a 2×4 matrix env with temperature (°C) in column 1–2 and pH in column 3–4, for two sites. Then compute rowMeans(env).\n\n\n\nAn array generalises a matrix to three (or more) dimensions:\n\n# Create a 2×2×2 array of counts: species × site × time\ncounts &lt;- array(1:8, dim = c(2,2,2),\n                dimnames = list(\n                  Species = c(\"SpA\",\"SpB\"),\n                  Site = c(\"A\",\"B\"),\n                  Time = c(\"T1\",\"T2\")))\ncounts\n\n, , Time = T1\n\n       Site\nSpecies A B\n    SpA 1 3\n    SpB 2 4\n\n, , Time = T2\n\n       Site\nSpecies A B\n    SpA 5 7\n    SpB 6 8\n\n\nArrays prove handy when you track multiple variables across both space and time.\n\n\n\n\n\n\nExercise 6\n\n\n\nDefine a 3×3×2 array representing chlorophyll (µg L⁻¹) at three depths and three stations across two months. Use any numeric values. Use dimnames() to label dimensions.\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nHow does the [] notation work with arrays? Demonstrate your understanding on your array with chlorophyll data.\n\n\n\nA data frame is like a spreadsheet: each column can be a different type:\n\nobs &lt;- data.frame(\n  SampleID = 1:4,\n  Species = c(\"Ant\",\"Bee\",\"Fly\",\"Wasp\"),\n  Wings = c(2, 2, 2, 2),\n  Mass_mg = c(2.3, 1.8, 0.5, 2.0),\n  stringsAsFactors = FALSE\n)\nstr(obs)\n\n'data.frame':   4 obs. of  4 variables:\n $ SampleID: int  1 2 3 4\n $ Species : chr  \"Ant\" \"Bee\" \"Fly\" \"Wasp\"\n $ Wings   : num  2 2 2 2\n $ Mass_mg : num  2.3 1.8 0.5 2\n\n\nYou can extract the Mass_mg vector:\n\nmean(obs$Mass_mg)\n\n[1] 1.65\n\n\nOr select rows by condition:\n\nobs[obs$Mass_mg &gt; 1, ]\n\n  SampleID Species Wings Mass_mg\n1        1     Ant     2     2.3\n2        2     Bee     2     1.8\n4        4    Wasp     2     2.0\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\nConstruct a data frame plants with columns Plot (A, B, C), SpeciesRichness (integer), and Biomass_g. Compute the overall mean biomass.\n\n\n\nRather than looping over each element, you apply functions to entire vectors:\n\nx &lt;- 1:10\nsqrt(x)         # returns vector of square roots\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nlog(x + 1)      # adds 1 to each element, then takes log\n\n [1] 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101 2.0794415\n [8] 2.1972246 2.3025851 2.3978953\n\n\nUnderneath, R’s internals run optimised C code. You write less and your script runs faster.\nContrast with an explicit loop:\n\nout &lt;- numeric(length(x))\nfor(i in seq_along(x)) {\n  out[i] &lt;- sqrt(x[i])\n}\n\nVectorised code tends to be clearer—and, often, shorter.\n\n\nIndicate purpose, date, author:\n\n# first_script.R\n# A simple demonstration of base R constructs\n# AJ, 2025-07-27\n\n\nUse blank lines or commented titles:\n\n# --- Vectors and summary stats ---\n\nYou may use Ctrl+Shift+R (Cmd+Shift+R on a Mac) to create a dialogue box where you may type your section headings.\n\nWrite outputs to disk when needed:\n\nwrite.csv(obs, \"observations.csv\", row.names = FALSE)\n\nLater, you’ll learn how to read such files back (read.csv()).\n\n\n\n\n\n\nFurther Practice\n\n\n\n\nSimulate 50 random normal body temperatures (mean = 37, sd = 0.5) and compute their summary statistics.\nCreate a matrix of two traits for five individuals; then extract the submatrix for individuals 2–4.\nBuild a data frame of bird counts per site and date; then find the date with highest total count.\n\nPlay around with the script. If you execute each block in your script and tweak the parameters, you’ll become more familiar with writing and organising code, and eventually it will become second nature. When you are comfortable, we’ll introduce file input/output and slightly more advanced data manipulation."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#the-assignment-operator",
    "href": "BDC334/Lab-R_RStudio.html#the-assignment-operator",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "We can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\n[1] 9\n\n\nTo type the assignment operator (&lt;-) press the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\n[1] FALSE\n\n\nN eval=FALSEote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself. :::\n\n\n\n\n\n\nExercise 1\n\n\n\nWhat are the values after each hashed statement in the following? Use the RStudio Console to determine these values:\n\nmass &lt;- 48\nage &lt;- 78\nmass &lt;- mass * 2.0 # mass? \nage &lt;- age - 17 # age? m\nmass_index &lt;- mass / age # mass_index?\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nUse R to calculate some simple mathematical expressions. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y. Display z in the console."
  },
  {
    "objectID": "BDC334/Lab-R_RStudio.html#more-console-functions",
    "href": "BDC334/Lab-R_RStudio.html#more-console-functions",
    "title": "Lab: R and RStudio",
    "section": "",
    "text": "RStudio supports the automatic completion of code using the Tab key. For example, type the three letters mas and then the Tab key. What happens?\nThe code completion feature also provides brief in-line help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl + Up to review the list (Cmd + Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane.\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source Editor using the buttons at the top-right or by double-clicking the title bar)"
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#getting-started",
    "href": "BDC334/Lab-02a-r_rstudio.html#getting-started",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Getting Started",
    "text": "Getting Started\nR is a software environment for statistical computation and graphics. It is free and open-source. It is a programming (or “scripting” or “coding”) language used by research statisticians, academics and their students, and by data “scientists” across a wide array or industries and organisations. To use it, you will need to install two pieces of software, both of which can be downloaded for free:\n\n\nR, which is the actual software that does the computations and graphics.\n\nChose your operating system, and select the most recent version, 4.5.1.\n\n\n\nRStudio, the Integrated Development Environment (IDE), which is what R runs within.\n\nYou must have R installed to use RStudio; RStudio by itself cannot do anything, like a car without it engine.\n\n\n\n\n\nThe R programme outside of RStudio.\n\n\n\nRStudio with R inside of it.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#why-r",
    "href": "BDC334/Lab-02a-r_rstudio.html#why-r",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Why R?",
    "text": "Why R?\nR is becoming increasingly popular. It is used, as I mentioned earlier, across academia as well as in industry. In particular, it has become especially popular among biologists and ecologists, since much of the analysis we want to carry out is readily executable within R. This is facilitated by the many add-on packages that ecologists have developed over the years.\nAdditionally, R is extremely powerful for the creation of graphics and figures, enabling us to visualise all of the data we have analysed. These graphical outputs are essential for effectively communicating our findings in publications.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#what-youll-find-here",
    "href": "BDC334/Lab-02a-r_rstudio.html#what-youll-find-here",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\nIn the following section, I will walk you through the basic use and operation of R. We shall look at several things, such as how the user interface works—in other words, how the RStudio IDE is organised. I will also show you how to set up a new Rproject, which is quite an essential step in keeping your work organised, especially as your analyses grow more complex over time.\nWe will cover how to create scripts, and how to save those scripts so that you can run them again in the future. This means that not only will you be able to reproduce your own analyses, but also modify or expand them as required. I will demonstrate how to create a few basic figures, so you will become familiar with visualising your data.\nQuite importantly, for your assignments and research over the coming weeks, you will use R as a system within which you can both write your scripts for data analysis, and also produce the documents required for communicating your findings. This includes output that is suitable for sharing with colleagues, whether in the form of reports, presentations, or publications.\nThe ability to integrate your code and your written explanation—what is often referred to as ‘reproducible research’—is a particular strength of R and RStudio. You will find that learning these skills is not only essential for your studies here, but also valuable for future scientific work.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#learning-r",
    "href": "BDC334/Lab-02a-r_rstudio.html#learning-r",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Learning R",
    "text": "Learning R\nLearning R is like learning another language—a spoken language like French or Finnish. R is also a language, and it requires a huge amount of practice and skill to achieve fluency in it. The most important thing when you’re learning R for the first time is to be patient with yourself. Many of the steps will require repeated iterations, working through examples, and, most crucially, solving your own problems. This process will help you become more familiar with R.\nYou are not expected to become fluent in it straight away, but the intention during this third-year course is that you will no longer feel apprehensive about using R. The key aspect, therefore, is to learn patience and to learn how to help yourself. That really is the only way to learn R: to have your own problem, which you are able to solve using some of the skills that I will teach you.\nRStudio has a large number of useful keyboard shortcuts. A list of these can be found using a keyboard shortcut – the keyboard shortcut to rule them all:\n\nOn Windows: Alt + Shift + K\n\nOn Mac: Option + Shift + K\n\n\nThe RStudio team has developed a number of “cheatsheets” for working with both R and RStudio. This particular cheatsheet for “Base” R will summarise many of the concepts in this document. (“Base” R is a name used to differentiate the practice of using built-in R functions, as opposed to using functions from outside packages, in particular, those from the tidyverse. More on this later.)",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#a-few-words-on-style",
    "href": "BDC334/Lab-02a-r_rstudio.html#a-few-words-on-style",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "A Few Words on Style",
    "text": "A Few Words on Style\nWhen writing scripts, it is good practice to follow a style guide. For example, where do spaces go? Do I use tabs or spaces? Do I orefer underscores or CamelCase when naming variables? No style guide is “correct,” but it helps to be aware of the general approaches people take. For me, the most important aspect is that you are consistent within your own code. This is something that we will pay a great deal of attention to when we mark your assignments.\n\n\nHadley Wickham Style Guide from Advanced R\n\nGoogle Style Guide",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#rstudio",
    "href": "BDC334/Lab-02a-r_rstudio.html#rstudio",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "RStudio",
    "text": "RStudio\nIn this section, we will have a look at the RStudio IDE. The IDE is where we are going to spend most of our time when we interact with R. You can think of the IDE as the body of a car: the seats, the steering wheel, all the luxuries, bells and whistles—essentially, all those various things that make up the experience of using the vehicle.\nR itself, on the other hand, is the engine. So, just as in a real car, if you take the engine out, all the seats, the bells and whistles, the steering wheel, and all the safety features mean absolutely nothing without that engine to power it. Similarly, with RStudio, you do need the R engine to operate RStudio. The IDE alone cannot run your code; it’s merely the interface and the facilitator, but the actual computations require the presence of R.\nGeneral Settings\nBefore we start using RStudio, let’s first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’) and select the tick boxes as shown in the figure below.\n\n\nRStudio preferences\n\nCustomising Appearance\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g., Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code.\n\n\nAppearance settings\n\nConfiguring Panes\nYou cannot rearrange panes (see below) in RStudio by dragging them, but you can alter their position via the Pane Layout tab in the ‘Tools’/‘Global Options’ (‘RStudio’/‘Preferences’ – for Mac). You may arrange the panes as you would prefer; however, we recommend that during the duration of this workshop you leave them in the default layout.\n\n\nRearranging the panes\n\nThe R Project\nA very nifty way of managing your workflow in RStudio is through the built-in functionality of the R project. We do not need to install any packages or change any settings to use these. Creating a new project is a very simple task, as well. This will prevent a lot of issues by ensuring we are doing things by the same standard. Better yet, an R project integrates seamlessly into version control software (e.g., GitHub) and allows for instant world class collaboration on any research project. We will cover the concepts and benefits of an R project more as we move through the course.\n\nIn the RStudio menu, find ‘File’ and then ‘New Project’.\nSelect ‘New Directory’ and then ‘New Project’.\nName the project ‘Intro_R_Workshop’ and save it in a location of your choice (make sure you understand your computer’s file system and where you are saving files).\nClick ‘Create Project’.\nCopy the data archive Archive.zip to the project directory and unzip it there.\nRename the unzipped folder to data.\n\nYour RStudio should now look like this:\n\n\nRStudio project\n\nNote the key points:\n\n❶ The project name is displayed in the top right corner of the RStudio window.\n❷ The name of the project workspace file is displayed in the Files pane.\n❸ The name of the data folder is displayed in the Files pane.\n❹ The project name is displayed in the title bar of the RStudio window (corresponding to the physical location on your computer).\n\n\n\n\n\n\n\nCopying Code from RStudio\n\n\n\nHere you saw RStudio execute the R code needed to install (using install.packages()) and load (using library()) the package, so if you want to include these in one of your programs, just copy the text it executes. Note that you need only install the current version of a package once, but it needs to be loaded at the beginning of each R session.\n\n\nThe Panes of RStudio\nRStudio has four main panes, each occupying a quadrant of your screen: Source Editor, Console, Workspace Browser (and History), and Plots (and Files, Packages, Help). These can also be adjusted under the ‘Preferences’ menu. Note that there might be subtle differences between RStudio installations on different operating systems. We will discuss each of the panes in turn.\nSource (Script) Editor\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it isn’t already)\nNow double click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (on Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon or Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18 and re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now. Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nThe # symbol\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.\n\n\nConsole\nThis is where you can type code that executes immediately.\nThe R console is an integral part of RStudio. In fact, the console is the main component of the software that is visible within R, the programme. In other words, when we use R outside of its integrated development, the console is essentially what we interact with.\nAlthough we can run our entire analysis within the console, we seldom do so. For that purpose, we use the Source Editor because our analysis is often comprised of many tens, hundreds, or even thousands of lines of executable code. Typically, in our day-to-day interaction with the R console, we use it to execute small programmes, each of which is usually no longer than about one line at a time. Alternatively, we might use the console to quickly and interactively check various objects stored within the R environment, or to perform small calculations on the fly, and so forth.\nThus, the console is typically reserved for one-off calculations—tasks that we do not need to retain for our future analysis at a later stage.\nWe will return to the Console later in Section 1.7 when we start practicing running code.\nEnvironment and History Panes\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values and functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\ncharacter(0)\n\n\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?\nFiles, Plots, Packages, Help, and Viewer Panes\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Help tab is particularly important as it allows you to search the R documentation for help and is where the help appears when you ask for it from the Console. Methods of getting help from the Console include will be discussed later in Section 1.9. The Packages tab shows you the packages that are installed and those that can be installed (see Section 1.11).\nThe Plot tab is where our figures will typically appear. Here’s a quick taste of what is to come–it shows already some of the things I mentioned above, including the use of the Console, loading packages, and so on. To reproduce Figure Figure 1 in the Plot tab, simply copy and paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.05)\ny &lt;- 2 * sin(2 * pi * (x - 1/4))\nplot(x, y, col = \"red\")\n\n\n\n\n\n\nFigure 1: A plot assembled with the base R plot fuction.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#sec-basic_calcs",
    "href": "BDC334/Lab-02a-r_rstudio.html#sec-basic_calcs",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Basic Calculations",
    "text": "Basic Calculations\n\n\n\n\n\n\nType it in!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really shouldn’t. The first reason is that you might unwittingly copy invisible PDF formatting codes into R, which will make your script fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your errors. This is an invaluable way of learning and taking shortcuts now will only hurt you in the long run.\n\n\nTo get started, we’ll use R like a simple calculator. You can type the command directly into the R Console and press Enter, and it will execute and display the result. Alternatively, you may type the command in the Source Editor. Making sure that your cursor is anywhere on the line that you want to execute, press Control + Enter if you are on a Windows computer, or Command + Enter if you are using a Macintosh. In both cases, the command you have typed in the Source Editor will be executed in the Console, and the output will be displayed there.\nAddition, Subtraction, Multiplication, and Division\nIn the R Console, start your calculation at the command prompt, &gt;, like this:\n&gt; 3 + 2\nBasic arithmetic is easy:\n\n\nMath\nR\nResult\n\n\n\n\\(3 + 2\\)\n3 + 2\n5\n\n\n\\(3 - 2\\)\n3 - 2\n1\n\n\n\\(3 \\cdot2\\)\n3 * 2\n6\n\n\n\\(3 / 2\\)\n3 / 2\n1.5\n\n\n\nNote that each line of the output for every calculation (e.g., 3 + 2) is indicated by [...], as we see here:\n\n3 + 2\n\n[1] 5\n\n\nAbove, the [1] indicates that the answer is a vector of one element.\nSimilarly, the commands for various basic mathematical operations are in the following tables:\nExponents\n\n\nMath\nR\nResult\n\n\n\n\\(3^2\\)\n3 ^ 2\n9\n\n\n\\(2^{(-3)}\\)\n2 ^ (-3)\n0.125\n\n\n\\(100^{1/2}\\)\n100 ^ (1 / 2)\n10\n\n\n\\(\\sqrt{100}\\)\nsqrt(100)\n10\n\n\nMathematical Constants\n\n\nMath\nR\nResult\n\n\n\n\\(\\pi\\)\npi\n3.1415927\n\n\n\\(e\\)\nexp(1)\n2.7182818\n\n\nLogarithms\nNote that we will use \\(\\ln\\) and \\(\\log\\) interchangeably to mean the natural logarithm. There is no ln() in R, instead it uses log() to mean the natural logarithm.\n\n\nMath\nR\nResult\n\n\n\n\\(\\log(e)\\)\nlog(exp(1))\n1\n\n\n\\(\\log_{10}(1000)\\)\nlog10(1000)\n3\n\n\n\\(\\log_{2}(8)\\)\nlog2(8)\n3\n\n\n\\(\\log_{4}(16)\\)\nlog(16, base = 4)\n2\n\n\nTrigonometry\n\n\nMath\nR\nResult\n\n\n\n\\(\\sin(\\pi / 2)\\)\nsin(pi / 2)\n1\n\n\n\\(\\cos(0)\\)\ncos(0)\n1\n\n\nThe Assignment Operator\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\n[1] 9\n\n\nTo type the assignment operator (&lt;-) press the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\n[1] FALSE\n\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\n\n\n\n\n\n\nExercise 1\n\n\n\nWhat are the values after each hashed statement in the following? Use the RStudio Console to determine these values:\n\nmass &lt;- 48\nage &lt;- 78\nmass &lt;- mass * 2.0 # mass? \nage &lt;- age - 17 # age? m\nmass_index &lt;- mass / age # mass_index?\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nUse R to calculate some simple mathematical expressions. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y. Display z in the console.\n\n\nMore About the Console\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters mas and then the Tab key. What happens?\nThe code completion feature also provides brief in-line help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl + Up to review the list (Cmd + Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane.\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source Editor using the buttons at the top-right or by double-clicking the title bar)",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#the-assignment-operator",
    "href": "BDC334/Lab-02a-r_rstudio.html#the-assignment-operator",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "The Assignment Operator",
    "text": "The Assignment Operator\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\n[1] 9\n\n\nTo type the assignment operator (&lt;-) press the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd&lt;-2\nd &lt; -2\n\n[1] FALSE\n\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\n\n\n\n\n\n\nExercise 1\n\n\n\nWhat are the values after each hashed statement in the following? Use the RStudio Console to determine these values:\n\nmass &lt;- 48\nage &lt;- 78\nmass &lt;- mass * 2.0 # mass? \nage &lt;- age - 17 # age? m\nmass_index &lt;- mass / age # mass_index?\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nUse R to calculate some simple mathematical expressions. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y. Display z in the console.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#more-console-functions",
    "href": "BDC334/Lab-02a-r_rstudio.html#more-console-functions",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "More Console Functions",
    "text": "More Console Functions\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters mas and then the Tab key. What happens?\nThe code completion feature also provides brief in-line help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl + Up to review the list (Cmd + Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane.\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source Editor using the buttons at the top-right or by double-clicking the title bar)",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#sec-help",
    "href": "BDC334/Lab-02a-r_rstudio.html#sec-help",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Getting Help",
    "text": "Getting Help\n\nR’s Help System\nIn using R as a calculator, we have seen a number of functions: sqrt(), exp(), log() and sin(). To get documentation about a function in R, simply put a question mark in front of the function name and RStudio will display the documentation, for example:\n\n?log\n?sin\n?mean\n?cor\n\nThis summons a help page in the RStudio Help tab divided into sections:\n\n\nDescription – a brief sketch of the function’s purpose;\n\nUsage – the arguments you saw via args();\n\nArguments – each argument spelled out, its type, and role;\n\nDetails – algorithmic notes or warnings;\n\nValue – what the function returns;\n\nExamples – runnable code illustrating common patterns.\n\nIf you prefer to search by keyword (“correlation” or “standard deviation”), use:\n\nhelp.search(\"correlation\")\n\n# or\n??\"standard deviation\"\n\nFinally, to run the examples embedded in a help file:\n\nexample(cor)\n# not executed as the output is volumnous\n\nWhat if the R Help System is Not Enough?\nFrequently one of the most difficult things to do when learning R is asking for help. First, you need to decide to ask for help, then you need to know how to ask for help. Your very first line of defense should be to Google your error message or a short description of your issue. (The ability to solve problems using this method is quickly becoming an extremely valuable skill.) If that fails, and it eventually will, you should ask for help. There are a number of things you should include when emailing an instructor, or posting to a help website such as Stack Exchange.\n\nDescribe what you expect the code to do.\nState the end goal you are trying to achieve. (Sometimes what you expect the code to do, is not what you want to actually do.)\nProvide the full text of any errors you have received.\nProvide enough code to recreate the error. Often for the purpose of this course, you could simply email your entire .R, .Rmd, .qmd file.\nSometimes it is also helpful to include a screenshot of your entire RStudio window when the error occurs.\n\nIf you follow these steps, you will get your issue resolved much quicker, and possibly learn more in the process. Do not be discouraged by running into errors and difficulties when learning R. (Or any technical skill.) It is simply part of the learning process.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#sec-packages",
    "href": "BDC334/Lab-02a-r_rstudio.html#sec-packages",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Installing Packages",
    "text": "Installing Packages\nThe most common functions used in R are contained within the base package; this makes R useful ‘out of the box.’ However, there is extensive additional functionality that is being expanded all the time through the use of packages. Packages are simply collections of code called functions that automate complex mathematical or statistical tasks. One of the most useful features of R is that users are continuously developing new packages and making them available for free. You can find a comprehensive list of available packages on the CRAN website. There are currently (2025-07-29) 22511 packages available for R!\nIf the thought of searching for and finding R packages is daunting, a good place to start is the R Task Views page. This page curates collections of packages for general tasks you might encounter, such as Experimental Design, Meta-Analysis, or Multivariate Analysis. Go and have a look for yourself, you might be surprised to find a good explanation of what you need.\nLet’s install our first package. After clicking ‘Tools’/‘Install Packages’, type in the package name tidyverse in the ‘Packages’ text box (note that it is case sensitive) and select the ‘Install’ button. The Console will run the code needed to install the package, and then provide some commentary on the installation of the package and any of its dependencies (i.e., other R packages needed to run the required package).\nThe installation process makes sure that the functions within the packages contained within the tidyverse are now available on your computer, but to avoid potential conflicts in the names of functions, it will not load these automatically. To make R ‘know’ about these functions in a particular session, you need either to load the package via ticking the checkbox for that package in the Packages tab, or execute:\n\nlibrary(tidyverse)\n\nTo prepare ourselves for the week ahead, let us also install the following packages. Here I demonstate the command line approach to achieve the same thing that can be done via the menu:\n\ninstall.packages(rmarkdown)\ninstall.packages(tidyverse)\n\nSince we will develop the habit of doing all of our analyses from R scripts, it is best practice to simply list all of the libraries to be loaded right at the start of your script. Comments may be used to remind your future-self (to quote Hadley Wickham) what those packages are for.\nOnce you close R, all the packages are closed and put back on the imaginary shelf. The next time you open R, you do not have to install the package again, but you do have to load any packages you intend to use by invoking library().",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#introducing-scripts",
    "href": "BDC334/Lab-02a-r_rstudio.html#introducing-scripts",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Introducing Scripts",
    "text": "Introducing Scripts\nBelow you’ll find a concise guide to writing your first R scripts. Each section combines exposition, runnable code snippets and brief exercises so you can test your understanding as you go.\nRemember the difference between scripts (in the Source Editor) and the ad-hoc functions run in the Console… When you open RStudio (or another editor), you see two ways to work:\n\n\nConsole: You type commands interactively. Results appear immediately. Useful for ad-hoc calculations.\n\nScript: A text file (often with .R extension, but .rmd and .qmd are also very common) where you assemble commands in order. You run chunks or the entire file. Scripts keep analyses reproducible.\n\nIn the Source Editor, begin a new script file. Save it as first_script.R within your Rproject. From now on, write code there, then press “Run” (or Ctrl + Enter) to execute. (Of course you can do all of this in the Console too, but that would be silly as you’ll typically not be able to retrieve any of the work done as you work on complex calculations.)\nIn our new script, we will explore the properties of four common ways to handle data within R: vectors, matrices, arrays, and dataframes.\nVectors: One-Dimensional Data\nA vector holds elements of a single type:\n\n# Body lengths (mm) measured on three frogs\nlengths &lt;- c(34.5, 29.8, 31.2)\nclass(lengths)        # \"numeric\"\n\n[1] \"numeric\"\n\n\nHere c() “combines” values into a vector. The name lengths now refers to that object in memory.\nWe can aply arithmetic to the vectors, or, for more complex statistical calculations, we can apply some built-in statistical functions:\n\nlengths * 0.001      # convert mm to meters\n\n[1] 0.0345 0.0298 0.0312\n\nmean(lengths)        # average length\n\n[1] 31.83333\n\nsd(lengths)          # standard deviation\n\n[1] 2.413158\n\n\nSince lengths is numeric, those operations apply to each element (vectorisation in action). No loop needed.\n\n\n\n\n\n\nExercise 3\n\n\n\nIn your script, create a numeric vector masses with values 1.2, 0.9, 1.5 (grams). Compute its mean and standard deviation.\n\n\nMatrices: Two-Dimensional Tables\nMatrices extend vectors by adding rows and columns. Every element shares the same type:\n\n# Suppose you measured length (mm) and mass (g) for three frogs\nmat &lt;- matrix(c(34.5, 29.8, 31.2, 1.2, 0.9, 1.5),\n              nrow = 3, byrow = FALSE)\ncolnames(mat) &lt;- c(\"Length_mm\", \"Mass_g\")\nrownames(mat) &lt;- paste0(\"Frog\", 1:3)\n\nYou’ll see:\n\nmat\n\n      Length_mm Mass_g\nFrog1      34.5    1.2\nFrog2      29.8    0.9\nFrog3      31.2    1.5\n\n\nTo compute column means:\n\ncolMeans(mat)\n\nLength_mm    Mass_g \n 31.83333   1.20000 \n\n\nRow sums:\n\nrowSums(mat)\n\nFrog1 Frog2 Frog3 \n 35.7  30.7  32.7 \n\n\nThe same may be achieved with the apply() function. Here, the argument MARGIN = 1 calculates along the first margin, which in R is always the rows. MARGIN = 2 applies the function to the columns:\n\n# Calculate row means\napply(mat, MARGIN = 1, FUN = mean)\n\nFrog1 Frog2 Frog3 \n17.85 15.35 16.35 \n\n# Calculate column means\napply(mat, MARGIN = 2, FUN = mean)\n\nLength_mm    Mass_g \n 31.83333   1.20000 \n\n\nAbove, we conveniently calculate the statistics all at once across rows and columns. But we can be more granular and access specific rows and columns individually. To do this we use the [] notation.\n\n# Access the rows\nmat[1, ] # extract to first row\n\nLength_mm    Mass_g \n     34.5       1.2 \n\nmean(mat[3, ]) # apply the mean function to the third row\n\n[1] 16.35\n\n# Access the columns\nmat[, 2]\n\nFrog1 Frog2 Frog3 \n  1.2   0.9   1.5 \n\nmean(mat[, 2])\n\n[1] 1.2\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nUsing the example with mat above, extract the element in the second row and the second column. In English, how would you describe the use of the [] notation?\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nBuild a 2×4 matrix env with temperature (°C) in column 1–2 and pH in column 3–4, for two sites. Then compute rowMeans(env).\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nIn your first_script.R, inspect the structure of two additional built-in functions—choose from median(), var(), or quantile(). Write down:\n\n\nThe name of each argument.\nIts default value (if any).\nNote one practical scenario (from the Examples section) where you might apply it in your field of study.\nNote how missing values are handled by default, and which argument controls that behavior.\n\n\nWrite a short code block (in your script) that applies your favourite function to a numeric vector of your choice, intentionally including at least one NA. Return the output of the calculation.\n\n\n\nArrays: Higher-Dimensional Data\nAn array generalises a matrix to three (or more) dimensions:\n\n# Create a 2×2×2 array of counts: species × site × time\ncounts &lt;- array(1:8, dim = c(2,2,2),\n                dimnames = list(\n                  Species = c(\"SpA\",\"SpB\"),\n                  Site = c(\"A\",\"B\"),\n                  Time = c(\"T1\",\"T2\")))\ncounts\n\n, , Time = T1\n\n       Site\nSpecies A B\n    SpA 1 3\n    SpB 2 4\n\n, , Time = T2\n\n       Site\nSpecies A B\n    SpA 5 7\n    SpB 6 8\n\n\nArrays prove handy when you track multiple variables across both space and time.\n\n\n\n\n\n\nExercise 7\n\n\n\nDefine a 3×3×2 array representing chlorophyll (µg L⁻¹) at three depths and three stations across two months. Use any numeric values. Use dimnames() to label dimensions.\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\nHow does the [] notation work with arrays? Demonstrate your understanding on your array with chlorophyll data.\n\n\nData Frames: Tabular, Mixed-Type Data\nA data frame is like a spreadsheet: each column can be a different type:\n\nobs &lt;- data.frame(\n  SampleID = 1:4,\n  Species = c(\"Ant\",\"Bee\",\"Fly\",\"Wasp\"),\n  Wings = c(2, 2, 2, 2),\n  Mass_mg = c(2.3, 1.8, 0.5, 2.0),\n  stringsAsFactors = FALSE\n)\nstr(obs)\n\n'data.frame':   4 obs. of  4 variables:\n $ SampleID: int  1 2 3 4\n $ Species : chr  \"Ant\" \"Bee\" \"Fly\" \"Wasp\"\n $ Wings   : num  2 2 2 2\n $ Mass_mg : num  2.3 1.8 0.5 2\n\n\nYou can extract the Mass_mg vector:\n\nmean(obs$Mass_mg)\n\n[1] 1.65\n\n\nOr select rows by condition:\n\nobs[obs$Mass_mg &gt; 1, ]\n\n  SampleID Species Wings Mass_mg\n1        1     Ant     2     2.3\n2        2     Bee     2     1.8\n4        4    Wasp     2     2.0\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\nConstruct a data frame plants with columns Plot (A, B, C), SpeciesRichness (integer), and Biomass_g. Compute the overall mean biomass.\n\n\nPrinciples of Vectorisation\nRather than looping over each element, you apply functions to entire vectors:\n\nx &lt;- 1:10\nsqrt(x)         # returns vector of square roots\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nlog(x + 1)      # adds 1 to each element, then takes log\n\n [1] 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101 2.0794415\n [8] 2.1972246 2.3025851 2.3978953\n\n\nUnderneath, R’s internals run optimised C code. You write less and your script runs faster.\nContrast with an explicit loop:\n\nout &lt;- numeric(length(x))\nfor(i in seq_along(x)) {\n  out[i] &lt;- sqrt(x[i])\n}\n\nVectorised code tends to be clearer—and, often, shorter.\nOrganising Your Script\nHeader comments\nIndicate purpose, date, author:\n\n# first_script.R\n# A simple demonstration of base R constructs\n# AJ, 2025-07-27\n\nLogical blocks\nUse blank lines or commented titles:\n\n# --- Vectors and summary stats ---\n\nYou may use Ctrl+Shift+R (Cmd+Shift+R on a Mac) to create a dialogue box where you may type your section headings.\nSave results\nWrite outputs to disk when needed:\n\nwrite.csv(obs, \"observations.csv\", row.names = FALSE)\n\nLater, you’ll learn how to read such files back (read.csv()).\n\n\n\n\n\n\nFurther Practice\n\n\n\n\nSimulate 50 random normal body temperatures (mean = 37, sd = 0.5) and compute their summary statistics.\nCreate a matrix of two traits for five individuals; then extract the submatrix for individuals 2–4.\nBuild a data frame of bird counts per site and date; then find the date with highest total count.\n\nPlay around with the script. If you execute each block in your script and tweak the parameters, you’ll become more familiar with writing and organising code, and eventually it will become second nature. When you are comfortable, we’ll introduce file input/output and slightly more advanced data manipulation.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#more-about-the-console",
    "href": "BDC334/Lab-02a-r_rstudio.html#more-about-the-console",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "More About the Console",
    "text": "More About the Console\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters mas and then the Tab key. What happens?\nThe code completion feature also provides brief in-line help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl + Up to review the list (Cmd + Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane.\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source Editor using the buttons at the top-right or by double-clicking the title bar)",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#built-in-r-functions",
    "href": "BDC334/Lab-02a-r_rstudio.html#built-in-r-functions",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Built-in R Functions",
    "text": "Built-in R Functions\nAbove we have seen a few basic math functions, such as sqrt(), log(), and sin(). There are many (1000s) others, including commonly-used ones like mean(), sd(), cor(), and so on.\nA conventional R function obeys a consistent anatomy. You invoke it by name, supply arguments (some with defaults), and receive a return answer (anything fromm a vector of length 1 or a complex summary of a model fit). Under the hood, the formal definition comprises a usage line (its signature or name), a set of arguments (with default values), and, where visible, a body (the code that executes). Let’s unpack this with two ubiquitous functions: mean() and cor().\nThe function’s name is always immediately followed by a set of matching brackets inside of which are the arguments. For example:\n\nmean(x, trim = 0, na.rm = FALSE, ...)\ncor(x, y = NULL, use = \"everything\", method = c(\"pearson\", \"kendall\", \"spearman\"))\n\nIf you know the name of a function but not its arguments, you can apply the args() function to a function’s name:\n\nargs(mean)\n\nfunction (x, ...) \nNULL\n\nargs(cor)\n\nfunction (x, y = NULL, use = \"everything\", method = c(\"pearson\", \n    \"kendall\", \"spearman\")) \nNULL\n\n\nThe first argument in both, x, is not followed by a = that assigns some value to it. In such cases that argument has no default value and the user must supply something. Here, x would be a vector in the case of mean(x, ...) or a vector, matrix, or dataframe in the case of cor(x, ...). To run the functions, the user must supply that input, but as far as the other arguments are concerned, the function should run fine with the default values (but you need to double check that they are appropriate). Sometimes we will also see ... inside of the function call, which means that other arguments may be provided to satisfy some deeper need of internal functions and so on.\nThat’s nice, but how do I know what the arguments do, and, if I encounter a new function that I don’t know, how do I learn more about it? Use R’s very powerful help system.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#introduction-to-r-markdown-in-quarto",
    "href": "BDC334/Lab-02a-r_rstudio.html#introduction-to-r-markdown-in-quarto",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Introduction to R Markdown in Quarto",
    "text": "Introduction to R Markdown in Quarto\nWhat is Markdown?\nMarkdown is a lightweight markup language that is designed for formatting text in a simple and readable way. It allows you to create formatted documents using a plain text editor, using symbols like # for headings, * or - for bullet lists, and other intuitive shortcuts. This makes it much easier to write well-structured documents compared to traditional word processors, especially for scientific and academic writing.\nR Markdown: Integrating Code and Text\nR Markdown is an extension of Markdown that allows you to embed code—such as R, Python, or Julia—directly into your text. This means you can integrate both your narrative (your explanations, interpretation, and discussion) and your code (data analysis, statistics, plots) into a single document. When this document is rendered, both the text and the outputs of your code (including tables and figures) are combined together into a final report.\nR Markdown is very useful in all areas of research because it allows you to:\n\nPrepare transparent, reproducible reports\nEmbed statistically rigorous analyses directly alongside your commentary\nSeamlessly incorporate tables and graphics generated by R\n\nWrite entre books\nEven this website, Tangled Bank, was written entirely in R Markdown (in Quarto–see below)\nUsing R Markdown in Quarto\nQuarto is a modern open-source scientific and technical publishing system. It is essentially the successor to the older R Markdown system, and supports a range of programming languages in addition to R.\nImportant Features\n\nWrite content in a human-readable format using Markdown\nInclude code chunks using triple backticks, specifying the language—e.g. ```{r} for R\nSupports citations and bibliographies\nAutomatically generates formatted outputs such as PDF, HTML, and Word\nDocument Structure\nA basic R Markdown (as implemented in Quarto) document has three main elements:\n\n\nYAML Header: At the very top, enclosed by three dashes ---, specifying basic document metadata (such as title, author, output format)\n\nNarrative Text: Written in Markdown, supporting headings, lists, emphasis, tables, and more\n\nCode Chunks: Segments of code embedded in the narrative and enclosed using triple backticks with curly braces indicating the language\nExample Skeleton\n---\ntitle: \"R Markdown and Quarto Demo\"\nauthor: \"AJ Smit\"\ndate: \"29/07/2025\"\nformat: \n  html:\n    code-fold: true\n---\n\n## Introduction\n\nThis study is about air quality.\n\n## Methods\n\n@fig-airquality further explores the impact of temperature on ozone level.\n\n```{r}\n#| label: fig-airquality\n#| fig-cap: \"Temperature and ozone level.\"\n#| fig-width: 6\n#| fig-height: 4\n#| fig-cap-location: bottom\n#| warning: false\n\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n```\n\n## Results\n\nThe results show that air has quality.\nWhen you render this file, you’ll see the following output (the HTML output shown):\n\n\nThe HTML output of the above Quarto document.\n\nSupported Output Formats\nBy changing the format option in the YAML header, you can export your report to different types including:\n\nPDF documents (provided you have LaTeX installed)\nHTML web pages\nWord (.docx) documents\n\nFor example:\nformat: pdf\nor\nformat: docx\nRendering the Document\n\nIn RStudio or VS Code, you can click the ‘Render’ or ‘Knit’ button to produce your desired output.\nYou can also use the command line: $ quarto render my_file.qmd$\n\nMore Detailed information\nPlease refer to the Markdown Basics page on the Quarto website for much more information about markdown.\nQuarto is extremely powerful and you’ll want to explore the Markdown Basics page thoroughly in your own time. Of immediate interest to most of you will be the page on Citations, or the other information under “Scholarly Writing” that you may access in the menu on the left of the page.\nYou will also have to explore the various YAML options, YAML meaning ‘Yet Another Markup Language’. You can specify these at the beginning of your document in the YAML block at the top, which allows you to define various options for how your HTML, Word document, PDF, or any of the many formats that Quarto can produce, will look. Please consult the reference section on the Quarto website for the various YAML options available (e.g., here the HTML YAML options), so that you can set up your document in the way you would like it to appear.\nOne thing to note about YAML is that it is incredibly particular about the way in which the various levels of indentation must appear in order for the YAML to be read correctly by your Quarto system and for the code to execute correctly. So, this is an excellent opportunity for you to pay attention to detail and ensure that your YAML is precisely structured according to the expectations of the example document provided.\nWhy Use R Markdown?\n\nEnsures your analyses are reproducible\n\nAllows collaboration between researchers\nCombines field notes, data analysis, results, and interpretation in one place",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/Lab-02a-r_rstudio.html#results",
    "href": "BDC334/Lab-02a-r_rstudio.html#results",
    "title": "Lab 2a. Introduction R & RStudio",
    "section": "Results",
    "text": "Results\nHere is the output of our analysis…\n\n#### Supported Output Formats\n\nBy changing the `format` option in the YAML header, you can export your report to different types including:\n\n- PDF documents (provided you have LaTeX installed)\n- HTML web pages\n- Word (.docx) documents\n\nFor example:\n```yaml\nformat: pdf\nor\nformat: docx\nRendering the Document\n\nIn RStudio or VS Code, you can click the ‘Render’ or ‘Knit’ button to produce your desired output.\nYou can also use the command line: $ quarto render my_file.qmd$\n\nWhy Use R Markdown in Biology and Ecology?\n\nEnsures your analyses are reproducible\n\nAllows collaboration between researchers\nCombines field notes, data analysis, results, and interpretation in one place\nSummary\nR Markdown in Quarto is a powerful tool for scientific writing. It is especially well-suited to experimental and field disciplines such as biology and ecology, where clear communication of both methods and analyses is critical. You are encouraged to experiment with different output types and to begin integrating your analysis code and comments into a unified document.",
    "crumbs": [
      "Home",
      "BDC334: Biogeography & Global Ecology",
      "Lab 2a. Introduction R & RStudio"
    ]
  },
  {
    "objectID": "BDC334/r_markdown_example.html",
    "href": "BDC334/r_markdown_example.html",
    "title": "R Markdown and Quarto Demo",
    "section": "",
    "text": "This study is about air quality."
  },
  {
    "objectID": "BDC334/r_markdown_example.html#introduction",
    "href": "BDC334/r_markdown_example.html#introduction",
    "title": "R Markdown and Quarto Demo",
    "section": "",
    "text": "This study is about air quality."
  },
  {
    "objectID": "BDC334/r_markdown_example.html#methods",
    "href": "BDC334/r_markdown_example.html#methods",
    "title": "R Markdown and Quarto Demo",
    "section": "Methods",
    "text": "Methods\nFigure 1 further explores the impact of temperature on ozone level.\n\nCodelibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\n\nFigure 1: Temperature and ozone level."
  },
  {
    "objectID": "BDC334/r_markdown_example.html#results",
    "href": "BDC334/r_markdown_example.html#results",
    "title": "R Markdown and Quarto Demo",
    "section": "Results",
    "text": "Results\nThe results show that air has quality."
  }
]